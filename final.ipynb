{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmarc\\.conda\\envs\\inteli_gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "#system\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "#ai\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import src\n",
    "importlib.reload(src)\n",
    "\n",
    "import src.utils.metrics as metrics\n",
    "import src.utils.gau as gaussian\n",
    "import src.utils.plots as plots\n",
    "\n",
    "from src.models import AE, RNN, LinearClassifier, get_latent_features, rnn_loss_function, ae_loss_function\n",
    "from src.training import EarlyStopping, train_ae, train_rnn, train_lp, augment_data\n",
    "\n",
    "from src.final_model import ae_loss_function, rnn_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamps_dataset = pd.read_pickle('data/5stamps_dataset.pkl')\n",
    "\n",
    "def rename_labels(dataset, old_value, new_value):\n",
    "    for key in dataset.keys():\n",
    "        if old_value in dataset[key]:\n",
    "            dataset[key][new_value] = dataset[key].pop(old_value)\n",
    "\n",
    "rename_labels(stamps_dataset, 'labels', 'class')\n",
    "rename_labels(stamps_dataset, 'science', 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = torch.tensor(stamps_dataset['Train']['template'], dtype=torch.float32)\n",
    "validation_template = torch.tensor(stamps_dataset['Validation']['template'], dtype=torch.float32)\n",
    "test_template = torch.tensor(stamps_dataset['Test']['template'], dtype=torch.float32)\n",
    "\n",
    "train_difference = torch.tensor(stamps_dataset['Train']['difference'], dtype=torch.float32)\n",
    "validation_difference = torch.tensor(stamps_dataset['Validation']['difference'], dtype=torch.float32)\n",
    "test_difference = torch.tensor(stamps_dataset['Test']['difference'], dtype=torch.float32)\n",
    "\n",
    "train_image = torch.tensor(stamps_dataset['Train']['images'], dtype=torch.float32)\n",
    "validation_image = torch.tensor(stamps_dataset['Validation']['images'], dtype=torch.float32)\n",
    "test_image = torch.tensor(stamps_dataset['Test']['images'], dtype=torch.float32)\n",
    "\n",
    "train_class_0 = torch.tensor(stamps_dataset['Train']['class'], dtype=torch.float32)\n",
    "validation_class_0 = torch.tensor(stamps_dataset['Validation']['class'], dtype=torch.float32)\n",
    "test_class_0 = torch.tensor(stamps_dataset['Test']['class'], dtype=torch.float32)\n",
    "\n",
    "train_template = train_template.unsqueeze(1).repeat(1, 5, 1, 1)\n",
    "validation_template = validation_template.unsqueeze(1).repeat(1, 5, 1, 1)\n",
    "test_template = test_template.unsqueeze(1).repeat(1, 5, 1, 1)\n",
    "\n",
    "\n",
    "train_dataset = torch.stack((train_template, train_image, train_difference), dim=3  )\n",
    "validation_dataset = torch.stack((validation_template, validation_difference, validation_difference), dim=3)\n",
    "test_dataset = torch.stack((test_template, test_image, test_difference), dim=3)\n",
    "\n",
    "train_template = train_template.unsqueeze(2)  # (samples, 5, 1, 21, 21)\n",
    "train_image = train_image.unsqueeze(2)        \n",
    "train_difference = train_difference.unsqueeze(2)  \n",
    "\n",
    "validation_template = validation_template.unsqueeze(2)\n",
    "validation_image = validation_image.unsqueeze(2)\n",
    "validation_difference = validation_difference.unsqueeze(2)\n",
    "\n",
    "test_template = test_template.unsqueeze(2)\n",
    "test_image = test_image.unsqueeze(2)\n",
    "test_difference = test_difference.unsqueeze(2)\n",
    "\n",
    "# Apilar los tensores a lo largo de la dimensi√≥n correcta\n",
    "train_dataset = torch.cat((train_template, train_image, train_difference), dim=2)\n",
    "validation_dataset = torch.cat((validation_template, validation_image, validation_difference), dim=2)\n",
    "test_dataset = torch.cat((test_template, test_image, test_difference), dim=2)\n",
    "\n",
    "# Crear los conjuntos de datos\n",
    "train_dataset = TensorDataset(train_dataset, train_class_0)\n",
    "validation_dataset = TensorDataset(validation_dataset, validation_class_0)\n",
    "test_dataset = TensorDataset(test_dataset, test_class_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn parameters\n",
    "rnn_type = 'LSTM'\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "\n",
    "# autoencoder params\n",
    "latent_dim = 128\n",
    "n_channels = 3\n",
    "n_classes = 3\n",
    "\n",
    "# training params\n",
    "ae_loss = ae_loss_function\n",
    "rnn_loss = rnn_loss_function\n",
    "\n",
    "max_epochs = 300\n",
    "max_time = 180 #minutos\n",
    "lr = 10e-5\n",
    "batch_size = 128\n",
    "dropout_prob = 0.35\n",
    "\n",
    "random_sampler = True\n",
    "use_gpu = True\n",
    "augmentation = False\n",
    "only_classifier = False\n",
    "early_stop = 10\n",
    "num_cpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setup finishied. Starting training...\n",
      "(0.01 min) Epoch 1/300 -- Iteration 9 - Batch 9/963 - Train loss: 0.0676  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.01 min) Epoch 1/300 -- Iteration 18 - Batch 18/963 - Train loss: 0.0577  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.01 min) Epoch 1/300 -- Iteration 27 - Batch 27/963 - Train loss: 0.0537  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.02 min) Epoch 1/300 -- Iteration 36 - Batch 36/963 - Train loss: 0.0512  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.02 min) Epoch 1/300 -- Iteration 45 - Batch 45/963 - Train loss: 0.0496  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.03 min) Epoch 1/300 -- Iteration 54 - Batch 54/963 - Train loss: 0.0481  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.03 min) Epoch 1/300 -- Iteration 63 - Batch 63/963 - Train loss: 0.0467  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.04 min) Epoch 1/300 -- Iteration 72 - Batch 72/963 - Train loss: 0.0455  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.04 min) Epoch 1/300 -- Iteration 81 - Batch 81/963 - Train loss: 0.0444  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.05 min) Epoch 1/300 -- Iteration 90 - Batch 90/963 - Train loss: 0.0435  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.05 min) Epoch 1/300 -- Iteration 99 - Batch 99/963 - Train loss: 0.0427  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.06 min) Epoch 1/300 -- Iteration 108 - Batch 108/963 - Train loss: 0.0419  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.06 min) Epoch 1/300 -- Iteration 117 - Batch 117/963 - Train loss: 0.0411  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.07 min) Epoch 1/300 -- Iteration 126 - Batch 126/963 - Train loss: 0.0404  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.07 min) Epoch 1/300 -- Iteration 135 - Batch 135/963 - Train loss: 0.0398  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 144 - Batch 144/963 - Train loss: 0.0391  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 153 - Batch 153/963 - Train loss: 0.0385  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 162 - Batch 162/963 - Train loss: 0.0380  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.09 min) Epoch 1/300 -- Iteration 171 - Batch 171/963 - Train loss: 0.0373  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.09 min) Epoch 1/300 -- Iteration 180 - Batch 180/963 - Train loss: 0.0368  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 189 - Batch 189/963 - Train loss: 0.0362  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 198 - Batch 198/963 - Train loss: 0.0356  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.11 min) Epoch 1/300 -- Iteration 207 - Batch 207/963 - Train loss: 0.0350  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.11 min) Epoch 1/300 -- Iteration 216 - Batch 216/963 - Train loss: 0.0344  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.12 min) Epoch 1/300 -- Iteration 225 - Batch 225/963 - Train loss: 0.0338  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.12 min) Epoch 1/300 -- Iteration 234 - Batch 234/963 - Train loss: 0.0333  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 243 - Batch 243/963 - Train loss: 0.0327  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 252 - Batch 252/963 - Train loss: 0.0322  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 261 - Batch 261/963 - Train loss: 0.0317  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.14 min) Epoch 1/300 -- Iteration 270 - Batch 270/963 - Train loss: 0.0312  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.14 min) Epoch 1/300 -- Iteration 279 - Batch 279/963 - Train loss: 0.0308  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.15 min) Epoch 1/300 -- Iteration 288 - Batch 288/963 - Train loss: 0.0303  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.15 min) Epoch 1/300 -- Iteration 297 - Batch 297/963 - Train loss: 0.0298  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.16 min) Epoch 1/300 -- Iteration 306 - Batch 306/963 - Train loss: 0.0294  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.16 min) Epoch 1/300 -- Iteration 315 - Batch 315/963 - Train loss: 0.0290  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 324 - Batch 324/963 - Train loss: 0.0286  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 333 - Batch 333/963 - Train loss: 0.0282  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 342 - Batch 342/963 - Train loss: 0.0278  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 351 - Batch 351/963 - Train loss: 0.0274  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 360 - Batch 360/963 - Train loss: 0.0271  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.19 min) Epoch 1/300 -- Iteration 369 - Batch 369/963 - Train loss: 0.0268  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.19 min) Epoch 1/300 -- Iteration 378 - Batch 378/963 - Train loss: 0.0264  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.20 min) Epoch 1/300 -- Iteration 387 - Batch 387/963 - Train loss: 0.0261  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.20 min) Epoch 1/300 -- Iteration 396 - Batch 396/963 - Train loss: 0.0258  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.21 min) Epoch 1/300 -- Iteration 405 - Batch 405/963 - Train loss: 0.0255  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.21 min) Epoch 1/300 -- Iteration 414 - Batch 414/963 - Train loss: 0.0252  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.22 min) Epoch 1/300 -- Iteration 423 - Batch 423/963 - Train loss: 0.0249  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.22 min) Epoch 1/300 -- Iteration 432 - Batch 432/963 - Train loss: 0.0247  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.23 min) Epoch 1/300 -- Iteration 441 - Batch 441/963 - Train loss: 0.0244  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.23 min) Epoch 1/300 -- Iteration 450 - Batch 450/963 - Train loss: 0.0242  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.23 min) Epoch 1/300 -- Iteration 459 - Batch 459/963 - Train loss: 0.0240  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.24 min) Epoch 1/300 -- Iteration 468 - Batch 468/963 - Train loss: 0.0237  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.24 min) Epoch 1/300 -- Iteration 477 - Batch 477/963 - Train loss: 0.0235  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.25 min) Epoch 1/300 -- Iteration 486 - Batch 486/963 - Train loss: 0.0233  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.25 min) Epoch 1/300 -- Iteration 495 - Batch 495/963 - Train loss: 0.0231  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.26 min) Epoch 1/300 -- Iteration 504 - Batch 504/963 - Train loss: 0.0228  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.26 min) Epoch 1/300 -- Iteration 513 - Batch 513/963 - Train loss: 0.0227  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.27 min) Epoch 1/300 -- Iteration 522 - Batch 522/963 - Train loss: 0.0225  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.27 min) Epoch 1/300 -- Iteration 531 - Batch 531/963 - Train loss: 0.0222  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.28 min) Epoch 1/300 -- Iteration 540 - Batch 540/963 - Train loss: 0.0221  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.28 min) Epoch 1/300 -- Iteration 549 - Batch 549/963 - Train loss: 0.0219  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.29 min) Epoch 1/300 -- Iteration 558 - Batch 558/963 - Train loss: 0.0217  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.29 min) Epoch 1/300 -- Iteration 567 - Batch 567/963 - Train loss: 0.0215  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.29 min) Epoch 1/300 -- Iteration 576 - Batch 576/963 - Train loss: 0.0214  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.30 min) Epoch 1/300 -- Iteration 585 - Batch 585/963 - Train loss: 0.0212  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.30 min) Epoch 1/300 -- Iteration 594 - Batch 594/963 - Train loss: 0.0210  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.31 min) Epoch 1/300 -- Iteration 603 - Batch 603/963 - Train loss: 0.0209  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.31 min) Epoch 1/300 -- Iteration 612 - Batch 612/963 - Train loss: 0.0207  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.32 min) Epoch 1/300 -- Iteration 621 - Batch 621/963 - Train loss: 0.0205  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.32 min) Epoch 1/300 -- Iteration 630 - Batch 630/963 - Train loss: 0.0204  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.33 min) Epoch 1/300 -- Iteration 639 - Batch 639/963 - Train loss: 0.0202  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.33 min) Epoch 1/300 -- Iteration 648 - Batch 648/963 - Train loss: 0.0201  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.34 min) Epoch 1/300 -- Iteration 657 - Batch 657/963 - Train loss: 0.0199  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.34 min) Epoch 1/300 -- Iteration 666 - Batch 666/963 - Train loss: 0.0198  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.34 min) Epoch 1/300 -- Iteration 675 - Batch 675/963 - Train loss: 0.0196  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.35 min) Epoch 1/300 -- Iteration 684 - Batch 684/963 - Train loss: 0.0195  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.35 min) Epoch 1/300 -- Iteration 693 - Batch 693/963 - Train loss: 0.0193  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.36 min) Epoch 1/300 -- Iteration 702 - Batch 702/963 - Train loss: 0.0192  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.36 min) Epoch 1/300 -- Iteration 711 - Batch 711/963 - Train loss: 0.0191  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.37 min) Epoch 1/300 -- Iteration 720 - Batch 720/963 - Train loss: 0.0189  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.37 min) Epoch 1/300 -- Iteration 729 - Batch 729/963 - Train loss: 0.0188  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.38 min) Epoch 1/300 -- Iteration 738 - Batch 738/963 - Train loss: 0.0187  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.38 min) Epoch 1/300 -- Iteration 747 - Batch 747/963 - Train loss: 0.0186  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.39 min) Epoch 1/300 -- Iteration 756 - Batch 756/963 - Train loss: 0.0185  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.39 min) Epoch 1/300 -- Iteration 765 - Batch 765/963 - Train loss: 0.0183  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.40 min) Epoch 1/300 -- Iteration 774 - Batch 774/963 - Train loss: 0.0182  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.40 min) Epoch 1/300 -- Iteration 783 - Batch 783/963 - Train loss: 0.0181  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.40 min) Epoch 1/300 -- Iteration 792 - Batch 792/963 - Train loss: 0.0180  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.41 min) Epoch 1/300 -- Iteration 801 - Batch 801/963 - Train loss: 0.0179  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.41 min) Epoch 1/300 -- Iteration 810 - Batch 810/963 - Train loss: 0.0178  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.42 min) Epoch 1/300 -- Iteration 819 - Batch 819/963 - Train loss: 0.0176  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.42 min) Epoch 1/300 -- Iteration 828 - Batch 828/963 - Train loss: 0.0176  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.43 min) Epoch 1/300 -- Iteration 837 - Batch 837/963 - Train loss: 0.0175  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.43 min) Epoch 1/300 -- Iteration 846 - Batch 846/963 - Train loss: 0.0174  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.44 min) Epoch 1/300 -- Iteration 855 - Batch 855/963 - Train loss: 0.0173  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.44 min) Epoch 1/300 -- Iteration 864 - Batch 864/963 - Train loss: 0.0172  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.45 min) Epoch 1/300 -- Iteration 873 - Batch 873/963 - Train loss: 0.0171  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.45 min) Epoch 1/300 -- Iteration 882 - Batch 882/963 - Train loss: 0.0169  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.45 min) Epoch 1/300 -- Iteration 891 - Batch 891/963 - Train loss: 0.0169  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.46 min) Epoch 1/300 -- Iteration 900 - Batch 900/963 - Train loss: 0.0168  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.46 min) Epoch 1/300 -- Iteration 909 - Batch 909/963 - Train loss: 0.0167  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.47 min) Epoch 1/300 -- Iteration 918 - Batch 918/963 - Train loss: 0.0166  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.47 min) Epoch 1/300 -- Iteration 927 - Batch 927/963 - Train loss: 0.0165  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.48 min) Epoch 1/300 -- Iteration 936 - Batch 936/963 - Train loss: 0.0164  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.48 min) Epoch 1/300 -- Iteration 945 - Batch 945/963 - Train loss: 0.0163  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.49 min) Epoch 1/300 -- Iteration 954 - Batch 954/963 - Train loss: 0.0162  - Train acc: -0.0000 - Val loss: 0.0000\n",
      "(0.49 min) Epoch 1/300 -- Iteration 963 - Batch 962/963 - Train loss: 0.0162  - Train acc: -0.0000 - Val loss: 0.0055 - Val acc: -0.0000\n",
      "(0.50 min) Epoch 2/300 -- Iteration 972 - Batch 9/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.50 min) Epoch 2/300 -- Iteration 981 - Batch 18/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.50 min) Epoch 2/300 -- Iteration 990 - Batch 27/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.51 min) Epoch 2/300 -- Iteration 999 - Batch 36/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.51 min) Epoch 2/300 -- Iteration 1008 - Batch 45/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.52 min) Epoch 2/300 -- Iteration 1017 - Batch 54/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.52 min) Epoch 2/300 -- Iteration 1026 - Batch 63/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.53 min) Epoch 2/300 -- Iteration 1035 - Batch 72/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.53 min) Epoch 2/300 -- Iteration 1044 - Batch 81/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.53 min) Epoch 2/300 -- Iteration 1053 - Batch 90/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.54 min) Epoch 2/300 -- Iteration 1062 - Batch 99/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.54 min) Epoch 2/300 -- Iteration 1071 - Batch 108/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.55 min) Epoch 2/300 -- Iteration 1080 - Batch 117/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.55 min) Epoch 2/300 -- Iteration 1089 - Batch 126/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.56 min) Epoch 2/300 -- Iteration 1098 - Batch 135/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.56 min) Epoch 2/300 -- Iteration 1107 - Batch 144/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.57 min) Epoch 2/300 -- Iteration 1116 - Batch 153/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.57 min) Epoch 2/300 -- Iteration 1125 - Batch 162/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.57 min) Epoch 2/300 -- Iteration 1134 - Batch 171/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.58 min) Epoch 2/300 -- Iteration 1143 - Batch 180/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.58 min) Epoch 2/300 -- Iteration 1152 - Batch 189/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.59 min) Epoch 2/300 -- Iteration 1161 - Batch 198/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.59 min) Epoch 2/300 -- Iteration 1170 - Batch 207/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.60 min) Epoch 2/300 -- Iteration 1179 - Batch 216/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.60 min) Epoch 2/300 -- Iteration 1188 - Batch 225/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.61 min) Epoch 2/300 -- Iteration 1197 - Batch 234/963 - Train loss: 0.0066  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.61 min) Epoch 2/300 -- Iteration 1206 - Batch 243/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.62 min) Epoch 2/300 -- Iteration 1215 - Batch 252/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.62 min) Epoch 2/300 -- Iteration 1224 - Batch 261/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.62 min) Epoch 2/300 -- Iteration 1233 - Batch 270/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.63 min) Epoch 2/300 -- Iteration 1242 - Batch 279/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.63 min) Epoch 2/300 -- Iteration 1251 - Batch 288/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.64 min) Epoch 2/300 -- Iteration 1260 - Batch 297/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.64 min) Epoch 2/300 -- Iteration 1269 - Batch 306/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.65 min) Epoch 2/300 -- Iteration 1278 - Batch 315/963 - Train loss: 0.0065  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.65 min) Epoch 2/300 -- Iteration 1287 - Batch 324/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.66 min) Epoch 2/300 -- Iteration 1296 - Batch 333/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.66 min) Epoch 2/300 -- Iteration 1305 - Batch 342/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.67 min) Epoch 2/300 -- Iteration 1314 - Batch 351/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.67 min) Epoch 2/300 -- Iteration 1323 - Batch 360/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.68 min) Epoch 2/300 -- Iteration 1332 - Batch 369/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.68 min) Epoch 2/300 -- Iteration 1341 - Batch 378/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.68 min) Epoch 2/300 -- Iteration 1350 - Batch 387/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.69 min) Epoch 2/300 -- Iteration 1359 - Batch 396/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.69 min) Epoch 2/300 -- Iteration 1368 - Batch 405/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.70 min) Epoch 2/300 -- Iteration 1377 - Batch 414/963 - Train loss: 0.0064  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.70 min) Epoch 2/300 -- Iteration 1386 - Batch 423/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.71 min) Epoch 2/300 -- Iteration 1395 - Batch 432/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.71 min) Epoch 2/300 -- Iteration 1404 - Batch 441/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.72 min) Epoch 2/300 -- Iteration 1413 - Batch 450/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.72 min) Epoch 2/300 -- Iteration 1422 - Batch 459/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.73 min) Epoch 2/300 -- Iteration 1431 - Batch 468/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.73 min) Epoch 2/300 -- Iteration 1440 - Batch 477/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.74 min) Epoch 2/300 -- Iteration 1449 - Batch 486/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.74 min) Epoch 2/300 -- Iteration 1458 - Batch 495/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.74 min) Epoch 2/300 -- Iteration 1467 - Batch 504/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.75 min) Epoch 2/300 -- Iteration 1476 - Batch 513/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.75 min) Epoch 2/300 -- Iteration 1485 - Batch 522/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.76 min) Epoch 2/300 -- Iteration 1494 - Batch 531/963 - Train loss: 0.0063  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.76 min) Epoch 2/300 -- Iteration 1503 - Batch 540/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.77 min) Epoch 2/300 -- Iteration 1512 - Batch 549/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.77 min) Epoch 2/300 -- Iteration 1521 - Batch 558/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.78 min) Epoch 2/300 -- Iteration 1530 - Batch 567/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.78 min) Epoch 2/300 -- Iteration 1539 - Batch 576/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.79 min) Epoch 2/300 -- Iteration 1548 - Batch 585/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.79 min) Epoch 2/300 -- Iteration 1557 - Batch 594/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.79 min) Epoch 2/300 -- Iteration 1566 - Batch 603/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.80 min) Epoch 2/300 -- Iteration 1575 - Batch 612/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.80 min) Epoch 2/300 -- Iteration 1584 - Batch 621/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.81 min) Epoch 2/300 -- Iteration 1593 - Batch 630/963 - Train loss: 0.0062  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.81 min) Epoch 2/300 -- Iteration 1602 - Batch 639/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.82 min) Epoch 2/300 -- Iteration 1611 - Batch 648/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.82 min) Epoch 2/300 -- Iteration 1620 - Batch 657/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.83 min) Epoch 2/300 -- Iteration 1629 - Batch 666/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.83 min) Epoch 2/300 -- Iteration 1638 - Batch 675/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.83 min) Epoch 2/300 -- Iteration 1647 - Batch 684/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.84 min) Epoch 2/300 -- Iteration 1656 - Batch 693/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.84 min) Epoch 2/300 -- Iteration 1665 - Batch 702/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.85 min) Epoch 2/300 -- Iteration 1674 - Batch 711/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.85 min) Epoch 2/300 -- Iteration 1683 - Batch 720/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.86 min) Epoch 2/300 -- Iteration 1692 - Batch 729/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.86 min) Epoch 2/300 -- Iteration 1701 - Batch 738/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.87 min) Epoch 2/300 -- Iteration 1710 - Batch 747/963 - Train loss: 0.0061  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.87 min) Epoch 2/300 -- Iteration 1719 - Batch 756/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.88 min) Epoch 2/300 -- Iteration 1728 - Batch 765/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.88 min) Epoch 2/300 -- Iteration 1737 - Batch 774/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.88 min) Epoch 2/300 -- Iteration 1746 - Batch 783/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.89 min) Epoch 2/300 -- Iteration 1755 - Batch 792/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.89 min) Epoch 2/300 -- Iteration 1764 - Batch 801/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.90 min) Epoch 2/300 -- Iteration 1773 - Batch 810/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.90 min) Epoch 2/300 -- Iteration 1782 - Batch 819/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.91 min) Epoch 2/300 -- Iteration 1791 - Batch 828/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.91 min) Epoch 2/300 -- Iteration 1800 - Batch 837/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.92 min) Epoch 2/300 -- Iteration 1809 - Batch 846/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.92 min) Epoch 2/300 -- Iteration 1818 - Batch 855/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.92 min) Epoch 2/300 -- Iteration 1827 - Batch 864/963 - Train loss: 0.0060  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.93 min) Epoch 2/300 -- Iteration 1836 - Batch 873/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.93 min) Epoch 2/300 -- Iteration 1845 - Batch 882/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.94 min) Epoch 2/300 -- Iteration 1854 - Batch 891/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.94 min) Epoch 2/300 -- Iteration 1863 - Batch 900/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.95 min) Epoch 2/300 -- Iteration 1872 - Batch 909/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.95 min) Epoch 2/300 -- Iteration 1881 - Batch 918/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.96 min) Epoch 2/300 -- Iteration 1890 - Batch 927/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.96 min) Epoch 2/300 -- Iteration 1899 - Batch 936/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.97 min) Epoch 2/300 -- Iteration 1908 - Batch 945/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.97 min) Epoch 2/300 -- Iteration 1917 - Batch 954/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0055\n",
      "(0.97 min) Epoch 2/300 -- Iteration 1926 - Batch 962/963 - Train loss: 0.0059  - Train acc: -0.0000 - Val loss: 0.0042 - Val acc: -0.0000\n",
      "(0.98 min) Epoch 3/300 -- Iteration 1935 - Batch 9/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(0.98 min) Epoch 3/300 -- Iteration 1944 - Batch 18/963 - Train loss: 0.0051  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(0.99 min) Epoch 3/300 -- Iteration 1953 - Batch 27/963 - Train loss: 0.0051  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(0.99 min) Epoch 3/300 -- Iteration 1962 - Batch 36/963 - Train loss: 0.0051  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.00 min) Epoch 3/300 -- Iteration 1971 - Batch 45/963 - Train loss: 0.0051  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.00 min) Epoch 3/300 -- Iteration 1980 - Batch 54/963 - Train loss: 0.0051  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.01 min) Epoch 3/300 -- Iteration 1989 - Batch 63/963 - Train loss: 0.0051  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.01 min) Epoch 3/300 -- Iteration 1998 - Batch 72/963 - Train loss: 0.0051  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.02 min) Epoch 3/300 -- Iteration 2007 - Batch 81/963 - Train loss: 0.0051  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.02 min) Epoch 3/300 -- Iteration 2016 - Batch 90/963 - Train loss: 0.0051  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.02 min) Epoch 3/300 -- Iteration 2025 - Batch 99/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.03 min) Epoch 3/300 -- Iteration 2034 - Batch 108/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.03 min) Epoch 3/300 -- Iteration 2043 - Batch 117/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.04 min) Epoch 3/300 -- Iteration 2052 - Batch 126/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.04 min) Epoch 3/300 -- Iteration 2061 - Batch 135/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.05 min) Epoch 3/300 -- Iteration 2070 - Batch 144/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.05 min) Epoch 3/300 -- Iteration 2079 - Batch 153/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.06 min) Epoch 3/300 -- Iteration 2088 - Batch 162/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.06 min) Epoch 3/300 -- Iteration 2097 - Batch 171/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.07 min) Epoch 3/300 -- Iteration 2106 - Batch 180/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.07 min) Epoch 3/300 -- Iteration 2115 - Batch 189/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.07 min) Epoch 3/300 -- Iteration 2124 - Batch 198/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.08 min) Epoch 3/300 -- Iteration 2133 - Batch 207/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.08 min) Epoch 3/300 -- Iteration 2142 - Batch 216/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.09 min) Epoch 3/300 -- Iteration 2151 - Batch 225/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.09 min) Epoch 3/300 -- Iteration 2160 - Batch 234/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.10 min) Epoch 3/300 -- Iteration 2169 - Batch 243/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.10 min) Epoch 3/300 -- Iteration 2178 - Batch 252/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.11 min) Epoch 3/300 -- Iteration 2187 - Batch 261/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.11 min) Epoch 3/300 -- Iteration 2196 - Batch 270/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.12 min) Epoch 3/300 -- Iteration 2205 - Batch 279/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.12 min) Epoch 3/300 -- Iteration 2214 - Batch 288/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.12 min) Epoch 3/300 -- Iteration 2223 - Batch 297/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.13 min) Epoch 3/300 -- Iteration 2232 - Batch 306/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.13 min) Epoch 3/300 -- Iteration 2241 - Batch 315/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.14 min) Epoch 3/300 -- Iteration 2250 - Batch 324/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.14 min) Epoch 3/300 -- Iteration 2259 - Batch 333/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.15 min) Epoch 3/300 -- Iteration 2268 - Batch 342/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.15 min) Epoch 3/300 -- Iteration 2277 - Batch 351/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.16 min) Epoch 3/300 -- Iteration 2286 - Batch 360/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.16 min) Epoch 3/300 -- Iteration 2295 - Batch 369/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.16 min) Epoch 3/300 -- Iteration 2304 - Batch 378/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.17 min) Epoch 3/300 -- Iteration 2313 - Batch 387/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.17 min) Epoch 3/300 -- Iteration 2322 - Batch 396/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.18 min) Epoch 3/300 -- Iteration 2331 - Batch 405/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.18 min) Epoch 3/300 -- Iteration 2340 - Batch 414/963 - Train loss: 0.0050  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.19 min) Epoch 3/300 -- Iteration 2349 - Batch 423/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.19 min) Epoch 3/300 -- Iteration 2358 - Batch 432/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.20 min) Epoch 3/300 -- Iteration 2367 - Batch 441/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.20 min) Epoch 3/300 -- Iteration 2376 - Batch 450/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.20 min) Epoch 3/300 -- Iteration 2385 - Batch 459/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.21 min) Epoch 3/300 -- Iteration 2394 - Batch 468/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.21 min) Epoch 3/300 -- Iteration 2403 - Batch 477/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.22 min) Epoch 3/300 -- Iteration 2412 - Batch 486/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.22 min) Epoch 3/300 -- Iteration 2421 - Batch 495/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.23 min) Epoch 3/300 -- Iteration 2430 - Batch 504/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.23 min) Epoch 3/300 -- Iteration 2439 - Batch 513/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.24 min) Epoch 3/300 -- Iteration 2448 - Batch 522/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.24 min) Epoch 3/300 -- Iteration 2457 - Batch 531/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.25 min) Epoch 3/300 -- Iteration 2466 - Batch 540/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.25 min) Epoch 3/300 -- Iteration 2475 - Batch 549/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.26 min) Epoch 3/300 -- Iteration 2484 - Batch 558/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.26 min) Epoch 3/300 -- Iteration 2493 - Batch 567/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.26 min) Epoch 3/300 -- Iteration 2502 - Batch 576/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.27 min) Epoch 3/300 -- Iteration 2511 - Batch 585/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.28 min) Epoch 3/300 -- Iteration 2520 - Batch 594/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.28 min) Epoch 3/300 -- Iteration 2529 - Batch 603/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.29 min) Epoch 3/300 -- Iteration 2538 - Batch 612/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.29 min) Epoch 3/300 -- Iteration 2547 - Batch 621/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.30 min) Epoch 3/300 -- Iteration 2556 - Batch 630/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.30 min) Epoch 3/300 -- Iteration 2565 - Batch 639/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.31 min) Epoch 3/300 -- Iteration 2574 - Batch 648/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.31 min) Epoch 3/300 -- Iteration 2583 - Batch 657/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.32 min) Epoch 3/300 -- Iteration 2592 - Batch 666/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.32 min) Epoch 3/300 -- Iteration 2601 - Batch 675/963 - Train loss: 0.0049  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.32 min) Epoch 3/300 -- Iteration 2610 - Batch 684/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.33 min) Epoch 3/300 -- Iteration 2619 - Batch 693/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.33 min) Epoch 3/300 -- Iteration 2628 - Batch 702/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.34 min) Epoch 3/300 -- Iteration 2637 - Batch 711/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.34 min) Epoch 3/300 -- Iteration 2646 - Batch 720/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.35 min) Epoch 3/300 -- Iteration 2655 - Batch 729/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.35 min) Epoch 3/300 -- Iteration 2664 - Batch 738/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.36 min) Epoch 3/300 -- Iteration 2673 - Batch 747/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.36 min) Epoch 3/300 -- Iteration 2682 - Batch 756/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.37 min) Epoch 3/300 -- Iteration 2691 - Batch 765/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.37 min) Epoch 3/300 -- Iteration 2700 - Batch 774/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.38 min) Epoch 3/300 -- Iteration 2709 - Batch 783/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.38 min) Epoch 3/300 -- Iteration 2718 - Batch 792/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.39 min) Epoch 3/300 -- Iteration 2727 - Batch 801/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.39 min) Epoch 3/300 -- Iteration 2736 - Batch 810/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.39 min) Epoch 3/300 -- Iteration 2745 - Batch 819/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.40 min) Epoch 3/300 -- Iteration 2754 - Batch 828/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.40 min) Epoch 3/300 -- Iteration 2763 - Batch 837/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.41 min) Epoch 3/300 -- Iteration 2772 - Batch 846/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.41 min) Epoch 3/300 -- Iteration 2781 - Batch 855/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.42 min) Epoch 3/300 -- Iteration 2790 - Batch 864/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.42 min) Epoch 3/300 -- Iteration 2799 - Batch 873/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.43 min) Epoch 3/300 -- Iteration 2808 - Batch 882/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.43 min) Epoch 3/300 -- Iteration 2817 - Batch 891/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.44 min) Epoch 3/300 -- Iteration 2826 - Batch 900/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.44 min) Epoch 3/300 -- Iteration 2835 - Batch 909/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.45 min) Epoch 3/300 -- Iteration 2844 - Batch 918/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.45 min) Epoch 3/300 -- Iteration 2853 - Batch 927/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.46 min) Epoch 3/300 -- Iteration 2862 - Batch 936/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.46 min) Epoch 3/300 -- Iteration 2871 - Batch 945/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.46 min) Epoch 3/300 -- Iteration 2880 - Batch 954/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0042\n",
      "(1.47 min) Epoch 3/300 -- Iteration 2889 - Batch 962/963 - Train loss: 0.0048  - Train acc: -0.0000 - Val loss: 0.0036 - Val acc: -0.0000\n",
      "(1.47 min) Epoch 4/300 -- Iteration 2898 - Batch 9/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.48 min) Epoch 4/300 -- Iteration 2907 - Batch 18/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.48 min) Epoch 4/300 -- Iteration 2916 - Batch 27/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.49 min) Epoch 4/300 -- Iteration 2925 - Batch 36/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.49 min) Epoch 4/300 -- Iteration 2934 - Batch 45/963 - Train loss: 0.0045  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.50 min) Epoch 4/300 -- Iteration 2943 - Batch 54/963 - Train loss: 0.0045  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.50 min) Epoch 4/300 -- Iteration 2952 - Batch 63/963 - Train loss: 0.0045  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.51 min) Epoch 4/300 -- Iteration 2961 - Batch 72/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.51 min) Epoch 4/300 -- Iteration 2970 - Batch 81/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.52 min) Epoch 4/300 -- Iteration 2979 - Batch 90/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.52 min) Epoch 4/300 -- Iteration 2988 - Batch 99/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.52 min) Epoch 4/300 -- Iteration 2997 - Batch 108/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.53 min) Epoch 4/300 -- Iteration 3006 - Batch 117/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.53 min) Epoch 4/300 -- Iteration 3015 - Batch 126/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.54 min) Epoch 4/300 -- Iteration 3024 - Batch 135/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.54 min) Epoch 4/300 -- Iteration 3033 - Batch 144/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.55 min) Epoch 4/300 -- Iteration 3042 - Batch 153/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.55 min) Epoch 4/300 -- Iteration 3051 - Batch 162/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.56 min) Epoch 4/300 -- Iteration 3060 - Batch 171/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.56 min) Epoch 4/300 -- Iteration 3069 - Batch 180/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.56 min) Epoch 4/300 -- Iteration 3078 - Batch 189/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.57 min) Epoch 4/300 -- Iteration 3087 - Batch 198/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.57 min) Epoch 4/300 -- Iteration 3096 - Batch 207/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.58 min) Epoch 4/300 -- Iteration 3105 - Batch 216/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.58 min) Epoch 4/300 -- Iteration 3114 - Batch 225/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.59 min) Epoch 4/300 -- Iteration 3123 - Batch 234/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.59 min) Epoch 4/300 -- Iteration 3132 - Batch 243/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.60 min) Epoch 4/300 -- Iteration 3141 - Batch 252/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.60 min) Epoch 4/300 -- Iteration 3150 - Batch 261/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.61 min) Epoch 4/300 -- Iteration 3159 - Batch 270/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.61 min) Epoch 4/300 -- Iteration 3168 - Batch 279/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.61 min) Epoch 4/300 -- Iteration 3177 - Batch 288/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.62 min) Epoch 4/300 -- Iteration 3186 - Batch 297/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.62 min) Epoch 4/300 -- Iteration 3195 - Batch 306/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.63 min) Epoch 4/300 -- Iteration 3204 - Batch 315/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.63 min) Epoch 4/300 -- Iteration 3213 - Batch 324/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.64 min) Epoch 4/300 -- Iteration 3222 - Batch 333/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.64 min) Epoch 4/300 -- Iteration 3231 - Batch 342/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.65 min) Epoch 4/300 -- Iteration 3240 - Batch 351/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.65 min) Epoch 4/300 -- Iteration 3249 - Batch 360/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.65 min) Epoch 4/300 -- Iteration 3258 - Batch 369/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.66 min) Epoch 4/300 -- Iteration 3267 - Batch 378/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.66 min) Epoch 4/300 -- Iteration 3276 - Batch 387/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.67 min) Epoch 4/300 -- Iteration 3285 - Batch 396/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.67 min) Epoch 4/300 -- Iteration 3294 - Batch 405/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.68 min) Epoch 4/300 -- Iteration 3303 - Batch 414/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.68 min) Epoch 4/300 -- Iteration 3312 - Batch 423/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.69 min) Epoch 4/300 -- Iteration 3321 - Batch 432/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.69 min) Epoch 4/300 -- Iteration 3330 - Batch 441/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.70 min) Epoch 4/300 -- Iteration 3339 - Batch 450/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.70 min) Epoch 4/300 -- Iteration 3348 - Batch 459/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.70 min) Epoch 4/300 -- Iteration 3357 - Batch 468/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.71 min) Epoch 4/300 -- Iteration 3366 - Batch 477/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.71 min) Epoch 4/300 -- Iteration 3375 - Batch 486/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.72 min) Epoch 4/300 -- Iteration 3384 - Batch 495/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.72 min) Epoch 4/300 -- Iteration 3393 - Batch 504/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.73 min) Epoch 4/300 -- Iteration 3402 - Batch 513/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.73 min) Epoch 4/300 -- Iteration 3411 - Batch 522/963 - Train loss: 0.0044  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.74 min) Epoch 4/300 -- Iteration 3420 - Batch 531/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.74 min) Epoch 4/300 -- Iteration 3429 - Batch 540/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.74 min) Epoch 4/300 -- Iteration 3438 - Batch 549/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.75 min) Epoch 4/300 -- Iteration 3447 - Batch 558/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.75 min) Epoch 4/300 -- Iteration 3456 - Batch 567/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.76 min) Epoch 4/300 -- Iteration 3465 - Batch 576/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.76 min) Epoch 4/300 -- Iteration 3474 - Batch 585/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.77 min) Epoch 4/300 -- Iteration 3483 - Batch 594/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.77 min) Epoch 4/300 -- Iteration 3492 - Batch 603/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.78 min) Epoch 4/300 -- Iteration 3501 - Batch 612/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.78 min) Epoch 4/300 -- Iteration 3510 - Batch 621/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.78 min) Epoch 4/300 -- Iteration 3519 - Batch 630/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.79 min) Epoch 4/300 -- Iteration 3528 - Batch 639/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.79 min) Epoch 4/300 -- Iteration 3537 - Batch 648/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.80 min) Epoch 4/300 -- Iteration 3546 - Batch 657/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.80 min) Epoch 4/300 -- Iteration 3555 - Batch 666/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.81 min) Epoch 4/300 -- Iteration 3564 - Batch 675/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.81 min) Epoch 4/300 -- Iteration 3573 - Batch 684/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.82 min) Epoch 4/300 -- Iteration 3582 - Batch 693/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.82 min) Epoch 4/300 -- Iteration 3591 - Batch 702/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.83 min) Epoch 4/300 -- Iteration 3600 - Batch 711/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.83 min) Epoch 4/300 -- Iteration 3609 - Batch 720/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.83 min) Epoch 4/300 -- Iteration 3618 - Batch 729/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.84 min) Epoch 4/300 -- Iteration 3627 - Batch 738/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.84 min) Epoch 4/300 -- Iteration 3636 - Batch 747/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.85 min) Epoch 4/300 -- Iteration 3645 - Batch 756/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.85 min) Epoch 4/300 -- Iteration 3654 - Batch 765/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.86 min) Epoch 4/300 -- Iteration 3663 - Batch 774/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.86 min) Epoch 4/300 -- Iteration 3672 - Batch 783/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.87 min) Epoch 4/300 -- Iteration 3681 - Batch 792/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.87 min) Epoch 4/300 -- Iteration 3690 - Batch 801/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.87 min) Epoch 4/300 -- Iteration 3699 - Batch 810/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.88 min) Epoch 4/300 -- Iteration 3708 - Batch 819/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.88 min) Epoch 4/300 -- Iteration 3717 - Batch 828/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.89 min) Epoch 4/300 -- Iteration 3726 - Batch 837/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.89 min) Epoch 4/300 -- Iteration 3735 - Batch 846/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.90 min) Epoch 4/300 -- Iteration 3744 - Batch 855/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.90 min) Epoch 4/300 -- Iteration 3753 - Batch 864/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.91 min) Epoch 4/300 -- Iteration 3762 - Batch 873/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.91 min) Epoch 4/300 -- Iteration 3771 - Batch 882/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.92 min) Epoch 4/300 -- Iteration 3780 - Batch 891/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.92 min) Epoch 4/300 -- Iteration 3789 - Batch 900/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.92 min) Epoch 4/300 -- Iteration 3798 - Batch 909/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.93 min) Epoch 4/300 -- Iteration 3807 - Batch 918/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.93 min) Epoch 4/300 -- Iteration 3816 - Batch 927/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.94 min) Epoch 4/300 -- Iteration 3825 - Batch 936/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.94 min) Epoch 4/300 -- Iteration 3834 - Batch 945/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.95 min) Epoch 4/300 -- Iteration 3843 - Batch 954/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(1.95 min) Epoch 4/300 -- Iteration 3852 - Batch 962/963 - Train loss: 0.0043  - Train acc: -0.0000 - Val loss: 0.0035 - Val acc: -0.0000\n",
      "(1.96 min) Epoch 5/300 -- Iteration 3861 - Batch 9/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(1.96 min) Epoch 5/300 -- Iteration 3870 - Batch 18/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(1.97 min) Epoch 5/300 -- Iteration 3879 - Batch 27/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(1.97 min) Epoch 5/300 -- Iteration 3888 - Batch 36/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(1.98 min) Epoch 5/300 -- Iteration 3897 - Batch 45/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(1.98 min) Epoch 5/300 -- Iteration 3906 - Batch 54/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(1.98 min) Epoch 5/300 -- Iteration 3915 - Batch 63/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(1.99 min) Epoch 5/300 -- Iteration 3924 - Batch 72/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(1.99 min) Epoch 5/300 -- Iteration 3933 - Batch 81/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.00 min) Epoch 5/300 -- Iteration 3942 - Batch 90/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.00 min) Epoch 5/300 -- Iteration 3951 - Batch 99/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.01 min) Epoch 5/300 -- Iteration 3960 - Batch 108/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.01 min) Epoch 5/300 -- Iteration 3969 - Batch 117/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.02 min) Epoch 5/300 -- Iteration 3978 - Batch 126/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.02 min) Epoch 5/300 -- Iteration 3987 - Batch 135/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.03 min) Epoch 5/300 -- Iteration 3996 - Batch 144/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.03 min) Epoch 5/300 -- Iteration 4005 - Batch 153/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.04 min) Epoch 5/300 -- Iteration 4014 - Batch 162/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.05 min) Epoch 5/300 -- Iteration 4023 - Batch 171/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.05 min) Epoch 5/300 -- Iteration 4032 - Batch 180/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.05 min) Epoch 5/300 -- Iteration 4041 - Batch 189/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.06 min) Epoch 5/300 -- Iteration 4050 - Batch 198/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.06 min) Epoch 5/300 -- Iteration 4059 - Batch 207/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.07 min) Epoch 5/300 -- Iteration 4068 - Batch 216/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.07 min) Epoch 5/300 -- Iteration 4077 - Batch 225/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.08 min) Epoch 5/300 -- Iteration 4086 - Batch 234/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.08 min) Epoch 5/300 -- Iteration 4095 - Batch 243/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.09 min) Epoch 5/300 -- Iteration 4104 - Batch 252/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.09 min) Epoch 5/300 -- Iteration 4113 - Batch 261/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.09 min) Epoch 5/300 -- Iteration 4122 - Batch 270/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.10 min) Epoch 5/300 -- Iteration 4131 - Batch 279/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.10 min) Epoch 5/300 -- Iteration 4140 - Batch 288/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.11 min) Epoch 5/300 -- Iteration 4149 - Batch 297/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.11 min) Epoch 5/300 -- Iteration 4158 - Batch 306/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.12 min) Epoch 5/300 -- Iteration 4167 - Batch 315/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.12 min) Epoch 5/300 -- Iteration 4176 - Batch 324/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.13 min) Epoch 5/300 -- Iteration 4185 - Batch 333/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.13 min) Epoch 5/300 -- Iteration 4194 - Batch 342/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.14 min) Epoch 5/300 -- Iteration 4203 - Batch 351/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.14 min) Epoch 5/300 -- Iteration 4212 - Batch 360/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.14 min) Epoch 5/300 -- Iteration 4221 - Batch 369/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.15 min) Epoch 5/300 -- Iteration 4230 - Batch 378/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.15 min) Epoch 5/300 -- Iteration 4239 - Batch 387/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.16 min) Epoch 5/300 -- Iteration 4248 - Batch 396/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.16 min) Epoch 5/300 -- Iteration 4257 - Batch 405/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.17 min) Epoch 5/300 -- Iteration 4266 - Batch 414/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.17 min) Epoch 5/300 -- Iteration 4275 - Batch 423/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.18 min) Epoch 5/300 -- Iteration 4284 - Batch 432/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.18 min) Epoch 5/300 -- Iteration 4293 - Batch 441/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.18 min) Epoch 5/300 -- Iteration 4302 - Batch 450/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.19 min) Epoch 5/300 -- Iteration 4311 - Batch 459/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.19 min) Epoch 5/300 -- Iteration 4320 - Batch 468/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.20 min) Epoch 5/300 -- Iteration 4329 - Batch 477/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.20 min) Epoch 5/300 -- Iteration 4338 - Batch 486/963 - Train loss: 0.0041  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.21 min) Epoch 5/300 -- Iteration 4347 - Batch 495/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.21 min) Epoch 5/300 -- Iteration 4356 - Batch 504/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.22 min) Epoch 5/300 -- Iteration 4365 - Batch 513/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.22 min) Epoch 5/300 -- Iteration 4374 - Batch 522/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.23 min) Epoch 5/300 -- Iteration 4383 - Batch 531/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.23 min) Epoch 5/300 -- Iteration 4392 - Batch 540/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.24 min) Epoch 5/300 -- Iteration 4401 - Batch 549/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.24 min) Epoch 5/300 -- Iteration 4410 - Batch 558/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.24 min) Epoch 5/300 -- Iteration 4419 - Batch 567/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.25 min) Epoch 5/300 -- Iteration 4428 - Batch 576/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.25 min) Epoch 5/300 -- Iteration 4437 - Batch 585/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.26 min) Epoch 5/300 -- Iteration 4446 - Batch 594/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.26 min) Epoch 5/300 -- Iteration 4455 - Batch 603/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.27 min) Epoch 5/300 -- Iteration 4464 - Batch 612/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.27 min) Epoch 5/300 -- Iteration 4473 - Batch 621/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.28 min) Epoch 5/300 -- Iteration 4482 - Batch 630/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.28 min) Epoch 5/300 -- Iteration 4491 - Batch 639/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.29 min) Epoch 5/300 -- Iteration 4500 - Batch 648/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.29 min) Epoch 5/300 -- Iteration 4509 - Batch 657/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.29 min) Epoch 5/300 -- Iteration 4518 - Batch 666/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.30 min) Epoch 5/300 -- Iteration 4527 - Batch 675/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.30 min) Epoch 5/300 -- Iteration 4536 - Batch 684/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.31 min) Epoch 5/300 -- Iteration 4545 - Batch 693/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.31 min) Epoch 5/300 -- Iteration 4554 - Batch 702/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.32 min) Epoch 5/300 -- Iteration 4563 - Batch 711/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.32 min) Epoch 5/300 -- Iteration 4572 - Batch 720/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.33 min) Epoch 5/300 -- Iteration 4581 - Batch 729/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.33 min) Epoch 5/300 -- Iteration 4590 - Batch 738/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.33 min) Epoch 5/300 -- Iteration 4599 - Batch 747/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.34 min) Epoch 5/300 -- Iteration 4608 - Batch 756/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.34 min) Epoch 5/300 -- Iteration 4617 - Batch 765/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.35 min) Epoch 5/300 -- Iteration 4626 - Batch 774/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.35 min) Epoch 5/300 -- Iteration 4635 - Batch 783/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.36 min) Epoch 5/300 -- Iteration 4644 - Batch 792/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.36 min) Epoch 5/300 -- Iteration 4653 - Batch 801/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.37 min) Epoch 5/300 -- Iteration 4662 - Batch 810/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.37 min) Epoch 5/300 -- Iteration 4671 - Batch 819/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.38 min) Epoch 5/300 -- Iteration 4680 - Batch 828/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.38 min) Epoch 5/300 -- Iteration 4689 - Batch 837/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.38 min) Epoch 5/300 -- Iteration 4698 - Batch 846/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.39 min) Epoch 5/300 -- Iteration 4707 - Batch 855/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.39 min) Epoch 5/300 -- Iteration 4716 - Batch 864/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.40 min) Epoch 5/300 -- Iteration 4725 - Batch 873/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.40 min) Epoch 5/300 -- Iteration 4734 - Batch 882/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.41 min) Epoch 5/300 -- Iteration 4743 - Batch 891/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.41 min) Epoch 5/300 -- Iteration 4752 - Batch 900/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.42 min) Epoch 5/300 -- Iteration 4761 - Batch 909/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.42 min) Epoch 5/300 -- Iteration 4770 - Batch 918/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.42 min) Epoch 5/300 -- Iteration 4779 - Batch 927/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.43 min) Epoch 5/300 -- Iteration 4788 - Batch 936/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.43 min) Epoch 5/300 -- Iteration 4797 - Batch 945/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.44 min) Epoch 5/300 -- Iteration 4806 - Batch 954/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0035\n",
      "(2.44 min) Epoch 5/300 -- Iteration 4815 - Batch 962/963 - Train loss: 0.0040  - Train acc: -0.0000 - Val loss: 0.0032 - Val acc: -0.0000\n",
      "(2.45 min) Epoch 6/300 -- Iteration 4824 - Batch 9/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.45 min) Epoch 6/300 -- Iteration 4833 - Batch 18/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.46 min) Epoch 6/300 -- Iteration 4842 - Batch 27/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.46 min) Epoch 6/300 -- Iteration 4851 - Batch 36/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.47 min) Epoch 6/300 -- Iteration 4860 - Batch 45/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.47 min) Epoch 6/300 -- Iteration 4869 - Batch 54/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.47 min) Epoch 6/300 -- Iteration 4878 - Batch 63/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.48 min) Epoch 6/300 -- Iteration 4887 - Batch 72/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.48 min) Epoch 6/300 -- Iteration 4896 - Batch 81/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.49 min) Epoch 6/300 -- Iteration 4905 - Batch 90/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.49 min) Epoch 6/300 -- Iteration 4914 - Batch 99/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.50 min) Epoch 6/300 -- Iteration 4923 - Batch 108/963 - Train loss: 0.0039  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.50 min) Epoch 6/300 -- Iteration 4932 - Batch 117/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.51 min) Epoch 6/300 -- Iteration 4941 - Batch 126/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.51 min) Epoch 6/300 -- Iteration 4950 - Batch 135/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.52 min) Epoch 6/300 -- Iteration 4959 - Batch 144/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.52 min) Epoch 6/300 -- Iteration 4968 - Batch 153/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.52 min) Epoch 6/300 -- Iteration 4977 - Batch 162/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.53 min) Epoch 6/300 -- Iteration 4986 - Batch 171/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.53 min) Epoch 6/300 -- Iteration 4995 - Batch 180/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.54 min) Epoch 6/300 -- Iteration 5004 - Batch 189/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.54 min) Epoch 6/300 -- Iteration 5013 - Batch 198/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.55 min) Epoch 6/300 -- Iteration 5022 - Batch 207/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.55 min) Epoch 6/300 -- Iteration 5031 - Batch 216/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.56 min) Epoch 6/300 -- Iteration 5040 - Batch 225/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.56 min) Epoch 6/300 -- Iteration 5049 - Batch 234/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.56 min) Epoch 6/300 -- Iteration 5058 - Batch 243/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.57 min) Epoch 6/300 -- Iteration 5067 - Batch 252/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.57 min) Epoch 6/300 -- Iteration 5076 - Batch 261/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.58 min) Epoch 6/300 -- Iteration 5085 - Batch 270/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.58 min) Epoch 6/300 -- Iteration 5094 - Batch 279/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.59 min) Epoch 6/300 -- Iteration 5103 - Batch 288/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.59 min) Epoch 6/300 -- Iteration 5112 - Batch 297/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.60 min) Epoch 6/300 -- Iteration 5121 - Batch 306/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.60 min) Epoch 6/300 -- Iteration 5130 - Batch 315/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.60 min) Epoch 6/300 -- Iteration 5139 - Batch 324/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.61 min) Epoch 6/300 -- Iteration 5148 - Batch 333/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.61 min) Epoch 6/300 -- Iteration 5157 - Batch 342/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.62 min) Epoch 6/300 -- Iteration 5166 - Batch 351/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.62 min) Epoch 6/300 -- Iteration 5175 - Batch 360/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.63 min) Epoch 6/300 -- Iteration 5184 - Batch 369/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.63 min) Epoch 6/300 -- Iteration 5193 - Batch 378/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.64 min) Epoch 6/300 -- Iteration 5202 - Batch 387/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.64 min) Epoch 6/300 -- Iteration 5211 - Batch 396/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.65 min) Epoch 6/300 -- Iteration 5220 - Batch 405/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.65 min) Epoch 6/300 -- Iteration 5229 - Batch 414/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.65 min) Epoch 6/300 -- Iteration 5238 - Batch 423/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.66 min) Epoch 6/300 -- Iteration 5247 - Batch 432/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.66 min) Epoch 6/300 -- Iteration 5256 - Batch 441/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.67 min) Epoch 6/300 -- Iteration 5265 - Batch 450/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.67 min) Epoch 6/300 -- Iteration 5274 - Batch 459/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.68 min) Epoch 6/300 -- Iteration 5283 - Batch 468/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.68 min) Epoch 6/300 -- Iteration 5292 - Batch 477/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.69 min) Epoch 6/300 -- Iteration 5301 - Batch 486/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.69 min) Epoch 6/300 -- Iteration 5310 - Batch 495/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.70 min) Epoch 6/300 -- Iteration 5319 - Batch 504/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.70 min) Epoch 6/300 -- Iteration 5328 - Batch 513/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.71 min) Epoch 6/300 -- Iteration 5337 - Batch 522/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.71 min) Epoch 6/300 -- Iteration 5346 - Batch 531/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.72 min) Epoch 6/300 -- Iteration 5355 - Batch 540/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.72 min) Epoch 6/300 -- Iteration 5364 - Batch 549/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.73 min) Epoch 6/300 -- Iteration 5373 - Batch 558/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.73 min) Epoch 6/300 -- Iteration 5382 - Batch 567/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.74 min) Epoch 6/300 -- Iteration 5391 - Batch 576/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.74 min) Epoch 6/300 -- Iteration 5400 - Batch 585/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.75 min) Epoch 6/300 -- Iteration 5409 - Batch 594/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.75 min) Epoch 6/300 -- Iteration 5418 - Batch 603/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.76 min) Epoch 6/300 -- Iteration 5427 - Batch 612/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.76 min) Epoch 6/300 -- Iteration 5436 - Batch 621/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.77 min) Epoch 6/300 -- Iteration 5445 - Batch 630/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.77 min) Epoch 6/300 -- Iteration 5454 - Batch 639/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.77 min) Epoch 6/300 -- Iteration 5463 - Batch 648/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.78 min) Epoch 6/300 -- Iteration 5472 - Batch 657/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.79 min) Epoch 6/300 -- Iteration 5481 - Batch 666/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.79 min) Epoch 6/300 -- Iteration 5490 - Batch 675/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.79 min) Epoch 6/300 -- Iteration 5499 - Batch 684/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.80 min) Epoch 6/300 -- Iteration 5508 - Batch 693/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.80 min) Epoch 6/300 -- Iteration 5517 - Batch 702/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.81 min) Epoch 6/300 -- Iteration 5526 - Batch 711/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.81 min) Epoch 6/300 -- Iteration 5535 - Batch 720/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.82 min) Epoch 6/300 -- Iteration 5544 - Batch 729/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.82 min) Epoch 6/300 -- Iteration 5553 - Batch 738/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.83 min) Epoch 6/300 -- Iteration 5562 - Batch 747/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.83 min) Epoch 6/300 -- Iteration 5571 - Batch 756/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.84 min) Epoch 6/300 -- Iteration 5580 - Batch 765/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.84 min) Epoch 6/300 -- Iteration 5589 - Batch 774/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.84 min) Epoch 6/300 -- Iteration 5598 - Batch 783/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.85 min) Epoch 6/300 -- Iteration 5607 - Batch 792/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.85 min) Epoch 6/300 -- Iteration 5616 - Batch 801/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.86 min) Epoch 6/300 -- Iteration 5625 - Batch 810/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.86 min) Epoch 6/300 -- Iteration 5634 - Batch 819/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.87 min) Epoch 6/300 -- Iteration 5643 - Batch 828/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.87 min) Epoch 6/300 -- Iteration 5652 - Batch 837/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.88 min) Epoch 6/300 -- Iteration 5661 - Batch 846/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.88 min) Epoch 6/300 -- Iteration 5670 - Batch 855/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.89 min) Epoch 6/300 -- Iteration 5679 - Batch 864/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.89 min) Epoch 6/300 -- Iteration 5688 - Batch 873/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.89 min) Epoch 6/300 -- Iteration 5697 - Batch 882/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.90 min) Epoch 6/300 -- Iteration 5706 - Batch 891/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.90 min) Epoch 6/300 -- Iteration 5715 - Batch 900/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.91 min) Epoch 6/300 -- Iteration 5724 - Batch 909/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.91 min) Epoch 6/300 -- Iteration 5733 - Batch 918/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.92 min) Epoch 6/300 -- Iteration 5742 - Batch 927/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.92 min) Epoch 6/300 -- Iteration 5751 - Batch 936/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.93 min) Epoch 6/300 -- Iteration 5760 - Batch 945/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.93 min) Epoch 6/300 -- Iteration 5769 - Batch 954/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.93 min) Epoch 6/300 -- Iteration 5778 - Batch 962/963 - Train loss: 0.0038  - Train acc: -0.0000 - Val loss: 0.0032 - Val acc: -0.0000\n",
      "(2.94 min) Epoch 7/300 -- Iteration 5787 - Batch 9/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.94 min) Epoch 7/300 -- Iteration 5796 - Batch 18/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.95 min) Epoch 7/300 -- Iteration 5805 - Batch 27/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.95 min) Epoch 7/300 -- Iteration 5814 - Batch 36/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.96 min) Epoch 7/300 -- Iteration 5823 - Batch 45/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.96 min) Epoch 7/300 -- Iteration 5832 - Batch 54/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.97 min) Epoch 7/300 -- Iteration 5841 - Batch 63/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.97 min) Epoch 7/300 -- Iteration 5850 - Batch 72/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.98 min) Epoch 7/300 -- Iteration 5859 - Batch 81/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.98 min) Epoch 7/300 -- Iteration 5868 - Batch 90/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.99 min) Epoch 7/300 -- Iteration 5877 - Batch 99/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.99 min) Epoch 7/300 -- Iteration 5886 - Batch 108/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(2.99 min) Epoch 7/300 -- Iteration 5895 - Batch 117/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.00 min) Epoch 7/300 -- Iteration 5904 - Batch 126/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.00 min) Epoch 7/300 -- Iteration 5913 - Batch 135/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.01 min) Epoch 7/300 -- Iteration 5922 - Batch 144/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.01 min) Epoch 7/300 -- Iteration 5931 - Batch 153/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.02 min) Epoch 7/300 -- Iteration 5940 - Batch 162/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.02 min) Epoch 7/300 -- Iteration 5949 - Batch 171/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.03 min) Epoch 7/300 -- Iteration 5958 - Batch 180/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.03 min) Epoch 7/300 -- Iteration 5967 - Batch 189/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.03 min) Epoch 7/300 -- Iteration 5976 - Batch 198/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.04 min) Epoch 7/300 -- Iteration 5985 - Batch 207/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.04 min) Epoch 7/300 -- Iteration 5994 - Batch 216/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.05 min) Epoch 7/300 -- Iteration 6003 - Batch 225/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.05 min) Epoch 7/300 -- Iteration 6012 - Batch 234/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.06 min) Epoch 7/300 -- Iteration 6021 - Batch 243/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.06 min) Epoch 7/300 -- Iteration 6030 - Batch 252/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.07 min) Epoch 7/300 -- Iteration 6039 - Batch 261/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.07 min) Epoch 7/300 -- Iteration 6048 - Batch 270/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.08 min) Epoch 7/300 -- Iteration 6057 - Batch 279/963 - Train loss: 0.0037  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.08 min) Epoch 7/300 -- Iteration 6066 - Batch 288/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.08 min) Epoch 7/300 -- Iteration 6075 - Batch 297/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.09 min) Epoch 7/300 -- Iteration 6084 - Batch 306/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.09 min) Epoch 7/300 -- Iteration 6093 - Batch 315/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.10 min) Epoch 7/300 -- Iteration 6102 - Batch 324/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.10 min) Epoch 7/300 -- Iteration 6111 - Batch 333/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.11 min) Epoch 7/300 -- Iteration 6120 - Batch 342/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.11 min) Epoch 7/300 -- Iteration 6129 - Batch 351/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.12 min) Epoch 7/300 -- Iteration 6138 - Batch 360/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.12 min) Epoch 7/300 -- Iteration 6147 - Batch 369/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.13 min) Epoch 7/300 -- Iteration 6156 - Batch 378/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.13 min) Epoch 7/300 -- Iteration 6165 - Batch 387/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.13 min) Epoch 7/300 -- Iteration 6174 - Batch 396/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.14 min) Epoch 7/300 -- Iteration 6183 - Batch 405/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.14 min) Epoch 7/300 -- Iteration 6192 - Batch 414/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.15 min) Epoch 7/300 -- Iteration 6201 - Batch 423/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.15 min) Epoch 7/300 -- Iteration 6210 - Batch 432/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.16 min) Epoch 7/300 -- Iteration 6219 - Batch 441/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.16 min) Epoch 7/300 -- Iteration 6228 - Batch 450/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.17 min) Epoch 7/300 -- Iteration 6237 - Batch 459/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.17 min) Epoch 7/300 -- Iteration 6246 - Batch 468/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.18 min) Epoch 7/300 -- Iteration 6255 - Batch 477/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.18 min) Epoch 7/300 -- Iteration 6264 - Batch 486/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.19 min) Epoch 7/300 -- Iteration 6273 - Batch 495/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.19 min) Epoch 7/300 -- Iteration 6282 - Batch 504/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.19 min) Epoch 7/300 -- Iteration 6291 - Batch 513/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.20 min) Epoch 7/300 -- Iteration 6300 - Batch 522/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.20 min) Epoch 7/300 -- Iteration 6309 - Batch 531/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.21 min) Epoch 7/300 -- Iteration 6318 - Batch 540/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.21 min) Epoch 7/300 -- Iteration 6327 - Batch 549/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.22 min) Epoch 7/300 -- Iteration 6336 - Batch 558/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.22 min) Epoch 7/300 -- Iteration 6345 - Batch 567/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.23 min) Epoch 7/300 -- Iteration 6354 - Batch 576/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.23 min) Epoch 7/300 -- Iteration 6363 - Batch 585/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.24 min) Epoch 7/300 -- Iteration 6372 - Batch 594/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.24 min) Epoch 7/300 -- Iteration 6381 - Batch 603/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.25 min) Epoch 7/300 -- Iteration 6390 - Batch 612/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.25 min) Epoch 7/300 -- Iteration 6399 - Batch 621/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.26 min) Epoch 7/300 -- Iteration 6408 - Batch 630/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.26 min) Epoch 7/300 -- Iteration 6417 - Batch 639/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.27 min) Epoch 7/300 -- Iteration 6426 - Batch 648/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.27 min) Epoch 7/300 -- Iteration 6435 - Batch 657/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.27 min) Epoch 7/300 -- Iteration 6444 - Batch 666/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.28 min) Epoch 7/300 -- Iteration 6453 - Batch 675/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.28 min) Epoch 7/300 -- Iteration 6462 - Batch 684/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.29 min) Epoch 7/300 -- Iteration 6471 - Batch 693/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.29 min) Epoch 7/300 -- Iteration 6480 - Batch 702/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.30 min) Epoch 7/300 -- Iteration 6489 - Batch 711/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.30 min) Epoch 7/300 -- Iteration 6498 - Batch 720/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.31 min) Epoch 7/300 -- Iteration 6507 - Batch 729/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.31 min) Epoch 7/300 -- Iteration 6516 - Batch 738/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.32 min) Epoch 7/300 -- Iteration 6525 - Batch 747/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.32 min) Epoch 7/300 -- Iteration 6534 - Batch 756/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.32 min) Epoch 7/300 -- Iteration 6543 - Batch 765/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.33 min) Epoch 7/300 -- Iteration 6552 - Batch 774/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.33 min) Epoch 7/300 -- Iteration 6561 - Batch 783/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.34 min) Epoch 7/300 -- Iteration 6570 - Batch 792/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.34 min) Epoch 7/300 -- Iteration 6579 - Batch 801/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.35 min) Epoch 7/300 -- Iteration 6588 - Batch 810/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.35 min) Epoch 7/300 -- Iteration 6597 - Batch 819/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.36 min) Epoch 7/300 -- Iteration 6606 - Batch 828/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.36 min) Epoch 7/300 -- Iteration 6615 - Batch 837/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.37 min) Epoch 7/300 -- Iteration 6624 - Batch 846/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.37 min) Epoch 7/300 -- Iteration 6633 - Batch 855/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.37 min) Epoch 7/300 -- Iteration 6642 - Batch 864/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.38 min) Epoch 7/300 -- Iteration 6651 - Batch 873/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.38 min) Epoch 7/300 -- Iteration 6660 - Batch 882/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.39 min) Epoch 7/300 -- Iteration 6669 - Batch 891/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.39 min) Epoch 7/300 -- Iteration 6678 - Batch 900/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.40 min) Epoch 7/300 -- Iteration 6687 - Batch 909/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.40 min) Epoch 7/300 -- Iteration 6696 - Batch 918/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.41 min) Epoch 7/300 -- Iteration 6705 - Batch 927/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.41 min) Epoch 7/300 -- Iteration 6714 - Batch 936/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.42 min) Epoch 7/300 -- Iteration 6723 - Batch 945/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.42 min) Epoch 7/300 -- Iteration 6732 - Batch 954/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(3.43 min) Epoch 7/300 -- Iteration 6741 - Batch 962/963 - Train loss: 0.0036  - Train acc: -0.0000 - Val loss: 0.0034 - Val acc: -0.0000\n",
      "(3.43 min) Epoch 8/300 -- Iteration 6750 - Batch 9/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.44 min) Epoch 8/300 -- Iteration 6759 - Batch 18/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.44 min) Epoch 8/300 -- Iteration 6768 - Batch 27/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.45 min) Epoch 8/300 -- Iteration 6777 - Batch 36/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.45 min) Epoch 8/300 -- Iteration 6786 - Batch 45/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.45 min) Epoch 8/300 -- Iteration 6795 - Batch 54/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.46 min) Epoch 8/300 -- Iteration 6804 - Batch 63/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.46 min) Epoch 8/300 -- Iteration 6813 - Batch 72/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.47 min) Epoch 8/300 -- Iteration 6822 - Batch 81/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.47 min) Epoch 8/300 -- Iteration 6831 - Batch 90/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.48 min) Epoch 8/300 -- Iteration 6840 - Batch 99/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.48 min) Epoch 8/300 -- Iteration 6849 - Batch 108/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.49 min) Epoch 8/300 -- Iteration 6858 - Batch 117/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.49 min) Epoch 8/300 -- Iteration 6867 - Batch 126/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.49 min) Epoch 8/300 -- Iteration 6876 - Batch 135/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.50 min) Epoch 8/300 -- Iteration 6885 - Batch 144/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.50 min) Epoch 8/300 -- Iteration 6894 - Batch 153/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.51 min) Epoch 8/300 -- Iteration 6903 - Batch 162/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.51 min) Epoch 8/300 -- Iteration 6912 - Batch 171/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.52 min) Epoch 8/300 -- Iteration 6921 - Batch 180/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.52 min) Epoch 8/300 -- Iteration 6930 - Batch 189/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.53 min) Epoch 8/300 -- Iteration 6939 - Batch 198/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.53 min) Epoch 8/300 -- Iteration 6948 - Batch 207/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.54 min) Epoch 8/300 -- Iteration 6957 - Batch 216/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.54 min) Epoch 8/300 -- Iteration 6966 - Batch 225/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.54 min) Epoch 8/300 -- Iteration 6975 - Batch 234/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.55 min) Epoch 8/300 -- Iteration 6984 - Batch 243/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.55 min) Epoch 8/300 -- Iteration 6993 - Batch 252/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.56 min) Epoch 8/300 -- Iteration 7002 - Batch 261/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.56 min) Epoch 8/300 -- Iteration 7011 - Batch 270/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.57 min) Epoch 8/300 -- Iteration 7020 - Batch 279/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.57 min) Epoch 8/300 -- Iteration 7029 - Batch 288/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.58 min) Epoch 8/300 -- Iteration 7038 - Batch 297/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.58 min) Epoch 8/300 -- Iteration 7047 - Batch 306/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.58 min) Epoch 8/300 -- Iteration 7056 - Batch 315/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.59 min) Epoch 8/300 -- Iteration 7065 - Batch 324/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.59 min) Epoch 8/300 -- Iteration 7074 - Batch 333/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.60 min) Epoch 8/300 -- Iteration 7083 - Batch 342/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.60 min) Epoch 8/300 -- Iteration 7092 - Batch 351/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.61 min) Epoch 8/300 -- Iteration 7101 - Batch 360/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.61 min) Epoch 8/300 -- Iteration 7110 - Batch 369/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.62 min) Epoch 8/300 -- Iteration 7119 - Batch 378/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.62 min) Epoch 8/300 -- Iteration 7128 - Batch 387/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.63 min) Epoch 8/300 -- Iteration 7137 - Batch 396/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.63 min) Epoch 8/300 -- Iteration 7146 - Batch 405/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.63 min) Epoch 8/300 -- Iteration 7155 - Batch 414/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.64 min) Epoch 8/300 -- Iteration 7164 - Batch 423/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.64 min) Epoch 8/300 -- Iteration 7173 - Batch 432/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.65 min) Epoch 8/300 -- Iteration 7182 - Batch 441/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.65 min) Epoch 8/300 -- Iteration 7191 - Batch 450/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.66 min) Epoch 8/300 -- Iteration 7200 - Batch 459/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.66 min) Epoch 8/300 -- Iteration 7209 - Batch 468/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.67 min) Epoch 8/300 -- Iteration 7218 - Batch 477/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.67 min) Epoch 8/300 -- Iteration 7227 - Batch 486/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.68 min) Epoch 8/300 -- Iteration 7236 - Batch 495/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.68 min) Epoch 8/300 -- Iteration 7245 - Batch 504/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.69 min) Epoch 8/300 -- Iteration 7254 - Batch 513/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.69 min) Epoch 8/300 -- Iteration 7263 - Batch 522/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.70 min) Epoch 8/300 -- Iteration 7272 - Batch 531/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.70 min) Epoch 8/300 -- Iteration 7281 - Batch 540/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.70 min) Epoch 8/300 -- Iteration 7290 - Batch 549/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.71 min) Epoch 8/300 -- Iteration 7299 - Batch 558/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.71 min) Epoch 8/300 -- Iteration 7308 - Batch 567/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.72 min) Epoch 8/300 -- Iteration 7317 - Batch 576/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.72 min) Epoch 8/300 -- Iteration 7326 - Batch 585/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.73 min) Epoch 8/300 -- Iteration 7335 - Batch 594/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.73 min) Epoch 8/300 -- Iteration 7344 - Batch 603/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.74 min) Epoch 8/300 -- Iteration 7353 - Batch 612/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.74 min) Epoch 8/300 -- Iteration 7362 - Batch 621/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.75 min) Epoch 8/300 -- Iteration 7371 - Batch 630/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.75 min) Epoch 8/300 -- Iteration 7380 - Batch 639/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.75 min) Epoch 8/300 -- Iteration 7389 - Batch 648/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.76 min) Epoch 8/300 -- Iteration 7398 - Batch 657/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.76 min) Epoch 8/300 -- Iteration 7407 - Batch 666/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.77 min) Epoch 8/300 -- Iteration 7416 - Batch 675/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.77 min) Epoch 8/300 -- Iteration 7425 - Batch 684/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.78 min) Epoch 8/300 -- Iteration 7434 - Batch 693/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.78 min) Epoch 8/300 -- Iteration 7443 - Batch 702/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.79 min) Epoch 8/300 -- Iteration 7452 - Batch 711/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.79 min) Epoch 8/300 -- Iteration 7461 - Batch 720/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.80 min) Epoch 8/300 -- Iteration 7470 - Batch 729/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.80 min) Epoch 8/300 -- Iteration 7479 - Batch 738/963 - Train loss: 0.0035  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.80 min) Epoch 8/300 -- Iteration 7488 - Batch 747/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.81 min) Epoch 8/300 -- Iteration 7497 - Batch 756/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.81 min) Epoch 8/300 -- Iteration 7506 - Batch 765/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.82 min) Epoch 8/300 -- Iteration 7515 - Batch 774/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.82 min) Epoch 8/300 -- Iteration 7524 - Batch 783/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.83 min) Epoch 8/300 -- Iteration 7533 - Batch 792/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.83 min) Epoch 8/300 -- Iteration 7542 - Batch 801/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.84 min) Epoch 8/300 -- Iteration 7551 - Batch 810/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.84 min) Epoch 8/300 -- Iteration 7560 - Batch 819/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.85 min) Epoch 8/300 -- Iteration 7569 - Batch 828/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.85 min) Epoch 8/300 -- Iteration 7578 - Batch 837/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.85 min) Epoch 8/300 -- Iteration 7587 - Batch 846/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.86 min) Epoch 8/300 -- Iteration 7596 - Batch 855/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.86 min) Epoch 8/300 -- Iteration 7605 - Batch 864/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.87 min) Epoch 8/300 -- Iteration 7614 - Batch 873/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.87 min) Epoch 8/300 -- Iteration 7623 - Batch 882/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.88 min) Epoch 8/300 -- Iteration 7632 - Batch 891/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.88 min) Epoch 8/300 -- Iteration 7641 - Batch 900/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.89 min) Epoch 8/300 -- Iteration 7650 - Batch 909/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.89 min) Epoch 8/300 -- Iteration 7659 - Batch 918/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.89 min) Epoch 8/300 -- Iteration 7668 - Batch 927/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.90 min) Epoch 8/300 -- Iteration 7677 - Batch 936/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.90 min) Epoch 8/300 -- Iteration 7686 - Batch 945/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.91 min) Epoch 8/300 -- Iteration 7695 - Batch 954/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.91 min) Epoch 8/300 -- Iteration 7704 - Batch 962/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034 - Val acc: -0.0000\n",
      "(3.92 min) Epoch 9/300 -- Iteration 7713 - Batch 9/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.92 min) Epoch 9/300 -- Iteration 7722 - Batch 18/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.93 min) Epoch 9/300 -- Iteration 7731 - Batch 27/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.93 min) Epoch 9/300 -- Iteration 7740 - Batch 36/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.94 min) Epoch 9/300 -- Iteration 7749 - Batch 45/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.94 min) Epoch 9/300 -- Iteration 7758 - Batch 54/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.95 min) Epoch 9/300 -- Iteration 7767 - Batch 63/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.95 min) Epoch 9/300 -- Iteration 7776 - Batch 72/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.96 min) Epoch 9/300 -- Iteration 7785 - Batch 81/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.96 min) Epoch 9/300 -- Iteration 7794 - Batch 90/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.97 min) Epoch 9/300 -- Iteration 7803 - Batch 99/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.97 min) Epoch 9/300 -- Iteration 7812 - Batch 108/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.98 min) Epoch 9/300 -- Iteration 7821 - Batch 117/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.98 min) Epoch 9/300 -- Iteration 7830 - Batch 126/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.99 min) Epoch 9/300 -- Iteration 7839 - Batch 135/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.99 min) Epoch 9/300 -- Iteration 7848 - Batch 144/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(3.99 min) Epoch 9/300 -- Iteration 7857 - Batch 153/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.00 min) Epoch 9/300 -- Iteration 7866 - Batch 162/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.00 min) Epoch 9/300 -- Iteration 7875 - Batch 171/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.01 min) Epoch 9/300 -- Iteration 7884 - Batch 180/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.01 min) Epoch 9/300 -- Iteration 7893 - Batch 189/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.02 min) Epoch 9/300 -- Iteration 7902 - Batch 198/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.02 min) Epoch 9/300 -- Iteration 7911 - Batch 207/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.03 min) Epoch 9/300 -- Iteration 7920 - Batch 216/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.03 min) Epoch 9/300 -- Iteration 7929 - Batch 225/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.04 min) Epoch 9/300 -- Iteration 7938 - Batch 234/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.04 min) Epoch 9/300 -- Iteration 7947 - Batch 243/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.04 min) Epoch 9/300 -- Iteration 7956 - Batch 252/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.05 min) Epoch 9/300 -- Iteration 7965 - Batch 261/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.05 min) Epoch 9/300 -- Iteration 7974 - Batch 270/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.06 min) Epoch 9/300 -- Iteration 7983 - Batch 279/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.06 min) Epoch 9/300 -- Iteration 7992 - Batch 288/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.07 min) Epoch 9/300 -- Iteration 8001 - Batch 297/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.07 min) Epoch 9/300 -- Iteration 8010 - Batch 306/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.08 min) Epoch 9/300 -- Iteration 8019 - Batch 315/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.08 min) Epoch 9/300 -- Iteration 8028 - Batch 324/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.09 min) Epoch 9/300 -- Iteration 8037 - Batch 333/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.09 min) Epoch 9/300 -- Iteration 8046 - Batch 342/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.10 min) Epoch 9/300 -- Iteration 8055 - Batch 351/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.10 min) Epoch 9/300 -- Iteration 8064 - Batch 360/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.10 min) Epoch 9/300 -- Iteration 8073 - Batch 369/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.11 min) Epoch 9/300 -- Iteration 8082 - Batch 378/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.11 min) Epoch 9/300 -- Iteration 8091 - Batch 387/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.12 min) Epoch 9/300 -- Iteration 8100 - Batch 396/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.12 min) Epoch 9/300 -- Iteration 8109 - Batch 405/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.13 min) Epoch 9/300 -- Iteration 8118 - Batch 414/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.13 min) Epoch 9/300 -- Iteration 8127 - Batch 423/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.14 min) Epoch 9/300 -- Iteration 8136 - Batch 432/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.14 min) Epoch 9/300 -- Iteration 8145 - Batch 441/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.14 min) Epoch 9/300 -- Iteration 8154 - Batch 450/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.15 min) Epoch 9/300 -- Iteration 8163 - Batch 459/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.15 min) Epoch 9/300 -- Iteration 8172 - Batch 468/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.16 min) Epoch 9/300 -- Iteration 8181 - Batch 477/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.16 min) Epoch 9/300 -- Iteration 8190 - Batch 486/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.17 min) Epoch 9/300 -- Iteration 8199 - Batch 495/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.17 min) Epoch 9/300 -- Iteration 8208 - Batch 504/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.18 min) Epoch 9/300 -- Iteration 8217 - Batch 513/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.18 min) Epoch 9/300 -- Iteration 8226 - Batch 522/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.19 min) Epoch 9/300 -- Iteration 8235 - Batch 531/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.19 min) Epoch 9/300 -- Iteration 8244 - Batch 540/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.20 min) Epoch 9/300 -- Iteration 8253 - Batch 549/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.20 min) Epoch 9/300 -- Iteration 8262 - Batch 558/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.21 min) Epoch 9/300 -- Iteration 8271 - Batch 567/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.21 min) Epoch 9/300 -- Iteration 8280 - Batch 576/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.21 min) Epoch 9/300 -- Iteration 8289 - Batch 585/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.22 min) Epoch 9/300 -- Iteration 8298 - Batch 594/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.22 min) Epoch 9/300 -- Iteration 8307 - Batch 603/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.23 min) Epoch 9/300 -- Iteration 8316 - Batch 612/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.23 min) Epoch 9/300 -- Iteration 8325 - Batch 621/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.24 min) Epoch 9/300 -- Iteration 8334 - Batch 630/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.24 min) Epoch 9/300 -- Iteration 8343 - Batch 639/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.25 min) Epoch 9/300 -- Iteration 8352 - Batch 648/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.25 min) Epoch 9/300 -- Iteration 8361 - Batch 657/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.26 min) Epoch 9/300 -- Iteration 8370 - Batch 666/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.26 min) Epoch 9/300 -- Iteration 8379 - Batch 675/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.27 min) Epoch 9/300 -- Iteration 8388 - Batch 684/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.27 min) Epoch 9/300 -- Iteration 8397 - Batch 693/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.27 min) Epoch 9/300 -- Iteration 8406 - Batch 702/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.28 min) Epoch 9/300 -- Iteration 8415 - Batch 711/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.28 min) Epoch 9/300 -- Iteration 8424 - Batch 720/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.29 min) Epoch 9/300 -- Iteration 8433 - Batch 729/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.29 min) Epoch 9/300 -- Iteration 8442 - Batch 738/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.30 min) Epoch 9/300 -- Iteration 8451 - Batch 747/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.30 min) Epoch 9/300 -- Iteration 8460 - Batch 756/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.31 min) Epoch 9/300 -- Iteration 8469 - Batch 765/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.31 min) Epoch 9/300 -- Iteration 8478 - Batch 774/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.31 min) Epoch 9/300 -- Iteration 8487 - Batch 783/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.32 min) Epoch 9/300 -- Iteration 8496 - Batch 792/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.32 min) Epoch 9/300 -- Iteration 8505 - Batch 801/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.33 min) Epoch 9/300 -- Iteration 8514 - Batch 810/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.33 min) Epoch 9/300 -- Iteration 8523 - Batch 819/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.34 min) Epoch 9/300 -- Iteration 8532 - Batch 828/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.34 min) Epoch 9/300 -- Iteration 8541 - Batch 837/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.35 min) Epoch 9/300 -- Iteration 8550 - Batch 846/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.35 min) Epoch 9/300 -- Iteration 8559 - Batch 855/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.36 min) Epoch 9/300 -- Iteration 8568 - Batch 864/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.36 min) Epoch 9/300 -- Iteration 8577 - Batch 873/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.36 min) Epoch 9/300 -- Iteration 8586 - Batch 882/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.37 min) Epoch 9/300 -- Iteration 8595 - Batch 891/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.37 min) Epoch 9/300 -- Iteration 8604 - Batch 900/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.38 min) Epoch 9/300 -- Iteration 8613 - Batch 909/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.38 min) Epoch 9/300 -- Iteration 8622 - Batch 918/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.39 min) Epoch 9/300 -- Iteration 8631 - Batch 927/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.39 min) Epoch 9/300 -- Iteration 8640 - Batch 936/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.40 min) Epoch 9/300 -- Iteration 8649 - Batch 945/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.40 min) Epoch 9/300 -- Iteration 8658 - Batch 954/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(4.40 min) Epoch 9/300 -- Iteration 8667 - Batch 962/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036 - Val acc: -0.0000\n",
      "(4.41 min) Epoch 10/300 -- Iteration 8676 - Batch 9/963 - Train loss: 0.0034  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.41 min) Epoch 10/300 -- Iteration 8685 - Batch 18/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.42 min) Epoch 10/300 -- Iteration 8694 - Batch 27/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.42 min) Epoch 10/300 -- Iteration 8703 - Batch 36/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.43 min) Epoch 10/300 -- Iteration 8712 - Batch 45/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.43 min) Epoch 10/300 -- Iteration 8721 - Batch 54/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.44 min) Epoch 10/300 -- Iteration 8730 - Batch 63/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.44 min) Epoch 10/300 -- Iteration 8739 - Batch 72/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.45 min) Epoch 10/300 -- Iteration 8748 - Batch 81/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.45 min) Epoch 10/300 -- Iteration 8757 - Batch 90/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.45 min) Epoch 10/300 -- Iteration 8766 - Batch 99/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.46 min) Epoch 10/300 -- Iteration 8775 - Batch 108/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.46 min) Epoch 10/300 -- Iteration 8784 - Batch 117/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.47 min) Epoch 10/300 -- Iteration 8793 - Batch 126/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.47 min) Epoch 10/300 -- Iteration 8802 - Batch 135/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.48 min) Epoch 10/300 -- Iteration 8811 - Batch 144/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.48 min) Epoch 10/300 -- Iteration 8820 - Batch 153/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.49 min) Epoch 10/300 -- Iteration 8829 - Batch 162/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.49 min) Epoch 10/300 -- Iteration 8838 - Batch 171/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.50 min) Epoch 10/300 -- Iteration 8847 - Batch 180/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.50 min) Epoch 10/300 -- Iteration 8856 - Batch 189/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.50 min) Epoch 10/300 -- Iteration 8865 - Batch 198/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.51 min) Epoch 10/300 -- Iteration 8874 - Batch 207/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.51 min) Epoch 10/300 -- Iteration 8883 - Batch 216/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.52 min) Epoch 10/300 -- Iteration 8892 - Batch 225/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.52 min) Epoch 10/300 -- Iteration 8901 - Batch 234/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.53 min) Epoch 10/300 -- Iteration 8910 - Batch 243/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.53 min) Epoch 10/300 -- Iteration 8919 - Batch 252/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.54 min) Epoch 10/300 -- Iteration 8928 - Batch 261/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.54 min) Epoch 10/300 -- Iteration 8937 - Batch 270/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.54 min) Epoch 10/300 -- Iteration 8946 - Batch 279/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.55 min) Epoch 10/300 -- Iteration 8955 - Batch 288/963 - Train loss: 0.0033  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.55 min) Epoch 10/300 -- Iteration 8964 - Batch 297/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.56 min) Epoch 10/300 -- Iteration 8973 - Batch 306/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.56 min) Epoch 10/300 -- Iteration 8982 - Batch 315/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.57 min) Epoch 10/300 -- Iteration 8991 - Batch 324/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.57 min) Epoch 10/300 -- Iteration 9000 - Batch 333/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.58 min) Epoch 10/300 -- Iteration 9009 - Batch 342/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.58 min) Epoch 10/300 -- Iteration 9018 - Batch 351/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.59 min) Epoch 10/300 -- Iteration 9027 - Batch 360/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.59 min) Epoch 10/300 -- Iteration 9036 - Batch 369/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.60 min) Epoch 10/300 -- Iteration 9045 - Batch 378/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.60 min) Epoch 10/300 -- Iteration 9054 - Batch 387/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.60 min) Epoch 10/300 -- Iteration 9063 - Batch 396/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.61 min) Epoch 10/300 -- Iteration 9072 - Batch 405/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.61 min) Epoch 10/300 -- Iteration 9081 - Batch 414/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.62 min) Epoch 10/300 -- Iteration 9090 - Batch 423/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.62 min) Epoch 10/300 -- Iteration 9099 - Batch 432/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.63 min) Epoch 10/300 -- Iteration 9108 - Batch 441/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.63 min) Epoch 10/300 -- Iteration 9117 - Batch 450/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.64 min) Epoch 10/300 -- Iteration 9126 - Batch 459/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.64 min) Epoch 10/300 -- Iteration 9135 - Batch 468/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.64 min) Epoch 10/300 -- Iteration 9144 - Batch 477/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.65 min) Epoch 10/300 -- Iteration 9153 - Batch 486/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.65 min) Epoch 10/300 -- Iteration 9162 - Batch 495/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.66 min) Epoch 10/300 -- Iteration 9171 - Batch 504/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.66 min) Epoch 10/300 -- Iteration 9180 - Batch 513/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.67 min) Epoch 10/300 -- Iteration 9189 - Batch 522/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.67 min) Epoch 10/300 -- Iteration 9198 - Batch 531/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.68 min) Epoch 10/300 -- Iteration 9207 - Batch 540/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.68 min) Epoch 10/300 -- Iteration 9216 - Batch 549/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.69 min) Epoch 10/300 -- Iteration 9225 - Batch 558/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.69 min) Epoch 10/300 -- Iteration 9234 - Batch 567/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.70 min) Epoch 10/300 -- Iteration 9243 - Batch 576/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.70 min) Epoch 10/300 -- Iteration 9252 - Batch 585/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.71 min) Epoch 10/300 -- Iteration 9261 - Batch 594/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.71 min) Epoch 10/300 -- Iteration 9270 - Batch 603/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.71 min) Epoch 10/300 -- Iteration 9279 - Batch 612/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.72 min) Epoch 10/300 -- Iteration 9288 - Batch 621/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.72 min) Epoch 10/300 -- Iteration 9297 - Batch 630/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.73 min) Epoch 10/300 -- Iteration 9306 - Batch 639/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.73 min) Epoch 10/300 -- Iteration 9315 - Batch 648/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.74 min) Epoch 10/300 -- Iteration 9324 - Batch 657/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.74 min) Epoch 10/300 -- Iteration 9333 - Batch 666/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.75 min) Epoch 10/300 -- Iteration 9342 - Batch 675/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.75 min) Epoch 10/300 -- Iteration 9351 - Batch 684/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.76 min) Epoch 10/300 -- Iteration 9360 - Batch 693/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.76 min) Epoch 10/300 -- Iteration 9369 - Batch 702/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.77 min) Epoch 10/300 -- Iteration 9378 - Batch 711/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.77 min) Epoch 10/300 -- Iteration 9387 - Batch 720/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.78 min) Epoch 10/300 -- Iteration 9396 - Batch 729/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.78 min) Epoch 10/300 -- Iteration 9405 - Batch 738/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.79 min) Epoch 10/300 -- Iteration 9414 - Batch 747/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.79 min) Epoch 10/300 -- Iteration 9423 - Batch 756/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.80 min) Epoch 10/300 -- Iteration 9432 - Batch 765/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.80 min) Epoch 10/300 -- Iteration 9441 - Batch 774/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.81 min) Epoch 10/300 -- Iteration 9450 - Batch 783/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.81 min) Epoch 10/300 -- Iteration 9459 - Batch 792/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.82 min) Epoch 10/300 -- Iteration 9468 - Batch 801/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.82 min) Epoch 10/300 -- Iteration 9477 - Batch 810/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.83 min) Epoch 10/300 -- Iteration 9486 - Batch 819/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.83 min) Epoch 10/300 -- Iteration 9495 - Batch 828/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.83 min) Epoch 10/300 -- Iteration 9504 - Batch 837/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.84 min) Epoch 10/300 -- Iteration 9513 - Batch 846/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.84 min) Epoch 10/300 -- Iteration 9522 - Batch 855/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.85 min) Epoch 10/300 -- Iteration 9531 - Batch 864/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.85 min) Epoch 10/300 -- Iteration 9540 - Batch 873/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.86 min) Epoch 10/300 -- Iteration 9549 - Batch 882/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.86 min) Epoch 10/300 -- Iteration 9558 - Batch 891/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.87 min) Epoch 10/300 -- Iteration 9567 - Batch 900/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.87 min) Epoch 10/300 -- Iteration 9576 - Batch 909/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.88 min) Epoch 10/300 -- Iteration 9585 - Batch 918/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.88 min) Epoch 10/300 -- Iteration 9594 - Batch 927/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.89 min) Epoch 10/300 -- Iteration 9603 - Batch 936/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.89 min) Epoch 10/300 -- Iteration 9612 - Batch 945/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.89 min) Epoch 10/300 -- Iteration 9621 - Batch 954/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0036\n",
      "(4.90 min) Epoch 10/300 -- Iteration 9630 - Batch 962/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033 - Val acc: -0.0000\n",
      "(4.90 min) Epoch 11/300 -- Iteration 9639 - Batch 9/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.91 min) Epoch 11/300 -- Iteration 9648 - Batch 18/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.91 min) Epoch 11/300 -- Iteration 9657 - Batch 27/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.92 min) Epoch 11/300 -- Iteration 9666 - Batch 36/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.92 min) Epoch 11/300 -- Iteration 9675 - Batch 45/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.93 min) Epoch 11/300 -- Iteration 9684 - Batch 54/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.93 min) Epoch 11/300 -- Iteration 9693 - Batch 63/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.94 min) Epoch 11/300 -- Iteration 9702 - Batch 72/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.94 min) Epoch 11/300 -- Iteration 9711 - Batch 81/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.94 min) Epoch 11/300 -- Iteration 9720 - Batch 90/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.95 min) Epoch 11/300 -- Iteration 9729 - Batch 99/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.95 min) Epoch 11/300 -- Iteration 9738 - Batch 108/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.96 min) Epoch 11/300 -- Iteration 9747 - Batch 117/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.96 min) Epoch 11/300 -- Iteration 9756 - Batch 126/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.97 min) Epoch 11/300 -- Iteration 9765 - Batch 135/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.97 min) Epoch 11/300 -- Iteration 9774 - Batch 144/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.98 min) Epoch 11/300 -- Iteration 9783 - Batch 153/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.98 min) Epoch 11/300 -- Iteration 9792 - Batch 162/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.99 min) Epoch 11/300 -- Iteration 9801 - Batch 171/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(4.99 min) Epoch 11/300 -- Iteration 9810 - Batch 180/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.00 min) Epoch 11/300 -- Iteration 9819 - Batch 189/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.00 min) Epoch 11/300 -- Iteration 9828 - Batch 198/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.00 min) Epoch 11/300 -- Iteration 9837 - Batch 207/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.01 min) Epoch 11/300 -- Iteration 9846 - Batch 216/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.01 min) Epoch 11/300 -- Iteration 9855 - Batch 225/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.02 min) Epoch 11/300 -- Iteration 9864 - Batch 234/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.02 min) Epoch 11/300 -- Iteration 9873 - Batch 243/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.03 min) Epoch 11/300 -- Iteration 9882 - Batch 252/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.03 min) Epoch 11/300 -- Iteration 9891 - Batch 261/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.04 min) Epoch 11/300 -- Iteration 9900 - Batch 270/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.04 min) Epoch 11/300 -- Iteration 9909 - Batch 279/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.04 min) Epoch 11/300 -- Iteration 9918 - Batch 288/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.05 min) Epoch 11/300 -- Iteration 9927 - Batch 297/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.05 min) Epoch 11/300 -- Iteration 9936 - Batch 306/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.06 min) Epoch 11/300 -- Iteration 9945 - Batch 315/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.06 min) Epoch 11/300 -- Iteration 9954 - Batch 324/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.07 min) Epoch 11/300 -- Iteration 9963 - Batch 333/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.07 min) Epoch 11/300 -- Iteration 9972 - Batch 342/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.08 min) Epoch 11/300 -- Iteration 9981 - Batch 351/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.08 min) Epoch 11/300 -- Iteration 9990 - Batch 360/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.08 min) Epoch 11/300 -- Iteration 9999 - Batch 369/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.09 min) Epoch 11/300 -- Iteration 10008 - Batch 378/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.09 min) Epoch 11/300 -- Iteration 10017 - Batch 387/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.10 min) Epoch 11/300 -- Iteration 10026 - Batch 396/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.10 min) Epoch 11/300 -- Iteration 10035 - Batch 405/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.11 min) Epoch 11/300 -- Iteration 10044 - Batch 414/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.11 min) Epoch 11/300 -- Iteration 10053 - Batch 423/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.12 min) Epoch 11/300 -- Iteration 10062 - Batch 432/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.12 min) Epoch 11/300 -- Iteration 10071 - Batch 441/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.13 min) Epoch 11/300 -- Iteration 10080 - Batch 450/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.13 min) Epoch 11/300 -- Iteration 10089 - Batch 459/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.13 min) Epoch 11/300 -- Iteration 10098 - Batch 468/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.14 min) Epoch 11/300 -- Iteration 10107 - Batch 477/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.14 min) Epoch 11/300 -- Iteration 10116 - Batch 486/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.15 min) Epoch 11/300 -- Iteration 10125 - Batch 495/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.15 min) Epoch 11/300 -- Iteration 10134 - Batch 504/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.16 min) Epoch 11/300 -- Iteration 10143 - Batch 513/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.16 min) Epoch 11/300 -- Iteration 10152 - Batch 522/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.17 min) Epoch 11/300 -- Iteration 10161 - Batch 531/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.17 min) Epoch 11/300 -- Iteration 10170 - Batch 540/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.17 min) Epoch 11/300 -- Iteration 10179 - Batch 549/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.18 min) Epoch 11/300 -- Iteration 10188 - Batch 558/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.18 min) Epoch 11/300 -- Iteration 10197 - Batch 567/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.19 min) Epoch 11/300 -- Iteration 10206 - Batch 576/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.19 min) Epoch 11/300 -- Iteration 10215 - Batch 585/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.20 min) Epoch 11/300 -- Iteration 10224 - Batch 594/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.20 min) Epoch 11/300 -- Iteration 10233 - Batch 603/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.21 min) Epoch 11/300 -- Iteration 10242 - Batch 612/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.21 min) Epoch 11/300 -- Iteration 10251 - Batch 621/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.21 min) Epoch 11/300 -- Iteration 10260 - Batch 630/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.22 min) Epoch 11/300 -- Iteration 10269 - Batch 639/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.22 min) Epoch 11/300 -- Iteration 10278 - Batch 648/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.23 min) Epoch 11/300 -- Iteration 10287 - Batch 657/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.23 min) Epoch 11/300 -- Iteration 10296 - Batch 666/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.24 min) Epoch 11/300 -- Iteration 10305 - Batch 675/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.24 min) Epoch 11/300 -- Iteration 10314 - Batch 684/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.25 min) Epoch 11/300 -- Iteration 10323 - Batch 693/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.25 min) Epoch 11/300 -- Iteration 10332 - Batch 702/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.25 min) Epoch 11/300 -- Iteration 10341 - Batch 711/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.26 min) Epoch 11/300 -- Iteration 10350 - Batch 720/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.26 min) Epoch 11/300 -- Iteration 10359 - Batch 729/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.27 min) Epoch 11/300 -- Iteration 10368 - Batch 738/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.27 min) Epoch 11/300 -- Iteration 10377 - Batch 747/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.28 min) Epoch 11/300 -- Iteration 10386 - Batch 756/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.28 min) Epoch 11/300 -- Iteration 10395 - Batch 765/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.29 min) Epoch 11/300 -- Iteration 10404 - Batch 774/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.29 min) Epoch 11/300 -- Iteration 10413 - Batch 783/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.30 min) Epoch 11/300 -- Iteration 10422 - Batch 792/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.30 min) Epoch 11/300 -- Iteration 10431 - Batch 801/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.30 min) Epoch 11/300 -- Iteration 10440 - Batch 810/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.31 min) Epoch 11/300 -- Iteration 10449 - Batch 819/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.31 min) Epoch 11/300 -- Iteration 10458 - Batch 828/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.32 min) Epoch 11/300 -- Iteration 10467 - Batch 837/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.32 min) Epoch 11/300 -- Iteration 10476 - Batch 846/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.33 min) Epoch 11/300 -- Iteration 10485 - Batch 855/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.33 min) Epoch 11/300 -- Iteration 10494 - Batch 864/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.34 min) Epoch 11/300 -- Iteration 10503 - Batch 873/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.34 min) Epoch 11/300 -- Iteration 10512 - Batch 882/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.34 min) Epoch 11/300 -- Iteration 10521 - Batch 891/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.35 min) Epoch 11/300 -- Iteration 10530 - Batch 900/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.35 min) Epoch 11/300 -- Iteration 10539 - Batch 909/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.36 min) Epoch 11/300 -- Iteration 10548 - Batch 918/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.36 min) Epoch 11/300 -- Iteration 10557 - Batch 927/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.37 min) Epoch 11/300 -- Iteration 10566 - Batch 936/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.37 min) Epoch 11/300 -- Iteration 10575 - Batch 945/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.38 min) Epoch 11/300 -- Iteration 10584 - Batch 954/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.38 min) Epoch 11/300 -- Iteration 10593 - Batch 962/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0034 - Val acc: -0.0000\n",
      "(5.39 min) Epoch 12/300 -- Iteration 10602 - Batch 9/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.39 min) Epoch 12/300 -- Iteration 10611 - Batch 18/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.40 min) Epoch 12/300 -- Iteration 10620 - Batch 27/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.40 min) Epoch 12/300 -- Iteration 10629 - Batch 36/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.40 min) Epoch 12/300 -- Iteration 10638 - Batch 45/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.41 min) Epoch 12/300 -- Iteration 10647 - Batch 54/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.41 min) Epoch 12/300 -- Iteration 10656 - Batch 63/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.42 min) Epoch 12/300 -- Iteration 10665 - Batch 72/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.42 min) Epoch 12/300 -- Iteration 10674 - Batch 81/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.43 min) Epoch 12/300 -- Iteration 10683 - Batch 90/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.43 min) Epoch 12/300 -- Iteration 10692 - Batch 99/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.44 min) Epoch 12/300 -- Iteration 10701 - Batch 108/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.44 min) Epoch 12/300 -- Iteration 10710 - Batch 117/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.45 min) Epoch 12/300 -- Iteration 10719 - Batch 126/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.45 min) Epoch 12/300 -- Iteration 10728 - Batch 135/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.46 min) Epoch 12/300 -- Iteration 10737 - Batch 144/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.46 min) Epoch 12/300 -- Iteration 10746 - Batch 153/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.46 min) Epoch 12/300 -- Iteration 10755 - Batch 162/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.47 min) Epoch 12/300 -- Iteration 10764 - Batch 171/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.47 min) Epoch 12/300 -- Iteration 10773 - Batch 180/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.48 min) Epoch 12/300 -- Iteration 10782 - Batch 189/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.48 min) Epoch 12/300 -- Iteration 10791 - Batch 198/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.49 min) Epoch 12/300 -- Iteration 10800 - Batch 207/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.49 min) Epoch 12/300 -- Iteration 10809 - Batch 216/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.50 min) Epoch 12/300 -- Iteration 10818 - Batch 225/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.50 min) Epoch 12/300 -- Iteration 10827 - Batch 234/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.50 min) Epoch 12/300 -- Iteration 10836 - Batch 243/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.51 min) Epoch 12/300 -- Iteration 10845 - Batch 252/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.51 min) Epoch 12/300 -- Iteration 10854 - Batch 261/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.52 min) Epoch 12/300 -- Iteration 10863 - Batch 270/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.52 min) Epoch 12/300 -- Iteration 10872 - Batch 279/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.53 min) Epoch 12/300 -- Iteration 10881 - Batch 288/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.53 min) Epoch 12/300 -- Iteration 10890 - Batch 297/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.54 min) Epoch 12/300 -- Iteration 10899 - Batch 306/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.54 min) Epoch 12/300 -- Iteration 10908 - Batch 315/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.55 min) Epoch 12/300 -- Iteration 10917 - Batch 324/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.55 min) Epoch 12/300 -- Iteration 10926 - Batch 333/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.55 min) Epoch 12/300 -- Iteration 10935 - Batch 342/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.56 min) Epoch 12/300 -- Iteration 10944 - Batch 351/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.56 min) Epoch 12/300 -- Iteration 10953 - Batch 360/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.57 min) Epoch 12/300 -- Iteration 10962 - Batch 369/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.57 min) Epoch 12/300 -- Iteration 10971 - Batch 378/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.58 min) Epoch 12/300 -- Iteration 10980 - Batch 387/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.58 min) Epoch 12/300 -- Iteration 10989 - Batch 396/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.59 min) Epoch 12/300 -- Iteration 10998 - Batch 405/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.59 min) Epoch 12/300 -- Iteration 11007 - Batch 414/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.60 min) Epoch 12/300 -- Iteration 11016 - Batch 423/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.60 min) Epoch 12/300 -- Iteration 11025 - Batch 432/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.60 min) Epoch 12/300 -- Iteration 11034 - Batch 441/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.61 min) Epoch 12/300 -- Iteration 11043 - Batch 450/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.61 min) Epoch 12/300 -- Iteration 11052 - Batch 459/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.62 min) Epoch 12/300 -- Iteration 11061 - Batch 468/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.62 min) Epoch 12/300 -- Iteration 11070 - Batch 477/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.63 min) Epoch 12/300 -- Iteration 11079 - Batch 486/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.63 min) Epoch 12/300 -- Iteration 11088 - Batch 495/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.64 min) Epoch 12/300 -- Iteration 11097 - Batch 504/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.64 min) Epoch 12/300 -- Iteration 11106 - Batch 513/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.64 min) Epoch 12/300 -- Iteration 11115 - Batch 522/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.65 min) Epoch 12/300 -- Iteration 11124 - Batch 531/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.65 min) Epoch 12/300 -- Iteration 11133 - Batch 540/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.66 min) Epoch 12/300 -- Iteration 11142 - Batch 549/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.66 min) Epoch 12/300 -- Iteration 11151 - Batch 558/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.67 min) Epoch 12/300 -- Iteration 11160 - Batch 567/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.67 min) Epoch 12/300 -- Iteration 11169 - Batch 576/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.68 min) Epoch 12/300 -- Iteration 11178 - Batch 585/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.68 min) Epoch 12/300 -- Iteration 11187 - Batch 594/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.69 min) Epoch 12/300 -- Iteration 11196 - Batch 603/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.69 min) Epoch 12/300 -- Iteration 11205 - Batch 612/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.69 min) Epoch 12/300 -- Iteration 11214 - Batch 621/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.70 min) Epoch 12/300 -- Iteration 11223 - Batch 630/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.70 min) Epoch 12/300 -- Iteration 11232 - Batch 639/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.71 min) Epoch 12/300 -- Iteration 11241 - Batch 648/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.71 min) Epoch 12/300 -- Iteration 11250 - Batch 657/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.72 min) Epoch 12/300 -- Iteration 11259 - Batch 666/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.72 min) Epoch 12/300 -- Iteration 11268 - Batch 675/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.73 min) Epoch 12/300 -- Iteration 11277 - Batch 684/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.73 min) Epoch 12/300 -- Iteration 11286 - Batch 693/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.73 min) Epoch 12/300 -- Iteration 11295 - Batch 702/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.74 min) Epoch 12/300 -- Iteration 11304 - Batch 711/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.74 min) Epoch 12/300 -- Iteration 11313 - Batch 720/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.75 min) Epoch 12/300 -- Iteration 11322 - Batch 729/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.75 min) Epoch 12/300 -- Iteration 11331 - Batch 738/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.76 min) Epoch 12/300 -- Iteration 11340 - Batch 747/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.76 min) Epoch 12/300 -- Iteration 11349 - Batch 756/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.77 min) Epoch 12/300 -- Iteration 11358 - Batch 765/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.77 min) Epoch 12/300 -- Iteration 11367 - Batch 774/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.77 min) Epoch 12/300 -- Iteration 11376 - Batch 783/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.78 min) Epoch 12/300 -- Iteration 11385 - Batch 792/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.78 min) Epoch 12/300 -- Iteration 11394 - Batch 801/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.79 min) Epoch 12/300 -- Iteration 11403 - Batch 810/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.79 min) Epoch 12/300 -- Iteration 11412 - Batch 819/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.80 min) Epoch 12/300 -- Iteration 11421 - Batch 828/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.80 min) Epoch 12/300 -- Iteration 11430 - Batch 837/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.81 min) Epoch 12/300 -- Iteration 11439 - Batch 846/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.81 min) Epoch 12/300 -- Iteration 11448 - Batch 855/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.82 min) Epoch 12/300 -- Iteration 11457 - Batch 864/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.82 min) Epoch 12/300 -- Iteration 11466 - Batch 873/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.82 min) Epoch 12/300 -- Iteration 11475 - Batch 882/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.83 min) Epoch 12/300 -- Iteration 11484 - Batch 891/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.83 min) Epoch 12/300 -- Iteration 11493 - Batch 900/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.84 min) Epoch 12/300 -- Iteration 11502 - Batch 909/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.84 min) Epoch 12/300 -- Iteration 11511 - Batch 918/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.85 min) Epoch 12/300 -- Iteration 11520 - Batch 927/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.85 min) Epoch 12/300 -- Iteration 11529 - Batch 936/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.85 min) Epoch 12/300 -- Iteration 11538 - Batch 945/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.86 min) Epoch 12/300 -- Iteration 11547 - Batch 954/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0034\n",
      "(5.86 min) Epoch 12/300 -- Iteration 11556 - Batch 962/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033 - Val acc: -0.0000\n",
      "(5.87 min) Epoch 13/300 -- Iteration 11565 - Batch 9/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.87 min) Epoch 13/300 -- Iteration 11574 - Batch 18/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.88 min) Epoch 13/300 -- Iteration 11583 - Batch 27/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.88 min) Epoch 13/300 -- Iteration 11592 - Batch 36/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.89 min) Epoch 13/300 -- Iteration 11601 - Batch 45/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.89 min) Epoch 13/300 -- Iteration 11610 - Batch 54/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.90 min) Epoch 13/300 -- Iteration 11619 - Batch 63/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.90 min) Epoch 13/300 -- Iteration 11628 - Batch 72/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.91 min) Epoch 13/300 -- Iteration 11637 - Batch 81/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.91 min) Epoch 13/300 -- Iteration 11646 - Batch 90/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.91 min) Epoch 13/300 -- Iteration 11655 - Batch 99/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.92 min) Epoch 13/300 -- Iteration 11664 - Batch 108/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.92 min) Epoch 13/300 -- Iteration 11673 - Batch 117/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.93 min) Epoch 13/300 -- Iteration 11682 - Batch 126/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.93 min) Epoch 13/300 -- Iteration 11691 - Batch 135/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.94 min) Epoch 13/300 -- Iteration 11700 - Batch 144/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.94 min) Epoch 13/300 -- Iteration 11709 - Batch 153/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.95 min) Epoch 13/300 -- Iteration 11718 - Batch 162/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.95 min) Epoch 13/300 -- Iteration 11727 - Batch 171/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.95 min) Epoch 13/300 -- Iteration 11736 - Batch 180/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.96 min) Epoch 13/300 -- Iteration 11745 - Batch 189/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.96 min) Epoch 13/300 -- Iteration 11754 - Batch 198/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.97 min) Epoch 13/300 -- Iteration 11763 - Batch 207/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.97 min) Epoch 13/300 -- Iteration 11772 - Batch 216/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.98 min) Epoch 13/300 -- Iteration 11781 - Batch 225/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.98 min) Epoch 13/300 -- Iteration 11790 - Batch 234/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.99 min) Epoch 13/300 -- Iteration 11799 - Batch 243/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.99 min) Epoch 13/300 -- Iteration 11808 - Batch 252/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(5.99 min) Epoch 13/300 -- Iteration 11817 - Batch 261/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.00 min) Epoch 13/300 -- Iteration 11826 - Batch 270/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.00 min) Epoch 13/300 -- Iteration 11835 - Batch 279/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.01 min) Epoch 13/300 -- Iteration 11844 - Batch 288/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.01 min) Epoch 13/300 -- Iteration 11853 - Batch 297/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.02 min) Epoch 13/300 -- Iteration 11862 - Batch 306/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.02 min) Epoch 13/300 -- Iteration 11871 - Batch 315/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.03 min) Epoch 13/300 -- Iteration 11880 - Batch 324/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.03 min) Epoch 13/300 -- Iteration 11889 - Batch 333/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.03 min) Epoch 13/300 -- Iteration 11898 - Batch 342/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.04 min) Epoch 13/300 -- Iteration 11907 - Batch 351/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.04 min) Epoch 13/300 -- Iteration 11916 - Batch 360/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.05 min) Epoch 13/300 -- Iteration 11925 - Batch 369/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.05 min) Epoch 13/300 -- Iteration 11934 - Batch 378/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.06 min) Epoch 13/300 -- Iteration 11943 - Batch 387/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.06 min) Epoch 13/300 -- Iteration 11952 - Batch 396/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.07 min) Epoch 13/300 -- Iteration 11961 - Batch 405/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.07 min) Epoch 13/300 -- Iteration 11970 - Batch 414/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.08 min) Epoch 13/300 -- Iteration 11979 - Batch 423/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.08 min) Epoch 13/300 -- Iteration 11988 - Batch 432/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.08 min) Epoch 13/300 -- Iteration 11997 - Batch 441/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.09 min) Epoch 13/300 -- Iteration 12006 - Batch 450/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.09 min) Epoch 13/300 -- Iteration 12015 - Batch 459/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.10 min) Epoch 13/300 -- Iteration 12024 - Batch 468/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.10 min) Epoch 13/300 -- Iteration 12033 - Batch 477/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.11 min) Epoch 13/300 -- Iteration 12042 - Batch 486/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.11 min) Epoch 13/300 -- Iteration 12051 - Batch 495/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.12 min) Epoch 13/300 -- Iteration 12060 - Batch 504/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.12 min) Epoch 13/300 -- Iteration 12069 - Batch 513/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.12 min) Epoch 13/300 -- Iteration 12078 - Batch 522/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.13 min) Epoch 13/300 -- Iteration 12087 - Batch 531/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.13 min) Epoch 13/300 -- Iteration 12096 - Batch 540/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.14 min) Epoch 13/300 -- Iteration 12105 - Batch 549/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.14 min) Epoch 13/300 -- Iteration 12114 - Batch 558/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.15 min) Epoch 13/300 -- Iteration 12123 - Batch 567/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.15 min) Epoch 13/300 -- Iteration 12132 - Batch 576/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.16 min) Epoch 13/300 -- Iteration 12141 - Batch 585/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.16 min) Epoch 13/300 -- Iteration 12150 - Batch 594/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.17 min) Epoch 13/300 -- Iteration 12159 - Batch 603/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.17 min) Epoch 13/300 -- Iteration 12168 - Batch 612/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.17 min) Epoch 13/300 -- Iteration 12177 - Batch 621/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.18 min) Epoch 13/300 -- Iteration 12186 - Batch 630/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.18 min) Epoch 13/300 -- Iteration 12195 - Batch 639/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.19 min) Epoch 13/300 -- Iteration 12204 - Batch 648/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.19 min) Epoch 13/300 -- Iteration 12213 - Batch 657/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.20 min) Epoch 13/300 -- Iteration 12222 - Batch 666/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.20 min) Epoch 13/300 -- Iteration 12231 - Batch 675/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.21 min) Epoch 13/300 -- Iteration 12240 - Batch 684/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.21 min) Epoch 13/300 -- Iteration 12249 - Batch 693/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.22 min) Epoch 13/300 -- Iteration 12258 - Batch 702/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.22 min) Epoch 13/300 -- Iteration 12267 - Batch 711/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.23 min) Epoch 13/300 -- Iteration 12276 - Batch 720/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.23 min) Epoch 13/300 -- Iteration 12285 - Batch 729/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.23 min) Epoch 13/300 -- Iteration 12294 - Batch 738/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.24 min) Epoch 13/300 -- Iteration 12303 - Batch 747/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.24 min) Epoch 13/300 -- Iteration 12312 - Batch 756/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.25 min) Epoch 13/300 -- Iteration 12321 - Batch 765/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.25 min) Epoch 13/300 -- Iteration 12330 - Batch 774/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.26 min) Epoch 13/300 -- Iteration 12339 - Batch 783/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.26 min) Epoch 13/300 -- Iteration 12348 - Batch 792/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.27 min) Epoch 13/300 -- Iteration 12357 - Batch 801/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.27 min) Epoch 13/300 -- Iteration 12366 - Batch 810/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.28 min) Epoch 13/300 -- Iteration 12375 - Batch 819/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.28 min) Epoch 13/300 -- Iteration 12384 - Batch 828/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.28 min) Epoch 13/300 -- Iteration 12393 - Batch 837/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.29 min) Epoch 13/300 -- Iteration 12402 - Batch 846/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.29 min) Epoch 13/300 -- Iteration 12411 - Batch 855/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.30 min) Epoch 13/300 -- Iteration 12420 - Batch 864/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.30 min) Epoch 13/300 -- Iteration 12429 - Batch 873/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.31 min) Epoch 13/300 -- Iteration 12438 - Batch 882/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.31 min) Epoch 13/300 -- Iteration 12447 - Batch 891/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.32 min) Epoch 13/300 -- Iteration 12456 - Batch 900/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.32 min) Epoch 13/300 -- Iteration 12465 - Batch 909/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.33 min) Epoch 13/300 -- Iteration 12474 - Batch 918/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.33 min) Epoch 13/300 -- Iteration 12483 - Batch 927/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.33 min) Epoch 13/300 -- Iteration 12492 - Batch 936/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.34 min) Epoch 13/300 -- Iteration 12501 - Batch 945/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.34 min) Epoch 13/300 -- Iteration 12510 - Batch 954/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.35 min) Epoch 13/300 -- Iteration 12519 - Batch 962/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033 - Val acc: -0.0000\n",
      "(6.35 min) Epoch 14/300 -- Iteration 12528 - Batch 9/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.36 min) Epoch 14/300 -- Iteration 12537 - Batch 18/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.36 min) Epoch 14/300 -- Iteration 12546 - Batch 27/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.37 min) Epoch 14/300 -- Iteration 12555 - Batch 36/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.37 min) Epoch 14/300 -- Iteration 12564 - Batch 45/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.38 min) Epoch 14/300 -- Iteration 12573 - Batch 54/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.38 min) Epoch 14/300 -- Iteration 12582 - Batch 63/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.38 min) Epoch 14/300 -- Iteration 12591 - Batch 72/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.39 min) Epoch 14/300 -- Iteration 12600 - Batch 81/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.39 min) Epoch 14/300 -- Iteration 12609 - Batch 90/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.40 min) Epoch 14/300 -- Iteration 12618 - Batch 99/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.40 min) Epoch 14/300 -- Iteration 12627 - Batch 108/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.41 min) Epoch 14/300 -- Iteration 12636 - Batch 117/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.41 min) Epoch 14/300 -- Iteration 12645 - Batch 126/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.42 min) Epoch 14/300 -- Iteration 12654 - Batch 135/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.42 min) Epoch 14/300 -- Iteration 12663 - Batch 144/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.43 min) Epoch 14/300 -- Iteration 12672 - Batch 153/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.43 min) Epoch 14/300 -- Iteration 12681 - Batch 162/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.43 min) Epoch 14/300 -- Iteration 12690 - Batch 171/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.44 min) Epoch 14/300 -- Iteration 12699 - Batch 180/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.44 min) Epoch 14/300 -- Iteration 12708 - Batch 189/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.45 min) Epoch 14/300 -- Iteration 12717 - Batch 198/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.45 min) Epoch 14/300 -- Iteration 12726 - Batch 207/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.46 min) Epoch 14/300 -- Iteration 12735 - Batch 216/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.46 min) Epoch 14/300 -- Iteration 12744 - Batch 225/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.47 min) Epoch 14/300 -- Iteration 12753 - Batch 234/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.47 min) Epoch 14/300 -- Iteration 12762 - Batch 243/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.47 min) Epoch 14/300 -- Iteration 12771 - Batch 252/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.48 min) Epoch 14/300 -- Iteration 12780 - Batch 261/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.48 min) Epoch 14/300 -- Iteration 12789 - Batch 270/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.49 min) Epoch 14/300 -- Iteration 12798 - Batch 279/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.49 min) Epoch 14/300 -- Iteration 12807 - Batch 288/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.50 min) Epoch 14/300 -- Iteration 12816 - Batch 297/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.50 min) Epoch 14/300 -- Iteration 12825 - Batch 306/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.51 min) Epoch 14/300 -- Iteration 12834 - Batch 315/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.51 min) Epoch 14/300 -- Iteration 12843 - Batch 324/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.51 min) Epoch 14/300 -- Iteration 12852 - Batch 333/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.52 min) Epoch 14/300 -- Iteration 12861 - Batch 342/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.52 min) Epoch 14/300 -- Iteration 12870 - Batch 351/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.53 min) Epoch 14/300 -- Iteration 12879 - Batch 360/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.53 min) Epoch 14/300 -- Iteration 12888 - Batch 369/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.54 min) Epoch 14/300 -- Iteration 12897 - Batch 378/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.54 min) Epoch 14/300 -- Iteration 12906 - Batch 387/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.55 min) Epoch 14/300 -- Iteration 12915 - Batch 396/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.55 min) Epoch 14/300 -- Iteration 12924 - Batch 405/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.55 min) Epoch 14/300 -- Iteration 12933 - Batch 414/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.56 min) Epoch 14/300 -- Iteration 12942 - Batch 423/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.56 min) Epoch 14/300 -- Iteration 12951 - Batch 432/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.57 min) Epoch 14/300 -- Iteration 12960 - Batch 441/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.57 min) Epoch 14/300 -- Iteration 12969 - Batch 450/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.58 min) Epoch 14/300 -- Iteration 12978 - Batch 459/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.58 min) Epoch 14/300 -- Iteration 12987 - Batch 468/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.59 min) Epoch 14/300 -- Iteration 12996 - Batch 477/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.59 min) Epoch 14/300 -- Iteration 13005 - Batch 486/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.60 min) Epoch 14/300 -- Iteration 13014 - Batch 495/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.60 min) Epoch 14/300 -- Iteration 13023 - Batch 504/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.60 min) Epoch 14/300 -- Iteration 13032 - Batch 513/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.61 min) Epoch 14/300 -- Iteration 13041 - Batch 522/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.61 min) Epoch 14/300 -- Iteration 13050 - Batch 531/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.62 min) Epoch 14/300 -- Iteration 13059 - Batch 540/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.62 min) Epoch 14/300 -- Iteration 13068 - Batch 549/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.63 min) Epoch 14/300 -- Iteration 13077 - Batch 558/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.63 min) Epoch 14/300 -- Iteration 13086 - Batch 567/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.64 min) Epoch 14/300 -- Iteration 13095 - Batch 576/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.64 min) Epoch 14/300 -- Iteration 13104 - Batch 585/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.64 min) Epoch 14/300 -- Iteration 13113 - Batch 594/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.65 min) Epoch 14/300 -- Iteration 13122 - Batch 603/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.65 min) Epoch 14/300 -- Iteration 13131 - Batch 612/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.66 min) Epoch 14/300 -- Iteration 13140 - Batch 621/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.66 min) Epoch 14/300 -- Iteration 13149 - Batch 630/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.66 min) Epoch 14/300 -- Iteration 13158 - Batch 639/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.67 min) Epoch 14/300 -- Iteration 13167 - Batch 648/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.67 min) Epoch 14/300 -- Iteration 13176 - Batch 657/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.68 min) Epoch 14/300 -- Iteration 13185 - Batch 666/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.68 min) Epoch 14/300 -- Iteration 13194 - Batch 675/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.69 min) Epoch 14/300 -- Iteration 13203 - Batch 684/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.69 min) Epoch 14/300 -- Iteration 13212 - Batch 693/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.69 min) Epoch 14/300 -- Iteration 13221 - Batch 702/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.70 min) Epoch 14/300 -- Iteration 13230 - Batch 711/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.70 min) Epoch 14/300 -- Iteration 13239 - Batch 720/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.71 min) Epoch 14/300 -- Iteration 13248 - Batch 729/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.71 min) Epoch 14/300 -- Iteration 13257 - Batch 738/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.72 min) Epoch 14/300 -- Iteration 13266 - Batch 747/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.72 min) Epoch 14/300 -- Iteration 13275 - Batch 756/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.72 min) Epoch 14/300 -- Iteration 13284 - Batch 765/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.73 min) Epoch 14/300 -- Iteration 13293 - Batch 774/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.73 min) Epoch 14/300 -- Iteration 13302 - Batch 783/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.74 min) Epoch 14/300 -- Iteration 13311 - Batch 792/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.74 min) Epoch 14/300 -- Iteration 13320 - Batch 801/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.75 min) Epoch 14/300 -- Iteration 13329 - Batch 810/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.75 min) Epoch 14/300 -- Iteration 13338 - Batch 819/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.75 min) Epoch 14/300 -- Iteration 13347 - Batch 828/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.76 min) Epoch 14/300 -- Iteration 13356 - Batch 837/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.76 min) Epoch 14/300 -- Iteration 13365 - Batch 846/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.77 min) Epoch 14/300 -- Iteration 13374 - Batch 855/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.77 min) Epoch 14/300 -- Iteration 13383 - Batch 864/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.78 min) Epoch 14/300 -- Iteration 13392 - Batch 873/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.78 min) Epoch 14/300 -- Iteration 13401 - Batch 882/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.79 min) Epoch 14/300 -- Iteration 13410 - Batch 891/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.79 min) Epoch 14/300 -- Iteration 13419 - Batch 900/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.80 min) Epoch 14/300 -- Iteration 13428 - Batch 909/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.80 min) Epoch 14/300 -- Iteration 13437 - Batch 918/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.80 min) Epoch 14/300 -- Iteration 13446 - Batch 927/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.81 min) Epoch 14/300 -- Iteration 13455 - Batch 936/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.81 min) Epoch 14/300 -- Iteration 13464 - Batch 945/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.82 min) Epoch 14/300 -- Iteration 13473 - Batch 954/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0033\n",
      "(6.82 min) Epoch 14/300 -- Iteration 13482 - Batch 962/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031 - Val acc: -0.0000\n",
      "(6.83 min) Epoch 15/300 -- Iteration 13491 - Batch 9/963 - Train loss: 0.0032  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.83 min) Epoch 15/300 -- Iteration 13500 - Batch 18/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.83 min) Epoch 15/300 -- Iteration 13509 - Batch 27/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.84 min) Epoch 15/300 -- Iteration 13518 - Batch 36/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.84 min) Epoch 15/300 -- Iteration 13527 - Batch 45/963 - Train loss: 0.0031  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.85 min) Epoch 15/300 -- Iteration 13536 - Batch 54/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.85 min) Epoch 15/300 -- Iteration 13545 - Batch 63/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.85 min) Epoch 15/300 -- Iteration 13554 - Batch 72/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.86 min) Epoch 15/300 -- Iteration 13563 - Batch 81/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.86 min) Epoch 15/300 -- Iteration 13572 - Batch 90/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.87 min) Epoch 15/300 -- Iteration 13581 - Batch 99/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.87 min) Epoch 15/300 -- Iteration 13590 - Batch 108/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.88 min) Epoch 15/300 -- Iteration 13599 - Batch 117/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.88 min) Epoch 15/300 -- Iteration 13608 - Batch 126/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.88 min) Epoch 15/300 -- Iteration 13617 - Batch 135/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.89 min) Epoch 15/300 -- Iteration 13626 - Batch 144/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.89 min) Epoch 15/300 -- Iteration 13635 - Batch 153/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.90 min) Epoch 15/300 -- Iteration 13644 - Batch 162/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.90 min) Epoch 15/300 -- Iteration 13653 - Batch 171/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.91 min) Epoch 15/300 -- Iteration 13662 - Batch 180/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.91 min) Epoch 15/300 -- Iteration 13671 - Batch 189/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.91 min) Epoch 15/300 -- Iteration 13680 - Batch 198/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.92 min) Epoch 15/300 -- Iteration 13689 - Batch 207/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.92 min) Epoch 15/300 -- Iteration 13698 - Batch 216/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.93 min) Epoch 15/300 -- Iteration 13707 - Batch 225/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.93 min) Epoch 15/300 -- Iteration 13716 - Batch 234/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.93 min) Epoch 15/300 -- Iteration 13725 - Batch 243/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.94 min) Epoch 15/300 -- Iteration 13734 - Batch 252/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.94 min) Epoch 15/300 -- Iteration 13743 - Batch 261/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.95 min) Epoch 15/300 -- Iteration 13752 - Batch 270/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.95 min) Epoch 15/300 -- Iteration 13761 - Batch 279/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.96 min) Epoch 15/300 -- Iteration 13770 - Batch 288/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.96 min) Epoch 15/300 -- Iteration 13779 - Batch 297/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.96 min) Epoch 15/300 -- Iteration 13788 - Batch 306/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.97 min) Epoch 15/300 -- Iteration 13797 - Batch 315/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.97 min) Epoch 15/300 -- Iteration 13806 - Batch 324/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.98 min) Epoch 15/300 -- Iteration 13815 - Batch 333/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.98 min) Epoch 15/300 -- Iteration 13824 - Batch 342/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.99 min) Epoch 15/300 -- Iteration 13833 - Batch 351/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.99 min) Epoch 15/300 -- Iteration 13842 - Batch 360/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(6.99 min) Epoch 15/300 -- Iteration 13851 - Batch 369/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.00 min) Epoch 15/300 -- Iteration 13860 - Batch 378/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.00 min) Epoch 15/300 -- Iteration 13869 - Batch 387/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.01 min) Epoch 15/300 -- Iteration 13878 - Batch 396/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.01 min) Epoch 15/300 -- Iteration 13887 - Batch 405/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.02 min) Epoch 15/300 -- Iteration 13896 - Batch 414/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.02 min) Epoch 15/300 -- Iteration 13905 - Batch 423/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.03 min) Epoch 15/300 -- Iteration 13914 - Batch 432/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.03 min) Epoch 15/300 -- Iteration 13923 - Batch 441/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.03 min) Epoch 15/300 -- Iteration 13932 - Batch 450/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.04 min) Epoch 15/300 -- Iteration 13941 - Batch 459/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.04 min) Epoch 15/300 -- Iteration 13950 - Batch 468/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.05 min) Epoch 15/300 -- Iteration 13959 - Batch 477/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.05 min) Epoch 15/300 -- Iteration 13968 - Batch 486/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.05 min) Epoch 15/300 -- Iteration 13977 - Batch 495/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.06 min) Epoch 15/300 -- Iteration 13986 - Batch 504/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.06 min) Epoch 15/300 -- Iteration 13995 - Batch 513/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.07 min) Epoch 15/300 -- Iteration 14004 - Batch 522/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.07 min) Epoch 15/300 -- Iteration 14013 - Batch 531/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.08 min) Epoch 15/300 -- Iteration 14022 - Batch 540/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.08 min) Epoch 15/300 -- Iteration 14031 - Batch 549/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.08 min) Epoch 15/300 -- Iteration 14040 - Batch 558/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.09 min) Epoch 15/300 -- Iteration 14049 - Batch 567/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.09 min) Epoch 15/300 -- Iteration 14058 - Batch 576/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.10 min) Epoch 15/300 -- Iteration 14067 - Batch 585/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.10 min) Epoch 15/300 -- Iteration 14076 - Batch 594/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.11 min) Epoch 15/300 -- Iteration 14085 - Batch 603/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.11 min) Epoch 15/300 -- Iteration 14094 - Batch 612/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.11 min) Epoch 15/300 -- Iteration 14103 - Batch 621/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.12 min) Epoch 15/300 -- Iteration 14112 - Batch 630/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.12 min) Epoch 15/300 -- Iteration 14121 - Batch 639/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.13 min) Epoch 15/300 -- Iteration 14130 - Batch 648/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.13 min) Epoch 15/300 -- Iteration 14139 - Batch 657/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.14 min) Epoch 15/300 -- Iteration 14148 - Batch 666/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.14 min) Epoch 15/300 -- Iteration 14157 - Batch 675/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.15 min) Epoch 15/300 -- Iteration 14166 - Batch 684/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.15 min) Epoch 15/300 -- Iteration 14175 - Batch 693/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.16 min) Epoch 15/300 -- Iteration 14184 - Batch 702/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.16 min) Epoch 15/300 -- Iteration 14193 - Batch 711/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.17 min) Epoch 15/300 -- Iteration 14202 - Batch 720/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.17 min) Epoch 15/300 -- Iteration 14211 - Batch 729/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.17 min) Epoch 15/300 -- Iteration 14220 - Batch 738/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.18 min) Epoch 15/300 -- Iteration 14229 - Batch 747/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.18 min) Epoch 15/300 -- Iteration 14238 - Batch 756/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.19 min) Epoch 15/300 -- Iteration 14247 - Batch 765/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.19 min) Epoch 15/300 -- Iteration 14256 - Batch 774/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.20 min) Epoch 15/300 -- Iteration 14265 - Batch 783/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.20 min) Epoch 15/300 -- Iteration 14274 - Batch 792/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.21 min) Epoch 15/300 -- Iteration 14283 - Batch 801/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.21 min) Epoch 15/300 -- Iteration 14292 - Batch 810/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.22 min) Epoch 15/300 -- Iteration 14301 - Batch 819/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.22 min) Epoch 15/300 -- Iteration 14310 - Batch 828/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.23 min) Epoch 15/300 -- Iteration 14319 - Batch 837/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.23 min) Epoch 15/300 -- Iteration 14328 - Batch 846/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.23 min) Epoch 15/300 -- Iteration 14337 - Batch 855/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.24 min) Epoch 15/300 -- Iteration 14346 - Batch 864/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.24 min) Epoch 15/300 -- Iteration 14355 - Batch 873/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.25 min) Epoch 15/300 -- Iteration 14364 - Batch 882/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.25 min) Epoch 15/300 -- Iteration 14373 - Batch 891/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.26 min) Epoch 15/300 -- Iteration 14382 - Batch 900/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.26 min) Epoch 15/300 -- Iteration 14391 - Batch 909/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.27 min) Epoch 15/300 -- Iteration 14400 - Batch 918/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.27 min) Epoch 15/300 -- Iteration 14409 - Batch 927/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.28 min) Epoch 15/300 -- Iteration 14418 - Batch 936/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.28 min) Epoch 15/300 -- Iteration 14427 - Batch 945/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.29 min) Epoch 15/300 -- Iteration 14436 - Batch 954/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(7.29 min) Epoch 15/300 -- Iteration 14445 - Batch 962/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030 - Val acc: -0.0000\n",
      "(7.30 min) Epoch 16/300 -- Iteration 14454 - Batch 9/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.30 min) Epoch 16/300 -- Iteration 14463 - Batch 18/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.30 min) Epoch 16/300 -- Iteration 14472 - Batch 27/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.31 min) Epoch 16/300 -- Iteration 14481 - Batch 36/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.31 min) Epoch 16/300 -- Iteration 14490 - Batch 45/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.32 min) Epoch 16/300 -- Iteration 14499 - Batch 54/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.32 min) Epoch 16/300 -- Iteration 14508 - Batch 63/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.33 min) Epoch 16/300 -- Iteration 14517 - Batch 72/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.33 min) Epoch 16/300 -- Iteration 14526 - Batch 81/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.34 min) Epoch 16/300 -- Iteration 14535 - Batch 90/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.34 min) Epoch 16/300 -- Iteration 14544 - Batch 99/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.35 min) Epoch 16/300 -- Iteration 14553 - Batch 108/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.35 min) Epoch 16/300 -- Iteration 14562 - Batch 117/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.36 min) Epoch 16/300 -- Iteration 14571 - Batch 126/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.36 min) Epoch 16/300 -- Iteration 14580 - Batch 135/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.37 min) Epoch 16/300 -- Iteration 14589 - Batch 144/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.37 min) Epoch 16/300 -- Iteration 14598 - Batch 153/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.37 min) Epoch 16/300 -- Iteration 14607 - Batch 162/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.38 min) Epoch 16/300 -- Iteration 14616 - Batch 171/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.38 min) Epoch 16/300 -- Iteration 14625 - Batch 180/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.39 min) Epoch 16/300 -- Iteration 14634 - Batch 189/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.39 min) Epoch 16/300 -- Iteration 14643 - Batch 198/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.40 min) Epoch 16/300 -- Iteration 14652 - Batch 207/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.40 min) Epoch 16/300 -- Iteration 14661 - Batch 216/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.41 min) Epoch 16/300 -- Iteration 14670 - Batch 225/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.41 min) Epoch 16/300 -- Iteration 14679 - Batch 234/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.42 min) Epoch 16/300 -- Iteration 14688 - Batch 243/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.42 min) Epoch 16/300 -- Iteration 14697 - Batch 252/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.43 min) Epoch 16/300 -- Iteration 14706 - Batch 261/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.43 min) Epoch 16/300 -- Iteration 14715 - Batch 270/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.43 min) Epoch 16/300 -- Iteration 14724 - Batch 279/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.44 min) Epoch 16/300 -- Iteration 14733 - Batch 288/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.44 min) Epoch 16/300 -- Iteration 14742 - Batch 297/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.45 min) Epoch 16/300 -- Iteration 14751 - Batch 306/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.45 min) Epoch 16/300 -- Iteration 14760 - Batch 315/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.46 min) Epoch 16/300 -- Iteration 14769 - Batch 324/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.46 min) Epoch 16/300 -- Iteration 14778 - Batch 333/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.47 min) Epoch 16/300 -- Iteration 14787 - Batch 342/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.47 min) Epoch 16/300 -- Iteration 14796 - Batch 351/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.48 min) Epoch 16/300 -- Iteration 14805 - Batch 360/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.48 min) Epoch 16/300 -- Iteration 14814 - Batch 369/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.48 min) Epoch 16/300 -- Iteration 14823 - Batch 378/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.49 min) Epoch 16/300 -- Iteration 14832 - Batch 387/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.49 min) Epoch 16/300 -- Iteration 14841 - Batch 396/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.50 min) Epoch 16/300 -- Iteration 14850 - Batch 405/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.50 min) Epoch 16/300 -- Iteration 14859 - Batch 414/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.51 min) Epoch 16/300 -- Iteration 14868 - Batch 423/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.51 min) Epoch 16/300 -- Iteration 14877 - Batch 432/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.52 min) Epoch 16/300 -- Iteration 14886 - Batch 441/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.52 min) Epoch 16/300 -- Iteration 14895 - Batch 450/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.53 min) Epoch 16/300 -- Iteration 14904 - Batch 459/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.53 min) Epoch 16/300 -- Iteration 14913 - Batch 468/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.53 min) Epoch 16/300 -- Iteration 14922 - Batch 477/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.54 min) Epoch 16/300 -- Iteration 14931 - Batch 486/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.54 min) Epoch 16/300 -- Iteration 14940 - Batch 495/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.55 min) Epoch 16/300 -- Iteration 14949 - Batch 504/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.55 min) Epoch 16/300 -- Iteration 14958 - Batch 513/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.56 min) Epoch 16/300 -- Iteration 14967 - Batch 522/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.56 min) Epoch 16/300 -- Iteration 14976 - Batch 531/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.57 min) Epoch 16/300 -- Iteration 14985 - Batch 540/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.57 min) Epoch 16/300 -- Iteration 14994 - Batch 549/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.57 min) Epoch 16/300 -- Iteration 15003 - Batch 558/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.58 min) Epoch 16/300 -- Iteration 15012 - Batch 567/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.58 min) Epoch 16/300 -- Iteration 15021 - Batch 576/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.59 min) Epoch 16/300 -- Iteration 15030 - Batch 585/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.59 min) Epoch 16/300 -- Iteration 15039 - Batch 594/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.60 min) Epoch 16/300 -- Iteration 15048 - Batch 603/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.60 min) Epoch 16/300 -- Iteration 15057 - Batch 612/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.61 min) Epoch 16/300 -- Iteration 15066 - Batch 621/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.61 min) Epoch 16/300 -- Iteration 15075 - Batch 630/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.62 min) Epoch 16/300 -- Iteration 15084 - Batch 639/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.62 min) Epoch 16/300 -- Iteration 15093 - Batch 648/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.62 min) Epoch 16/300 -- Iteration 15102 - Batch 657/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.63 min) Epoch 16/300 -- Iteration 15111 - Batch 666/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.63 min) Epoch 16/300 -- Iteration 15120 - Batch 675/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.64 min) Epoch 16/300 -- Iteration 15129 - Batch 684/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.64 min) Epoch 16/300 -- Iteration 15138 - Batch 693/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.64 min) Epoch 16/300 -- Iteration 15147 - Batch 702/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.65 min) Epoch 16/300 -- Iteration 15156 - Batch 711/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.65 min) Epoch 16/300 -- Iteration 15165 - Batch 720/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.66 min) Epoch 16/300 -- Iteration 15174 - Batch 729/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.66 min) Epoch 16/300 -- Iteration 15183 - Batch 738/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.67 min) Epoch 16/300 -- Iteration 15192 - Batch 747/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.67 min) Epoch 16/300 -- Iteration 15201 - Batch 756/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.67 min) Epoch 16/300 -- Iteration 15210 - Batch 765/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.68 min) Epoch 16/300 -- Iteration 15219 - Batch 774/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.68 min) Epoch 16/300 -- Iteration 15228 - Batch 783/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.69 min) Epoch 16/300 -- Iteration 15237 - Batch 792/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.69 min) Epoch 16/300 -- Iteration 15246 - Batch 801/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.69 min) Epoch 16/300 -- Iteration 15255 - Batch 810/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.70 min) Epoch 16/300 -- Iteration 15264 - Batch 819/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.70 min) Epoch 16/300 -- Iteration 15273 - Batch 828/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.71 min) Epoch 16/300 -- Iteration 15282 - Batch 837/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.71 min) Epoch 16/300 -- Iteration 15291 - Batch 846/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.71 min) Epoch 16/300 -- Iteration 15300 - Batch 855/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.72 min) Epoch 16/300 -- Iteration 15309 - Batch 864/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.72 min) Epoch 16/300 -- Iteration 15318 - Batch 873/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.73 min) Epoch 16/300 -- Iteration 15327 - Batch 882/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.73 min) Epoch 16/300 -- Iteration 15336 - Batch 891/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.74 min) Epoch 16/300 -- Iteration 15345 - Batch 900/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.74 min) Epoch 16/300 -- Iteration 15354 - Batch 909/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.74 min) Epoch 16/300 -- Iteration 15363 - Batch 918/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.75 min) Epoch 16/300 -- Iteration 15372 - Batch 927/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.75 min) Epoch 16/300 -- Iteration 15381 - Batch 936/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.76 min) Epoch 16/300 -- Iteration 15390 - Batch 945/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.76 min) Epoch 16/300 -- Iteration 15399 - Batch 954/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(7.76 min) Epoch 16/300 -- Iteration 15408 - Batch 962/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032 - Val acc: -0.0000\n",
      "(7.77 min) Epoch 17/300 -- Iteration 15417 - Batch 9/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.77 min) Epoch 17/300 -- Iteration 15426 - Batch 18/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.78 min) Epoch 17/300 -- Iteration 15435 - Batch 27/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.78 min) Epoch 17/300 -- Iteration 15444 - Batch 36/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.79 min) Epoch 17/300 -- Iteration 15453 - Batch 45/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.79 min) Epoch 17/300 -- Iteration 15462 - Batch 54/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.79 min) Epoch 17/300 -- Iteration 15471 - Batch 63/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.80 min) Epoch 17/300 -- Iteration 15480 - Batch 72/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.80 min) Epoch 17/300 -- Iteration 15489 - Batch 81/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.81 min) Epoch 17/300 -- Iteration 15498 - Batch 90/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.81 min) Epoch 17/300 -- Iteration 15507 - Batch 99/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.82 min) Epoch 17/300 -- Iteration 15516 - Batch 108/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.82 min) Epoch 17/300 -- Iteration 15525 - Batch 117/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.83 min) Epoch 17/300 -- Iteration 15534 - Batch 126/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.83 min) Epoch 17/300 -- Iteration 15543 - Batch 135/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.83 min) Epoch 17/300 -- Iteration 15552 - Batch 144/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.84 min) Epoch 17/300 -- Iteration 15561 - Batch 153/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.84 min) Epoch 17/300 -- Iteration 15570 - Batch 162/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.85 min) Epoch 17/300 -- Iteration 15579 - Batch 171/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.85 min) Epoch 17/300 -- Iteration 15588 - Batch 180/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.85 min) Epoch 17/300 -- Iteration 15597 - Batch 189/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.86 min) Epoch 17/300 -- Iteration 15606 - Batch 198/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.86 min) Epoch 17/300 -- Iteration 15615 - Batch 207/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.87 min) Epoch 17/300 -- Iteration 15624 - Batch 216/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.87 min) Epoch 17/300 -- Iteration 15633 - Batch 225/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.88 min) Epoch 17/300 -- Iteration 15642 - Batch 234/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.88 min) Epoch 17/300 -- Iteration 15651 - Batch 243/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.89 min) Epoch 17/300 -- Iteration 15660 - Batch 252/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.89 min) Epoch 17/300 -- Iteration 15669 - Batch 261/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.89 min) Epoch 17/300 -- Iteration 15678 - Batch 270/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.90 min) Epoch 17/300 -- Iteration 15687 - Batch 279/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.90 min) Epoch 17/300 -- Iteration 15696 - Batch 288/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.91 min) Epoch 17/300 -- Iteration 15705 - Batch 297/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.91 min) Epoch 17/300 -- Iteration 15714 - Batch 306/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.92 min) Epoch 17/300 -- Iteration 15723 - Batch 315/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.92 min) Epoch 17/300 -- Iteration 15732 - Batch 324/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.92 min) Epoch 17/300 -- Iteration 15741 - Batch 333/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.93 min) Epoch 17/300 -- Iteration 15750 - Batch 342/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.93 min) Epoch 17/300 -- Iteration 15759 - Batch 351/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.94 min) Epoch 17/300 -- Iteration 15768 - Batch 360/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.94 min) Epoch 17/300 -- Iteration 15777 - Batch 369/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.95 min) Epoch 17/300 -- Iteration 15786 - Batch 378/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.95 min) Epoch 17/300 -- Iteration 15795 - Batch 387/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.95 min) Epoch 17/300 -- Iteration 15804 - Batch 396/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.96 min) Epoch 17/300 -- Iteration 15813 - Batch 405/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.96 min) Epoch 17/300 -- Iteration 15822 - Batch 414/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.97 min) Epoch 17/300 -- Iteration 15831 - Batch 423/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.97 min) Epoch 17/300 -- Iteration 15840 - Batch 432/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.98 min) Epoch 17/300 -- Iteration 15849 - Batch 441/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.98 min) Epoch 17/300 -- Iteration 15858 - Batch 450/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.99 min) Epoch 17/300 -- Iteration 15867 - Batch 459/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.99 min) Epoch 17/300 -- Iteration 15876 - Batch 468/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(7.99 min) Epoch 17/300 -- Iteration 15885 - Batch 477/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.00 min) Epoch 17/300 -- Iteration 15894 - Batch 486/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.00 min) Epoch 17/300 -- Iteration 15903 - Batch 495/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.01 min) Epoch 17/300 -- Iteration 15912 - Batch 504/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.01 min) Epoch 17/300 -- Iteration 15921 - Batch 513/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.02 min) Epoch 17/300 -- Iteration 15930 - Batch 522/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.02 min) Epoch 17/300 -- Iteration 15939 - Batch 531/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.02 min) Epoch 17/300 -- Iteration 15948 - Batch 540/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.03 min) Epoch 17/300 -- Iteration 15957 - Batch 549/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.03 min) Epoch 17/300 -- Iteration 15966 - Batch 558/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.04 min) Epoch 17/300 -- Iteration 15975 - Batch 567/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.04 min) Epoch 17/300 -- Iteration 15984 - Batch 576/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.05 min) Epoch 17/300 -- Iteration 15993 - Batch 585/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.05 min) Epoch 17/300 -- Iteration 16002 - Batch 594/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.05 min) Epoch 17/300 -- Iteration 16011 - Batch 603/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.06 min) Epoch 17/300 -- Iteration 16020 - Batch 612/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.06 min) Epoch 17/300 -- Iteration 16029 - Batch 621/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.07 min) Epoch 17/300 -- Iteration 16038 - Batch 630/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.07 min) Epoch 17/300 -- Iteration 16047 - Batch 639/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.08 min) Epoch 17/300 -- Iteration 16056 - Batch 648/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.08 min) Epoch 17/300 -- Iteration 16065 - Batch 657/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.09 min) Epoch 17/300 -- Iteration 16074 - Batch 666/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.09 min) Epoch 17/300 -- Iteration 16083 - Batch 675/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.09 min) Epoch 17/300 -- Iteration 16092 - Batch 684/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.10 min) Epoch 17/300 -- Iteration 16101 - Batch 693/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.10 min) Epoch 17/300 -- Iteration 16110 - Batch 702/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.11 min) Epoch 17/300 -- Iteration 16119 - Batch 711/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.11 min) Epoch 17/300 -- Iteration 16128 - Batch 720/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.11 min) Epoch 17/300 -- Iteration 16137 - Batch 729/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.12 min) Epoch 17/300 -- Iteration 16146 - Batch 738/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.12 min) Epoch 17/300 -- Iteration 16155 - Batch 747/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.13 min) Epoch 17/300 -- Iteration 16164 - Batch 756/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.13 min) Epoch 17/300 -- Iteration 16173 - Batch 765/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.13 min) Epoch 17/300 -- Iteration 16182 - Batch 774/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.14 min) Epoch 17/300 -- Iteration 16191 - Batch 783/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.14 min) Epoch 17/300 -- Iteration 16200 - Batch 792/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.15 min) Epoch 17/300 -- Iteration 16209 - Batch 801/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.15 min) Epoch 17/300 -- Iteration 16218 - Batch 810/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.16 min) Epoch 17/300 -- Iteration 16227 - Batch 819/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.16 min) Epoch 17/300 -- Iteration 16236 - Batch 828/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.17 min) Epoch 17/300 -- Iteration 16245 - Batch 837/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.17 min) Epoch 17/300 -- Iteration 16254 - Batch 846/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.17 min) Epoch 17/300 -- Iteration 16263 - Batch 855/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.18 min) Epoch 17/300 -- Iteration 16272 - Batch 864/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.18 min) Epoch 17/300 -- Iteration 16281 - Batch 873/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.19 min) Epoch 17/300 -- Iteration 16290 - Batch 882/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.19 min) Epoch 17/300 -- Iteration 16299 - Batch 891/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.19 min) Epoch 17/300 -- Iteration 16308 - Batch 900/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.20 min) Epoch 17/300 -- Iteration 16317 - Batch 909/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.20 min) Epoch 17/300 -- Iteration 16326 - Batch 918/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.21 min) Epoch 17/300 -- Iteration 16335 - Batch 927/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.21 min) Epoch 17/300 -- Iteration 16344 - Batch 936/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.22 min) Epoch 17/300 -- Iteration 16353 - Batch 945/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.22 min) Epoch 17/300 -- Iteration 16362 - Batch 954/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(8.22 min) Epoch 17/300 -- Iteration 16371 - Batch 962/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031 - Val acc: -0.0000\n",
      "(8.23 min) Epoch 18/300 -- Iteration 16380 - Batch 9/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.23 min) Epoch 18/300 -- Iteration 16389 - Batch 18/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.24 min) Epoch 18/300 -- Iteration 16398 - Batch 27/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.24 min) Epoch 18/300 -- Iteration 16407 - Batch 36/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.24 min) Epoch 18/300 -- Iteration 16416 - Batch 45/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.25 min) Epoch 18/300 -- Iteration 16425 - Batch 54/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.25 min) Epoch 18/300 -- Iteration 16434 - Batch 63/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.26 min) Epoch 18/300 -- Iteration 16443 - Batch 72/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.26 min) Epoch 18/300 -- Iteration 16452 - Batch 81/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.27 min) Epoch 18/300 -- Iteration 16461 - Batch 90/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.27 min) Epoch 18/300 -- Iteration 16470 - Batch 99/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.27 min) Epoch 18/300 -- Iteration 16479 - Batch 108/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.28 min) Epoch 18/300 -- Iteration 16488 - Batch 117/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.28 min) Epoch 18/300 -- Iteration 16497 - Batch 126/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.29 min) Epoch 18/300 -- Iteration 16506 - Batch 135/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.29 min) Epoch 18/300 -- Iteration 16515 - Batch 144/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.30 min) Epoch 18/300 -- Iteration 16524 - Batch 153/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.30 min) Epoch 18/300 -- Iteration 16533 - Batch 162/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.31 min) Epoch 18/300 -- Iteration 16542 - Batch 171/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.31 min) Epoch 18/300 -- Iteration 16551 - Batch 180/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.31 min) Epoch 18/300 -- Iteration 16560 - Batch 189/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.32 min) Epoch 18/300 -- Iteration 16569 - Batch 198/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.32 min) Epoch 18/300 -- Iteration 16578 - Batch 207/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.33 min) Epoch 18/300 -- Iteration 16587 - Batch 216/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.33 min) Epoch 18/300 -- Iteration 16596 - Batch 225/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.34 min) Epoch 18/300 -- Iteration 16605 - Batch 234/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.34 min) Epoch 18/300 -- Iteration 16614 - Batch 243/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.35 min) Epoch 18/300 -- Iteration 16623 - Batch 252/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.35 min) Epoch 18/300 -- Iteration 16632 - Batch 261/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.35 min) Epoch 18/300 -- Iteration 16641 - Batch 270/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.36 min) Epoch 18/300 -- Iteration 16650 - Batch 279/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.36 min) Epoch 18/300 -- Iteration 16659 - Batch 288/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.37 min) Epoch 18/300 -- Iteration 16668 - Batch 297/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.37 min) Epoch 18/300 -- Iteration 16677 - Batch 306/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.37 min) Epoch 18/300 -- Iteration 16686 - Batch 315/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.38 min) Epoch 18/300 -- Iteration 16695 - Batch 324/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.38 min) Epoch 18/300 -- Iteration 16704 - Batch 333/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.39 min) Epoch 18/300 -- Iteration 16713 - Batch 342/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.39 min) Epoch 18/300 -- Iteration 16722 - Batch 351/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.39 min) Epoch 18/300 -- Iteration 16731 - Batch 360/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.40 min) Epoch 18/300 -- Iteration 16740 - Batch 369/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.40 min) Epoch 18/300 -- Iteration 16749 - Batch 378/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.41 min) Epoch 18/300 -- Iteration 16758 - Batch 387/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.41 min) Epoch 18/300 -- Iteration 16767 - Batch 396/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.42 min) Epoch 18/300 -- Iteration 16776 - Batch 405/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.42 min) Epoch 18/300 -- Iteration 16785 - Batch 414/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.42 min) Epoch 18/300 -- Iteration 16794 - Batch 423/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.43 min) Epoch 18/300 -- Iteration 16803 - Batch 432/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.43 min) Epoch 18/300 -- Iteration 16812 - Batch 441/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.44 min) Epoch 18/300 -- Iteration 16821 - Batch 450/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.44 min) Epoch 18/300 -- Iteration 16830 - Batch 459/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.44 min) Epoch 18/300 -- Iteration 16839 - Batch 468/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.45 min) Epoch 18/300 -- Iteration 16848 - Batch 477/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.45 min) Epoch 18/300 -- Iteration 16857 - Batch 486/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.46 min) Epoch 18/300 -- Iteration 16866 - Batch 495/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.46 min) Epoch 18/300 -- Iteration 16875 - Batch 504/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.46 min) Epoch 18/300 -- Iteration 16884 - Batch 513/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.47 min) Epoch 18/300 -- Iteration 16893 - Batch 522/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.47 min) Epoch 18/300 -- Iteration 16902 - Batch 531/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.48 min) Epoch 18/300 -- Iteration 16911 - Batch 540/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.48 min) Epoch 18/300 -- Iteration 16920 - Batch 549/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.49 min) Epoch 18/300 -- Iteration 16929 - Batch 558/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.49 min) Epoch 18/300 -- Iteration 16938 - Batch 567/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.49 min) Epoch 18/300 -- Iteration 16947 - Batch 576/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.50 min) Epoch 18/300 -- Iteration 16956 - Batch 585/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.50 min) Epoch 18/300 -- Iteration 16965 - Batch 594/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.51 min) Epoch 18/300 -- Iteration 16974 - Batch 603/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.51 min) Epoch 18/300 -- Iteration 16983 - Batch 612/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.51 min) Epoch 18/300 -- Iteration 16992 - Batch 621/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.52 min) Epoch 18/300 -- Iteration 17001 - Batch 630/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.52 min) Epoch 18/300 -- Iteration 17010 - Batch 639/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.53 min) Epoch 18/300 -- Iteration 17019 - Batch 648/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.53 min) Epoch 18/300 -- Iteration 17028 - Batch 657/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.53 min) Epoch 18/300 -- Iteration 17037 - Batch 666/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.54 min) Epoch 18/300 -- Iteration 17046 - Batch 675/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.54 min) Epoch 18/300 -- Iteration 17055 - Batch 684/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.55 min) Epoch 18/300 -- Iteration 17064 - Batch 693/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.55 min) Epoch 18/300 -- Iteration 17073 - Batch 702/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.56 min) Epoch 18/300 -- Iteration 17082 - Batch 711/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.56 min) Epoch 18/300 -- Iteration 17091 - Batch 720/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.56 min) Epoch 18/300 -- Iteration 17100 - Batch 729/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.57 min) Epoch 18/300 -- Iteration 17109 - Batch 738/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.57 min) Epoch 18/300 -- Iteration 17118 - Batch 747/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.58 min) Epoch 18/300 -- Iteration 17127 - Batch 756/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.58 min) Epoch 18/300 -- Iteration 17136 - Batch 765/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.58 min) Epoch 18/300 -- Iteration 17145 - Batch 774/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.59 min) Epoch 18/300 -- Iteration 17154 - Batch 783/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.59 min) Epoch 18/300 -- Iteration 17163 - Batch 792/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.60 min) Epoch 18/300 -- Iteration 17172 - Batch 801/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.60 min) Epoch 18/300 -- Iteration 17181 - Batch 810/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.60 min) Epoch 18/300 -- Iteration 17190 - Batch 819/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.61 min) Epoch 18/300 -- Iteration 17199 - Batch 828/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.61 min) Epoch 18/300 -- Iteration 17208 - Batch 837/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.62 min) Epoch 18/300 -- Iteration 17217 - Batch 846/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.62 min) Epoch 18/300 -- Iteration 17226 - Batch 855/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.63 min) Epoch 18/300 -- Iteration 17235 - Batch 864/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.63 min) Epoch 18/300 -- Iteration 17244 - Batch 873/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.63 min) Epoch 18/300 -- Iteration 17253 - Batch 882/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.64 min) Epoch 18/300 -- Iteration 17262 - Batch 891/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.64 min) Epoch 18/300 -- Iteration 17271 - Batch 900/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.65 min) Epoch 18/300 -- Iteration 17280 - Batch 909/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.65 min) Epoch 18/300 -- Iteration 17289 - Batch 918/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.65 min) Epoch 18/300 -- Iteration 17298 - Batch 927/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.66 min) Epoch 18/300 -- Iteration 17307 - Batch 936/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.66 min) Epoch 18/300 -- Iteration 17316 - Batch 945/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.67 min) Epoch 18/300 -- Iteration 17325 - Batch 954/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.67 min) Epoch 18/300 -- Iteration 17334 - Batch 962/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031 - Val acc: -0.0000\n",
      "(8.68 min) Epoch 19/300 -- Iteration 17343 - Batch 9/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.68 min) Epoch 19/300 -- Iteration 17352 - Batch 18/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.68 min) Epoch 19/300 -- Iteration 17361 - Batch 27/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.69 min) Epoch 19/300 -- Iteration 17370 - Batch 36/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.69 min) Epoch 19/300 -- Iteration 17379 - Batch 45/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.70 min) Epoch 19/300 -- Iteration 17388 - Batch 54/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.70 min) Epoch 19/300 -- Iteration 17397 - Batch 63/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.71 min) Epoch 19/300 -- Iteration 17406 - Batch 72/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.71 min) Epoch 19/300 -- Iteration 17415 - Batch 81/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.72 min) Epoch 19/300 -- Iteration 17424 - Batch 90/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.72 min) Epoch 19/300 -- Iteration 17433 - Batch 99/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.72 min) Epoch 19/300 -- Iteration 17442 - Batch 108/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.73 min) Epoch 19/300 -- Iteration 17451 - Batch 117/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.73 min) Epoch 19/300 -- Iteration 17460 - Batch 126/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.74 min) Epoch 19/300 -- Iteration 17469 - Batch 135/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.74 min) Epoch 19/300 -- Iteration 17478 - Batch 144/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.75 min) Epoch 19/300 -- Iteration 17487 - Batch 153/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.75 min) Epoch 19/300 -- Iteration 17496 - Batch 162/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.76 min) Epoch 19/300 -- Iteration 17505 - Batch 171/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.76 min) Epoch 19/300 -- Iteration 17514 - Batch 180/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.77 min) Epoch 19/300 -- Iteration 17523 - Batch 189/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.77 min) Epoch 19/300 -- Iteration 17532 - Batch 198/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.78 min) Epoch 19/300 -- Iteration 17541 - Batch 207/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.78 min) Epoch 19/300 -- Iteration 17550 - Batch 216/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.79 min) Epoch 19/300 -- Iteration 17559 - Batch 225/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.79 min) Epoch 19/300 -- Iteration 17568 - Batch 234/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.79 min) Epoch 19/300 -- Iteration 17577 - Batch 243/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.80 min) Epoch 19/300 -- Iteration 17586 - Batch 252/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.80 min) Epoch 19/300 -- Iteration 17595 - Batch 261/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.81 min) Epoch 19/300 -- Iteration 17604 - Batch 270/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.81 min) Epoch 19/300 -- Iteration 17613 - Batch 279/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.82 min) Epoch 19/300 -- Iteration 17622 - Batch 288/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.82 min) Epoch 19/300 -- Iteration 17631 - Batch 297/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.83 min) Epoch 19/300 -- Iteration 17640 - Batch 306/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.83 min) Epoch 19/300 -- Iteration 17649 - Batch 315/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.84 min) Epoch 19/300 -- Iteration 17658 - Batch 324/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.84 min) Epoch 19/300 -- Iteration 17667 - Batch 333/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.85 min) Epoch 19/300 -- Iteration 17676 - Batch 342/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.85 min) Epoch 19/300 -- Iteration 17685 - Batch 351/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.86 min) Epoch 19/300 -- Iteration 17694 - Batch 360/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.86 min) Epoch 19/300 -- Iteration 17703 - Batch 369/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.86 min) Epoch 19/300 -- Iteration 17712 - Batch 378/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.87 min) Epoch 19/300 -- Iteration 17721 - Batch 387/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.87 min) Epoch 19/300 -- Iteration 17730 - Batch 396/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.88 min) Epoch 19/300 -- Iteration 17739 - Batch 405/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.88 min) Epoch 19/300 -- Iteration 17748 - Batch 414/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.89 min) Epoch 19/300 -- Iteration 17757 - Batch 423/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.89 min) Epoch 19/300 -- Iteration 17766 - Batch 432/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.90 min) Epoch 19/300 -- Iteration 17775 - Batch 441/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.90 min) Epoch 19/300 -- Iteration 17784 - Batch 450/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.91 min) Epoch 19/300 -- Iteration 17793 - Batch 459/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.91 min) Epoch 19/300 -- Iteration 17802 - Batch 468/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.91 min) Epoch 19/300 -- Iteration 17811 - Batch 477/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.92 min) Epoch 19/300 -- Iteration 17820 - Batch 486/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.92 min) Epoch 19/300 -- Iteration 17829 - Batch 495/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.93 min) Epoch 19/300 -- Iteration 17838 - Batch 504/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.93 min) Epoch 19/300 -- Iteration 17847 - Batch 513/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.94 min) Epoch 19/300 -- Iteration 17856 - Batch 522/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.94 min) Epoch 19/300 -- Iteration 17865 - Batch 531/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.95 min) Epoch 19/300 -- Iteration 17874 - Batch 540/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.95 min) Epoch 19/300 -- Iteration 17883 - Batch 549/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.96 min) Epoch 19/300 -- Iteration 17892 - Batch 558/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.96 min) Epoch 19/300 -- Iteration 17901 - Batch 567/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.96 min) Epoch 19/300 -- Iteration 17910 - Batch 576/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.97 min) Epoch 19/300 -- Iteration 17919 - Batch 585/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.97 min) Epoch 19/300 -- Iteration 17928 - Batch 594/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.98 min) Epoch 19/300 -- Iteration 17937 - Batch 603/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.98 min) Epoch 19/300 -- Iteration 17946 - Batch 612/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.99 min) Epoch 19/300 -- Iteration 17955 - Batch 621/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(8.99 min) Epoch 19/300 -- Iteration 17964 - Batch 630/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.00 min) Epoch 19/300 -- Iteration 17973 - Batch 639/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.00 min) Epoch 19/300 -- Iteration 17982 - Batch 648/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.00 min) Epoch 19/300 -- Iteration 17991 - Batch 657/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.01 min) Epoch 19/300 -- Iteration 18000 - Batch 666/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.01 min) Epoch 19/300 -- Iteration 18009 - Batch 675/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.02 min) Epoch 19/300 -- Iteration 18018 - Batch 684/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.02 min) Epoch 19/300 -- Iteration 18027 - Batch 693/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.03 min) Epoch 19/300 -- Iteration 18036 - Batch 702/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.03 min) Epoch 19/300 -- Iteration 18045 - Batch 711/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.04 min) Epoch 19/300 -- Iteration 18054 - Batch 720/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.04 min) Epoch 19/300 -- Iteration 18063 - Batch 729/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.04 min) Epoch 19/300 -- Iteration 18072 - Batch 738/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.05 min) Epoch 19/300 -- Iteration 18081 - Batch 747/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.05 min) Epoch 19/300 -- Iteration 18090 - Batch 756/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.06 min) Epoch 19/300 -- Iteration 18099 - Batch 765/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.06 min) Epoch 19/300 -- Iteration 18108 - Batch 774/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.07 min) Epoch 19/300 -- Iteration 18117 - Batch 783/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.07 min) Epoch 19/300 -- Iteration 18126 - Batch 792/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.08 min) Epoch 19/300 -- Iteration 18135 - Batch 801/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.08 min) Epoch 19/300 -- Iteration 18144 - Batch 810/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.09 min) Epoch 19/300 -- Iteration 18153 - Batch 819/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.09 min) Epoch 19/300 -- Iteration 18162 - Batch 828/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.09 min) Epoch 19/300 -- Iteration 18171 - Batch 837/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.10 min) Epoch 19/300 -- Iteration 18180 - Batch 846/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.10 min) Epoch 19/300 -- Iteration 18189 - Batch 855/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.11 min) Epoch 19/300 -- Iteration 18198 - Batch 864/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.11 min) Epoch 19/300 -- Iteration 18207 - Batch 873/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.12 min) Epoch 19/300 -- Iteration 18216 - Batch 882/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.12 min) Epoch 19/300 -- Iteration 18225 - Batch 891/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.13 min) Epoch 19/300 -- Iteration 18234 - Batch 900/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.13 min) Epoch 19/300 -- Iteration 18243 - Batch 909/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.13 min) Epoch 19/300 -- Iteration 18252 - Batch 918/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.14 min) Epoch 19/300 -- Iteration 18261 - Batch 927/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.14 min) Epoch 19/300 -- Iteration 18270 - Batch 936/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.15 min) Epoch 19/300 -- Iteration 18279 - Batch 945/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.15 min) Epoch 19/300 -- Iteration 18288 - Batch 954/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(9.16 min) Epoch 19/300 -- Iteration 18297 - Batch 962/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029 - Val acc: -0.0000\n",
      "(9.16 min) Epoch 20/300 -- Iteration 18306 - Batch 9/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.17 min) Epoch 20/300 -- Iteration 18315 - Batch 18/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.17 min) Epoch 20/300 -- Iteration 18324 - Batch 27/963 - Train loss: 0.0030  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.18 min) Epoch 20/300 -- Iteration 18333 - Batch 36/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.18 min) Epoch 20/300 -- Iteration 18342 - Batch 45/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.18 min) Epoch 20/300 -- Iteration 18351 - Batch 54/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.19 min) Epoch 20/300 -- Iteration 18360 - Batch 63/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.19 min) Epoch 20/300 -- Iteration 18369 - Batch 72/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.20 min) Epoch 20/300 -- Iteration 18378 - Batch 81/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.20 min) Epoch 20/300 -- Iteration 18387 - Batch 90/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.21 min) Epoch 20/300 -- Iteration 18396 - Batch 99/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.21 min) Epoch 20/300 -- Iteration 18405 - Batch 108/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.22 min) Epoch 20/300 -- Iteration 18414 - Batch 117/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.22 min) Epoch 20/300 -- Iteration 18423 - Batch 126/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.22 min) Epoch 20/300 -- Iteration 18432 - Batch 135/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.23 min) Epoch 20/300 -- Iteration 18441 - Batch 144/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.23 min) Epoch 20/300 -- Iteration 18450 - Batch 153/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.24 min) Epoch 20/300 -- Iteration 18459 - Batch 162/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.24 min) Epoch 20/300 -- Iteration 18468 - Batch 171/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.25 min) Epoch 20/300 -- Iteration 18477 - Batch 180/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.25 min) Epoch 20/300 -- Iteration 18486 - Batch 189/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.26 min) Epoch 20/300 -- Iteration 18495 - Batch 198/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.26 min) Epoch 20/300 -- Iteration 18504 - Batch 207/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.26 min) Epoch 20/300 -- Iteration 18513 - Batch 216/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.27 min) Epoch 20/300 -- Iteration 18522 - Batch 225/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.27 min) Epoch 20/300 -- Iteration 18531 - Batch 234/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.28 min) Epoch 20/300 -- Iteration 18540 - Batch 243/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.28 min) Epoch 20/300 -- Iteration 18549 - Batch 252/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.29 min) Epoch 20/300 -- Iteration 18558 - Batch 261/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.29 min) Epoch 20/300 -- Iteration 18567 - Batch 270/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.30 min) Epoch 20/300 -- Iteration 18576 - Batch 279/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.30 min) Epoch 20/300 -- Iteration 18585 - Batch 288/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.30 min) Epoch 20/300 -- Iteration 18594 - Batch 297/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.31 min) Epoch 20/300 -- Iteration 18603 - Batch 306/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.31 min) Epoch 20/300 -- Iteration 18612 - Batch 315/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.32 min) Epoch 20/300 -- Iteration 18621 - Batch 324/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.32 min) Epoch 20/300 -- Iteration 18630 - Batch 333/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.33 min) Epoch 20/300 -- Iteration 18639 - Batch 342/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.33 min) Epoch 20/300 -- Iteration 18648 - Batch 351/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.34 min) Epoch 20/300 -- Iteration 18657 - Batch 360/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.34 min) Epoch 20/300 -- Iteration 18666 - Batch 369/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.34 min) Epoch 20/300 -- Iteration 18675 - Batch 378/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.35 min) Epoch 20/300 -- Iteration 18684 - Batch 387/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.35 min) Epoch 20/300 -- Iteration 18693 - Batch 396/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.36 min) Epoch 20/300 -- Iteration 18702 - Batch 405/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.36 min) Epoch 20/300 -- Iteration 18711 - Batch 414/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.37 min) Epoch 20/300 -- Iteration 18720 - Batch 423/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.37 min) Epoch 20/300 -- Iteration 18729 - Batch 432/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.38 min) Epoch 20/300 -- Iteration 18738 - Batch 441/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.38 min) Epoch 20/300 -- Iteration 18747 - Batch 450/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.38 min) Epoch 20/300 -- Iteration 18756 - Batch 459/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.39 min) Epoch 20/300 -- Iteration 18765 - Batch 468/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.39 min) Epoch 20/300 -- Iteration 18774 - Batch 477/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.40 min) Epoch 20/300 -- Iteration 18783 - Batch 486/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.40 min) Epoch 20/300 -- Iteration 18792 - Batch 495/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.41 min) Epoch 20/300 -- Iteration 18801 - Batch 504/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.41 min) Epoch 20/300 -- Iteration 18810 - Batch 513/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.42 min) Epoch 20/300 -- Iteration 18819 - Batch 522/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.42 min) Epoch 20/300 -- Iteration 18828 - Batch 531/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.42 min) Epoch 20/300 -- Iteration 18837 - Batch 540/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.43 min) Epoch 20/300 -- Iteration 18846 - Batch 549/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.43 min) Epoch 20/300 -- Iteration 18855 - Batch 558/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.44 min) Epoch 20/300 -- Iteration 18864 - Batch 567/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.44 min) Epoch 20/300 -- Iteration 18873 - Batch 576/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.45 min) Epoch 20/300 -- Iteration 18882 - Batch 585/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.45 min) Epoch 20/300 -- Iteration 18891 - Batch 594/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.46 min) Epoch 20/300 -- Iteration 18900 - Batch 603/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.46 min) Epoch 20/300 -- Iteration 18909 - Batch 612/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.47 min) Epoch 20/300 -- Iteration 18918 - Batch 621/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.47 min) Epoch 20/300 -- Iteration 18927 - Batch 630/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.47 min) Epoch 20/300 -- Iteration 18936 - Batch 639/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.48 min) Epoch 20/300 -- Iteration 18945 - Batch 648/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.48 min) Epoch 20/300 -- Iteration 18954 - Batch 657/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.49 min) Epoch 20/300 -- Iteration 18963 - Batch 666/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.49 min) Epoch 20/300 -- Iteration 18972 - Batch 675/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.50 min) Epoch 20/300 -- Iteration 18981 - Batch 684/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.50 min) Epoch 20/300 -- Iteration 18990 - Batch 693/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.51 min) Epoch 20/300 -- Iteration 18999 - Batch 702/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.51 min) Epoch 20/300 -- Iteration 19008 - Batch 711/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.51 min) Epoch 20/300 -- Iteration 19017 - Batch 720/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.52 min) Epoch 20/300 -- Iteration 19026 - Batch 729/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.52 min) Epoch 20/300 -- Iteration 19035 - Batch 738/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.53 min) Epoch 20/300 -- Iteration 19044 - Batch 747/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.53 min) Epoch 20/300 -- Iteration 19053 - Batch 756/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.54 min) Epoch 20/300 -- Iteration 19062 - Batch 765/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.54 min) Epoch 20/300 -- Iteration 19071 - Batch 774/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.55 min) Epoch 20/300 -- Iteration 19080 - Batch 783/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.55 min) Epoch 20/300 -- Iteration 19089 - Batch 792/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.55 min) Epoch 20/300 -- Iteration 19098 - Batch 801/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.56 min) Epoch 20/300 -- Iteration 19107 - Batch 810/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.56 min) Epoch 20/300 -- Iteration 19116 - Batch 819/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.57 min) Epoch 20/300 -- Iteration 19125 - Batch 828/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.57 min) Epoch 20/300 -- Iteration 19134 - Batch 837/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.58 min) Epoch 20/300 -- Iteration 19143 - Batch 846/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.58 min) Epoch 20/300 -- Iteration 19152 - Batch 855/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.59 min) Epoch 20/300 -- Iteration 19161 - Batch 864/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.59 min) Epoch 20/300 -- Iteration 19170 - Batch 873/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.60 min) Epoch 20/300 -- Iteration 19179 - Batch 882/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.60 min) Epoch 20/300 -- Iteration 19188 - Batch 891/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.60 min) Epoch 20/300 -- Iteration 19197 - Batch 900/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.61 min) Epoch 20/300 -- Iteration 19206 - Batch 909/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.61 min) Epoch 20/300 -- Iteration 19215 - Batch 918/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.62 min) Epoch 20/300 -- Iteration 19224 - Batch 927/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.62 min) Epoch 20/300 -- Iteration 19233 - Batch 936/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.63 min) Epoch 20/300 -- Iteration 19242 - Batch 945/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.63 min) Epoch 20/300 -- Iteration 19251 - Batch 954/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(9.64 min) Epoch 20/300 -- Iteration 19260 - Batch 962/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032 - Val acc: -0.0000\n",
      "(9.64 min) Epoch 21/300 -- Iteration 19269 - Batch 9/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.65 min) Epoch 21/300 -- Iteration 19278 - Batch 18/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.65 min) Epoch 21/300 -- Iteration 19287 - Batch 27/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.66 min) Epoch 21/300 -- Iteration 19296 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.66 min) Epoch 21/300 -- Iteration 19305 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.67 min) Epoch 21/300 -- Iteration 19314 - Batch 54/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.67 min) Epoch 21/300 -- Iteration 19323 - Batch 63/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.68 min) Epoch 21/300 -- Iteration 19332 - Batch 72/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.68 min) Epoch 21/300 -- Iteration 19341 - Batch 81/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.69 min) Epoch 21/300 -- Iteration 19350 - Batch 90/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.69 min) Epoch 21/300 -- Iteration 19359 - Batch 99/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.69 min) Epoch 21/300 -- Iteration 19368 - Batch 108/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.70 min) Epoch 21/300 -- Iteration 19377 - Batch 117/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.70 min) Epoch 21/300 -- Iteration 19386 - Batch 126/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.71 min) Epoch 21/300 -- Iteration 19395 - Batch 135/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.71 min) Epoch 21/300 -- Iteration 19404 - Batch 144/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.72 min) Epoch 21/300 -- Iteration 19413 - Batch 153/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.72 min) Epoch 21/300 -- Iteration 19422 - Batch 162/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.73 min) Epoch 21/300 -- Iteration 19431 - Batch 171/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.73 min) Epoch 21/300 -- Iteration 19440 - Batch 180/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.73 min) Epoch 21/300 -- Iteration 19449 - Batch 189/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.74 min) Epoch 21/300 -- Iteration 19458 - Batch 198/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.74 min) Epoch 21/300 -- Iteration 19467 - Batch 207/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.75 min) Epoch 21/300 -- Iteration 19476 - Batch 216/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.75 min) Epoch 21/300 -- Iteration 19485 - Batch 225/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.76 min) Epoch 21/300 -- Iteration 19494 - Batch 234/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.76 min) Epoch 21/300 -- Iteration 19503 - Batch 243/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.77 min) Epoch 21/300 -- Iteration 19512 - Batch 252/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.77 min) Epoch 21/300 -- Iteration 19521 - Batch 261/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.78 min) Epoch 21/300 -- Iteration 19530 - Batch 270/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.78 min) Epoch 21/300 -- Iteration 19539 - Batch 279/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.78 min) Epoch 21/300 -- Iteration 19548 - Batch 288/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.79 min) Epoch 21/300 -- Iteration 19557 - Batch 297/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.79 min) Epoch 21/300 -- Iteration 19566 - Batch 306/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.80 min) Epoch 21/300 -- Iteration 19575 - Batch 315/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.80 min) Epoch 21/300 -- Iteration 19584 - Batch 324/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.81 min) Epoch 21/300 -- Iteration 19593 - Batch 333/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.81 min) Epoch 21/300 -- Iteration 19602 - Batch 342/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.82 min) Epoch 21/300 -- Iteration 19611 - Batch 351/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.82 min) Epoch 21/300 -- Iteration 19620 - Batch 360/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.83 min) Epoch 21/300 -- Iteration 19629 - Batch 369/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.83 min) Epoch 21/300 -- Iteration 19638 - Batch 378/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.83 min) Epoch 21/300 -- Iteration 19647 - Batch 387/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.84 min) Epoch 21/300 -- Iteration 19656 - Batch 396/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.84 min) Epoch 21/300 -- Iteration 19665 - Batch 405/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.85 min) Epoch 21/300 -- Iteration 19674 - Batch 414/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.85 min) Epoch 21/300 -- Iteration 19683 - Batch 423/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.86 min) Epoch 21/300 -- Iteration 19692 - Batch 432/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.86 min) Epoch 21/300 -- Iteration 19701 - Batch 441/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.87 min) Epoch 21/300 -- Iteration 19710 - Batch 450/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.87 min) Epoch 21/300 -- Iteration 19719 - Batch 459/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.87 min) Epoch 21/300 -- Iteration 19728 - Batch 468/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.88 min) Epoch 21/300 -- Iteration 19737 - Batch 477/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.88 min) Epoch 21/300 -- Iteration 19746 - Batch 486/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.89 min) Epoch 21/300 -- Iteration 19755 - Batch 495/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.89 min) Epoch 21/300 -- Iteration 19764 - Batch 504/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.90 min) Epoch 21/300 -- Iteration 19773 - Batch 513/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.90 min) Epoch 21/300 -- Iteration 19782 - Batch 522/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.91 min) Epoch 21/300 -- Iteration 19791 - Batch 531/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.91 min) Epoch 21/300 -- Iteration 19800 - Batch 540/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.92 min) Epoch 21/300 -- Iteration 19809 - Batch 549/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.92 min) Epoch 21/300 -- Iteration 19818 - Batch 558/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.92 min) Epoch 21/300 -- Iteration 19827 - Batch 567/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.93 min) Epoch 21/300 -- Iteration 19836 - Batch 576/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.93 min) Epoch 21/300 -- Iteration 19845 - Batch 585/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.94 min) Epoch 21/300 -- Iteration 19854 - Batch 594/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.94 min) Epoch 21/300 -- Iteration 19863 - Batch 603/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.95 min) Epoch 21/300 -- Iteration 19872 - Batch 612/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.95 min) Epoch 21/300 -- Iteration 19881 - Batch 621/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.96 min) Epoch 21/300 -- Iteration 19890 - Batch 630/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.96 min) Epoch 21/300 -- Iteration 19899 - Batch 639/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.97 min) Epoch 21/300 -- Iteration 19908 - Batch 648/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.97 min) Epoch 21/300 -- Iteration 19917 - Batch 657/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.98 min) Epoch 21/300 -- Iteration 19926 - Batch 666/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.98 min) Epoch 21/300 -- Iteration 19935 - Batch 675/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.99 min) Epoch 21/300 -- Iteration 19944 - Batch 684/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.99 min) Epoch 21/300 -- Iteration 19953 - Batch 693/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(9.99 min) Epoch 21/300 -- Iteration 19962 - Batch 702/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.00 min) Epoch 21/300 -- Iteration 19971 - Batch 711/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.00 min) Epoch 21/300 -- Iteration 19980 - Batch 720/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.01 min) Epoch 21/300 -- Iteration 19989 - Batch 729/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.01 min) Epoch 21/300 -- Iteration 19998 - Batch 738/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.02 min) Epoch 21/300 -- Iteration 20007 - Batch 747/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.02 min) Epoch 21/300 -- Iteration 20016 - Batch 756/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.03 min) Epoch 21/300 -- Iteration 20025 - Batch 765/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.03 min) Epoch 21/300 -- Iteration 20034 - Batch 774/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.04 min) Epoch 21/300 -- Iteration 20043 - Batch 783/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.04 min) Epoch 21/300 -- Iteration 20052 - Batch 792/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.05 min) Epoch 21/300 -- Iteration 20061 - Batch 801/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.05 min) Epoch 21/300 -- Iteration 20070 - Batch 810/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.05 min) Epoch 21/300 -- Iteration 20079 - Batch 819/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.06 min) Epoch 21/300 -- Iteration 20088 - Batch 828/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.06 min) Epoch 21/300 -- Iteration 20097 - Batch 837/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.07 min) Epoch 21/300 -- Iteration 20106 - Batch 846/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.07 min) Epoch 21/300 -- Iteration 20115 - Batch 855/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.08 min) Epoch 21/300 -- Iteration 20124 - Batch 864/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.08 min) Epoch 21/300 -- Iteration 20133 - Batch 873/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.09 min) Epoch 21/300 -- Iteration 20142 - Batch 882/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.09 min) Epoch 21/300 -- Iteration 20151 - Batch 891/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.10 min) Epoch 21/300 -- Iteration 20160 - Batch 900/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.10 min) Epoch 21/300 -- Iteration 20169 - Batch 909/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.11 min) Epoch 21/300 -- Iteration 20178 - Batch 918/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.11 min) Epoch 21/300 -- Iteration 20187 - Batch 927/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.12 min) Epoch 21/300 -- Iteration 20196 - Batch 936/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.12 min) Epoch 21/300 -- Iteration 20205 - Batch 945/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.13 min) Epoch 21/300 -- Iteration 20214 - Batch 954/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0032\n",
      "(10.13 min) Epoch 21/300 -- Iteration 20223 - Batch 962/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031 - Val acc: -0.0000\n",
      "(10.14 min) Epoch 22/300 -- Iteration 20232 - Batch 9/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.14 min) Epoch 22/300 -- Iteration 20241 - Batch 18/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.15 min) Epoch 22/300 -- Iteration 20250 - Batch 27/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.15 min) Epoch 22/300 -- Iteration 20259 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.16 min) Epoch 22/300 -- Iteration 20268 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.16 min) Epoch 22/300 -- Iteration 20277 - Batch 54/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.17 min) Epoch 22/300 -- Iteration 20286 - Batch 63/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.17 min) Epoch 22/300 -- Iteration 20295 - Batch 72/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.17 min) Epoch 22/300 -- Iteration 20304 - Batch 81/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.18 min) Epoch 22/300 -- Iteration 20313 - Batch 90/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.18 min) Epoch 22/300 -- Iteration 20322 - Batch 99/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.19 min) Epoch 22/300 -- Iteration 20331 - Batch 108/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.19 min) Epoch 22/300 -- Iteration 20340 - Batch 117/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.20 min) Epoch 22/300 -- Iteration 20349 - Batch 126/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.20 min) Epoch 22/300 -- Iteration 20358 - Batch 135/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.21 min) Epoch 22/300 -- Iteration 20367 - Batch 144/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.21 min) Epoch 22/300 -- Iteration 20376 - Batch 153/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.22 min) Epoch 22/300 -- Iteration 20385 - Batch 162/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.22 min) Epoch 22/300 -- Iteration 20394 - Batch 171/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.23 min) Epoch 22/300 -- Iteration 20403 - Batch 180/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.23 min) Epoch 22/300 -- Iteration 20412 - Batch 189/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.23 min) Epoch 22/300 -- Iteration 20421 - Batch 198/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.24 min) Epoch 22/300 -- Iteration 20430 - Batch 207/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.24 min) Epoch 22/300 -- Iteration 20439 - Batch 216/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.25 min) Epoch 22/300 -- Iteration 20448 - Batch 225/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.25 min) Epoch 22/300 -- Iteration 20457 - Batch 234/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.26 min) Epoch 22/300 -- Iteration 20466 - Batch 243/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.26 min) Epoch 22/300 -- Iteration 20475 - Batch 252/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.27 min) Epoch 22/300 -- Iteration 20484 - Batch 261/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.27 min) Epoch 22/300 -- Iteration 20493 - Batch 270/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.28 min) Epoch 22/300 -- Iteration 20502 - Batch 279/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.28 min) Epoch 22/300 -- Iteration 20511 - Batch 288/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.29 min) Epoch 22/300 -- Iteration 20520 - Batch 297/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.29 min) Epoch 22/300 -- Iteration 20529 - Batch 306/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.29 min) Epoch 22/300 -- Iteration 20538 - Batch 315/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.30 min) Epoch 22/300 -- Iteration 20547 - Batch 324/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.30 min) Epoch 22/300 -- Iteration 20556 - Batch 333/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.31 min) Epoch 22/300 -- Iteration 20565 - Batch 342/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.31 min) Epoch 22/300 -- Iteration 20574 - Batch 351/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.32 min) Epoch 22/300 -- Iteration 20583 - Batch 360/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.32 min) Epoch 22/300 -- Iteration 20592 - Batch 369/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.33 min) Epoch 22/300 -- Iteration 20601 - Batch 378/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.33 min) Epoch 22/300 -- Iteration 20610 - Batch 387/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.34 min) Epoch 22/300 -- Iteration 20619 - Batch 396/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.34 min) Epoch 22/300 -- Iteration 20628 - Batch 405/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.35 min) Epoch 22/300 -- Iteration 20637 - Batch 414/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.35 min) Epoch 22/300 -- Iteration 20646 - Batch 423/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.35 min) Epoch 22/300 -- Iteration 20655 - Batch 432/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.36 min) Epoch 22/300 -- Iteration 20664 - Batch 441/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.36 min) Epoch 22/300 -- Iteration 20673 - Batch 450/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.37 min) Epoch 22/300 -- Iteration 20682 - Batch 459/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.37 min) Epoch 22/300 -- Iteration 20691 - Batch 468/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.38 min) Epoch 22/300 -- Iteration 20700 - Batch 477/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.38 min) Epoch 22/300 -- Iteration 20709 - Batch 486/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.39 min) Epoch 22/300 -- Iteration 20718 - Batch 495/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.39 min) Epoch 22/300 -- Iteration 20727 - Batch 504/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.39 min) Epoch 22/300 -- Iteration 20736 - Batch 513/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.40 min) Epoch 22/300 -- Iteration 20745 - Batch 522/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.40 min) Epoch 22/300 -- Iteration 20754 - Batch 531/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.41 min) Epoch 22/300 -- Iteration 20763 - Batch 540/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.41 min) Epoch 22/300 -- Iteration 20772 - Batch 549/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.42 min) Epoch 22/300 -- Iteration 20781 - Batch 558/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.42 min) Epoch 22/300 -- Iteration 20790 - Batch 567/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.43 min) Epoch 22/300 -- Iteration 20799 - Batch 576/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.43 min) Epoch 22/300 -- Iteration 20808 - Batch 585/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.44 min) Epoch 22/300 -- Iteration 20817 - Batch 594/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.44 min) Epoch 22/300 -- Iteration 20826 - Batch 603/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.44 min) Epoch 22/300 -- Iteration 20835 - Batch 612/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.45 min) Epoch 22/300 -- Iteration 20844 - Batch 621/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.45 min) Epoch 22/300 -- Iteration 20853 - Batch 630/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.46 min) Epoch 22/300 -- Iteration 20862 - Batch 639/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.46 min) Epoch 22/300 -- Iteration 20871 - Batch 648/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.47 min) Epoch 22/300 -- Iteration 20880 - Batch 657/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.47 min) Epoch 22/300 -- Iteration 20889 - Batch 666/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.48 min) Epoch 22/300 -- Iteration 20898 - Batch 675/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.48 min) Epoch 22/300 -- Iteration 20907 - Batch 684/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.48 min) Epoch 22/300 -- Iteration 20916 - Batch 693/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.49 min) Epoch 22/300 -- Iteration 20925 - Batch 702/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.49 min) Epoch 22/300 -- Iteration 20934 - Batch 711/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.50 min) Epoch 22/300 -- Iteration 20943 - Batch 720/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.50 min) Epoch 22/300 -- Iteration 20952 - Batch 729/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.51 min) Epoch 22/300 -- Iteration 20961 - Batch 738/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.51 min) Epoch 22/300 -- Iteration 20970 - Batch 747/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.52 min) Epoch 22/300 -- Iteration 20979 - Batch 756/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.52 min) Epoch 22/300 -- Iteration 20988 - Batch 765/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.53 min) Epoch 22/300 -- Iteration 20997 - Batch 774/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.53 min) Epoch 22/300 -- Iteration 21006 - Batch 783/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.53 min) Epoch 22/300 -- Iteration 21015 - Batch 792/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.54 min) Epoch 22/300 -- Iteration 21024 - Batch 801/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.54 min) Epoch 22/300 -- Iteration 21033 - Batch 810/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.55 min) Epoch 22/300 -- Iteration 21042 - Batch 819/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.55 min) Epoch 22/300 -- Iteration 21051 - Batch 828/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.56 min) Epoch 22/300 -- Iteration 21060 - Batch 837/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.56 min) Epoch 22/300 -- Iteration 21069 - Batch 846/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.57 min) Epoch 22/300 -- Iteration 21078 - Batch 855/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.57 min) Epoch 22/300 -- Iteration 21087 - Batch 864/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.58 min) Epoch 22/300 -- Iteration 21096 - Batch 873/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.58 min) Epoch 22/300 -- Iteration 21105 - Batch 882/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.58 min) Epoch 22/300 -- Iteration 21114 - Batch 891/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.59 min) Epoch 22/300 -- Iteration 21123 - Batch 900/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.59 min) Epoch 22/300 -- Iteration 21132 - Batch 909/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.60 min) Epoch 22/300 -- Iteration 21141 - Batch 918/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.60 min) Epoch 22/300 -- Iteration 21150 - Batch 927/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.61 min) Epoch 22/300 -- Iteration 21159 - Batch 936/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.61 min) Epoch 22/300 -- Iteration 21168 - Batch 945/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.62 min) Epoch 22/300 -- Iteration 21177 - Batch 954/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(10.62 min) Epoch 22/300 -- Iteration 21186 - Batch 962/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029 - Val acc: -0.0000\n",
      "(10.63 min) Epoch 23/300 -- Iteration 21195 - Batch 9/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.63 min) Epoch 23/300 -- Iteration 21204 - Batch 18/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.64 min) Epoch 23/300 -- Iteration 21213 - Batch 27/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.64 min) Epoch 23/300 -- Iteration 21222 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.65 min) Epoch 23/300 -- Iteration 21231 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.65 min) Epoch 23/300 -- Iteration 21240 - Batch 54/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.66 min) Epoch 23/300 -- Iteration 21249 - Batch 63/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.66 min) Epoch 23/300 -- Iteration 21258 - Batch 72/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.66 min) Epoch 23/300 -- Iteration 21267 - Batch 81/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.67 min) Epoch 23/300 -- Iteration 21276 - Batch 90/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.67 min) Epoch 23/300 -- Iteration 21285 - Batch 99/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.68 min) Epoch 23/300 -- Iteration 21294 - Batch 108/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.68 min) Epoch 23/300 -- Iteration 21303 - Batch 117/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.69 min) Epoch 23/300 -- Iteration 21312 - Batch 126/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.69 min) Epoch 23/300 -- Iteration 21321 - Batch 135/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.70 min) Epoch 23/300 -- Iteration 21330 - Batch 144/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.70 min) Epoch 23/300 -- Iteration 21339 - Batch 153/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.71 min) Epoch 23/300 -- Iteration 21348 - Batch 162/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.71 min) Epoch 23/300 -- Iteration 21357 - Batch 171/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.71 min) Epoch 23/300 -- Iteration 21366 - Batch 180/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.72 min) Epoch 23/300 -- Iteration 21375 - Batch 189/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.72 min) Epoch 23/300 -- Iteration 21384 - Batch 198/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.73 min) Epoch 23/300 -- Iteration 21393 - Batch 207/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.73 min) Epoch 23/300 -- Iteration 21402 - Batch 216/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.74 min) Epoch 23/300 -- Iteration 21411 - Batch 225/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.74 min) Epoch 23/300 -- Iteration 21420 - Batch 234/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.75 min) Epoch 23/300 -- Iteration 21429 - Batch 243/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.75 min) Epoch 23/300 -- Iteration 21438 - Batch 252/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.76 min) Epoch 23/300 -- Iteration 21447 - Batch 261/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.76 min) Epoch 23/300 -- Iteration 21456 - Batch 270/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.76 min) Epoch 23/300 -- Iteration 21465 - Batch 279/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.77 min) Epoch 23/300 -- Iteration 21474 - Batch 288/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.77 min) Epoch 23/300 -- Iteration 21483 - Batch 297/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.78 min) Epoch 23/300 -- Iteration 21492 - Batch 306/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.78 min) Epoch 23/300 -- Iteration 21501 - Batch 315/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.79 min) Epoch 23/300 -- Iteration 21510 - Batch 324/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.79 min) Epoch 23/300 -- Iteration 21519 - Batch 333/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.80 min) Epoch 23/300 -- Iteration 21528 - Batch 342/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.80 min) Epoch 23/300 -- Iteration 21537 - Batch 351/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.81 min) Epoch 23/300 -- Iteration 21546 - Batch 360/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.81 min) Epoch 23/300 -- Iteration 21555 - Batch 369/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.81 min) Epoch 23/300 -- Iteration 21564 - Batch 378/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.82 min) Epoch 23/300 -- Iteration 21573 - Batch 387/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.82 min) Epoch 23/300 -- Iteration 21582 - Batch 396/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.83 min) Epoch 23/300 -- Iteration 21591 - Batch 405/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.83 min) Epoch 23/300 -- Iteration 21600 - Batch 414/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.84 min) Epoch 23/300 -- Iteration 21609 - Batch 423/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.84 min) Epoch 23/300 -- Iteration 21618 - Batch 432/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.85 min) Epoch 23/300 -- Iteration 21627 - Batch 441/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.85 min) Epoch 23/300 -- Iteration 21636 - Batch 450/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.85 min) Epoch 23/300 -- Iteration 21645 - Batch 459/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.86 min) Epoch 23/300 -- Iteration 21654 - Batch 468/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.86 min) Epoch 23/300 -- Iteration 21663 - Batch 477/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.87 min) Epoch 23/300 -- Iteration 21672 - Batch 486/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.87 min) Epoch 23/300 -- Iteration 21681 - Batch 495/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.88 min) Epoch 23/300 -- Iteration 21690 - Batch 504/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.88 min) Epoch 23/300 -- Iteration 21699 - Batch 513/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.89 min) Epoch 23/300 -- Iteration 21708 - Batch 522/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.89 min) Epoch 23/300 -- Iteration 21717 - Batch 531/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.90 min) Epoch 23/300 -- Iteration 21726 - Batch 540/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.90 min) Epoch 23/300 -- Iteration 21735 - Batch 549/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.90 min) Epoch 23/300 -- Iteration 21744 - Batch 558/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.91 min) Epoch 23/300 -- Iteration 21753 - Batch 567/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.91 min) Epoch 23/300 -- Iteration 21762 - Batch 576/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.92 min) Epoch 23/300 -- Iteration 21771 - Batch 585/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.92 min) Epoch 23/300 -- Iteration 21780 - Batch 594/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.93 min) Epoch 23/300 -- Iteration 21789 - Batch 603/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.93 min) Epoch 23/300 -- Iteration 21798 - Batch 612/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.94 min) Epoch 23/300 -- Iteration 21807 - Batch 621/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.94 min) Epoch 23/300 -- Iteration 21816 - Batch 630/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.94 min) Epoch 23/300 -- Iteration 21825 - Batch 639/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.95 min) Epoch 23/300 -- Iteration 21834 - Batch 648/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.95 min) Epoch 23/300 -- Iteration 21843 - Batch 657/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.96 min) Epoch 23/300 -- Iteration 21852 - Batch 666/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.96 min) Epoch 23/300 -- Iteration 21861 - Batch 675/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.97 min) Epoch 23/300 -- Iteration 21870 - Batch 684/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.97 min) Epoch 23/300 -- Iteration 21879 - Batch 693/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.98 min) Epoch 23/300 -- Iteration 21888 - Batch 702/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.98 min) Epoch 23/300 -- Iteration 21897 - Batch 711/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.99 min) Epoch 23/300 -- Iteration 21906 - Batch 720/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.99 min) Epoch 23/300 -- Iteration 21915 - Batch 729/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(10.99 min) Epoch 23/300 -- Iteration 21924 - Batch 738/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.00 min) Epoch 23/300 -- Iteration 21933 - Batch 747/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.00 min) Epoch 23/300 -- Iteration 21942 - Batch 756/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.01 min) Epoch 23/300 -- Iteration 21951 - Batch 765/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.01 min) Epoch 23/300 -- Iteration 21960 - Batch 774/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.02 min) Epoch 23/300 -- Iteration 21969 - Batch 783/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.02 min) Epoch 23/300 -- Iteration 21978 - Batch 792/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.03 min) Epoch 23/300 -- Iteration 21987 - Batch 801/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.03 min) Epoch 23/300 -- Iteration 21996 - Batch 810/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.03 min) Epoch 23/300 -- Iteration 22005 - Batch 819/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.04 min) Epoch 23/300 -- Iteration 22014 - Batch 828/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.04 min) Epoch 23/300 -- Iteration 22023 - Batch 837/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.05 min) Epoch 23/300 -- Iteration 22032 - Batch 846/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.05 min) Epoch 23/300 -- Iteration 22041 - Batch 855/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.06 min) Epoch 23/300 -- Iteration 22050 - Batch 864/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.06 min) Epoch 23/300 -- Iteration 22059 - Batch 873/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.07 min) Epoch 23/300 -- Iteration 22068 - Batch 882/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.07 min) Epoch 23/300 -- Iteration 22077 - Batch 891/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.07 min) Epoch 23/300 -- Iteration 22086 - Batch 900/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.08 min) Epoch 23/300 -- Iteration 22095 - Batch 909/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.08 min) Epoch 23/300 -- Iteration 22104 - Batch 918/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.09 min) Epoch 23/300 -- Iteration 22113 - Batch 927/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.09 min) Epoch 23/300 -- Iteration 22122 - Batch 936/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.10 min) Epoch 23/300 -- Iteration 22131 - Batch 945/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.10 min) Epoch 23/300 -- Iteration 22140 - Batch 954/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(11.11 min) Epoch 23/300 -- Iteration 22149 - Batch 962/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030 - Val acc: -0.0000\n",
      "(11.11 min) Epoch 24/300 -- Iteration 22158 - Batch 9/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.12 min) Epoch 24/300 -- Iteration 22167 - Batch 18/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.12 min) Epoch 24/300 -- Iteration 22176 - Batch 27/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.13 min) Epoch 24/300 -- Iteration 22185 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.13 min) Epoch 24/300 -- Iteration 22194 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.14 min) Epoch 24/300 -- Iteration 22203 - Batch 54/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.14 min) Epoch 24/300 -- Iteration 22212 - Batch 63/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.14 min) Epoch 24/300 -- Iteration 22221 - Batch 72/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.15 min) Epoch 24/300 -- Iteration 22230 - Batch 81/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.15 min) Epoch 24/300 -- Iteration 22239 - Batch 90/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.16 min) Epoch 24/300 -- Iteration 22248 - Batch 99/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.16 min) Epoch 24/300 -- Iteration 22257 - Batch 108/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.17 min) Epoch 24/300 -- Iteration 22266 - Batch 117/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.17 min) Epoch 24/300 -- Iteration 22275 - Batch 126/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.18 min) Epoch 24/300 -- Iteration 22284 - Batch 135/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.18 min) Epoch 24/300 -- Iteration 22293 - Batch 144/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.19 min) Epoch 24/300 -- Iteration 22302 - Batch 153/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.19 min) Epoch 24/300 -- Iteration 22311 - Batch 162/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.20 min) Epoch 24/300 -- Iteration 22320 - Batch 171/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.20 min) Epoch 24/300 -- Iteration 22329 - Batch 180/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.21 min) Epoch 24/300 -- Iteration 22338 - Batch 189/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.21 min) Epoch 24/300 -- Iteration 22347 - Batch 198/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.21 min) Epoch 24/300 -- Iteration 22356 - Batch 207/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.22 min) Epoch 24/300 -- Iteration 22365 - Batch 216/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.22 min) Epoch 24/300 -- Iteration 22374 - Batch 225/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.23 min) Epoch 24/300 -- Iteration 22383 - Batch 234/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.23 min) Epoch 24/300 -- Iteration 22392 - Batch 243/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.24 min) Epoch 24/300 -- Iteration 22401 - Batch 252/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.24 min) Epoch 24/300 -- Iteration 22410 - Batch 261/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.25 min) Epoch 24/300 -- Iteration 22419 - Batch 270/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.25 min) Epoch 24/300 -- Iteration 22428 - Batch 279/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.26 min) Epoch 24/300 -- Iteration 22437 - Batch 288/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.26 min) Epoch 24/300 -- Iteration 22446 - Batch 297/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.27 min) Epoch 24/300 -- Iteration 22455 - Batch 306/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.27 min) Epoch 24/300 -- Iteration 22464 - Batch 315/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.27 min) Epoch 24/300 -- Iteration 22473 - Batch 324/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.28 min) Epoch 24/300 -- Iteration 22482 - Batch 333/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.28 min) Epoch 24/300 -- Iteration 22491 - Batch 342/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.29 min) Epoch 24/300 -- Iteration 22500 - Batch 351/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.29 min) Epoch 24/300 -- Iteration 22509 - Batch 360/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.30 min) Epoch 24/300 -- Iteration 22518 - Batch 369/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.30 min) Epoch 24/300 -- Iteration 22527 - Batch 378/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.31 min) Epoch 24/300 -- Iteration 22536 - Batch 387/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.31 min) Epoch 24/300 -- Iteration 22545 - Batch 396/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.32 min) Epoch 24/300 -- Iteration 22554 - Batch 405/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.32 min) Epoch 24/300 -- Iteration 22563 - Batch 414/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.33 min) Epoch 24/300 -- Iteration 22572 - Batch 423/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.33 min) Epoch 24/300 -- Iteration 22581 - Batch 432/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.33 min) Epoch 24/300 -- Iteration 22590 - Batch 441/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.34 min) Epoch 24/300 -- Iteration 22599 - Batch 450/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.34 min) Epoch 24/300 -- Iteration 22608 - Batch 459/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.35 min) Epoch 24/300 -- Iteration 22617 - Batch 468/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.35 min) Epoch 24/300 -- Iteration 22626 - Batch 477/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.36 min) Epoch 24/300 -- Iteration 22635 - Batch 486/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.36 min) Epoch 24/300 -- Iteration 22644 - Batch 495/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.37 min) Epoch 24/300 -- Iteration 22653 - Batch 504/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.37 min) Epoch 24/300 -- Iteration 22662 - Batch 513/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.38 min) Epoch 24/300 -- Iteration 22671 - Batch 522/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.38 min) Epoch 24/300 -- Iteration 22680 - Batch 531/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.38 min) Epoch 24/300 -- Iteration 22689 - Batch 540/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.39 min) Epoch 24/300 -- Iteration 22698 - Batch 549/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.39 min) Epoch 24/300 -- Iteration 22707 - Batch 558/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.40 min) Epoch 24/300 -- Iteration 22716 - Batch 567/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.40 min) Epoch 24/300 -- Iteration 22725 - Batch 576/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.41 min) Epoch 24/300 -- Iteration 22734 - Batch 585/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.41 min) Epoch 24/300 -- Iteration 22743 - Batch 594/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.41 min) Epoch 24/300 -- Iteration 22752 - Batch 603/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.42 min) Epoch 24/300 -- Iteration 22761 - Batch 612/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.42 min) Epoch 24/300 -- Iteration 22770 - Batch 621/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.43 min) Epoch 24/300 -- Iteration 22779 - Batch 630/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.43 min) Epoch 24/300 -- Iteration 22788 - Batch 639/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.44 min) Epoch 24/300 -- Iteration 22797 - Batch 648/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.44 min) Epoch 24/300 -- Iteration 22806 - Batch 657/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.45 min) Epoch 24/300 -- Iteration 22815 - Batch 666/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.45 min) Epoch 24/300 -- Iteration 22824 - Batch 675/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.46 min) Epoch 24/300 -- Iteration 22833 - Batch 684/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.46 min) Epoch 24/300 -- Iteration 22842 - Batch 693/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.46 min) Epoch 24/300 -- Iteration 22851 - Batch 702/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.47 min) Epoch 24/300 -- Iteration 22860 - Batch 711/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.47 min) Epoch 24/300 -- Iteration 22869 - Batch 720/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.48 min) Epoch 24/300 -- Iteration 22878 - Batch 729/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.48 min) Epoch 24/300 -- Iteration 22887 - Batch 738/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.49 min) Epoch 24/300 -- Iteration 22896 - Batch 747/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.49 min) Epoch 24/300 -- Iteration 22905 - Batch 756/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.50 min) Epoch 24/300 -- Iteration 22914 - Batch 765/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.50 min) Epoch 24/300 -- Iteration 22923 - Batch 774/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.50 min) Epoch 24/300 -- Iteration 22932 - Batch 783/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.51 min) Epoch 24/300 -- Iteration 22941 - Batch 792/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.51 min) Epoch 24/300 -- Iteration 22950 - Batch 801/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.52 min) Epoch 24/300 -- Iteration 22959 - Batch 810/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.52 min) Epoch 24/300 -- Iteration 22968 - Batch 819/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.53 min) Epoch 24/300 -- Iteration 22977 - Batch 828/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.53 min) Epoch 24/300 -- Iteration 22986 - Batch 837/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.54 min) Epoch 24/300 -- Iteration 22995 - Batch 846/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.54 min) Epoch 24/300 -- Iteration 23004 - Batch 855/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.54 min) Epoch 24/300 -- Iteration 23013 - Batch 864/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.55 min) Epoch 24/300 -- Iteration 23022 - Batch 873/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.55 min) Epoch 24/300 -- Iteration 23031 - Batch 882/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.56 min) Epoch 24/300 -- Iteration 23040 - Batch 891/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.56 min) Epoch 24/300 -- Iteration 23049 - Batch 900/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.57 min) Epoch 24/300 -- Iteration 23058 - Batch 909/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.57 min) Epoch 24/300 -- Iteration 23067 - Batch 918/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.58 min) Epoch 24/300 -- Iteration 23076 - Batch 927/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.58 min) Epoch 24/300 -- Iteration 23085 - Batch 936/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.59 min) Epoch 24/300 -- Iteration 23094 - Batch 945/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.59 min) Epoch 24/300 -- Iteration 23103 - Batch 954/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0030\n",
      "(11.59 min) Epoch 24/300 -- Iteration 23112 - Batch 962/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031 - Val acc: -0.0000\n",
      "(11.60 min) Epoch 25/300 -- Iteration 23121 - Batch 9/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.60 min) Epoch 25/300 -- Iteration 23130 - Batch 18/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.61 min) Epoch 25/300 -- Iteration 23139 - Batch 27/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.61 min) Epoch 25/300 -- Iteration 23148 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.62 min) Epoch 25/300 -- Iteration 23157 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.62 min) Epoch 25/300 -- Iteration 23166 - Batch 54/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.63 min) Epoch 25/300 -- Iteration 23175 - Batch 63/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.63 min) Epoch 25/300 -- Iteration 23184 - Batch 72/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.64 min) Epoch 25/300 -- Iteration 23193 - Batch 81/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.64 min) Epoch 25/300 -- Iteration 23202 - Batch 90/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.65 min) Epoch 25/300 -- Iteration 23211 - Batch 99/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.65 min) Epoch 25/300 -- Iteration 23220 - Batch 108/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.65 min) Epoch 25/300 -- Iteration 23229 - Batch 117/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.66 min) Epoch 25/300 -- Iteration 23238 - Batch 126/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.66 min) Epoch 25/300 -- Iteration 23247 - Batch 135/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.67 min) Epoch 25/300 -- Iteration 23256 - Batch 144/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.67 min) Epoch 25/300 -- Iteration 23265 - Batch 153/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.68 min) Epoch 25/300 -- Iteration 23274 - Batch 162/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.68 min) Epoch 25/300 -- Iteration 23283 - Batch 171/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.69 min) Epoch 25/300 -- Iteration 23292 - Batch 180/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.69 min) Epoch 25/300 -- Iteration 23301 - Batch 189/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.69 min) Epoch 25/300 -- Iteration 23310 - Batch 198/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.70 min) Epoch 25/300 -- Iteration 23319 - Batch 207/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.70 min) Epoch 25/300 -- Iteration 23328 - Batch 216/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.71 min) Epoch 25/300 -- Iteration 23337 - Batch 225/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.71 min) Epoch 25/300 -- Iteration 23346 - Batch 234/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.72 min) Epoch 25/300 -- Iteration 23355 - Batch 243/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.72 min) Epoch 25/300 -- Iteration 23364 - Batch 252/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.73 min) Epoch 25/300 -- Iteration 23373 - Batch 261/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.73 min) Epoch 25/300 -- Iteration 23382 - Batch 270/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.73 min) Epoch 25/300 -- Iteration 23391 - Batch 279/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.74 min) Epoch 25/300 -- Iteration 23400 - Batch 288/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.74 min) Epoch 25/300 -- Iteration 23409 - Batch 297/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.75 min) Epoch 25/300 -- Iteration 23418 - Batch 306/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.75 min) Epoch 25/300 -- Iteration 23427 - Batch 315/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.76 min) Epoch 25/300 -- Iteration 23436 - Batch 324/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.76 min) Epoch 25/300 -- Iteration 23445 - Batch 333/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.77 min) Epoch 25/300 -- Iteration 23454 - Batch 342/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.77 min) Epoch 25/300 -- Iteration 23463 - Batch 351/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.78 min) Epoch 25/300 -- Iteration 23472 - Batch 360/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.78 min) Epoch 25/300 -- Iteration 23481 - Batch 369/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.79 min) Epoch 25/300 -- Iteration 23490 - Batch 378/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.79 min) Epoch 25/300 -- Iteration 23499 - Batch 387/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.79 min) Epoch 25/300 -- Iteration 23508 - Batch 396/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.80 min) Epoch 25/300 -- Iteration 23517 - Batch 405/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.80 min) Epoch 25/300 -- Iteration 23526 - Batch 414/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.81 min) Epoch 25/300 -- Iteration 23535 - Batch 423/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.81 min) Epoch 25/300 -- Iteration 23544 - Batch 432/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.82 min) Epoch 25/300 -- Iteration 23553 - Batch 441/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.82 min) Epoch 25/300 -- Iteration 23562 - Batch 450/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.83 min) Epoch 25/300 -- Iteration 23571 - Batch 459/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.83 min) Epoch 25/300 -- Iteration 23580 - Batch 468/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.84 min) Epoch 25/300 -- Iteration 23589 - Batch 477/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.84 min) Epoch 25/300 -- Iteration 23598 - Batch 486/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.84 min) Epoch 25/300 -- Iteration 23607 - Batch 495/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.85 min) Epoch 25/300 -- Iteration 23616 - Batch 504/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.85 min) Epoch 25/300 -- Iteration 23625 - Batch 513/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.86 min) Epoch 25/300 -- Iteration 23634 - Batch 522/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.86 min) Epoch 25/300 -- Iteration 23643 - Batch 531/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.87 min) Epoch 25/300 -- Iteration 23652 - Batch 540/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.87 min) Epoch 25/300 -- Iteration 23661 - Batch 549/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.88 min) Epoch 25/300 -- Iteration 23670 - Batch 558/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.88 min) Epoch 25/300 -- Iteration 23679 - Batch 567/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.89 min) Epoch 25/300 -- Iteration 23688 - Batch 576/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.89 min) Epoch 25/300 -- Iteration 23697 - Batch 585/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.89 min) Epoch 25/300 -- Iteration 23706 - Batch 594/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.90 min) Epoch 25/300 -- Iteration 23715 - Batch 603/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.90 min) Epoch 25/300 -- Iteration 23724 - Batch 612/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.91 min) Epoch 25/300 -- Iteration 23733 - Batch 621/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.91 min) Epoch 25/300 -- Iteration 23742 - Batch 630/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.92 min) Epoch 25/300 -- Iteration 23751 - Batch 639/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.92 min) Epoch 25/300 -- Iteration 23760 - Batch 648/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.93 min) Epoch 25/300 -- Iteration 23769 - Batch 657/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.93 min) Epoch 25/300 -- Iteration 23778 - Batch 666/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.94 min) Epoch 25/300 -- Iteration 23787 - Batch 675/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.94 min) Epoch 25/300 -- Iteration 23796 - Batch 684/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.94 min) Epoch 25/300 -- Iteration 23805 - Batch 693/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.95 min) Epoch 25/300 -- Iteration 23814 - Batch 702/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.95 min) Epoch 25/300 -- Iteration 23823 - Batch 711/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.96 min) Epoch 25/300 -- Iteration 23832 - Batch 720/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.96 min) Epoch 25/300 -- Iteration 23841 - Batch 729/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.97 min) Epoch 25/300 -- Iteration 23850 - Batch 738/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.97 min) Epoch 25/300 -- Iteration 23859 - Batch 747/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.98 min) Epoch 25/300 -- Iteration 23868 - Batch 756/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.98 min) Epoch 25/300 -- Iteration 23877 - Batch 765/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.98 min) Epoch 25/300 -- Iteration 23886 - Batch 774/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.99 min) Epoch 25/300 -- Iteration 23895 - Batch 783/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(11.99 min) Epoch 25/300 -- Iteration 23904 - Batch 792/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.00 min) Epoch 25/300 -- Iteration 23913 - Batch 801/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.00 min) Epoch 25/300 -- Iteration 23922 - Batch 810/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.01 min) Epoch 25/300 -- Iteration 23931 - Batch 819/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.01 min) Epoch 25/300 -- Iteration 23940 - Batch 828/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.02 min) Epoch 25/300 -- Iteration 23949 - Batch 837/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.02 min) Epoch 25/300 -- Iteration 23958 - Batch 846/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.02 min) Epoch 25/300 -- Iteration 23967 - Batch 855/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.03 min) Epoch 25/300 -- Iteration 23976 - Batch 864/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.03 min) Epoch 25/300 -- Iteration 23985 - Batch 873/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.04 min) Epoch 25/300 -- Iteration 23994 - Batch 882/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.04 min) Epoch 25/300 -- Iteration 24003 - Batch 891/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.05 min) Epoch 25/300 -- Iteration 24012 - Batch 900/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.05 min) Epoch 25/300 -- Iteration 24021 - Batch 909/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.06 min) Epoch 25/300 -- Iteration 24030 - Batch 918/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.06 min) Epoch 25/300 -- Iteration 24039 - Batch 927/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.06 min) Epoch 25/300 -- Iteration 24048 - Batch 936/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.07 min) Epoch 25/300 -- Iteration 24057 - Batch 945/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.07 min) Epoch 25/300 -- Iteration 24066 - Batch 954/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.08 min) Epoch 25/300 -- Iteration 24075 - Batch 962/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031 - Val acc: -0.0000\n",
      "(12.08 min) Epoch 26/300 -- Iteration 24084 - Batch 9/963 - Train loss: 0.0029  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.09 min) Epoch 26/300 -- Iteration 24093 - Batch 18/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.09 min) Epoch 26/300 -- Iteration 24102 - Batch 27/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.10 min) Epoch 26/300 -- Iteration 24111 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.10 min) Epoch 26/300 -- Iteration 24120 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.10 min) Epoch 26/300 -- Iteration 24129 - Batch 54/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.11 min) Epoch 26/300 -- Iteration 24138 - Batch 63/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.11 min) Epoch 26/300 -- Iteration 24147 - Batch 72/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.12 min) Epoch 26/300 -- Iteration 24156 - Batch 81/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.12 min) Epoch 26/300 -- Iteration 24165 - Batch 90/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.13 min) Epoch 26/300 -- Iteration 24174 - Batch 99/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.13 min) Epoch 26/300 -- Iteration 24183 - Batch 108/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.14 min) Epoch 26/300 -- Iteration 24192 - Batch 117/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.14 min) Epoch 26/300 -- Iteration 24201 - Batch 126/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.14 min) Epoch 26/300 -- Iteration 24210 - Batch 135/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.15 min) Epoch 26/300 -- Iteration 24219 - Batch 144/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.15 min) Epoch 26/300 -- Iteration 24228 - Batch 153/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.16 min) Epoch 26/300 -- Iteration 24237 - Batch 162/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.16 min) Epoch 26/300 -- Iteration 24246 - Batch 171/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.17 min) Epoch 26/300 -- Iteration 24255 - Batch 180/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.17 min) Epoch 26/300 -- Iteration 24264 - Batch 189/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.17 min) Epoch 26/300 -- Iteration 24273 - Batch 198/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.18 min) Epoch 26/300 -- Iteration 24282 - Batch 207/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.18 min) Epoch 26/300 -- Iteration 24291 - Batch 216/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.19 min) Epoch 26/300 -- Iteration 24300 - Batch 225/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.19 min) Epoch 26/300 -- Iteration 24309 - Batch 234/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.20 min) Epoch 26/300 -- Iteration 24318 - Batch 243/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.20 min) Epoch 26/300 -- Iteration 24327 - Batch 252/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.21 min) Epoch 26/300 -- Iteration 24336 - Batch 261/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.21 min) Epoch 26/300 -- Iteration 24345 - Batch 270/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.21 min) Epoch 26/300 -- Iteration 24354 - Batch 279/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.22 min) Epoch 26/300 -- Iteration 24363 - Batch 288/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.22 min) Epoch 26/300 -- Iteration 24372 - Batch 297/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.23 min) Epoch 26/300 -- Iteration 24381 - Batch 306/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.23 min) Epoch 26/300 -- Iteration 24390 - Batch 315/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.24 min) Epoch 26/300 -- Iteration 24399 - Batch 324/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.24 min) Epoch 26/300 -- Iteration 24408 - Batch 333/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.25 min) Epoch 26/300 -- Iteration 24417 - Batch 342/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.25 min) Epoch 26/300 -- Iteration 24426 - Batch 351/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.25 min) Epoch 26/300 -- Iteration 24435 - Batch 360/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.26 min) Epoch 26/300 -- Iteration 24444 - Batch 369/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.26 min) Epoch 26/300 -- Iteration 24453 - Batch 378/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.27 min) Epoch 26/300 -- Iteration 24462 - Batch 387/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.27 min) Epoch 26/300 -- Iteration 24471 - Batch 396/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.28 min) Epoch 26/300 -- Iteration 24480 - Batch 405/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.28 min) Epoch 26/300 -- Iteration 24489 - Batch 414/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.29 min) Epoch 26/300 -- Iteration 24498 - Batch 423/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.29 min) Epoch 26/300 -- Iteration 24507 - Batch 432/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.30 min) Epoch 26/300 -- Iteration 24516 - Batch 441/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.30 min) Epoch 26/300 -- Iteration 24525 - Batch 450/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.31 min) Epoch 26/300 -- Iteration 24534 - Batch 459/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.31 min) Epoch 26/300 -- Iteration 24543 - Batch 468/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.31 min) Epoch 26/300 -- Iteration 24552 - Batch 477/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.32 min) Epoch 26/300 -- Iteration 24561 - Batch 486/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.32 min) Epoch 26/300 -- Iteration 24570 - Batch 495/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.33 min) Epoch 26/300 -- Iteration 24579 - Batch 504/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.33 min) Epoch 26/300 -- Iteration 24588 - Batch 513/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.34 min) Epoch 26/300 -- Iteration 24597 - Batch 522/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.34 min) Epoch 26/300 -- Iteration 24606 - Batch 531/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.35 min) Epoch 26/300 -- Iteration 24615 - Batch 540/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.35 min) Epoch 26/300 -- Iteration 24624 - Batch 549/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.35 min) Epoch 26/300 -- Iteration 24633 - Batch 558/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.36 min) Epoch 26/300 -- Iteration 24642 - Batch 567/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.36 min) Epoch 26/300 -- Iteration 24651 - Batch 576/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.37 min) Epoch 26/300 -- Iteration 24660 - Batch 585/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.37 min) Epoch 26/300 -- Iteration 24669 - Batch 594/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.38 min) Epoch 26/300 -- Iteration 24678 - Batch 603/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.38 min) Epoch 26/300 -- Iteration 24687 - Batch 612/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.39 min) Epoch 26/300 -- Iteration 24696 - Batch 621/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.39 min) Epoch 26/300 -- Iteration 24705 - Batch 630/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.39 min) Epoch 26/300 -- Iteration 24714 - Batch 639/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.40 min) Epoch 26/300 -- Iteration 24723 - Batch 648/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.40 min) Epoch 26/300 -- Iteration 24732 - Batch 657/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.41 min) Epoch 26/300 -- Iteration 24741 - Batch 666/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.41 min) Epoch 26/300 -- Iteration 24750 - Batch 675/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.42 min) Epoch 26/300 -- Iteration 24759 - Batch 684/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.42 min) Epoch 26/300 -- Iteration 24768 - Batch 693/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.43 min) Epoch 26/300 -- Iteration 24777 - Batch 702/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.43 min) Epoch 26/300 -- Iteration 24786 - Batch 711/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.44 min) Epoch 26/300 -- Iteration 24795 - Batch 720/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.44 min) Epoch 26/300 -- Iteration 24804 - Batch 729/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.44 min) Epoch 26/300 -- Iteration 24813 - Batch 738/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.45 min) Epoch 26/300 -- Iteration 24822 - Batch 747/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.45 min) Epoch 26/300 -- Iteration 24831 - Batch 756/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.46 min) Epoch 26/300 -- Iteration 24840 - Batch 765/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.46 min) Epoch 26/300 -- Iteration 24849 - Batch 774/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.47 min) Epoch 26/300 -- Iteration 24858 - Batch 783/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.47 min) Epoch 26/300 -- Iteration 24867 - Batch 792/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.48 min) Epoch 26/300 -- Iteration 24876 - Batch 801/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.48 min) Epoch 26/300 -- Iteration 24885 - Batch 810/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.49 min) Epoch 26/300 -- Iteration 24894 - Batch 819/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.49 min) Epoch 26/300 -- Iteration 24903 - Batch 828/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.49 min) Epoch 26/300 -- Iteration 24912 - Batch 837/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.50 min) Epoch 26/300 -- Iteration 24921 - Batch 846/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.50 min) Epoch 26/300 -- Iteration 24930 - Batch 855/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.51 min) Epoch 26/300 -- Iteration 24939 - Batch 864/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.51 min) Epoch 26/300 -- Iteration 24948 - Batch 873/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.52 min) Epoch 26/300 -- Iteration 24957 - Batch 882/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.52 min) Epoch 26/300 -- Iteration 24966 - Batch 891/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.53 min) Epoch 26/300 -- Iteration 24975 - Batch 900/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.53 min) Epoch 26/300 -- Iteration 24984 - Batch 909/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.53 min) Epoch 26/300 -- Iteration 24993 - Batch 918/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.54 min) Epoch 26/300 -- Iteration 25002 - Batch 927/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.54 min) Epoch 26/300 -- Iteration 25011 - Batch 936/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.55 min) Epoch 26/300 -- Iteration 25020 - Batch 945/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.55 min) Epoch 26/300 -- Iteration 25029 - Batch 954/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0031\n",
      "(12.56 min) Epoch 26/300 -- Iteration 25038 - Batch 962/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027 - Val acc: -0.0000\n",
      "(12.56 min) Epoch 27/300 -- Iteration 25047 - Batch 9/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.57 min) Epoch 27/300 -- Iteration 25056 - Batch 18/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.57 min) Epoch 27/300 -- Iteration 25065 - Batch 27/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.58 min) Epoch 27/300 -- Iteration 25074 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.58 min) Epoch 27/300 -- Iteration 25083 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.58 min) Epoch 27/300 -- Iteration 25092 - Batch 54/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.59 min) Epoch 27/300 -- Iteration 25101 - Batch 63/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.59 min) Epoch 27/300 -- Iteration 25110 - Batch 72/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.60 min) Epoch 27/300 -- Iteration 25119 - Batch 81/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.60 min) Epoch 27/300 -- Iteration 25128 - Batch 90/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.61 min) Epoch 27/300 -- Iteration 25137 - Batch 99/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.61 min) Epoch 27/300 -- Iteration 25146 - Batch 108/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.62 min) Epoch 27/300 -- Iteration 25155 - Batch 117/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.62 min) Epoch 27/300 -- Iteration 25164 - Batch 126/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.63 min) Epoch 27/300 -- Iteration 25173 - Batch 135/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.63 min) Epoch 27/300 -- Iteration 25182 - Batch 144/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.63 min) Epoch 27/300 -- Iteration 25191 - Batch 153/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.64 min) Epoch 27/300 -- Iteration 25200 - Batch 162/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.64 min) Epoch 27/300 -- Iteration 25209 - Batch 171/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.65 min) Epoch 27/300 -- Iteration 25218 - Batch 180/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.65 min) Epoch 27/300 -- Iteration 25227 - Batch 189/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.66 min) Epoch 27/300 -- Iteration 25236 - Batch 198/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.66 min) Epoch 27/300 -- Iteration 25245 - Batch 207/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.67 min) Epoch 27/300 -- Iteration 25254 - Batch 216/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.67 min) Epoch 27/300 -- Iteration 25263 - Batch 225/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.67 min) Epoch 27/300 -- Iteration 25272 - Batch 234/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.68 min) Epoch 27/300 -- Iteration 25281 - Batch 243/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.68 min) Epoch 27/300 -- Iteration 25290 - Batch 252/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.69 min) Epoch 27/300 -- Iteration 25299 - Batch 261/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.69 min) Epoch 27/300 -- Iteration 25308 - Batch 270/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.70 min) Epoch 27/300 -- Iteration 25317 - Batch 279/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.70 min) Epoch 27/300 -- Iteration 25326 - Batch 288/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.71 min) Epoch 27/300 -- Iteration 25335 - Batch 297/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.71 min) Epoch 27/300 -- Iteration 25344 - Batch 306/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.71 min) Epoch 27/300 -- Iteration 25353 - Batch 315/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.72 min) Epoch 27/300 -- Iteration 25362 - Batch 324/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.72 min) Epoch 27/300 -- Iteration 25371 - Batch 333/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.73 min) Epoch 27/300 -- Iteration 25380 - Batch 342/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.73 min) Epoch 27/300 -- Iteration 25389 - Batch 351/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.74 min) Epoch 27/300 -- Iteration 25398 - Batch 360/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.74 min) Epoch 27/300 -- Iteration 25407 - Batch 369/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.75 min) Epoch 27/300 -- Iteration 25416 - Batch 378/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.75 min) Epoch 27/300 -- Iteration 25425 - Batch 387/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.76 min) Epoch 27/300 -- Iteration 25434 - Batch 396/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.76 min) Epoch 27/300 -- Iteration 25443 - Batch 405/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.77 min) Epoch 27/300 -- Iteration 25452 - Batch 414/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.77 min) Epoch 27/300 -- Iteration 25461 - Batch 423/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.78 min) Epoch 27/300 -- Iteration 25470 - Batch 432/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.78 min) Epoch 27/300 -- Iteration 25479 - Batch 441/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.79 min) Epoch 27/300 -- Iteration 25488 - Batch 450/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.79 min) Epoch 27/300 -- Iteration 25497 - Batch 459/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.80 min) Epoch 27/300 -- Iteration 25506 - Batch 468/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.80 min) Epoch 27/300 -- Iteration 25515 - Batch 477/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.81 min) Epoch 27/300 -- Iteration 25524 - Batch 486/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.81 min) Epoch 27/300 -- Iteration 25533 - Batch 495/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.81 min) Epoch 27/300 -- Iteration 25542 - Batch 504/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.82 min) Epoch 27/300 -- Iteration 25551 - Batch 513/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.82 min) Epoch 27/300 -- Iteration 25560 - Batch 522/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.83 min) Epoch 27/300 -- Iteration 25569 - Batch 531/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.83 min) Epoch 27/300 -- Iteration 25578 - Batch 540/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.84 min) Epoch 27/300 -- Iteration 25587 - Batch 549/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.84 min) Epoch 27/300 -- Iteration 25596 - Batch 558/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.85 min) Epoch 27/300 -- Iteration 25605 - Batch 567/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.85 min) Epoch 27/300 -- Iteration 25614 - Batch 576/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.86 min) Epoch 27/300 -- Iteration 25623 - Batch 585/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.86 min) Epoch 27/300 -- Iteration 25632 - Batch 594/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.87 min) Epoch 27/300 -- Iteration 25641 - Batch 603/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.87 min) Epoch 27/300 -- Iteration 25650 - Batch 612/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.87 min) Epoch 27/300 -- Iteration 25659 - Batch 621/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.88 min) Epoch 27/300 -- Iteration 25668 - Batch 630/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.88 min) Epoch 27/300 -- Iteration 25677 - Batch 639/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.89 min) Epoch 27/300 -- Iteration 25686 - Batch 648/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.89 min) Epoch 27/300 -- Iteration 25695 - Batch 657/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.90 min) Epoch 27/300 -- Iteration 25704 - Batch 666/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.90 min) Epoch 27/300 -- Iteration 25713 - Batch 675/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.91 min) Epoch 27/300 -- Iteration 25722 - Batch 684/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.91 min) Epoch 27/300 -- Iteration 25731 - Batch 693/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.92 min) Epoch 27/300 -- Iteration 25740 - Batch 702/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.92 min) Epoch 27/300 -- Iteration 25749 - Batch 711/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.93 min) Epoch 27/300 -- Iteration 25758 - Batch 720/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.93 min) Epoch 27/300 -- Iteration 25767 - Batch 729/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.94 min) Epoch 27/300 -- Iteration 25776 - Batch 738/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.94 min) Epoch 27/300 -- Iteration 25785 - Batch 747/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.94 min) Epoch 27/300 -- Iteration 25794 - Batch 756/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.95 min) Epoch 27/300 -- Iteration 25803 - Batch 765/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.95 min) Epoch 27/300 -- Iteration 25812 - Batch 774/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.96 min) Epoch 27/300 -- Iteration 25821 - Batch 783/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.96 min) Epoch 27/300 -- Iteration 25830 - Batch 792/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.97 min) Epoch 27/300 -- Iteration 25839 - Batch 801/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.97 min) Epoch 27/300 -- Iteration 25848 - Batch 810/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.98 min) Epoch 27/300 -- Iteration 25857 - Batch 819/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.98 min) Epoch 27/300 -- Iteration 25866 - Batch 828/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.98 min) Epoch 27/300 -- Iteration 25875 - Batch 837/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.99 min) Epoch 27/300 -- Iteration 25884 - Batch 846/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(12.99 min) Epoch 27/300 -- Iteration 25893 - Batch 855/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.00 min) Epoch 27/300 -- Iteration 25902 - Batch 864/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.00 min) Epoch 27/300 -- Iteration 25911 - Batch 873/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.01 min) Epoch 27/300 -- Iteration 25920 - Batch 882/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.01 min) Epoch 27/300 -- Iteration 25929 - Batch 891/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.02 min) Epoch 27/300 -- Iteration 25938 - Batch 900/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.02 min) Epoch 27/300 -- Iteration 25947 - Batch 909/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.02 min) Epoch 27/300 -- Iteration 25956 - Batch 918/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.03 min) Epoch 27/300 -- Iteration 25965 - Batch 927/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.03 min) Epoch 27/300 -- Iteration 25974 - Batch 936/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.04 min) Epoch 27/300 -- Iteration 25983 - Batch 945/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.04 min) Epoch 27/300 -- Iteration 25992 - Batch 954/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(13.05 min) Epoch 27/300 -- Iteration 26001 - Batch 962/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028 - Val acc: -0.0000\n",
      "(13.05 min) Epoch 28/300 -- Iteration 26010 - Batch 9/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.06 min) Epoch 28/300 -- Iteration 26019 - Batch 18/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.06 min) Epoch 28/300 -- Iteration 26028 - Batch 27/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.07 min) Epoch 28/300 -- Iteration 26037 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.07 min) Epoch 28/300 -- Iteration 26046 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.07 min) Epoch 28/300 -- Iteration 26055 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.08 min) Epoch 28/300 -- Iteration 26064 - Batch 63/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.08 min) Epoch 28/300 -- Iteration 26073 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.09 min) Epoch 28/300 -- Iteration 26082 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.09 min) Epoch 28/300 -- Iteration 26091 - Batch 90/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.10 min) Epoch 28/300 -- Iteration 26100 - Batch 99/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.10 min) Epoch 28/300 -- Iteration 26109 - Batch 108/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.11 min) Epoch 28/300 -- Iteration 26118 - Batch 117/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.11 min) Epoch 28/300 -- Iteration 26127 - Batch 126/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.11 min) Epoch 28/300 -- Iteration 26136 - Batch 135/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.12 min) Epoch 28/300 -- Iteration 26145 - Batch 144/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.12 min) Epoch 28/300 -- Iteration 26154 - Batch 153/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.13 min) Epoch 28/300 -- Iteration 26163 - Batch 162/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.13 min) Epoch 28/300 -- Iteration 26172 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.14 min) Epoch 28/300 -- Iteration 26181 - Batch 180/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.14 min) Epoch 28/300 -- Iteration 26190 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.14 min) Epoch 28/300 -- Iteration 26199 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.15 min) Epoch 28/300 -- Iteration 26208 - Batch 207/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.15 min) Epoch 28/300 -- Iteration 26217 - Batch 216/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.16 min) Epoch 28/300 -- Iteration 26226 - Batch 225/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.16 min) Epoch 28/300 -- Iteration 26235 - Batch 234/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.17 min) Epoch 28/300 -- Iteration 26244 - Batch 243/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.17 min) Epoch 28/300 -- Iteration 26253 - Batch 252/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.18 min) Epoch 28/300 -- Iteration 26262 - Batch 261/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.18 min) Epoch 28/300 -- Iteration 26271 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.19 min) Epoch 28/300 -- Iteration 26280 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.19 min) Epoch 28/300 -- Iteration 26289 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.20 min) Epoch 28/300 -- Iteration 26298 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.20 min) Epoch 28/300 -- Iteration 26307 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.21 min) Epoch 28/300 -- Iteration 26316 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.21 min) Epoch 28/300 -- Iteration 26325 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.22 min) Epoch 28/300 -- Iteration 26334 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.22 min) Epoch 28/300 -- Iteration 26343 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.22 min) Epoch 28/300 -- Iteration 26352 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.23 min) Epoch 28/300 -- Iteration 26361 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.23 min) Epoch 28/300 -- Iteration 26370 - Batch 369/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.24 min) Epoch 28/300 -- Iteration 26379 - Batch 378/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.24 min) Epoch 28/300 -- Iteration 26388 - Batch 387/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.25 min) Epoch 28/300 -- Iteration 26397 - Batch 396/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.25 min) Epoch 28/300 -- Iteration 26406 - Batch 405/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.26 min) Epoch 28/300 -- Iteration 26415 - Batch 414/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.26 min) Epoch 28/300 -- Iteration 26424 - Batch 423/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.27 min) Epoch 28/300 -- Iteration 26433 - Batch 432/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.27 min) Epoch 28/300 -- Iteration 26442 - Batch 441/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.28 min) Epoch 28/300 -- Iteration 26451 - Batch 450/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.28 min) Epoch 28/300 -- Iteration 26460 - Batch 459/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.28 min) Epoch 28/300 -- Iteration 26469 - Batch 468/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.29 min) Epoch 28/300 -- Iteration 26478 - Batch 477/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.29 min) Epoch 28/300 -- Iteration 26487 - Batch 486/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.30 min) Epoch 28/300 -- Iteration 26496 - Batch 495/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.30 min) Epoch 28/300 -- Iteration 26505 - Batch 504/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.31 min) Epoch 28/300 -- Iteration 26514 - Batch 513/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.31 min) Epoch 28/300 -- Iteration 26523 - Batch 522/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.32 min) Epoch 28/300 -- Iteration 26532 - Batch 531/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.32 min) Epoch 28/300 -- Iteration 26541 - Batch 540/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.33 min) Epoch 28/300 -- Iteration 26550 - Batch 549/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.33 min) Epoch 28/300 -- Iteration 26559 - Batch 558/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.34 min) Epoch 28/300 -- Iteration 26568 - Batch 567/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.34 min) Epoch 28/300 -- Iteration 26577 - Batch 576/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.34 min) Epoch 28/300 -- Iteration 26586 - Batch 585/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.35 min) Epoch 28/300 -- Iteration 26595 - Batch 594/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.35 min) Epoch 28/300 -- Iteration 26604 - Batch 603/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.36 min) Epoch 28/300 -- Iteration 26613 - Batch 612/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.36 min) Epoch 28/300 -- Iteration 26622 - Batch 621/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.37 min) Epoch 28/300 -- Iteration 26631 - Batch 630/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.37 min) Epoch 28/300 -- Iteration 26640 - Batch 639/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.38 min) Epoch 28/300 -- Iteration 26649 - Batch 648/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.38 min) Epoch 28/300 -- Iteration 26658 - Batch 657/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.39 min) Epoch 28/300 -- Iteration 26667 - Batch 666/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.39 min) Epoch 28/300 -- Iteration 26676 - Batch 675/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.40 min) Epoch 28/300 -- Iteration 26685 - Batch 684/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.40 min) Epoch 28/300 -- Iteration 26694 - Batch 693/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.41 min) Epoch 28/300 -- Iteration 26703 - Batch 702/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.41 min) Epoch 28/300 -- Iteration 26712 - Batch 711/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.41 min) Epoch 28/300 -- Iteration 26721 - Batch 720/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.42 min) Epoch 28/300 -- Iteration 26730 - Batch 729/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.42 min) Epoch 28/300 -- Iteration 26739 - Batch 738/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.43 min) Epoch 28/300 -- Iteration 26748 - Batch 747/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.43 min) Epoch 28/300 -- Iteration 26757 - Batch 756/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.44 min) Epoch 28/300 -- Iteration 26766 - Batch 765/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.44 min) Epoch 28/300 -- Iteration 26775 - Batch 774/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.45 min) Epoch 28/300 -- Iteration 26784 - Batch 783/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.45 min) Epoch 28/300 -- Iteration 26793 - Batch 792/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.46 min) Epoch 28/300 -- Iteration 26802 - Batch 801/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.46 min) Epoch 28/300 -- Iteration 26811 - Batch 810/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.46 min) Epoch 28/300 -- Iteration 26820 - Batch 819/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.47 min) Epoch 28/300 -- Iteration 26829 - Batch 828/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.47 min) Epoch 28/300 -- Iteration 26838 - Batch 837/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.48 min) Epoch 28/300 -- Iteration 26847 - Batch 846/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.48 min) Epoch 28/300 -- Iteration 26856 - Batch 855/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.49 min) Epoch 28/300 -- Iteration 26865 - Batch 864/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.49 min) Epoch 28/300 -- Iteration 26874 - Batch 873/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.50 min) Epoch 28/300 -- Iteration 26883 - Batch 882/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.50 min) Epoch 28/300 -- Iteration 26892 - Batch 891/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.51 min) Epoch 28/300 -- Iteration 26901 - Batch 900/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.51 min) Epoch 28/300 -- Iteration 26910 - Batch 909/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.52 min) Epoch 28/300 -- Iteration 26919 - Batch 918/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.52 min) Epoch 28/300 -- Iteration 26928 - Batch 927/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.52 min) Epoch 28/300 -- Iteration 26937 - Batch 936/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.53 min) Epoch 28/300 -- Iteration 26946 - Batch 945/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.53 min) Epoch 28/300 -- Iteration 26955 - Batch 954/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(13.54 min) Epoch 28/300 -- Iteration 26964 - Batch 962/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029 - Val acc: -0.0000\n",
      "(13.54 min) Epoch 29/300 -- Iteration 26973 - Batch 9/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.55 min) Epoch 29/300 -- Iteration 26982 - Batch 18/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.55 min) Epoch 29/300 -- Iteration 26991 - Batch 27/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.56 min) Epoch 29/300 -- Iteration 27000 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.56 min) Epoch 29/300 -- Iteration 27009 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.57 min) Epoch 29/300 -- Iteration 27018 - Batch 54/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.57 min) Epoch 29/300 -- Iteration 27027 - Batch 63/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.58 min) Epoch 29/300 -- Iteration 27036 - Batch 72/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.58 min) Epoch 29/300 -- Iteration 27045 - Batch 81/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.59 min) Epoch 29/300 -- Iteration 27054 - Batch 90/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.59 min) Epoch 29/300 -- Iteration 27063 - Batch 99/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.59 min) Epoch 29/300 -- Iteration 27072 - Batch 108/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.60 min) Epoch 29/300 -- Iteration 27081 - Batch 117/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.60 min) Epoch 29/300 -- Iteration 27090 - Batch 126/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.61 min) Epoch 29/300 -- Iteration 27099 - Batch 135/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.61 min) Epoch 29/300 -- Iteration 27108 - Batch 144/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.62 min) Epoch 29/300 -- Iteration 27117 - Batch 153/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.62 min) Epoch 29/300 -- Iteration 27126 - Batch 162/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.63 min) Epoch 29/300 -- Iteration 27135 - Batch 171/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.63 min) Epoch 29/300 -- Iteration 27144 - Batch 180/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.63 min) Epoch 29/300 -- Iteration 27153 - Batch 189/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.64 min) Epoch 29/300 -- Iteration 27162 - Batch 198/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.64 min) Epoch 29/300 -- Iteration 27171 - Batch 207/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.65 min) Epoch 29/300 -- Iteration 27180 - Batch 216/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.65 min) Epoch 29/300 -- Iteration 27189 - Batch 225/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.66 min) Epoch 29/300 -- Iteration 27198 - Batch 234/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.66 min) Epoch 29/300 -- Iteration 27207 - Batch 243/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.67 min) Epoch 29/300 -- Iteration 27216 - Batch 252/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.67 min) Epoch 29/300 -- Iteration 27225 - Batch 261/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.67 min) Epoch 29/300 -- Iteration 27234 - Batch 270/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.68 min) Epoch 29/300 -- Iteration 27243 - Batch 279/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.68 min) Epoch 29/300 -- Iteration 27252 - Batch 288/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.69 min) Epoch 29/300 -- Iteration 27261 - Batch 297/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.69 min) Epoch 29/300 -- Iteration 27270 - Batch 306/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.70 min) Epoch 29/300 -- Iteration 27279 - Batch 315/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.70 min) Epoch 29/300 -- Iteration 27288 - Batch 324/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.71 min) Epoch 29/300 -- Iteration 27297 - Batch 333/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.71 min) Epoch 29/300 -- Iteration 27306 - Batch 342/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.71 min) Epoch 29/300 -- Iteration 27315 - Batch 351/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.72 min) Epoch 29/300 -- Iteration 27324 - Batch 360/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.72 min) Epoch 29/300 -- Iteration 27333 - Batch 369/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.73 min) Epoch 29/300 -- Iteration 27342 - Batch 378/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.73 min) Epoch 29/300 -- Iteration 27351 - Batch 387/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.74 min) Epoch 29/300 -- Iteration 27360 - Batch 396/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.74 min) Epoch 29/300 -- Iteration 27369 - Batch 405/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.75 min) Epoch 29/300 -- Iteration 27378 - Batch 414/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.75 min) Epoch 29/300 -- Iteration 27387 - Batch 423/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.75 min) Epoch 29/300 -- Iteration 27396 - Batch 432/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.76 min) Epoch 29/300 -- Iteration 27405 - Batch 441/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.76 min) Epoch 29/300 -- Iteration 27414 - Batch 450/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.77 min) Epoch 29/300 -- Iteration 27423 - Batch 459/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.77 min) Epoch 29/300 -- Iteration 27432 - Batch 468/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.78 min) Epoch 29/300 -- Iteration 27441 - Batch 477/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.78 min) Epoch 29/300 -- Iteration 27450 - Batch 486/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.79 min) Epoch 29/300 -- Iteration 27459 - Batch 495/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.79 min) Epoch 29/300 -- Iteration 27468 - Batch 504/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.80 min) Epoch 29/300 -- Iteration 27477 - Batch 513/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.80 min) Epoch 29/300 -- Iteration 27486 - Batch 522/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.80 min) Epoch 29/300 -- Iteration 27495 - Batch 531/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.81 min) Epoch 29/300 -- Iteration 27504 - Batch 540/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.81 min) Epoch 29/300 -- Iteration 27513 - Batch 549/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.82 min) Epoch 29/300 -- Iteration 27522 - Batch 558/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.82 min) Epoch 29/300 -- Iteration 27531 - Batch 567/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.83 min) Epoch 29/300 -- Iteration 27540 - Batch 576/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.83 min) Epoch 29/300 -- Iteration 27549 - Batch 585/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.84 min) Epoch 29/300 -- Iteration 27558 - Batch 594/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.84 min) Epoch 29/300 -- Iteration 27567 - Batch 603/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.85 min) Epoch 29/300 -- Iteration 27576 - Batch 612/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.85 min) Epoch 29/300 -- Iteration 27585 - Batch 621/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.85 min) Epoch 29/300 -- Iteration 27594 - Batch 630/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.86 min) Epoch 29/300 -- Iteration 27603 - Batch 639/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.86 min) Epoch 29/300 -- Iteration 27612 - Batch 648/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.87 min) Epoch 29/300 -- Iteration 27621 - Batch 657/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.87 min) Epoch 29/300 -- Iteration 27630 - Batch 666/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.88 min) Epoch 29/300 -- Iteration 27639 - Batch 675/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.88 min) Epoch 29/300 -- Iteration 27648 - Batch 684/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.89 min) Epoch 29/300 -- Iteration 27657 - Batch 693/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.89 min) Epoch 29/300 -- Iteration 27666 - Batch 702/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.89 min) Epoch 29/300 -- Iteration 27675 - Batch 711/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.90 min) Epoch 29/300 -- Iteration 27684 - Batch 720/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.90 min) Epoch 29/300 -- Iteration 27693 - Batch 729/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.91 min) Epoch 29/300 -- Iteration 27702 - Batch 738/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.91 min) Epoch 29/300 -- Iteration 27711 - Batch 747/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.92 min) Epoch 29/300 -- Iteration 27720 - Batch 756/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.92 min) Epoch 29/300 -- Iteration 27729 - Batch 765/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.93 min) Epoch 29/300 -- Iteration 27738 - Batch 774/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.93 min) Epoch 29/300 -- Iteration 27747 - Batch 783/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.94 min) Epoch 29/300 -- Iteration 27756 - Batch 792/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.94 min) Epoch 29/300 -- Iteration 27765 - Batch 801/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.95 min) Epoch 29/300 -- Iteration 27774 - Batch 810/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.95 min) Epoch 29/300 -- Iteration 27783 - Batch 819/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.95 min) Epoch 29/300 -- Iteration 27792 - Batch 828/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.96 min) Epoch 29/300 -- Iteration 27801 - Batch 837/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.96 min) Epoch 29/300 -- Iteration 27810 - Batch 846/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.97 min) Epoch 29/300 -- Iteration 27819 - Batch 855/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.97 min) Epoch 29/300 -- Iteration 27828 - Batch 864/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.98 min) Epoch 29/300 -- Iteration 27837 - Batch 873/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.98 min) Epoch 29/300 -- Iteration 27846 - Batch 882/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.99 min) Epoch 29/300 -- Iteration 27855 - Batch 891/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(13.99 min) Epoch 29/300 -- Iteration 27864 - Batch 900/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(14.00 min) Epoch 29/300 -- Iteration 27873 - Batch 909/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(14.00 min) Epoch 29/300 -- Iteration 27882 - Batch 918/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(14.00 min) Epoch 29/300 -- Iteration 27891 - Batch 927/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(14.01 min) Epoch 29/300 -- Iteration 27900 - Batch 936/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(14.01 min) Epoch 29/300 -- Iteration 27909 - Batch 945/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(14.02 min) Epoch 29/300 -- Iteration 27918 - Batch 954/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0029\n",
      "(14.02 min) Epoch 29/300 -- Iteration 27927 - Batch 962/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028 - Val acc: -0.0000\n",
      "(14.03 min) Epoch 30/300 -- Iteration 27936 - Batch 9/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.03 min) Epoch 30/300 -- Iteration 27945 - Batch 18/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.04 min) Epoch 30/300 -- Iteration 27954 - Batch 27/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.04 min) Epoch 30/300 -- Iteration 27963 - Batch 36/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.05 min) Epoch 30/300 -- Iteration 27972 - Batch 45/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.05 min) Epoch 30/300 -- Iteration 27981 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.06 min) Epoch 30/300 -- Iteration 27990 - Batch 63/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.06 min) Epoch 30/300 -- Iteration 27999 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.06 min) Epoch 30/300 -- Iteration 28008 - Batch 81/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.07 min) Epoch 30/300 -- Iteration 28017 - Batch 90/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.07 min) Epoch 30/300 -- Iteration 28026 - Batch 99/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.08 min) Epoch 30/300 -- Iteration 28035 - Batch 108/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.08 min) Epoch 30/300 -- Iteration 28044 - Batch 117/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.09 min) Epoch 30/300 -- Iteration 28053 - Batch 126/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.09 min) Epoch 30/300 -- Iteration 28062 - Batch 135/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.10 min) Epoch 30/300 -- Iteration 28071 - Batch 144/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.10 min) Epoch 30/300 -- Iteration 28080 - Batch 153/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.10 min) Epoch 30/300 -- Iteration 28089 - Batch 162/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.11 min) Epoch 30/300 -- Iteration 28098 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.11 min) Epoch 30/300 -- Iteration 28107 - Batch 180/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.12 min) Epoch 30/300 -- Iteration 28116 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.12 min) Epoch 30/300 -- Iteration 28125 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.13 min) Epoch 30/300 -- Iteration 28134 - Batch 207/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.13 min) Epoch 30/300 -- Iteration 28143 - Batch 216/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.14 min) Epoch 30/300 -- Iteration 28152 - Batch 225/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.14 min) Epoch 30/300 -- Iteration 28161 - Batch 234/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.15 min) Epoch 30/300 -- Iteration 28170 - Batch 243/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.15 min) Epoch 30/300 -- Iteration 28179 - Batch 252/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.15 min) Epoch 30/300 -- Iteration 28188 - Batch 261/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.16 min) Epoch 30/300 -- Iteration 28197 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.16 min) Epoch 30/300 -- Iteration 28206 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.17 min) Epoch 30/300 -- Iteration 28215 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.17 min) Epoch 30/300 -- Iteration 28224 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.18 min) Epoch 30/300 -- Iteration 28233 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.18 min) Epoch 30/300 -- Iteration 28242 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.19 min) Epoch 30/300 -- Iteration 28251 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.19 min) Epoch 30/300 -- Iteration 28260 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.19 min) Epoch 30/300 -- Iteration 28269 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.20 min) Epoch 30/300 -- Iteration 28278 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.20 min) Epoch 30/300 -- Iteration 28287 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.21 min) Epoch 30/300 -- Iteration 28296 - Batch 369/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.21 min) Epoch 30/300 -- Iteration 28305 - Batch 378/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.22 min) Epoch 30/300 -- Iteration 28314 - Batch 387/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.22 min) Epoch 30/300 -- Iteration 28323 - Batch 396/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.23 min) Epoch 30/300 -- Iteration 28332 - Batch 405/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.23 min) Epoch 30/300 -- Iteration 28341 - Batch 414/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.23 min) Epoch 30/300 -- Iteration 28350 - Batch 423/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.24 min) Epoch 30/300 -- Iteration 28359 - Batch 432/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.24 min) Epoch 30/300 -- Iteration 28368 - Batch 441/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.25 min) Epoch 30/300 -- Iteration 28377 - Batch 450/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.25 min) Epoch 30/300 -- Iteration 28386 - Batch 459/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.26 min) Epoch 30/300 -- Iteration 28395 - Batch 468/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.26 min) Epoch 30/300 -- Iteration 28404 - Batch 477/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.27 min) Epoch 30/300 -- Iteration 28413 - Batch 486/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.27 min) Epoch 30/300 -- Iteration 28422 - Batch 495/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.27 min) Epoch 30/300 -- Iteration 28431 - Batch 504/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.28 min) Epoch 30/300 -- Iteration 28440 - Batch 513/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.28 min) Epoch 30/300 -- Iteration 28449 - Batch 522/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.29 min) Epoch 30/300 -- Iteration 28458 - Batch 531/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.29 min) Epoch 30/300 -- Iteration 28467 - Batch 540/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.30 min) Epoch 30/300 -- Iteration 28476 - Batch 549/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.30 min) Epoch 30/300 -- Iteration 28485 - Batch 558/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.30 min) Epoch 30/300 -- Iteration 28494 - Batch 567/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.31 min) Epoch 30/300 -- Iteration 28503 - Batch 576/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.31 min) Epoch 30/300 -- Iteration 28512 - Batch 585/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.32 min) Epoch 30/300 -- Iteration 28521 - Batch 594/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.32 min) Epoch 30/300 -- Iteration 28530 - Batch 603/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.33 min) Epoch 30/300 -- Iteration 28539 - Batch 612/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.33 min) Epoch 30/300 -- Iteration 28548 - Batch 621/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.34 min) Epoch 30/300 -- Iteration 28557 - Batch 630/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.34 min) Epoch 30/300 -- Iteration 28566 - Batch 639/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.34 min) Epoch 30/300 -- Iteration 28575 - Batch 648/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.35 min) Epoch 30/300 -- Iteration 28584 - Batch 657/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.35 min) Epoch 30/300 -- Iteration 28593 - Batch 666/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.36 min) Epoch 30/300 -- Iteration 28602 - Batch 675/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.36 min) Epoch 30/300 -- Iteration 28611 - Batch 684/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.37 min) Epoch 30/300 -- Iteration 28620 - Batch 693/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.37 min) Epoch 30/300 -- Iteration 28629 - Batch 702/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.38 min) Epoch 30/300 -- Iteration 28638 - Batch 711/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.38 min) Epoch 30/300 -- Iteration 28647 - Batch 720/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.39 min) Epoch 30/300 -- Iteration 28656 - Batch 729/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.39 min) Epoch 30/300 -- Iteration 28665 - Batch 738/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.39 min) Epoch 30/300 -- Iteration 28674 - Batch 747/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.40 min) Epoch 30/300 -- Iteration 28683 - Batch 756/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.40 min) Epoch 30/300 -- Iteration 28692 - Batch 765/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.41 min) Epoch 30/300 -- Iteration 28701 - Batch 774/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.41 min) Epoch 30/300 -- Iteration 28710 - Batch 783/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.42 min) Epoch 30/300 -- Iteration 28719 - Batch 792/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.42 min) Epoch 30/300 -- Iteration 28728 - Batch 801/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.42 min) Epoch 30/300 -- Iteration 28737 - Batch 810/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.43 min) Epoch 30/300 -- Iteration 28746 - Batch 819/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.43 min) Epoch 30/300 -- Iteration 28755 - Batch 828/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.44 min) Epoch 30/300 -- Iteration 28764 - Batch 837/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.44 min) Epoch 30/300 -- Iteration 28773 - Batch 846/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.45 min) Epoch 30/300 -- Iteration 28782 - Batch 855/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.45 min) Epoch 30/300 -- Iteration 28791 - Batch 864/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.46 min) Epoch 30/300 -- Iteration 28800 - Batch 873/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.46 min) Epoch 30/300 -- Iteration 28809 - Batch 882/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.46 min) Epoch 30/300 -- Iteration 28818 - Batch 891/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.47 min) Epoch 30/300 -- Iteration 28827 - Batch 900/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.47 min) Epoch 30/300 -- Iteration 28836 - Batch 909/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.48 min) Epoch 30/300 -- Iteration 28845 - Batch 918/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.48 min) Epoch 30/300 -- Iteration 28854 - Batch 927/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.49 min) Epoch 30/300 -- Iteration 28863 - Batch 936/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.49 min) Epoch 30/300 -- Iteration 28872 - Batch 945/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.50 min) Epoch 30/300 -- Iteration 28881 - Batch 954/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.50 min) Epoch 30/300 -- Iteration 28890 - Batch 962/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027 - Val acc: -0.0000\n",
      "(14.51 min) Epoch 31/300 -- Iteration 28899 - Batch 9/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.51 min) Epoch 31/300 -- Iteration 28908 - Batch 18/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.51 min) Epoch 31/300 -- Iteration 28917 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.52 min) Epoch 31/300 -- Iteration 28926 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.52 min) Epoch 31/300 -- Iteration 28935 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.53 min) Epoch 31/300 -- Iteration 28944 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.53 min) Epoch 31/300 -- Iteration 28953 - Batch 63/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.54 min) Epoch 31/300 -- Iteration 28962 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.54 min) Epoch 31/300 -- Iteration 28971 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.55 min) Epoch 31/300 -- Iteration 28980 - Batch 90/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.55 min) Epoch 31/300 -- Iteration 28989 - Batch 99/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.56 min) Epoch 31/300 -- Iteration 28998 - Batch 108/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.56 min) Epoch 31/300 -- Iteration 29007 - Batch 117/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.56 min) Epoch 31/300 -- Iteration 29016 - Batch 126/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.57 min) Epoch 31/300 -- Iteration 29025 - Batch 135/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.57 min) Epoch 31/300 -- Iteration 29034 - Batch 144/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.58 min) Epoch 31/300 -- Iteration 29043 - Batch 153/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.58 min) Epoch 31/300 -- Iteration 29052 - Batch 162/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.59 min) Epoch 31/300 -- Iteration 29061 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.59 min) Epoch 31/300 -- Iteration 29070 - Batch 180/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.59 min) Epoch 31/300 -- Iteration 29079 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.60 min) Epoch 31/300 -- Iteration 29088 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.60 min) Epoch 31/300 -- Iteration 29097 - Batch 207/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.61 min) Epoch 31/300 -- Iteration 29106 - Batch 216/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.61 min) Epoch 31/300 -- Iteration 29115 - Batch 225/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.62 min) Epoch 31/300 -- Iteration 29124 - Batch 234/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.62 min) Epoch 31/300 -- Iteration 29133 - Batch 243/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.63 min) Epoch 31/300 -- Iteration 29142 - Batch 252/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.63 min) Epoch 31/300 -- Iteration 29151 - Batch 261/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.63 min) Epoch 31/300 -- Iteration 29160 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.64 min) Epoch 31/300 -- Iteration 29169 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.64 min) Epoch 31/300 -- Iteration 29178 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.65 min) Epoch 31/300 -- Iteration 29187 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.65 min) Epoch 31/300 -- Iteration 29196 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.66 min) Epoch 31/300 -- Iteration 29205 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.66 min) Epoch 31/300 -- Iteration 29214 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.67 min) Epoch 31/300 -- Iteration 29223 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.67 min) Epoch 31/300 -- Iteration 29232 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.67 min) Epoch 31/300 -- Iteration 29241 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.68 min) Epoch 31/300 -- Iteration 29250 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.68 min) Epoch 31/300 -- Iteration 29259 - Batch 369/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.69 min) Epoch 31/300 -- Iteration 29268 - Batch 378/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.69 min) Epoch 31/300 -- Iteration 29277 - Batch 387/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.70 min) Epoch 31/300 -- Iteration 29286 - Batch 396/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.70 min) Epoch 31/300 -- Iteration 29295 - Batch 405/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.71 min) Epoch 31/300 -- Iteration 29304 - Batch 414/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.71 min) Epoch 31/300 -- Iteration 29313 - Batch 423/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.72 min) Epoch 31/300 -- Iteration 29322 - Batch 432/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.72 min) Epoch 31/300 -- Iteration 29331 - Batch 441/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.72 min) Epoch 31/300 -- Iteration 29340 - Batch 450/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.73 min) Epoch 31/300 -- Iteration 29349 - Batch 459/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.73 min) Epoch 31/300 -- Iteration 29358 - Batch 468/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.74 min) Epoch 31/300 -- Iteration 29367 - Batch 477/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.74 min) Epoch 31/300 -- Iteration 29376 - Batch 486/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.75 min) Epoch 31/300 -- Iteration 29385 - Batch 495/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.75 min) Epoch 31/300 -- Iteration 29394 - Batch 504/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.76 min) Epoch 31/300 -- Iteration 29403 - Batch 513/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.76 min) Epoch 31/300 -- Iteration 29412 - Batch 522/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.77 min) Epoch 31/300 -- Iteration 29421 - Batch 531/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.77 min) Epoch 31/300 -- Iteration 29430 - Batch 540/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.78 min) Epoch 31/300 -- Iteration 29439 - Batch 549/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.78 min) Epoch 31/300 -- Iteration 29448 - Batch 558/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.78 min) Epoch 31/300 -- Iteration 29457 - Batch 567/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.79 min) Epoch 31/300 -- Iteration 29466 - Batch 576/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.79 min) Epoch 31/300 -- Iteration 29475 - Batch 585/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.80 min) Epoch 31/300 -- Iteration 29484 - Batch 594/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.80 min) Epoch 31/300 -- Iteration 29493 - Batch 603/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.81 min) Epoch 31/300 -- Iteration 29502 - Batch 612/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.81 min) Epoch 31/300 -- Iteration 29511 - Batch 621/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.82 min) Epoch 31/300 -- Iteration 29520 - Batch 630/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.82 min) Epoch 31/300 -- Iteration 29529 - Batch 639/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.82 min) Epoch 31/300 -- Iteration 29538 - Batch 648/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.83 min) Epoch 31/300 -- Iteration 29547 - Batch 657/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.83 min) Epoch 31/300 -- Iteration 29556 - Batch 666/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.84 min) Epoch 31/300 -- Iteration 29565 - Batch 675/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.84 min) Epoch 31/300 -- Iteration 29574 - Batch 684/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.85 min) Epoch 31/300 -- Iteration 29583 - Batch 693/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.85 min) Epoch 31/300 -- Iteration 29592 - Batch 702/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.86 min) Epoch 31/300 -- Iteration 29601 - Batch 711/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.86 min) Epoch 31/300 -- Iteration 29610 - Batch 720/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.86 min) Epoch 31/300 -- Iteration 29619 - Batch 729/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.87 min) Epoch 31/300 -- Iteration 29628 - Batch 738/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.87 min) Epoch 31/300 -- Iteration 29637 - Batch 747/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.88 min) Epoch 31/300 -- Iteration 29646 - Batch 756/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.88 min) Epoch 31/300 -- Iteration 29655 - Batch 765/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.89 min) Epoch 31/300 -- Iteration 29664 - Batch 774/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.89 min) Epoch 31/300 -- Iteration 29673 - Batch 783/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.90 min) Epoch 31/300 -- Iteration 29682 - Batch 792/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.90 min) Epoch 31/300 -- Iteration 29691 - Batch 801/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.91 min) Epoch 31/300 -- Iteration 29700 - Batch 810/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.91 min) Epoch 31/300 -- Iteration 29709 - Batch 819/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.91 min) Epoch 31/300 -- Iteration 29718 - Batch 828/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.92 min) Epoch 31/300 -- Iteration 29727 - Batch 837/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.92 min) Epoch 31/300 -- Iteration 29736 - Batch 846/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.93 min) Epoch 31/300 -- Iteration 29745 - Batch 855/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.93 min) Epoch 31/300 -- Iteration 29754 - Batch 864/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.94 min) Epoch 31/300 -- Iteration 29763 - Batch 873/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.94 min) Epoch 31/300 -- Iteration 29772 - Batch 882/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.94 min) Epoch 31/300 -- Iteration 29781 - Batch 891/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.95 min) Epoch 31/300 -- Iteration 29790 - Batch 900/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.95 min) Epoch 31/300 -- Iteration 29799 - Batch 909/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.96 min) Epoch 31/300 -- Iteration 29808 - Batch 918/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.96 min) Epoch 31/300 -- Iteration 29817 - Batch 927/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.97 min) Epoch 31/300 -- Iteration 29826 - Batch 936/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.97 min) Epoch 31/300 -- Iteration 29835 - Batch 945/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.98 min) Epoch 31/300 -- Iteration 29844 - Batch 954/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(14.98 min) Epoch 31/300 -- Iteration 29853 - Batch 962/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028 - Val acc: -0.0000\n",
      "(14.99 min) Epoch 32/300 -- Iteration 29862 - Batch 9/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.99 min) Epoch 32/300 -- Iteration 29871 - Batch 18/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(14.99 min) Epoch 32/300 -- Iteration 29880 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.00 min) Epoch 32/300 -- Iteration 29889 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.00 min) Epoch 32/300 -- Iteration 29898 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.01 min) Epoch 32/300 -- Iteration 29907 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.01 min) Epoch 32/300 -- Iteration 29916 - Batch 63/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.02 min) Epoch 32/300 -- Iteration 29925 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.02 min) Epoch 32/300 -- Iteration 29934 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.03 min) Epoch 32/300 -- Iteration 29943 - Batch 90/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.03 min) Epoch 32/300 -- Iteration 29952 - Batch 99/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.03 min) Epoch 32/300 -- Iteration 29961 - Batch 108/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.04 min) Epoch 32/300 -- Iteration 29970 - Batch 117/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.04 min) Epoch 32/300 -- Iteration 29979 - Batch 126/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.05 min) Epoch 32/300 -- Iteration 29988 - Batch 135/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.05 min) Epoch 32/300 -- Iteration 29997 - Batch 144/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.06 min) Epoch 32/300 -- Iteration 30006 - Batch 153/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.06 min) Epoch 32/300 -- Iteration 30015 - Batch 162/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.07 min) Epoch 32/300 -- Iteration 30024 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.07 min) Epoch 32/300 -- Iteration 30033 - Batch 180/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.07 min) Epoch 32/300 -- Iteration 30042 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.08 min) Epoch 32/300 -- Iteration 30051 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.08 min) Epoch 32/300 -- Iteration 30060 - Batch 207/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.09 min) Epoch 32/300 -- Iteration 30069 - Batch 216/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.09 min) Epoch 32/300 -- Iteration 30078 - Batch 225/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.10 min) Epoch 32/300 -- Iteration 30087 - Batch 234/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.10 min) Epoch 32/300 -- Iteration 30096 - Batch 243/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.11 min) Epoch 32/300 -- Iteration 30105 - Batch 252/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.11 min) Epoch 32/300 -- Iteration 30114 - Batch 261/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.11 min) Epoch 32/300 -- Iteration 30123 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.12 min) Epoch 32/300 -- Iteration 30132 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.12 min) Epoch 32/300 -- Iteration 30141 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.13 min) Epoch 32/300 -- Iteration 30150 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.13 min) Epoch 32/300 -- Iteration 30159 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.14 min) Epoch 32/300 -- Iteration 30168 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.14 min) Epoch 32/300 -- Iteration 30177 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.15 min) Epoch 32/300 -- Iteration 30186 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.15 min) Epoch 32/300 -- Iteration 30195 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.16 min) Epoch 32/300 -- Iteration 30204 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.16 min) Epoch 32/300 -- Iteration 30213 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.16 min) Epoch 32/300 -- Iteration 30222 - Batch 369/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.17 min) Epoch 32/300 -- Iteration 30231 - Batch 378/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.17 min) Epoch 32/300 -- Iteration 30240 - Batch 387/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.18 min) Epoch 32/300 -- Iteration 30249 - Batch 396/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.18 min) Epoch 32/300 -- Iteration 30258 - Batch 405/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.19 min) Epoch 32/300 -- Iteration 30267 - Batch 414/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.19 min) Epoch 32/300 -- Iteration 30276 - Batch 423/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.20 min) Epoch 32/300 -- Iteration 30285 - Batch 432/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.20 min) Epoch 32/300 -- Iteration 30294 - Batch 441/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.20 min) Epoch 32/300 -- Iteration 30303 - Batch 450/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.21 min) Epoch 32/300 -- Iteration 30312 - Batch 459/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.21 min) Epoch 32/300 -- Iteration 30321 - Batch 468/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.22 min) Epoch 32/300 -- Iteration 30330 - Batch 477/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.22 min) Epoch 32/300 -- Iteration 30339 - Batch 486/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.23 min) Epoch 32/300 -- Iteration 30348 - Batch 495/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.23 min) Epoch 32/300 -- Iteration 30357 - Batch 504/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.24 min) Epoch 32/300 -- Iteration 30366 - Batch 513/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.24 min) Epoch 32/300 -- Iteration 30375 - Batch 522/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.24 min) Epoch 32/300 -- Iteration 30384 - Batch 531/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.25 min) Epoch 32/300 -- Iteration 30393 - Batch 540/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.25 min) Epoch 32/300 -- Iteration 30402 - Batch 549/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.26 min) Epoch 32/300 -- Iteration 30411 - Batch 558/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.26 min) Epoch 32/300 -- Iteration 30420 - Batch 567/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.27 min) Epoch 32/300 -- Iteration 30429 - Batch 576/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.27 min) Epoch 32/300 -- Iteration 30438 - Batch 585/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.27 min) Epoch 32/300 -- Iteration 30447 - Batch 594/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.28 min) Epoch 32/300 -- Iteration 30456 - Batch 603/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.28 min) Epoch 32/300 -- Iteration 30465 - Batch 612/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.29 min) Epoch 32/300 -- Iteration 30474 - Batch 621/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.29 min) Epoch 32/300 -- Iteration 30483 - Batch 630/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.30 min) Epoch 32/300 -- Iteration 30492 - Batch 639/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.30 min) Epoch 32/300 -- Iteration 30501 - Batch 648/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.31 min) Epoch 32/300 -- Iteration 30510 - Batch 657/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.31 min) Epoch 32/300 -- Iteration 30519 - Batch 666/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.32 min) Epoch 32/300 -- Iteration 30528 - Batch 675/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.32 min) Epoch 32/300 -- Iteration 30537 - Batch 684/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.32 min) Epoch 32/300 -- Iteration 30546 - Batch 693/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.33 min) Epoch 32/300 -- Iteration 30555 - Batch 702/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.33 min) Epoch 32/300 -- Iteration 30564 - Batch 711/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.34 min) Epoch 32/300 -- Iteration 30573 - Batch 720/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.34 min) Epoch 32/300 -- Iteration 30582 - Batch 729/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.35 min) Epoch 32/300 -- Iteration 30591 - Batch 738/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.35 min) Epoch 32/300 -- Iteration 30600 - Batch 747/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.36 min) Epoch 32/300 -- Iteration 30609 - Batch 756/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.36 min) Epoch 32/300 -- Iteration 30618 - Batch 765/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.36 min) Epoch 32/300 -- Iteration 30627 - Batch 774/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.37 min) Epoch 32/300 -- Iteration 30636 - Batch 783/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.37 min) Epoch 32/300 -- Iteration 30645 - Batch 792/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.38 min) Epoch 32/300 -- Iteration 30654 - Batch 801/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.38 min) Epoch 32/300 -- Iteration 30663 - Batch 810/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.39 min) Epoch 32/300 -- Iteration 30672 - Batch 819/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.39 min) Epoch 32/300 -- Iteration 30681 - Batch 828/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.40 min) Epoch 32/300 -- Iteration 30690 - Batch 837/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.40 min) Epoch 32/300 -- Iteration 30699 - Batch 846/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.40 min) Epoch 32/300 -- Iteration 30708 - Batch 855/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.41 min) Epoch 32/300 -- Iteration 30717 - Batch 864/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.41 min) Epoch 32/300 -- Iteration 30726 - Batch 873/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.42 min) Epoch 32/300 -- Iteration 30735 - Batch 882/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.42 min) Epoch 32/300 -- Iteration 30744 - Batch 891/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.43 min) Epoch 32/300 -- Iteration 30753 - Batch 900/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.43 min) Epoch 32/300 -- Iteration 30762 - Batch 909/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.43 min) Epoch 32/300 -- Iteration 30771 - Batch 918/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.44 min) Epoch 32/300 -- Iteration 30780 - Batch 927/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.44 min) Epoch 32/300 -- Iteration 30789 - Batch 936/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.45 min) Epoch 32/300 -- Iteration 30798 - Batch 945/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.45 min) Epoch 32/300 -- Iteration 30807 - Batch 954/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.46 min) Epoch 32/300 -- Iteration 30816 - Batch 962/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028 - Val acc: -0.0000\n",
      "(15.46 min) Epoch 33/300 -- Iteration 30825 - Batch 9/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.47 min) Epoch 33/300 -- Iteration 30834 - Batch 18/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.47 min) Epoch 33/300 -- Iteration 30843 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.48 min) Epoch 33/300 -- Iteration 30852 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.48 min) Epoch 33/300 -- Iteration 30861 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.48 min) Epoch 33/300 -- Iteration 30870 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.49 min) Epoch 33/300 -- Iteration 30879 - Batch 63/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.49 min) Epoch 33/300 -- Iteration 30888 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.50 min) Epoch 33/300 -- Iteration 30897 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.50 min) Epoch 33/300 -- Iteration 30906 - Batch 90/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.51 min) Epoch 33/300 -- Iteration 30915 - Batch 99/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.51 min) Epoch 33/300 -- Iteration 30924 - Batch 108/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.52 min) Epoch 33/300 -- Iteration 30933 - Batch 117/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.52 min) Epoch 33/300 -- Iteration 30942 - Batch 126/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.53 min) Epoch 33/300 -- Iteration 30951 - Batch 135/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.53 min) Epoch 33/300 -- Iteration 30960 - Batch 144/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.54 min) Epoch 33/300 -- Iteration 30969 - Batch 153/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.54 min) Epoch 33/300 -- Iteration 30978 - Batch 162/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.54 min) Epoch 33/300 -- Iteration 30987 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.55 min) Epoch 33/300 -- Iteration 30996 - Batch 180/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.55 min) Epoch 33/300 -- Iteration 31005 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.56 min) Epoch 33/300 -- Iteration 31014 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.56 min) Epoch 33/300 -- Iteration 31023 - Batch 207/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.57 min) Epoch 33/300 -- Iteration 31032 - Batch 216/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.57 min) Epoch 33/300 -- Iteration 31041 - Batch 225/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.58 min) Epoch 33/300 -- Iteration 31050 - Batch 234/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.58 min) Epoch 33/300 -- Iteration 31059 - Batch 243/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.59 min) Epoch 33/300 -- Iteration 31068 - Batch 252/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.59 min) Epoch 33/300 -- Iteration 31077 - Batch 261/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.59 min) Epoch 33/300 -- Iteration 31086 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.60 min) Epoch 33/300 -- Iteration 31095 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.60 min) Epoch 33/300 -- Iteration 31104 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.61 min) Epoch 33/300 -- Iteration 31113 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.61 min) Epoch 33/300 -- Iteration 31122 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.62 min) Epoch 33/300 -- Iteration 31131 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.62 min) Epoch 33/300 -- Iteration 31140 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.62 min) Epoch 33/300 -- Iteration 31149 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.63 min) Epoch 33/300 -- Iteration 31158 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.63 min) Epoch 33/300 -- Iteration 31167 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.64 min) Epoch 33/300 -- Iteration 31176 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.64 min) Epoch 33/300 -- Iteration 31185 - Batch 369/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.65 min) Epoch 33/300 -- Iteration 31194 - Batch 378/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.65 min) Epoch 33/300 -- Iteration 31203 - Batch 387/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.66 min) Epoch 33/300 -- Iteration 31212 - Batch 396/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.66 min) Epoch 33/300 -- Iteration 31221 - Batch 405/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.66 min) Epoch 33/300 -- Iteration 31230 - Batch 414/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.67 min) Epoch 33/300 -- Iteration 31239 - Batch 423/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.67 min) Epoch 33/300 -- Iteration 31248 - Batch 432/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.68 min) Epoch 33/300 -- Iteration 31257 - Batch 441/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.68 min) Epoch 33/300 -- Iteration 31266 - Batch 450/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.69 min) Epoch 33/300 -- Iteration 31275 - Batch 459/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.69 min) Epoch 33/300 -- Iteration 31284 - Batch 468/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.70 min) Epoch 33/300 -- Iteration 31293 - Batch 477/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.70 min) Epoch 33/300 -- Iteration 31302 - Batch 486/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.71 min) Epoch 33/300 -- Iteration 31311 - Batch 495/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.71 min) Epoch 33/300 -- Iteration 31320 - Batch 504/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.72 min) Epoch 33/300 -- Iteration 31329 - Batch 513/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.72 min) Epoch 33/300 -- Iteration 31338 - Batch 522/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.73 min) Epoch 33/300 -- Iteration 31347 - Batch 531/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.73 min) Epoch 33/300 -- Iteration 31356 - Batch 540/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.73 min) Epoch 33/300 -- Iteration 31365 - Batch 549/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.74 min) Epoch 33/300 -- Iteration 31374 - Batch 558/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.74 min) Epoch 33/300 -- Iteration 31383 - Batch 567/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.75 min) Epoch 33/300 -- Iteration 31392 - Batch 576/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.75 min) Epoch 33/300 -- Iteration 31401 - Batch 585/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.76 min) Epoch 33/300 -- Iteration 31410 - Batch 594/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.76 min) Epoch 33/300 -- Iteration 31419 - Batch 603/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.77 min) Epoch 33/300 -- Iteration 31428 - Batch 612/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.77 min) Epoch 33/300 -- Iteration 31437 - Batch 621/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.78 min) Epoch 33/300 -- Iteration 31446 - Batch 630/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.78 min) Epoch 33/300 -- Iteration 31455 - Batch 639/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.79 min) Epoch 33/300 -- Iteration 31464 - Batch 648/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.79 min) Epoch 33/300 -- Iteration 31473 - Batch 657/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.80 min) Epoch 33/300 -- Iteration 31482 - Batch 666/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.80 min) Epoch 33/300 -- Iteration 31491 - Batch 675/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.81 min) Epoch 33/300 -- Iteration 31500 - Batch 684/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.81 min) Epoch 33/300 -- Iteration 31509 - Batch 693/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.82 min) Epoch 33/300 -- Iteration 31518 - Batch 702/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.82 min) Epoch 33/300 -- Iteration 31527 - Batch 711/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.83 min) Epoch 33/300 -- Iteration 31536 - Batch 720/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.83 min) Epoch 33/300 -- Iteration 31545 - Batch 729/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.83 min) Epoch 33/300 -- Iteration 31554 - Batch 738/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.84 min) Epoch 33/300 -- Iteration 31563 - Batch 747/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.84 min) Epoch 33/300 -- Iteration 31572 - Batch 756/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.85 min) Epoch 33/300 -- Iteration 31581 - Batch 765/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.85 min) Epoch 33/300 -- Iteration 31590 - Batch 774/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.86 min) Epoch 33/300 -- Iteration 31599 - Batch 783/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.86 min) Epoch 33/300 -- Iteration 31608 - Batch 792/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.87 min) Epoch 33/300 -- Iteration 31617 - Batch 801/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.87 min) Epoch 33/300 -- Iteration 31626 - Batch 810/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.87 min) Epoch 33/300 -- Iteration 31635 - Batch 819/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.88 min) Epoch 33/300 -- Iteration 31644 - Batch 828/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.88 min) Epoch 33/300 -- Iteration 31653 - Batch 837/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.89 min) Epoch 33/300 -- Iteration 31662 - Batch 846/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.89 min) Epoch 33/300 -- Iteration 31671 - Batch 855/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.90 min) Epoch 33/300 -- Iteration 31680 - Batch 864/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.90 min) Epoch 33/300 -- Iteration 31689 - Batch 873/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.91 min) Epoch 33/300 -- Iteration 31698 - Batch 882/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.91 min) Epoch 33/300 -- Iteration 31707 - Batch 891/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.92 min) Epoch 33/300 -- Iteration 31716 - Batch 900/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.92 min) Epoch 33/300 -- Iteration 31725 - Batch 909/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.93 min) Epoch 33/300 -- Iteration 31734 - Batch 918/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.93 min) Epoch 33/300 -- Iteration 31743 - Batch 927/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.94 min) Epoch 33/300 -- Iteration 31752 - Batch 936/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.94 min) Epoch 33/300 -- Iteration 31761 - Batch 945/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.95 min) Epoch 33/300 -- Iteration 31770 - Batch 954/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0028\n",
      "(15.95 min) Epoch 33/300 -- Iteration 31779 - Batch 962/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026 - Val acc: -0.0000\n",
      "(15.96 min) Epoch 34/300 -- Iteration 31788 - Batch 9/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(15.96 min) Epoch 34/300 -- Iteration 31797 - Batch 18/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(15.97 min) Epoch 34/300 -- Iteration 31806 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(15.97 min) Epoch 34/300 -- Iteration 31815 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(15.98 min) Epoch 34/300 -- Iteration 31824 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(15.98 min) Epoch 34/300 -- Iteration 31833 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(15.98 min) Epoch 34/300 -- Iteration 31842 - Batch 63/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(15.99 min) Epoch 34/300 -- Iteration 31851 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(15.99 min) Epoch 34/300 -- Iteration 31860 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.00 min) Epoch 34/300 -- Iteration 31869 - Batch 90/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.00 min) Epoch 34/300 -- Iteration 31878 - Batch 99/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.01 min) Epoch 34/300 -- Iteration 31887 - Batch 108/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.01 min) Epoch 34/300 -- Iteration 31896 - Batch 117/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.02 min) Epoch 34/300 -- Iteration 31905 - Batch 126/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.02 min) Epoch 34/300 -- Iteration 31914 - Batch 135/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.03 min) Epoch 34/300 -- Iteration 31923 - Batch 144/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.03 min) Epoch 34/300 -- Iteration 31932 - Batch 153/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.04 min) Epoch 34/300 -- Iteration 31941 - Batch 162/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.04 min) Epoch 34/300 -- Iteration 31950 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.04 min) Epoch 34/300 -- Iteration 31959 - Batch 180/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.05 min) Epoch 34/300 -- Iteration 31968 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.05 min) Epoch 34/300 -- Iteration 31977 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.06 min) Epoch 34/300 -- Iteration 31986 - Batch 207/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.06 min) Epoch 34/300 -- Iteration 31995 - Batch 216/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.07 min) Epoch 34/300 -- Iteration 32004 - Batch 225/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.07 min) Epoch 34/300 -- Iteration 32013 - Batch 234/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.08 min) Epoch 34/300 -- Iteration 32022 - Batch 243/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.08 min) Epoch 34/300 -- Iteration 32031 - Batch 252/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.09 min) Epoch 34/300 -- Iteration 32040 - Batch 261/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.09 min) Epoch 34/300 -- Iteration 32049 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.09 min) Epoch 34/300 -- Iteration 32058 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.10 min) Epoch 34/300 -- Iteration 32067 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.10 min) Epoch 34/300 -- Iteration 32076 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.11 min) Epoch 34/300 -- Iteration 32085 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.11 min) Epoch 34/300 -- Iteration 32094 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.12 min) Epoch 34/300 -- Iteration 32103 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.12 min) Epoch 34/300 -- Iteration 32112 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.13 min) Epoch 34/300 -- Iteration 32121 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.13 min) Epoch 34/300 -- Iteration 32130 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.13 min) Epoch 34/300 -- Iteration 32139 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.14 min) Epoch 34/300 -- Iteration 32148 - Batch 369/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.14 min) Epoch 34/300 -- Iteration 32157 - Batch 378/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.15 min) Epoch 34/300 -- Iteration 32166 - Batch 387/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.15 min) Epoch 34/300 -- Iteration 32175 - Batch 396/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.16 min) Epoch 34/300 -- Iteration 32184 - Batch 405/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.16 min) Epoch 34/300 -- Iteration 32193 - Batch 414/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.17 min) Epoch 34/300 -- Iteration 32202 - Batch 423/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.17 min) Epoch 34/300 -- Iteration 32211 - Batch 432/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.18 min) Epoch 34/300 -- Iteration 32220 - Batch 441/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.18 min) Epoch 34/300 -- Iteration 32229 - Batch 450/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.19 min) Epoch 34/300 -- Iteration 32238 - Batch 459/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.19 min) Epoch 34/300 -- Iteration 32247 - Batch 468/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.20 min) Epoch 34/300 -- Iteration 32256 - Batch 477/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.20 min) Epoch 34/300 -- Iteration 32265 - Batch 486/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.20 min) Epoch 34/300 -- Iteration 32274 - Batch 495/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.21 min) Epoch 34/300 -- Iteration 32283 - Batch 504/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.21 min) Epoch 34/300 -- Iteration 32292 - Batch 513/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.22 min) Epoch 34/300 -- Iteration 32301 - Batch 522/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.22 min) Epoch 34/300 -- Iteration 32310 - Batch 531/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.23 min) Epoch 34/300 -- Iteration 32319 - Batch 540/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.23 min) Epoch 34/300 -- Iteration 32328 - Batch 549/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.24 min) Epoch 34/300 -- Iteration 32337 - Batch 558/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.24 min) Epoch 34/300 -- Iteration 32346 - Batch 567/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.25 min) Epoch 34/300 -- Iteration 32355 - Batch 576/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.25 min) Epoch 34/300 -- Iteration 32364 - Batch 585/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.26 min) Epoch 34/300 -- Iteration 32373 - Batch 594/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.26 min) Epoch 34/300 -- Iteration 32382 - Batch 603/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.26 min) Epoch 34/300 -- Iteration 32391 - Batch 612/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.27 min) Epoch 34/300 -- Iteration 32400 - Batch 621/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.27 min) Epoch 34/300 -- Iteration 32409 - Batch 630/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.28 min) Epoch 34/300 -- Iteration 32418 - Batch 639/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.28 min) Epoch 34/300 -- Iteration 32427 - Batch 648/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.29 min) Epoch 34/300 -- Iteration 32436 - Batch 657/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.29 min) Epoch 34/300 -- Iteration 32445 - Batch 666/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.30 min) Epoch 34/300 -- Iteration 32454 - Batch 675/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.30 min) Epoch 34/300 -- Iteration 32463 - Batch 684/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.30 min) Epoch 34/300 -- Iteration 32472 - Batch 693/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.31 min) Epoch 34/300 -- Iteration 32481 - Batch 702/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.31 min) Epoch 34/300 -- Iteration 32490 - Batch 711/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.32 min) Epoch 34/300 -- Iteration 32499 - Batch 720/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.32 min) Epoch 34/300 -- Iteration 32508 - Batch 729/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.33 min) Epoch 34/300 -- Iteration 32517 - Batch 738/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.33 min) Epoch 34/300 -- Iteration 32526 - Batch 747/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.34 min) Epoch 34/300 -- Iteration 32535 - Batch 756/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.34 min) Epoch 34/300 -- Iteration 32544 - Batch 765/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.34 min) Epoch 34/300 -- Iteration 32553 - Batch 774/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.35 min) Epoch 34/300 -- Iteration 32562 - Batch 783/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.35 min) Epoch 34/300 -- Iteration 32571 - Batch 792/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.36 min) Epoch 34/300 -- Iteration 32580 - Batch 801/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.36 min) Epoch 34/300 -- Iteration 32589 - Batch 810/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.37 min) Epoch 34/300 -- Iteration 32598 - Batch 819/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.37 min) Epoch 34/300 -- Iteration 32607 - Batch 828/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.38 min) Epoch 34/300 -- Iteration 32616 - Batch 837/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.38 min) Epoch 34/300 -- Iteration 32625 - Batch 846/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.38 min) Epoch 34/300 -- Iteration 32634 - Batch 855/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.39 min) Epoch 34/300 -- Iteration 32643 - Batch 864/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.39 min) Epoch 34/300 -- Iteration 32652 - Batch 873/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.40 min) Epoch 34/300 -- Iteration 32661 - Batch 882/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.40 min) Epoch 34/300 -- Iteration 32670 - Batch 891/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.41 min) Epoch 34/300 -- Iteration 32679 - Batch 900/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.41 min) Epoch 34/300 -- Iteration 32688 - Batch 909/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.42 min) Epoch 34/300 -- Iteration 32697 - Batch 918/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.42 min) Epoch 34/300 -- Iteration 32706 - Batch 927/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.42 min) Epoch 34/300 -- Iteration 32715 - Batch 936/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.43 min) Epoch 34/300 -- Iteration 32724 - Batch 945/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.43 min) Epoch 34/300 -- Iteration 32733 - Batch 954/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.44 min) Epoch 34/300 -- Iteration 32742 - Batch 962/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026 - Val acc: -0.0000\n",
      "(16.44 min) Epoch 35/300 -- Iteration 32751 - Batch 9/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.45 min) Epoch 35/300 -- Iteration 32760 - Batch 18/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.45 min) Epoch 35/300 -- Iteration 32769 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.46 min) Epoch 35/300 -- Iteration 32778 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.46 min) Epoch 35/300 -- Iteration 32787 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.46 min) Epoch 35/300 -- Iteration 32796 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.47 min) Epoch 35/300 -- Iteration 32805 - Batch 63/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.47 min) Epoch 35/300 -- Iteration 32814 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.48 min) Epoch 35/300 -- Iteration 32823 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.48 min) Epoch 35/300 -- Iteration 32832 - Batch 90/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.49 min) Epoch 35/300 -- Iteration 32841 - Batch 99/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.49 min) Epoch 35/300 -- Iteration 32850 - Batch 108/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.50 min) Epoch 35/300 -- Iteration 32859 - Batch 117/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.50 min) Epoch 35/300 -- Iteration 32868 - Batch 126/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.50 min) Epoch 35/300 -- Iteration 32877 - Batch 135/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.51 min) Epoch 35/300 -- Iteration 32886 - Batch 144/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.51 min) Epoch 35/300 -- Iteration 32895 - Batch 153/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.52 min) Epoch 35/300 -- Iteration 32904 - Batch 162/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.52 min) Epoch 35/300 -- Iteration 32913 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.53 min) Epoch 35/300 -- Iteration 32922 - Batch 180/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.53 min) Epoch 35/300 -- Iteration 32931 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.54 min) Epoch 35/300 -- Iteration 32940 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.54 min) Epoch 35/300 -- Iteration 32949 - Batch 207/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.54 min) Epoch 35/300 -- Iteration 32958 - Batch 216/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.55 min) Epoch 35/300 -- Iteration 32967 - Batch 225/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.55 min) Epoch 35/300 -- Iteration 32976 - Batch 234/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.56 min) Epoch 35/300 -- Iteration 32985 - Batch 243/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.56 min) Epoch 35/300 -- Iteration 32994 - Batch 252/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.57 min) Epoch 35/300 -- Iteration 33003 - Batch 261/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.57 min) Epoch 35/300 -- Iteration 33012 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.58 min) Epoch 35/300 -- Iteration 33021 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.58 min) Epoch 35/300 -- Iteration 33030 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.58 min) Epoch 35/300 -- Iteration 33039 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.59 min) Epoch 35/300 -- Iteration 33048 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.59 min) Epoch 35/300 -- Iteration 33057 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.60 min) Epoch 35/300 -- Iteration 33066 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.60 min) Epoch 35/300 -- Iteration 33075 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.61 min) Epoch 35/300 -- Iteration 33084 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.61 min) Epoch 35/300 -- Iteration 33093 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.62 min) Epoch 35/300 -- Iteration 33102 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.62 min) Epoch 35/300 -- Iteration 33111 - Batch 369/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.63 min) Epoch 35/300 -- Iteration 33120 - Batch 378/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.63 min) Epoch 35/300 -- Iteration 33129 - Batch 387/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.63 min) Epoch 35/300 -- Iteration 33138 - Batch 396/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.64 min) Epoch 35/300 -- Iteration 33147 - Batch 405/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.64 min) Epoch 35/300 -- Iteration 33156 - Batch 414/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.65 min) Epoch 35/300 -- Iteration 33165 - Batch 423/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.65 min) Epoch 35/300 -- Iteration 33174 - Batch 432/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.66 min) Epoch 35/300 -- Iteration 33183 - Batch 441/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.66 min) Epoch 35/300 -- Iteration 33192 - Batch 450/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.67 min) Epoch 35/300 -- Iteration 33201 - Batch 459/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.67 min) Epoch 35/300 -- Iteration 33210 - Batch 468/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.67 min) Epoch 35/300 -- Iteration 33219 - Batch 477/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.68 min) Epoch 35/300 -- Iteration 33228 - Batch 486/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.68 min) Epoch 35/300 -- Iteration 33237 - Batch 495/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.69 min) Epoch 35/300 -- Iteration 33246 - Batch 504/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.69 min) Epoch 35/300 -- Iteration 33255 - Batch 513/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.70 min) Epoch 35/300 -- Iteration 33264 - Batch 522/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.70 min) Epoch 35/300 -- Iteration 33273 - Batch 531/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.71 min) Epoch 35/300 -- Iteration 33282 - Batch 540/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.71 min) Epoch 35/300 -- Iteration 33291 - Batch 549/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.71 min) Epoch 35/300 -- Iteration 33300 - Batch 558/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.72 min) Epoch 35/300 -- Iteration 33309 - Batch 567/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.72 min) Epoch 35/300 -- Iteration 33318 - Batch 576/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.73 min) Epoch 35/300 -- Iteration 33327 - Batch 585/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.73 min) Epoch 35/300 -- Iteration 33336 - Batch 594/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.74 min) Epoch 35/300 -- Iteration 33345 - Batch 603/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.74 min) Epoch 35/300 -- Iteration 33354 - Batch 612/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.74 min) Epoch 35/300 -- Iteration 33363 - Batch 621/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.75 min) Epoch 35/300 -- Iteration 33372 - Batch 630/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.75 min) Epoch 35/300 -- Iteration 33381 - Batch 639/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.76 min) Epoch 35/300 -- Iteration 33390 - Batch 648/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.76 min) Epoch 35/300 -- Iteration 33399 - Batch 657/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.77 min) Epoch 35/300 -- Iteration 33408 - Batch 666/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.77 min) Epoch 35/300 -- Iteration 33417 - Batch 675/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.78 min) Epoch 35/300 -- Iteration 33426 - Batch 684/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.78 min) Epoch 35/300 -- Iteration 33435 - Batch 693/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.78 min) Epoch 35/300 -- Iteration 33444 - Batch 702/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.79 min) Epoch 35/300 -- Iteration 33453 - Batch 711/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.79 min) Epoch 35/300 -- Iteration 33462 - Batch 720/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.80 min) Epoch 35/300 -- Iteration 33471 - Batch 729/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.80 min) Epoch 35/300 -- Iteration 33480 - Batch 738/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.81 min) Epoch 35/300 -- Iteration 33489 - Batch 747/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.81 min) Epoch 35/300 -- Iteration 33498 - Batch 756/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.82 min) Epoch 35/300 -- Iteration 33507 - Batch 765/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.82 min) Epoch 35/300 -- Iteration 33516 - Batch 774/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.83 min) Epoch 35/300 -- Iteration 33525 - Batch 783/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.83 min) Epoch 35/300 -- Iteration 33534 - Batch 792/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.84 min) Epoch 35/300 -- Iteration 33543 - Batch 801/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.84 min) Epoch 35/300 -- Iteration 33552 - Batch 810/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.85 min) Epoch 35/300 -- Iteration 33561 - Batch 819/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.85 min) Epoch 35/300 -- Iteration 33570 - Batch 828/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.85 min) Epoch 35/300 -- Iteration 33579 - Batch 837/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.86 min) Epoch 35/300 -- Iteration 33588 - Batch 846/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.86 min) Epoch 35/300 -- Iteration 33597 - Batch 855/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.87 min) Epoch 35/300 -- Iteration 33606 - Batch 864/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.87 min) Epoch 35/300 -- Iteration 33615 - Batch 873/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.88 min) Epoch 35/300 -- Iteration 33624 - Batch 882/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.88 min) Epoch 35/300 -- Iteration 33633 - Batch 891/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.89 min) Epoch 35/300 -- Iteration 33642 - Batch 900/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.89 min) Epoch 35/300 -- Iteration 33651 - Batch 909/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.90 min) Epoch 35/300 -- Iteration 33660 - Batch 918/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.90 min) Epoch 35/300 -- Iteration 33669 - Batch 927/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.91 min) Epoch 35/300 -- Iteration 33678 - Batch 936/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.91 min) Epoch 35/300 -- Iteration 33687 - Batch 945/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.91 min) Epoch 35/300 -- Iteration 33696 - Batch 954/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.92 min) Epoch 35/300 -- Iteration 33705 - Batch 962/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026 - Val acc: -0.0000\n",
      "(16.92 min) Epoch 36/300 -- Iteration 33714 - Batch 9/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.93 min) Epoch 36/300 -- Iteration 33723 - Batch 18/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.93 min) Epoch 36/300 -- Iteration 33732 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.94 min) Epoch 36/300 -- Iteration 33741 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.94 min) Epoch 36/300 -- Iteration 33750 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.95 min) Epoch 36/300 -- Iteration 33759 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.95 min) Epoch 36/300 -- Iteration 33768 - Batch 63/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.96 min) Epoch 36/300 -- Iteration 33777 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.96 min) Epoch 36/300 -- Iteration 33786 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.97 min) Epoch 36/300 -- Iteration 33795 - Batch 90/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.97 min) Epoch 36/300 -- Iteration 33804 - Batch 99/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.98 min) Epoch 36/300 -- Iteration 33813 - Batch 108/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.98 min) Epoch 36/300 -- Iteration 33822 - Batch 117/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.99 min) Epoch 36/300 -- Iteration 33831 - Batch 126/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.99 min) Epoch 36/300 -- Iteration 33840 - Batch 135/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(16.99 min) Epoch 36/300 -- Iteration 33849 - Batch 144/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.00 min) Epoch 36/300 -- Iteration 33858 - Batch 153/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.00 min) Epoch 36/300 -- Iteration 33867 - Batch 162/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.01 min) Epoch 36/300 -- Iteration 33876 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.01 min) Epoch 36/300 -- Iteration 33885 - Batch 180/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.02 min) Epoch 36/300 -- Iteration 33894 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.02 min) Epoch 36/300 -- Iteration 33903 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.03 min) Epoch 36/300 -- Iteration 33912 - Batch 207/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.03 min) Epoch 36/300 -- Iteration 33921 - Batch 216/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.04 min) Epoch 36/300 -- Iteration 33930 - Batch 225/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.04 min) Epoch 36/300 -- Iteration 33939 - Batch 234/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.05 min) Epoch 36/300 -- Iteration 33948 - Batch 243/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.05 min) Epoch 36/300 -- Iteration 33957 - Batch 252/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.06 min) Epoch 36/300 -- Iteration 33966 - Batch 261/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.06 min) Epoch 36/300 -- Iteration 33975 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.06 min) Epoch 36/300 -- Iteration 33984 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.07 min) Epoch 36/300 -- Iteration 33993 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.07 min) Epoch 36/300 -- Iteration 34002 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.08 min) Epoch 36/300 -- Iteration 34011 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.08 min) Epoch 36/300 -- Iteration 34020 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.09 min) Epoch 36/300 -- Iteration 34029 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.09 min) Epoch 36/300 -- Iteration 34038 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.10 min) Epoch 36/300 -- Iteration 34047 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.10 min) Epoch 36/300 -- Iteration 34056 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.11 min) Epoch 36/300 -- Iteration 34065 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.11 min) Epoch 36/300 -- Iteration 34074 - Batch 369/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.12 min) Epoch 36/300 -- Iteration 34083 - Batch 378/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.12 min) Epoch 36/300 -- Iteration 34092 - Batch 387/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.13 min) Epoch 36/300 -- Iteration 34101 - Batch 396/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.13 min) Epoch 36/300 -- Iteration 34110 - Batch 405/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.14 min) Epoch 36/300 -- Iteration 34119 - Batch 414/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.14 min) Epoch 36/300 -- Iteration 34128 - Batch 423/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.14 min) Epoch 36/300 -- Iteration 34137 - Batch 432/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.15 min) Epoch 36/300 -- Iteration 34146 - Batch 441/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.15 min) Epoch 36/300 -- Iteration 34155 - Batch 450/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.16 min) Epoch 36/300 -- Iteration 34164 - Batch 459/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.16 min) Epoch 36/300 -- Iteration 34173 - Batch 468/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.17 min) Epoch 36/300 -- Iteration 34182 - Batch 477/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.17 min) Epoch 36/300 -- Iteration 34191 - Batch 486/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.18 min) Epoch 36/300 -- Iteration 34200 - Batch 495/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.18 min) Epoch 36/300 -- Iteration 34209 - Batch 504/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.19 min) Epoch 36/300 -- Iteration 34218 - Batch 513/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.19 min) Epoch 36/300 -- Iteration 34227 - Batch 522/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.20 min) Epoch 36/300 -- Iteration 34236 - Batch 531/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.20 min) Epoch 36/300 -- Iteration 34245 - Batch 540/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.21 min) Epoch 36/300 -- Iteration 34254 - Batch 549/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.21 min) Epoch 36/300 -- Iteration 34263 - Batch 558/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.21 min) Epoch 36/300 -- Iteration 34272 - Batch 567/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.22 min) Epoch 36/300 -- Iteration 34281 - Batch 576/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.22 min) Epoch 36/300 -- Iteration 34290 - Batch 585/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.23 min) Epoch 36/300 -- Iteration 34299 - Batch 594/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.23 min) Epoch 36/300 -- Iteration 34308 - Batch 603/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.24 min) Epoch 36/300 -- Iteration 34317 - Batch 612/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.24 min) Epoch 36/300 -- Iteration 34326 - Batch 621/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.25 min) Epoch 36/300 -- Iteration 34335 - Batch 630/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.25 min) Epoch 36/300 -- Iteration 34344 - Batch 639/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.26 min) Epoch 36/300 -- Iteration 34353 - Batch 648/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.26 min) Epoch 36/300 -- Iteration 34362 - Batch 657/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.27 min) Epoch 36/300 -- Iteration 34371 - Batch 666/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.27 min) Epoch 36/300 -- Iteration 34380 - Batch 675/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.27 min) Epoch 36/300 -- Iteration 34389 - Batch 684/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.28 min) Epoch 36/300 -- Iteration 34398 - Batch 693/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.28 min) Epoch 36/300 -- Iteration 34407 - Batch 702/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.29 min) Epoch 36/300 -- Iteration 34416 - Batch 711/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.29 min) Epoch 36/300 -- Iteration 34425 - Batch 720/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.30 min) Epoch 36/300 -- Iteration 34434 - Batch 729/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.30 min) Epoch 36/300 -- Iteration 34443 - Batch 738/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.31 min) Epoch 36/300 -- Iteration 34452 - Batch 747/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.31 min) Epoch 36/300 -- Iteration 34461 - Batch 756/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.32 min) Epoch 36/300 -- Iteration 34470 - Batch 765/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.32 min) Epoch 36/300 -- Iteration 34479 - Batch 774/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.32 min) Epoch 36/300 -- Iteration 34488 - Batch 783/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.33 min) Epoch 36/300 -- Iteration 34497 - Batch 792/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.33 min) Epoch 36/300 -- Iteration 34506 - Batch 801/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.34 min) Epoch 36/300 -- Iteration 34515 - Batch 810/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.34 min) Epoch 36/300 -- Iteration 34524 - Batch 819/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.35 min) Epoch 36/300 -- Iteration 34533 - Batch 828/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.35 min) Epoch 36/300 -- Iteration 34542 - Batch 837/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.36 min) Epoch 36/300 -- Iteration 34551 - Batch 846/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.36 min) Epoch 36/300 -- Iteration 34560 - Batch 855/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.36 min) Epoch 36/300 -- Iteration 34569 - Batch 864/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.37 min) Epoch 36/300 -- Iteration 34578 - Batch 873/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.37 min) Epoch 36/300 -- Iteration 34587 - Batch 882/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.38 min) Epoch 36/300 -- Iteration 34596 - Batch 891/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.38 min) Epoch 36/300 -- Iteration 34605 - Batch 900/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.39 min) Epoch 36/300 -- Iteration 34614 - Batch 909/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.39 min) Epoch 36/300 -- Iteration 34623 - Batch 918/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.40 min) Epoch 36/300 -- Iteration 34632 - Batch 927/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.40 min) Epoch 36/300 -- Iteration 34641 - Batch 936/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.40 min) Epoch 36/300 -- Iteration 34650 - Batch 945/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.41 min) Epoch 36/300 -- Iteration 34659 - Batch 954/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.41 min) Epoch 36/300 -- Iteration 34668 - Batch 962/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027 - Val acc: -0.0000\n",
      "(17.42 min) Epoch 37/300 -- Iteration 34677 - Batch 9/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.42 min) Epoch 37/300 -- Iteration 34686 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.43 min) Epoch 37/300 -- Iteration 34695 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.43 min) Epoch 37/300 -- Iteration 34704 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.44 min) Epoch 37/300 -- Iteration 34713 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.44 min) Epoch 37/300 -- Iteration 34722 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.45 min) Epoch 37/300 -- Iteration 34731 - Batch 63/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.45 min) Epoch 37/300 -- Iteration 34740 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.46 min) Epoch 37/300 -- Iteration 34749 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.46 min) Epoch 37/300 -- Iteration 34758 - Batch 90/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.47 min) Epoch 37/300 -- Iteration 34767 - Batch 99/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.47 min) Epoch 37/300 -- Iteration 34776 - Batch 108/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.48 min) Epoch 37/300 -- Iteration 34785 - Batch 117/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.48 min) Epoch 37/300 -- Iteration 34794 - Batch 126/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.48 min) Epoch 37/300 -- Iteration 34803 - Batch 135/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.49 min) Epoch 37/300 -- Iteration 34812 - Batch 144/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.49 min) Epoch 37/300 -- Iteration 34821 - Batch 153/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.50 min) Epoch 37/300 -- Iteration 34830 - Batch 162/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.50 min) Epoch 37/300 -- Iteration 34839 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.51 min) Epoch 37/300 -- Iteration 34848 - Batch 180/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.51 min) Epoch 37/300 -- Iteration 34857 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.52 min) Epoch 37/300 -- Iteration 34866 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.52 min) Epoch 37/300 -- Iteration 34875 - Batch 207/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.53 min) Epoch 37/300 -- Iteration 34884 - Batch 216/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.53 min) Epoch 37/300 -- Iteration 34893 - Batch 225/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.54 min) Epoch 37/300 -- Iteration 34902 - Batch 234/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.54 min) Epoch 37/300 -- Iteration 34911 - Batch 243/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.55 min) Epoch 37/300 -- Iteration 34920 - Batch 252/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.55 min) Epoch 37/300 -- Iteration 34929 - Batch 261/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.55 min) Epoch 37/300 -- Iteration 34938 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.56 min) Epoch 37/300 -- Iteration 34947 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.56 min) Epoch 37/300 -- Iteration 34956 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.57 min) Epoch 37/300 -- Iteration 34965 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.57 min) Epoch 37/300 -- Iteration 34974 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.58 min) Epoch 37/300 -- Iteration 34983 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.58 min) Epoch 37/300 -- Iteration 34992 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.59 min) Epoch 37/300 -- Iteration 35001 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.59 min) Epoch 37/300 -- Iteration 35010 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.60 min) Epoch 37/300 -- Iteration 35019 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.60 min) Epoch 37/300 -- Iteration 35028 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.60 min) Epoch 37/300 -- Iteration 35037 - Batch 369/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.61 min) Epoch 37/300 -- Iteration 35046 - Batch 378/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.61 min) Epoch 37/300 -- Iteration 35055 - Batch 387/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.62 min) Epoch 37/300 -- Iteration 35064 - Batch 396/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.62 min) Epoch 37/300 -- Iteration 35073 - Batch 405/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.63 min) Epoch 37/300 -- Iteration 35082 - Batch 414/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.63 min) Epoch 37/300 -- Iteration 35091 - Batch 423/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.64 min) Epoch 37/300 -- Iteration 35100 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.64 min) Epoch 37/300 -- Iteration 35109 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.65 min) Epoch 37/300 -- Iteration 35118 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.65 min) Epoch 37/300 -- Iteration 35127 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.65 min) Epoch 37/300 -- Iteration 35136 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.66 min) Epoch 37/300 -- Iteration 35145 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.66 min) Epoch 37/300 -- Iteration 35154 - Batch 486/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.67 min) Epoch 37/300 -- Iteration 35163 - Batch 495/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.67 min) Epoch 37/300 -- Iteration 35172 - Batch 504/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.68 min) Epoch 37/300 -- Iteration 35181 - Batch 513/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.68 min) Epoch 37/300 -- Iteration 35190 - Batch 522/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.69 min) Epoch 37/300 -- Iteration 35199 - Batch 531/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.69 min) Epoch 37/300 -- Iteration 35208 - Batch 540/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.69 min) Epoch 37/300 -- Iteration 35217 - Batch 549/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.70 min) Epoch 37/300 -- Iteration 35226 - Batch 558/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.70 min) Epoch 37/300 -- Iteration 35235 - Batch 567/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.71 min) Epoch 37/300 -- Iteration 35244 - Batch 576/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.71 min) Epoch 37/300 -- Iteration 35253 - Batch 585/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.72 min) Epoch 37/300 -- Iteration 35262 - Batch 594/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.72 min) Epoch 37/300 -- Iteration 35271 - Batch 603/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.73 min) Epoch 37/300 -- Iteration 35280 - Batch 612/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.73 min) Epoch 37/300 -- Iteration 35289 - Batch 621/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.73 min) Epoch 37/300 -- Iteration 35298 - Batch 630/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.74 min) Epoch 37/300 -- Iteration 35307 - Batch 639/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.74 min) Epoch 37/300 -- Iteration 35316 - Batch 648/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.75 min) Epoch 37/300 -- Iteration 35325 - Batch 657/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.75 min) Epoch 37/300 -- Iteration 35334 - Batch 666/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.76 min) Epoch 37/300 -- Iteration 35343 - Batch 675/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.76 min) Epoch 37/300 -- Iteration 35352 - Batch 684/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.77 min) Epoch 37/300 -- Iteration 35361 - Batch 693/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.77 min) Epoch 37/300 -- Iteration 35370 - Batch 702/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.77 min) Epoch 37/300 -- Iteration 35379 - Batch 711/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.78 min) Epoch 37/300 -- Iteration 35388 - Batch 720/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.78 min) Epoch 37/300 -- Iteration 35397 - Batch 729/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.79 min) Epoch 37/300 -- Iteration 35406 - Batch 738/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.79 min) Epoch 37/300 -- Iteration 35415 - Batch 747/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.80 min) Epoch 37/300 -- Iteration 35424 - Batch 756/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.80 min) Epoch 37/300 -- Iteration 35433 - Batch 765/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.81 min) Epoch 37/300 -- Iteration 35442 - Batch 774/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.81 min) Epoch 37/300 -- Iteration 35451 - Batch 783/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.82 min) Epoch 37/300 -- Iteration 35460 - Batch 792/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.82 min) Epoch 37/300 -- Iteration 35469 - Batch 801/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.82 min) Epoch 37/300 -- Iteration 35478 - Batch 810/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.83 min) Epoch 37/300 -- Iteration 35487 - Batch 819/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.83 min) Epoch 37/300 -- Iteration 35496 - Batch 828/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.84 min) Epoch 37/300 -- Iteration 35505 - Batch 837/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.84 min) Epoch 37/300 -- Iteration 35514 - Batch 846/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.85 min) Epoch 37/300 -- Iteration 35523 - Batch 855/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.85 min) Epoch 37/300 -- Iteration 35532 - Batch 864/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.86 min) Epoch 37/300 -- Iteration 35541 - Batch 873/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.86 min) Epoch 37/300 -- Iteration 35550 - Batch 882/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.87 min) Epoch 37/300 -- Iteration 35559 - Batch 891/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.87 min) Epoch 37/300 -- Iteration 35568 - Batch 900/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.88 min) Epoch 37/300 -- Iteration 35577 - Batch 909/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.88 min) Epoch 37/300 -- Iteration 35586 - Batch 918/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.89 min) Epoch 37/300 -- Iteration 35595 - Batch 927/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.89 min) Epoch 37/300 -- Iteration 35604 - Batch 936/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.89 min) Epoch 37/300 -- Iteration 35613 - Batch 945/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.90 min) Epoch 37/300 -- Iteration 35622 - Batch 954/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0027\n",
      "(17.90 min) Epoch 37/300 -- Iteration 35631 - Batch 962/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026 - Val acc: -0.0000\n",
      "(17.91 min) Epoch 38/300 -- Iteration 35640 - Batch 9/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.91 min) Epoch 38/300 -- Iteration 35649 - Batch 18/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.92 min) Epoch 38/300 -- Iteration 35658 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.92 min) Epoch 38/300 -- Iteration 35667 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.93 min) Epoch 38/300 -- Iteration 35676 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.93 min) Epoch 38/300 -- Iteration 35685 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.94 min) Epoch 38/300 -- Iteration 35694 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.94 min) Epoch 38/300 -- Iteration 35703 - Batch 72/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.95 min) Epoch 38/300 -- Iteration 35712 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.95 min) Epoch 38/300 -- Iteration 35721 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.96 min) Epoch 38/300 -- Iteration 35730 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.96 min) Epoch 38/300 -- Iteration 35739 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.97 min) Epoch 38/300 -- Iteration 35748 - Batch 117/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.97 min) Epoch 38/300 -- Iteration 35757 - Batch 126/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.98 min) Epoch 38/300 -- Iteration 35766 - Batch 135/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.98 min) Epoch 38/300 -- Iteration 35775 - Batch 144/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.98 min) Epoch 38/300 -- Iteration 35784 - Batch 153/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.99 min) Epoch 38/300 -- Iteration 35793 - Batch 162/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(17.99 min) Epoch 38/300 -- Iteration 35802 - Batch 171/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.00 min) Epoch 38/300 -- Iteration 35811 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.00 min) Epoch 38/300 -- Iteration 35820 - Batch 189/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.01 min) Epoch 38/300 -- Iteration 35829 - Batch 198/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.01 min) Epoch 38/300 -- Iteration 35838 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.02 min) Epoch 38/300 -- Iteration 35847 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.02 min) Epoch 38/300 -- Iteration 35856 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.02 min) Epoch 38/300 -- Iteration 35865 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.03 min) Epoch 38/300 -- Iteration 35874 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.03 min) Epoch 38/300 -- Iteration 35883 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.04 min) Epoch 38/300 -- Iteration 35892 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.04 min) Epoch 38/300 -- Iteration 35901 - Batch 270/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.05 min) Epoch 38/300 -- Iteration 35910 - Batch 279/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.05 min) Epoch 38/300 -- Iteration 35919 - Batch 288/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.06 min) Epoch 38/300 -- Iteration 35928 - Batch 297/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.06 min) Epoch 38/300 -- Iteration 35937 - Batch 306/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.07 min) Epoch 38/300 -- Iteration 35946 - Batch 315/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.07 min) Epoch 38/300 -- Iteration 35955 - Batch 324/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.07 min) Epoch 38/300 -- Iteration 35964 - Batch 333/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.08 min) Epoch 38/300 -- Iteration 35973 - Batch 342/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.08 min) Epoch 38/300 -- Iteration 35982 - Batch 351/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.09 min) Epoch 38/300 -- Iteration 35991 - Batch 360/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.09 min) Epoch 38/300 -- Iteration 36000 - Batch 369/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.10 min) Epoch 38/300 -- Iteration 36009 - Batch 378/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.10 min) Epoch 38/300 -- Iteration 36018 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.11 min) Epoch 38/300 -- Iteration 36027 - Batch 396/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.11 min) Epoch 38/300 -- Iteration 36036 - Batch 405/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.11 min) Epoch 38/300 -- Iteration 36045 - Batch 414/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.12 min) Epoch 38/300 -- Iteration 36054 - Batch 423/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.12 min) Epoch 38/300 -- Iteration 36063 - Batch 432/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.13 min) Epoch 38/300 -- Iteration 36072 - Batch 441/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.13 min) Epoch 38/300 -- Iteration 36081 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.14 min) Epoch 38/300 -- Iteration 36090 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.14 min) Epoch 38/300 -- Iteration 36099 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.15 min) Epoch 38/300 -- Iteration 36108 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.15 min) Epoch 38/300 -- Iteration 36117 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.15 min) Epoch 38/300 -- Iteration 36126 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.16 min) Epoch 38/300 -- Iteration 36135 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.16 min) Epoch 38/300 -- Iteration 36144 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.17 min) Epoch 38/300 -- Iteration 36153 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.17 min) Epoch 38/300 -- Iteration 36162 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.18 min) Epoch 38/300 -- Iteration 36171 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.18 min) Epoch 38/300 -- Iteration 36180 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.19 min) Epoch 38/300 -- Iteration 36189 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.19 min) Epoch 38/300 -- Iteration 36198 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.19 min) Epoch 38/300 -- Iteration 36207 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.20 min) Epoch 38/300 -- Iteration 36216 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.20 min) Epoch 38/300 -- Iteration 36225 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.21 min) Epoch 38/300 -- Iteration 36234 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.21 min) Epoch 38/300 -- Iteration 36243 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.22 min) Epoch 38/300 -- Iteration 36252 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.22 min) Epoch 38/300 -- Iteration 36261 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.23 min) Epoch 38/300 -- Iteration 36270 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.23 min) Epoch 38/300 -- Iteration 36279 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.23 min) Epoch 38/300 -- Iteration 36288 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.24 min) Epoch 38/300 -- Iteration 36297 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.24 min) Epoch 38/300 -- Iteration 36306 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.25 min) Epoch 38/300 -- Iteration 36315 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.25 min) Epoch 38/300 -- Iteration 36324 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.26 min) Epoch 38/300 -- Iteration 36333 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.26 min) Epoch 38/300 -- Iteration 36342 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.27 min) Epoch 38/300 -- Iteration 36351 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.27 min) Epoch 38/300 -- Iteration 36360 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.27 min) Epoch 38/300 -- Iteration 36369 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.28 min) Epoch 38/300 -- Iteration 36378 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.28 min) Epoch 38/300 -- Iteration 36387 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.29 min) Epoch 38/300 -- Iteration 36396 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.29 min) Epoch 38/300 -- Iteration 36405 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.30 min) Epoch 38/300 -- Iteration 36414 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.30 min) Epoch 38/300 -- Iteration 36423 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.31 min) Epoch 38/300 -- Iteration 36432 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.31 min) Epoch 38/300 -- Iteration 36441 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.31 min) Epoch 38/300 -- Iteration 36450 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.32 min) Epoch 38/300 -- Iteration 36459 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.32 min) Epoch 38/300 -- Iteration 36468 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.33 min) Epoch 38/300 -- Iteration 36477 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.33 min) Epoch 38/300 -- Iteration 36486 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.34 min) Epoch 38/300 -- Iteration 36495 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.34 min) Epoch 38/300 -- Iteration 36504 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.35 min) Epoch 38/300 -- Iteration 36513 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.35 min) Epoch 38/300 -- Iteration 36522 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.35 min) Epoch 38/300 -- Iteration 36531 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.36 min) Epoch 38/300 -- Iteration 36540 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.36 min) Epoch 38/300 -- Iteration 36549 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.37 min) Epoch 38/300 -- Iteration 36558 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.37 min) Epoch 38/300 -- Iteration 36567 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.38 min) Epoch 38/300 -- Iteration 36576 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.38 min) Epoch 38/300 -- Iteration 36585 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0026\n",
      "(18.38 min) Epoch 38/300 -- Iteration 36594 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025 - Val acc: -0.0000\n",
      "(18.39 min) Epoch 39/300 -- Iteration 36603 - Batch 9/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.40 min) Epoch 39/300 -- Iteration 36612 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.40 min) Epoch 39/300 -- Iteration 36621 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.40 min) Epoch 39/300 -- Iteration 36630 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.41 min) Epoch 39/300 -- Iteration 36639 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.41 min) Epoch 39/300 -- Iteration 36648 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.42 min) Epoch 39/300 -- Iteration 36657 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.42 min) Epoch 39/300 -- Iteration 36666 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.43 min) Epoch 39/300 -- Iteration 36675 - Batch 81/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.43 min) Epoch 39/300 -- Iteration 36684 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.44 min) Epoch 39/300 -- Iteration 36693 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.44 min) Epoch 39/300 -- Iteration 36702 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.44 min) Epoch 39/300 -- Iteration 36711 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.45 min) Epoch 39/300 -- Iteration 36720 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.45 min) Epoch 39/300 -- Iteration 36729 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.46 min) Epoch 39/300 -- Iteration 36738 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.46 min) Epoch 39/300 -- Iteration 36747 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.47 min) Epoch 39/300 -- Iteration 36756 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.47 min) Epoch 39/300 -- Iteration 36765 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.48 min) Epoch 39/300 -- Iteration 36774 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.48 min) Epoch 39/300 -- Iteration 36783 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.48 min) Epoch 39/300 -- Iteration 36792 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.49 min) Epoch 39/300 -- Iteration 36801 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.49 min) Epoch 39/300 -- Iteration 36810 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.50 min) Epoch 39/300 -- Iteration 36819 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.50 min) Epoch 39/300 -- Iteration 36828 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.51 min) Epoch 39/300 -- Iteration 36837 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.51 min) Epoch 39/300 -- Iteration 36846 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.52 min) Epoch 39/300 -- Iteration 36855 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.52 min) Epoch 39/300 -- Iteration 36864 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.52 min) Epoch 39/300 -- Iteration 36873 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.53 min) Epoch 39/300 -- Iteration 36882 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.53 min) Epoch 39/300 -- Iteration 36891 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.54 min) Epoch 39/300 -- Iteration 36900 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.54 min) Epoch 39/300 -- Iteration 36909 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.55 min) Epoch 39/300 -- Iteration 36918 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.55 min) Epoch 39/300 -- Iteration 36927 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.55 min) Epoch 39/300 -- Iteration 36936 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.56 min) Epoch 39/300 -- Iteration 36945 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.56 min) Epoch 39/300 -- Iteration 36954 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.57 min) Epoch 39/300 -- Iteration 36963 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.57 min) Epoch 39/300 -- Iteration 36972 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.58 min) Epoch 39/300 -- Iteration 36981 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.58 min) Epoch 39/300 -- Iteration 36990 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.59 min) Epoch 39/300 -- Iteration 36999 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.59 min) Epoch 39/300 -- Iteration 37008 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.59 min) Epoch 39/300 -- Iteration 37017 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.60 min) Epoch 39/300 -- Iteration 37026 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.60 min) Epoch 39/300 -- Iteration 37035 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.61 min) Epoch 39/300 -- Iteration 37044 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.61 min) Epoch 39/300 -- Iteration 37053 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.62 min) Epoch 39/300 -- Iteration 37062 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.62 min) Epoch 39/300 -- Iteration 37071 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.63 min) Epoch 39/300 -- Iteration 37080 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.63 min) Epoch 39/300 -- Iteration 37089 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.63 min) Epoch 39/300 -- Iteration 37098 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.64 min) Epoch 39/300 -- Iteration 37107 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.64 min) Epoch 39/300 -- Iteration 37116 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.65 min) Epoch 39/300 -- Iteration 37125 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.65 min) Epoch 39/300 -- Iteration 37134 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.66 min) Epoch 39/300 -- Iteration 37143 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.66 min) Epoch 39/300 -- Iteration 37152 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.67 min) Epoch 39/300 -- Iteration 37161 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.67 min) Epoch 39/300 -- Iteration 37170 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.67 min) Epoch 39/300 -- Iteration 37179 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.68 min) Epoch 39/300 -- Iteration 37188 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.68 min) Epoch 39/300 -- Iteration 37197 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.69 min) Epoch 39/300 -- Iteration 37206 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.69 min) Epoch 39/300 -- Iteration 37215 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.70 min) Epoch 39/300 -- Iteration 37224 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.70 min) Epoch 39/300 -- Iteration 37233 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.71 min) Epoch 39/300 -- Iteration 37242 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.71 min) Epoch 39/300 -- Iteration 37251 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.71 min) Epoch 39/300 -- Iteration 37260 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.72 min) Epoch 39/300 -- Iteration 37269 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.72 min) Epoch 39/300 -- Iteration 37278 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.73 min) Epoch 39/300 -- Iteration 37287 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.73 min) Epoch 39/300 -- Iteration 37296 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.74 min) Epoch 39/300 -- Iteration 37305 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.74 min) Epoch 39/300 -- Iteration 37314 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.75 min) Epoch 39/300 -- Iteration 37323 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.75 min) Epoch 39/300 -- Iteration 37332 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.75 min) Epoch 39/300 -- Iteration 37341 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.76 min) Epoch 39/300 -- Iteration 37350 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.76 min) Epoch 39/300 -- Iteration 37359 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.77 min) Epoch 39/300 -- Iteration 37368 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.77 min) Epoch 39/300 -- Iteration 37377 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.78 min) Epoch 39/300 -- Iteration 37386 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.78 min) Epoch 39/300 -- Iteration 37395 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.79 min) Epoch 39/300 -- Iteration 37404 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.79 min) Epoch 39/300 -- Iteration 37413 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.79 min) Epoch 39/300 -- Iteration 37422 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.80 min) Epoch 39/300 -- Iteration 37431 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.80 min) Epoch 39/300 -- Iteration 37440 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.81 min) Epoch 39/300 -- Iteration 37449 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.81 min) Epoch 39/300 -- Iteration 37458 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.82 min) Epoch 39/300 -- Iteration 37467 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.82 min) Epoch 39/300 -- Iteration 37476 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.83 min) Epoch 39/300 -- Iteration 37485 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.83 min) Epoch 39/300 -- Iteration 37494 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.83 min) Epoch 39/300 -- Iteration 37503 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.84 min) Epoch 39/300 -- Iteration 37512 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.84 min) Epoch 39/300 -- Iteration 37521 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.85 min) Epoch 39/300 -- Iteration 37530 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.85 min) Epoch 39/300 -- Iteration 37539 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.86 min) Epoch 39/300 -- Iteration 37548 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.86 min) Epoch 39/300 -- Iteration 37557 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025 - Val acc: -0.0000\n",
      "(18.87 min) Epoch 40/300 -- Iteration 37566 - Batch 9/963 - Train loss: 0.0028  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.87 min) Epoch 40/300 -- Iteration 37575 - Batch 18/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.88 min) Epoch 40/300 -- Iteration 37584 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.88 min) Epoch 40/300 -- Iteration 37593 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.88 min) Epoch 40/300 -- Iteration 37602 - Batch 45/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.89 min) Epoch 40/300 -- Iteration 37611 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.89 min) Epoch 40/300 -- Iteration 37620 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.90 min) Epoch 40/300 -- Iteration 37629 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.90 min) Epoch 40/300 -- Iteration 37638 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.91 min) Epoch 40/300 -- Iteration 37647 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.91 min) Epoch 40/300 -- Iteration 37656 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.92 min) Epoch 40/300 -- Iteration 37665 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.92 min) Epoch 40/300 -- Iteration 37674 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.92 min) Epoch 40/300 -- Iteration 37683 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.93 min) Epoch 40/300 -- Iteration 37692 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.93 min) Epoch 40/300 -- Iteration 37701 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.94 min) Epoch 40/300 -- Iteration 37710 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.94 min) Epoch 40/300 -- Iteration 37719 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.95 min) Epoch 40/300 -- Iteration 37728 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.95 min) Epoch 40/300 -- Iteration 37737 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.96 min) Epoch 40/300 -- Iteration 37746 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.96 min) Epoch 40/300 -- Iteration 37755 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.97 min) Epoch 40/300 -- Iteration 37764 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.97 min) Epoch 40/300 -- Iteration 37773 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.97 min) Epoch 40/300 -- Iteration 37782 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.98 min) Epoch 40/300 -- Iteration 37791 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.98 min) Epoch 40/300 -- Iteration 37800 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.99 min) Epoch 40/300 -- Iteration 37809 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(18.99 min) Epoch 40/300 -- Iteration 37818 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.00 min) Epoch 40/300 -- Iteration 37827 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.00 min) Epoch 40/300 -- Iteration 37836 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.01 min) Epoch 40/300 -- Iteration 37845 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.01 min) Epoch 40/300 -- Iteration 37854 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.01 min) Epoch 40/300 -- Iteration 37863 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.02 min) Epoch 40/300 -- Iteration 37872 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.02 min) Epoch 40/300 -- Iteration 37881 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.03 min) Epoch 40/300 -- Iteration 37890 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.03 min) Epoch 40/300 -- Iteration 37899 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.04 min) Epoch 40/300 -- Iteration 37908 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.04 min) Epoch 40/300 -- Iteration 37917 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.05 min) Epoch 40/300 -- Iteration 37926 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.05 min) Epoch 40/300 -- Iteration 37935 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.05 min) Epoch 40/300 -- Iteration 37944 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.06 min) Epoch 40/300 -- Iteration 37953 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.06 min) Epoch 40/300 -- Iteration 37962 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.07 min) Epoch 40/300 -- Iteration 37971 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.07 min) Epoch 40/300 -- Iteration 37980 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.08 min) Epoch 40/300 -- Iteration 37989 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.08 min) Epoch 40/300 -- Iteration 37998 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.09 min) Epoch 40/300 -- Iteration 38007 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.09 min) Epoch 40/300 -- Iteration 38016 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.09 min) Epoch 40/300 -- Iteration 38025 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.10 min) Epoch 40/300 -- Iteration 38034 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.10 min) Epoch 40/300 -- Iteration 38043 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.11 min) Epoch 40/300 -- Iteration 38052 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.11 min) Epoch 40/300 -- Iteration 38061 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.12 min) Epoch 40/300 -- Iteration 38070 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.12 min) Epoch 40/300 -- Iteration 38079 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.13 min) Epoch 40/300 -- Iteration 38088 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.13 min) Epoch 40/300 -- Iteration 38097 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.13 min) Epoch 40/300 -- Iteration 38106 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.14 min) Epoch 40/300 -- Iteration 38115 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.14 min) Epoch 40/300 -- Iteration 38124 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.15 min) Epoch 40/300 -- Iteration 38133 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.15 min) Epoch 40/300 -- Iteration 38142 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.16 min) Epoch 40/300 -- Iteration 38151 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.16 min) Epoch 40/300 -- Iteration 38160 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.17 min) Epoch 40/300 -- Iteration 38169 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.17 min) Epoch 40/300 -- Iteration 38178 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.17 min) Epoch 40/300 -- Iteration 38187 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.18 min) Epoch 40/300 -- Iteration 38196 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.18 min) Epoch 40/300 -- Iteration 38205 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.19 min) Epoch 40/300 -- Iteration 38214 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.19 min) Epoch 40/300 -- Iteration 38223 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.20 min) Epoch 40/300 -- Iteration 38232 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.20 min) Epoch 40/300 -- Iteration 38241 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.21 min) Epoch 40/300 -- Iteration 38250 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.21 min) Epoch 40/300 -- Iteration 38259 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.21 min) Epoch 40/300 -- Iteration 38268 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.22 min) Epoch 40/300 -- Iteration 38277 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.22 min) Epoch 40/300 -- Iteration 38286 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.23 min) Epoch 40/300 -- Iteration 38295 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.23 min) Epoch 40/300 -- Iteration 38304 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.24 min) Epoch 40/300 -- Iteration 38313 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.24 min) Epoch 40/300 -- Iteration 38322 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.25 min) Epoch 40/300 -- Iteration 38331 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.25 min) Epoch 40/300 -- Iteration 38340 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.25 min) Epoch 40/300 -- Iteration 38349 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.26 min) Epoch 40/300 -- Iteration 38358 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.26 min) Epoch 40/300 -- Iteration 38367 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.27 min) Epoch 40/300 -- Iteration 38376 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.27 min) Epoch 40/300 -- Iteration 38385 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.28 min) Epoch 40/300 -- Iteration 38394 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.28 min) Epoch 40/300 -- Iteration 38403 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.29 min) Epoch 40/300 -- Iteration 38412 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.29 min) Epoch 40/300 -- Iteration 38421 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.29 min) Epoch 40/300 -- Iteration 38430 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.30 min) Epoch 40/300 -- Iteration 38439 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.30 min) Epoch 40/300 -- Iteration 38448 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.31 min) Epoch 40/300 -- Iteration 38457 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.31 min) Epoch 40/300 -- Iteration 38466 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.32 min) Epoch 40/300 -- Iteration 38475 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.32 min) Epoch 40/300 -- Iteration 38484 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.33 min) Epoch 40/300 -- Iteration 38493 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.33 min) Epoch 40/300 -- Iteration 38502 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.33 min) Epoch 40/300 -- Iteration 38511 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0025\n",
      "(19.34 min) Epoch 40/300 -- Iteration 38520 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023 - Val acc: -0.0000\n",
      "(19.34 min) Epoch 41/300 -- Iteration 38529 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.35 min) Epoch 41/300 -- Iteration 38538 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.35 min) Epoch 41/300 -- Iteration 38547 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.36 min) Epoch 41/300 -- Iteration 38556 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.36 min) Epoch 41/300 -- Iteration 38565 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.37 min) Epoch 41/300 -- Iteration 38574 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.37 min) Epoch 41/300 -- Iteration 38583 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.37 min) Epoch 41/300 -- Iteration 38592 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.38 min) Epoch 41/300 -- Iteration 38601 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.38 min) Epoch 41/300 -- Iteration 38610 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.39 min) Epoch 41/300 -- Iteration 38619 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.39 min) Epoch 41/300 -- Iteration 38628 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.40 min) Epoch 41/300 -- Iteration 38637 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.40 min) Epoch 41/300 -- Iteration 38646 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.41 min) Epoch 41/300 -- Iteration 38655 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.41 min) Epoch 41/300 -- Iteration 38664 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.41 min) Epoch 41/300 -- Iteration 38673 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.42 min) Epoch 41/300 -- Iteration 38682 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.42 min) Epoch 41/300 -- Iteration 38691 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.43 min) Epoch 41/300 -- Iteration 38700 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.43 min) Epoch 41/300 -- Iteration 38709 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.44 min) Epoch 41/300 -- Iteration 38718 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.44 min) Epoch 41/300 -- Iteration 38727 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.45 min) Epoch 41/300 -- Iteration 38736 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.45 min) Epoch 41/300 -- Iteration 38745 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.45 min) Epoch 41/300 -- Iteration 38754 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.46 min) Epoch 41/300 -- Iteration 38763 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.46 min) Epoch 41/300 -- Iteration 38772 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.47 min) Epoch 41/300 -- Iteration 38781 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.47 min) Epoch 41/300 -- Iteration 38790 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.48 min) Epoch 41/300 -- Iteration 38799 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.48 min) Epoch 41/300 -- Iteration 38808 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.49 min) Epoch 41/300 -- Iteration 38817 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.49 min) Epoch 41/300 -- Iteration 38826 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.49 min) Epoch 41/300 -- Iteration 38835 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.50 min) Epoch 41/300 -- Iteration 38844 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.50 min) Epoch 41/300 -- Iteration 38853 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.51 min) Epoch 41/300 -- Iteration 38862 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.51 min) Epoch 41/300 -- Iteration 38871 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.52 min) Epoch 41/300 -- Iteration 38880 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.52 min) Epoch 41/300 -- Iteration 38889 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.53 min) Epoch 41/300 -- Iteration 38898 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.53 min) Epoch 41/300 -- Iteration 38907 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.53 min) Epoch 41/300 -- Iteration 38916 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.54 min) Epoch 41/300 -- Iteration 38925 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.54 min) Epoch 41/300 -- Iteration 38934 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.55 min) Epoch 41/300 -- Iteration 38943 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.55 min) Epoch 41/300 -- Iteration 38952 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.56 min) Epoch 41/300 -- Iteration 38961 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.56 min) Epoch 41/300 -- Iteration 38970 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.57 min) Epoch 41/300 -- Iteration 38979 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.57 min) Epoch 41/300 -- Iteration 38988 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.57 min) Epoch 41/300 -- Iteration 38997 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.58 min) Epoch 41/300 -- Iteration 39006 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.58 min) Epoch 41/300 -- Iteration 39015 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.59 min) Epoch 41/300 -- Iteration 39024 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.59 min) Epoch 41/300 -- Iteration 39033 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.60 min) Epoch 41/300 -- Iteration 39042 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.60 min) Epoch 41/300 -- Iteration 39051 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.61 min) Epoch 41/300 -- Iteration 39060 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.61 min) Epoch 41/300 -- Iteration 39069 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.61 min) Epoch 41/300 -- Iteration 39078 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.62 min) Epoch 41/300 -- Iteration 39087 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.62 min) Epoch 41/300 -- Iteration 39096 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.63 min) Epoch 41/300 -- Iteration 39105 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.63 min) Epoch 41/300 -- Iteration 39114 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.64 min) Epoch 41/300 -- Iteration 39123 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.64 min) Epoch 41/300 -- Iteration 39132 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.65 min) Epoch 41/300 -- Iteration 39141 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.65 min) Epoch 41/300 -- Iteration 39150 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.65 min) Epoch 41/300 -- Iteration 39159 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.66 min) Epoch 41/300 -- Iteration 39168 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.66 min) Epoch 41/300 -- Iteration 39177 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.67 min) Epoch 41/300 -- Iteration 39186 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.67 min) Epoch 41/300 -- Iteration 39195 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.68 min) Epoch 41/300 -- Iteration 39204 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.68 min) Epoch 41/300 -- Iteration 39213 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.69 min) Epoch 41/300 -- Iteration 39222 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.69 min) Epoch 41/300 -- Iteration 39231 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.69 min) Epoch 41/300 -- Iteration 39240 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.70 min) Epoch 41/300 -- Iteration 39249 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.70 min) Epoch 41/300 -- Iteration 39258 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.71 min) Epoch 41/300 -- Iteration 39267 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.71 min) Epoch 41/300 -- Iteration 39276 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.72 min) Epoch 41/300 -- Iteration 39285 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.72 min) Epoch 41/300 -- Iteration 39294 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.73 min) Epoch 41/300 -- Iteration 39303 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.73 min) Epoch 41/300 -- Iteration 39312 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.73 min) Epoch 41/300 -- Iteration 39321 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.74 min) Epoch 41/300 -- Iteration 39330 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.74 min) Epoch 41/300 -- Iteration 39339 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.75 min) Epoch 41/300 -- Iteration 39348 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.75 min) Epoch 41/300 -- Iteration 39357 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.76 min) Epoch 41/300 -- Iteration 39366 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.76 min) Epoch 41/300 -- Iteration 39375 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.77 min) Epoch 41/300 -- Iteration 39384 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.77 min) Epoch 41/300 -- Iteration 39393 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.77 min) Epoch 41/300 -- Iteration 39402 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.78 min) Epoch 41/300 -- Iteration 39411 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.78 min) Epoch 41/300 -- Iteration 39420 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.79 min) Epoch 41/300 -- Iteration 39429 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.79 min) Epoch 41/300 -- Iteration 39438 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.80 min) Epoch 41/300 -- Iteration 39447 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.80 min) Epoch 41/300 -- Iteration 39456 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.81 min) Epoch 41/300 -- Iteration 39465 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.81 min) Epoch 41/300 -- Iteration 39474 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0023\n",
      "(19.81 min) Epoch 41/300 -- Iteration 39483 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022 - Val acc: -0.0000\n",
      "(19.82 min) Epoch 42/300 -- Iteration 39492 - Batch 9/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.83 min) Epoch 42/300 -- Iteration 39501 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.83 min) Epoch 42/300 -- Iteration 39510 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.83 min) Epoch 42/300 -- Iteration 39519 - Batch 36/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.84 min) Epoch 42/300 -- Iteration 39528 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.84 min) Epoch 42/300 -- Iteration 39537 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.85 min) Epoch 42/300 -- Iteration 39546 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.85 min) Epoch 42/300 -- Iteration 39555 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.86 min) Epoch 42/300 -- Iteration 39564 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.86 min) Epoch 42/300 -- Iteration 39573 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.87 min) Epoch 42/300 -- Iteration 39582 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.87 min) Epoch 42/300 -- Iteration 39591 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.88 min) Epoch 42/300 -- Iteration 39600 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.88 min) Epoch 42/300 -- Iteration 39609 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.89 min) Epoch 42/300 -- Iteration 39618 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.89 min) Epoch 42/300 -- Iteration 39627 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.89 min) Epoch 42/300 -- Iteration 39636 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.90 min) Epoch 42/300 -- Iteration 39645 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.90 min) Epoch 42/300 -- Iteration 39654 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.91 min) Epoch 42/300 -- Iteration 39663 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.91 min) Epoch 42/300 -- Iteration 39672 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.92 min) Epoch 42/300 -- Iteration 39681 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.92 min) Epoch 42/300 -- Iteration 39690 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.93 min) Epoch 42/300 -- Iteration 39699 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.93 min) Epoch 42/300 -- Iteration 39708 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.94 min) Epoch 42/300 -- Iteration 39717 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.94 min) Epoch 42/300 -- Iteration 39726 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.95 min) Epoch 42/300 -- Iteration 39735 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.95 min) Epoch 42/300 -- Iteration 39744 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.96 min) Epoch 42/300 -- Iteration 39753 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.96 min) Epoch 42/300 -- Iteration 39762 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.97 min) Epoch 42/300 -- Iteration 39771 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.97 min) Epoch 42/300 -- Iteration 39780 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.97 min) Epoch 42/300 -- Iteration 39789 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.98 min) Epoch 42/300 -- Iteration 39798 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.98 min) Epoch 42/300 -- Iteration 39807 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.99 min) Epoch 42/300 -- Iteration 39816 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(19.99 min) Epoch 42/300 -- Iteration 39825 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.00 min) Epoch 42/300 -- Iteration 39834 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.00 min) Epoch 42/300 -- Iteration 39843 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.01 min) Epoch 42/300 -- Iteration 39852 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.01 min) Epoch 42/300 -- Iteration 39861 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.02 min) Epoch 42/300 -- Iteration 39870 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.02 min) Epoch 42/300 -- Iteration 39879 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.03 min) Epoch 42/300 -- Iteration 39888 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.03 min) Epoch 42/300 -- Iteration 39897 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.04 min) Epoch 42/300 -- Iteration 39906 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.04 min) Epoch 42/300 -- Iteration 39915 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.05 min) Epoch 42/300 -- Iteration 39924 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.05 min) Epoch 42/300 -- Iteration 39933 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.05 min) Epoch 42/300 -- Iteration 39942 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.06 min) Epoch 42/300 -- Iteration 39951 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.06 min) Epoch 42/300 -- Iteration 39960 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.07 min) Epoch 42/300 -- Iteration 39969 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.07 min) Epoch 42/300 -- Iteration 39978 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.08 min) Epoch 42/300 -- Iteration 39987 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.08 min) Epoch 42/300 -- Iteration 39996 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.09 min) Epoch 42/300 -- Iteration 40005 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.09 min) Epoch 42/300 -- Iteration 40014 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.09 min) Epoch 42/300 -- Iteration 40023 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.10 min) Epoch 42/300 -- Iteration 40032 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.10 min) Epoch 42/300 -- Iteration 40041 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.11 min) Epoch 42/300 -- Iteration 40050 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.11 min) Epoch 42/300 -- Iteration 40059 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.12 min) Epoch 42/300 -- Iteration 40068 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.12 min) Epoch 42/300 -- Iteration 40077 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.13 min) Epoch 42/300 -- Iteration 40086 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.13 min) Epoch 42/300 -- Iteration 40095 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.13 min) Epoch 42/300 -- Iteration 40104 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.14 min) Epoch 42/300 -- Iteration 40113 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.14 min) Epoch 42/300 -- Iteration 40122 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.15 min) Epoch 42/300 -- Iteration 40131 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.15 min) Epoch 42/300 -- Iteration 40140 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.16 min) Epoch 42/300 -- Iteration 40149 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.16 min) Epoch 42/300 -- Iteration 40158 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.17 min) Epoch 42/300 -- Iteration 40167 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.17 min) Epoch 42/300 -- Iteration 40176 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.17 min) Epoch 42/300 -- Iteration 40185 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.18 min) Epoch 42/300 -- Iteration 40194 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.18 min) Epoch 42/300 -- Iteration 40203 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.19 min) Epoch 42/300 -- Iteration 40212 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.19 min) Epoch 42/300 -- Iteration 40221 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.20 min) Epoch 42/300 -- Iteration 40230 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.20 min) Epoch 42/300 -- Iteration 40239 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.21 min) Epoch 42/300 -- Iteration 40248 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.21 min) Epoch 42/300 -- Iteration 40257 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.22 min) Epoch 42/300 -- Iteration 40266 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.22 min) Epoch 42/300 -- Iteration 40275 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.22 min) Epoch 42/300 -- Iteration 40284 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.23 min) Epoch 42/300 -- Iteration 40293 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.23 min) Epoch 42/300 -- Iteration 40302 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.24 min) Epoch 42/300 -- Iteration 40311 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.24 min) Epoch 42/300 -- Iteration 40320 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.25 min) Epoch 42/300 -- Iteration 40329 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.25 min) Epoch 42/300 -- Iteration 40338 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.26 min) Epoch 42/300 -- Iteration 40347 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.26 min) Epoch 42/300 -- Iteration 40356 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.26 min) Epoch 42/300 -- Iteration 40365 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.27 min) Epoch 42/300 -- Iteration 40374 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.27 min) Epoch 42/300 -- Iteration 40383 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.28 min) Epoch 42/300 -- Iteration 40392 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.28 min) Epoch 42/300 -- Iteration 40401 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.29 min) Epoch 42/300 -- Iteration 40410 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.29 min) Epoch 42/300 -- Iteration 40419 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.30 min) Epoch 42/300 -- Iteration 40428 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.30 min) Epoch 42/300 -- Iteration 40437 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(20.31 min) Epoch 42/300 -- Iteration 40446 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021 - Val acc: -0.0000\n",
      "(20.31 min) Epoch 43/300 -- Iteration 40455 - Batch 9/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.32 min) Epoch 43/300 -- Iteration 40464 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.32 min) Epoch 43/300 -- Iteration 40473 - Batch 27/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.32 min) Epoch 43/300 -- Iteration 40482 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.33 min) Epoch 43/300 -- Iteration 40491 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.33 min) Epoch 43/300 -- Iteration 40500 - Batch 54/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.34 min) Epoch 43/300 -- Iteration 40509 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.34 min) Epoch 43/300 -- Iteration 40518 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.35 min) Epoch 43/300 -- Iteration 40527 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.35 min) Epoch 43/300 -- Iteration 40536 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.36 min) Epoch 43/300 -- Iteration 40545 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.36 min) Epoch 43/300 -- Iteration 40554 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.37 min) Epoch 43/300 -- Iteration 40563 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.37 min) Epoch 43/300 -- Iteration 40572 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.37 min) Epoch 43/300 -- Iteration 40581 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.38 min) Epoch 43/300 -- Iteration 40590 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.38 min) Epoch 43/300 -- Iteration 40599 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.39 min) Epoch 43/300 -- Iteration 40608 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.39 min) Epoch 43/300 -- Iteration 40617 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.40 min) Epoch 43/300 -- Iteration 40626 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.40 min) Epoch 43/300 -- Iteration 40635 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.41 min) Epoch 43/300 -- Iteration 40644 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.41 min) Epoch 43/300 -- Iteration 40653 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.41 min) Epoch 43/300 -- Iteration 40662 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.42 min) Epoch 43/300 -- Iteration 40671 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.42 min) Epoch 43/300 -- Iteration 40680 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.43 min) Epoch 43/300 -- Iteration 40689 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.43 min) Epoch 43/300 -- Iteration 40698 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.44 min) Epoch 43/300 -- Iteration 40707 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.44 min) Epoch 43/300 -- Iteration 40716 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.45 min) Epoch 43/300 -- Iteration 40725 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.45 min) Epoch 43/300 -- Iteration 40734 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.46 min) Epoch 43/300 -- Iteration 40743 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.46 min) Epoch 43/300 -- Iteration 40752 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.46 min) Epoch 43/300 -- Iteration 40761 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.47 min) Epoch 43/300 -- Iteration 40770 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.47 min) Epoch 43/300 -- Iteration 40779 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.48 min) Epoch 43/300 -- Iteration 40788 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.48 min) Epoch 43/300 -- Iteration 40797 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.49 min) Epoch 43/300 -- Iteration 40806 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.49 min) Epoch 43/300 -- Iteration 40815 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.50 min) Epoch 43/300 -- Iteration 40824 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.50 min) Epoch 43/300 -- Iteration 40833 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.50 min) Epoch 43/300 -- Iteration 40842 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.51 min) Epoch 43/300 -- Iteration 40851 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.51 min) Epoch 43/300 -- Iteration 40860 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.52 min) Epoch 43/300 -- Iteration 40869 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.52 min) Epoch 43/300 -- Iteration 40878 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.53 min) Epoch 43/300 -- Iteration 40887 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.53 min) Epoch 43/300 -- Iteration 40896 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.54 min) Epoch 43/300 -- Iteration 40905 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.54 min) Epoch 43/300 -- Iteration 40914 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.55 min) Epoch 43/300 -- Iteration 40923 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.55 min) Epoch 43/300 -- Iteration 40932 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.55 min) Epoch 43/300 -- Iteration 40941 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.56 min) Epoch 43/300 -- Iteration 40950 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.56 min) Epoch 43/300 -- Iteration 40959 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.57 min) Epoch 43/300 -- Iteration 40968 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.57 min) Epoch 43/300 -- Iteration 40977 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.58 min) Epoch 43/300 -- Iteration 40986 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.58 min) Epoch 43/300 -- Iteration 40995 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.59 min) Epoch 43/300 -- Iteration 41004 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.59 min) Epoch 43/300 -- Iteration 41013 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.59 min) Epoch 43/300 -- Iteration 41022 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.60 min) Epoch 43/300 -- Iteration 41031 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.60 min) Epoch 43/300 -- Iteration 41040 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.61 min) Epoch 43/300 -- Iteration 41049 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.61 min) Epoch 43/300 -- Iteration 41058 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.62 min) Epoch 43/300 -- Iteration 41067 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.62 min) Epoch 43/300 -- Iteration 41076 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.63 min) Epoch 43/300 -- Iteration 41085 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.63 min) Epoch 43/300 -- Iteration 41094 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.64 min) Epoch 43/300 -- Iteration 41103 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.64 min) Epoch 43/300 -- Iteration 41112 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.64 min) Epoch 43/300 -- Iteration 41121 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.65 min) Epoch 43/300 -- Iteration 41130 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.65 min) Epoch 43/300 -- Iteration 41139 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.66 min) Epoch 43/300 -- Iteration 41148 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.66 min) Epoch 43/300 -- Iteration 41157 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.67 min) Epoch 43/300 -- Iteration 41166 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.67 min) Epoch 43/300 -- Iteration 41175 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.68 min) Epoch 43/300 -- Iteration 41184 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.68 min) Epoch 43/300 -- Iteration 41193 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.68 min) Epoch 43/300 -- Iteration 41202 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.69 min) Epoch 43/300 -- Iteration 41211 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.69 min) Epoch 43/300 -- Iteration 41220 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.70 min) Epoch 43/300 -- Iteration 41229 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.70 min) Epoch 43/300 -- Iteration 41238 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.71 min) Epoch 43/300 -- Iteration 41247 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.71 min) Epoch 43/300 -- Iteration 41256 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.72 min) Epoch 43/300 -- Iteration 41265 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.72 min) Epoch 43/300 -- Iteration 41274 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.73 min) Epoch 43/300 -- Iteration 41283 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.73 min) Epoch 43/300 -- Iteration 41292 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.73 min) Epoch 43/300 -- Iteration 41301 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.74 min) Epoch 43/300 -- Iteration 41310 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.74 min) Epoch 43/300 -- Iteration 41319 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.75 min) Epoch 43/300 -- Iteration 41328 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.75 min) Epoch 43/300 -- Iteration 41337 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.76 min) Epoch 43/300 -- Iteration 41346 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.76 min) Epoch 43/300 -- Iteration 41355 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.77 min) Epoch 43/300 -- Iteration 41364 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.77 min) Epoch 43/300 -- Iteration 41373 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.78 min) Epoch 43/300 -- Iteration 41382 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.78 min) Epoch 43/300 -- Iteration 41391 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.78 min) Epoch 43/300 -- Iteration 41400 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.79 min) Epoch 43/300 -- Iteration 41409 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021 - Val acc: -0.0000\n",
      "(20.79 min) Epoch 44/300 -- Iteration 41418 - Batch 9/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.80 min) Epoch 44/300 -- Iteration 41427 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.80 min) Epoch 44/300 -- Iteration 41436 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.81 min) Epoch 44/300 -- Iteration 41445 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.81 min) Epoch 44/300 -- Iteration 41454 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.82 min) Epoch 44/300 -- Iteration 41463 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.82 min) Epoch 44/300 -- Iteration 41472 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.83 min) Epoch 44/300 -- Iteration 41481 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.83 min) Epoch 44/300 -- Iteration 41490 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.83 min) Epoch 44/300 -- Iteration 41499 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.84 min) Epoch 44/300 -- Iteration 41508 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.84 min) Epoch 44/300 -- Iteration 41517 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.85 min) Epoch 44/300 -- Iteration 41526 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.85 min) Epoch 44/300 -- Iteration 41535 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.86 min) Epoch 44/300 -- Iteration 41544 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.86 min) Epoch 44/300 -- Iteration 41553 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.87 min) Epoch 44/300 -- Iteration 41562 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.87 min) Epoch 44/300 -- Iteration 41571 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.88 min) Epoch 44/300 -- Iteration 41580 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.88 min) Epoch 44/300 -- Iteration 41589 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.88 min) Epoch 44/300 -- Iteration 41598 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.89 min) Epoch 44/300 -- Iteration 41607 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.89 min) Epoch 44/300 -- Iteration 41616 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.90 min) Epoch 44/300 -- Iteration 41625 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.90 min) Epoch 44/300 -- Iteration 41634 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.91 min) Epoch 44/300 -- Iteration 41643 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.91 min) Epoch 44/300 -- Iteration 41652 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.92 min) Epoch 44/300 -- Iteration 41661 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.92 min) Epoch 44/300 -- Iteration 41670 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.93 min) Epoch 44/300 -- Iteration 41679 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.93 min) Epoch 44/300 -- Iteration 41688 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.94 min) Epoch 44/300 -- Iteration 41697 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.94 min) Epoch 44/300 -- Iteration 41706 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.94 min) Epoch 44/300 -- Iteration 41715 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.95 min) Epoch 44/300 -- Iteration 41724 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.95 min) Epoch 44/300 -- Iteration 41733 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.96 min) Epoch 44/300 -- Iteration 41742 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.96 min) Epoch 44/300 -- Iteration 41751 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.97 min) Epoch 44/300 -- Iteration 41760 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.97 min) Epoch 44/300 -- Iteration 41769 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.98 min) Epoch 44/300 -- Iteration 41778 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.98 min) Epoch 44/300 -- Iteration 41787 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.99 min) Epoch 44/300 -- Iteration 41796 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.99 min) Epoch 44/300 -- Iteration 41805 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(20.99 min) Epoch 44/300 -- Iteration 41814 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.00 min) Epoch 44/300 -- Iteration 41823 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.00 min) Epoch 44/300 -- Iteration 41832 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.01 min) Epoch 44/300 -- Iteration 41841 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.01 min) Epoch 44/300 -- Iteration 41850 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.02 min) Epoch 44/300 -- Iteration 41859 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.02 min) Epoch 44/300 -- Iteration 41868 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.03 min) Epoch 44/300 -- Iteration 41877 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.03 min) Epoch 44/300 -- Iteration 41886 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.04 min) Epoch 44/300 -- Iteration 41895 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.04 min) Epoch 44/300 -- Iteration 41904 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.04 min) Epoch 44/300 -- Iteration 41913 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.05 min) Epoch 44/300 -- Iteration 41922 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.05 min) Epoch 44/300 -- Iteration 41931 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.06 min) Epoch 44/300 -- Iteration 41940 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.06 min) Epoch 44/300 -- Iteration 41949 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.07 min) Epoch 44/300 -- Iteration 41958 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.07 min) Epoch 44/300 -- Iteration 41967 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.08 min) Epoch 44/300 -- Iteration 41976 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.08 min) Epoch 44/300 -- Iteration 41985 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.08 min) Epoch 44/300 -- Iteration 41994 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.09 min) Epoch 44/300 -- Iteration 42003 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.09 min) Epoch 44/300 -- Iteration 42012 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.10 min) Epoch 44/300 -- Iteration 42021 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.10 min) Epoch 44/300 -- Iteration 42030 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.11 min) Epoch 44/300 -- Iteration 42039 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.11 min) Epoch 44/300 -- Iteration 42048 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.12 min) Epoch 44/300 -- Iteration 42057 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.12 min) Epoch 44/300 -- Iteration 42066 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.13 min) Epoch 44/300 -- Iteration 42075 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.13 min) Epoch 44/300 -- Iteration 42084 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.14 min) Epoch 44/300 -- Iteration 42093 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.14 min) Epoch 44/300 -- Iteration 42102 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.14 min) Epoch 44/300 -- Iteration 42111 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.15 min) Epoch 44/300 -- Iteration 42120 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.15 min) Epoch 44/300 -- Iteration 42129 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.16 min) Epoch 44/300 -- Iteration 42138 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.16 min) Epoch 44/300 -- Iteration 42147 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.17 min) Epoch 44/300 -- Iteration 42156 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.17 min) Epoch 44/300 -- Iteration 42165 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.18 min) Epoch 44/300 -- Iteration 42174 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.18 min) Epoch 44/300 -- Iteration 42183 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.19 min) Epoch 44/300 -- Iteration 42192 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.19 min) Epoch 44/300 -- Iteration 42201 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.19 min) Epoch 44/300 -- Iteration 42210 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.20 min) Epoch 44/300 -- Iteration 42219 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.20 min) Epoch 44/300 -- Iteration 42228 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.21 min) Epoch 44/300 -- Iteration 42237 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.21 min) Epoch 44/300 -- Iteration 42246 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.22 min) Epoch 44/300 -- Iteration 42255 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.22 min) Epoch 44/300 -- Iteration 42264 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.23 min) Epoch 44/300 -- Iteration 42273 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.23 min) Epoch 44/300 -- Iteration 42282 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.24 min) Epoch 44/300 -- Iteration 42291 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.24 min) Epoch 44/300 -- Iteration 42300 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.24 min) Epoch 44/300 -- Iteration 42309 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.25 min) Epoch 44/300 -- Iteration 42318 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.25 min) Epoch 44/300 -- Iteration 42327 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.26 min) Epoch 44/300 -- Iteration 42336 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.26 min) Epoch 44/300 -- Iteration 42345 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.27 min) Epoch 44/300 -- Iteration 42354 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.27 min) Epoch 44/300 -- Iteration 42363 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.28 min) Epoch 44/300 -- Iteration 42372 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(21.28 min) Epoch 45/300 -- Iteration 42381 - Batch 9/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.29 min) Epoch 45/300 -- Iteration 42390 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.29 min) Epoch 45/300 -- Iteration 42399 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.29 min) Epoch 45/300 -- Iteration 42408 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.30 min) Epoch 45/300 -- Iteration 42417 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.30 min) Epoch 45/300 -- Iteration 42426 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.31 min) Epoch 45/300 -- Iteration 42435 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.31 min) Epoch 45/300 -- Iteration 42444 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.32 min) Epoch 45/300 -- Iteration 42453 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.32 min) Epoch 45/300 -- Iteration 42462 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.33 min) Epoch 45/300 -- Iteration 42471 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.33 min) Epoch 45/300 -- Iteration 42480 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.33 min) Epoch 45/300 -- Iteration 42489 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.34 min) Epoch 45/300 -- Iteration 42498 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.34 min) Epoch 45/300 -- Iteration 42507 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.35 min) Epoch 45/300 -- Iteration 42516 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.35 min) Epoch 45/300 -- Iteration 42525 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.36 min) Epoch 45/300 -- Iteration 42534 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.36 min) Epoch 45/300 -- Iteration 42543 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.37 min) Epoch 45/300 -- Iteration 42552 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.37 min) Epoch 45/300 -- Iteration 42561 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.37 min) Epoch 45/300 -- Iteration 42570 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.38 min) Epoch 45/300 -- Iteration 42579 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.38 min) Epoch 45/300 -- Iteration 42588 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.39 min) Epoch 45/300 -- Iteration 42597 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.39 min) Epoch 45/300 -- Iteration 42606 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.40 min) Epoch 45/300 -- Iteration 42615 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.40 min) Epoch 45/300 -- Iteration 42624 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.41 min) Epoch 45/300 -- Iteration 42633 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.41 min) Epoch 45/300 -- Iteration 42642 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.41 min) Epoch 45/300 -- Iteration 42651 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.42 min) Epoch 45/300 -- Iteration 42660 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.42 min) Epoch 45/300 -- Iteration 42669 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.43 min) Epoch 45/300 -- Iteration 42678 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.43 min) Epoch 45/300 -- Iteration 42687 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.44 min) Epoch 45/300 -- Iteration 42696 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.44 min) Epoch 45/300 -- Iteration 42705 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.45 min) Epoch 45/300 -- Iteration 42714 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.45 min) Epoch 45/300 -- Iteration 42723 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.45 min) Epoch 45/300 -- Iteration 42732 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.46 min) Epoch 45/300 -- Iteration 42741 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.46 min) Epoch 45/300 -- Iteration 42750 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.47 min) Epoch 45/300 -- Iteration 42759 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.47 min) Epoch 45/300 -- Iteration 42768 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.48 min) Epoch 45/300 -- Iteration 42777 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.48 min) Epoch 45/300 -- Iteration 42786 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.49 min) Epoch 45/300 -- Iteration 42795 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.49 min) Epoch 45/300 -- Iteration 42804 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.50 min) Epoch 45/300 -- Iteration 42813 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.50 min) Epoch 45/300 -- Iteration 42822 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.50 min) Epoch 45/300 -- Iteration 42831 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.51 min) Epoch 45/300 -- Iteration 42840 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.51 min) Epoch 45/300 -- Iteration 42849 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.52 min) Epoch 45/300 -- Iteration 42858 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.52 min) Epoch 45/300 -- Iteration 42867 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.53 min) Epoch 45/300 -- Iteration 42876 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.53 min) Epoch 45/300 -- Iteration 42885 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.54 min) Epoch 45/300 -- Iteration 42894 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.54 min) Epoch 45/300 -- Iteration 42903 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.54 min) Epoch 45/300 -- Iteration 42912 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.55 min) Epoch 45/300 -- Iteration 42921 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.55 min) Epoch 45/300 -- Iteration 42930 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.56 min) Epoch 45/300 -- Iteration 42939 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.56 min) Epoch 45/300 -- Iteration 42948 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.57 min) Epoch 45/300 -- Iteration 42957 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.57 min) Epoch 45/300 -- Iteration 42966 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.58 min) Epoch 45/300 -- Iteration 42975 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.58 min) Epoch 45/300 -- Iteration 42984 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.58 min) Epoch 45/300 -- Iteration 42993 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.59 min) Epoch 45/300 -- Iteration 43002 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.59 min) Epoch 45/300 -- Iteration 43011 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.60 min) Epoch 45/300 -- Iteration 43020 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.60 min) Epoch 45/300 -- Iteration 43029 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.61 min) Epoch 45/300 -- Iteration 43038 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.61 min) Epoch 45/300 -- Iteration 43047 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.62 min) Epoch 45/300 -- Iteration 43056 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.62 min) Epoch 45/300 -- Iteration 43065 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.62 min) Epoch 45/300 -- Iteration 43074 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.63 min) Epoch 45/300 -- Iteration 43083 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.63 min) Epoch 45/300 -- Iteration 43092 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.64 min) Epoch 45/300 -- Iteration 43101 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.64 min) Epoch 45/300 -- Iteration 43110 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.65 min) Epoch 45/300 -- Iteration 43119 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.65 min) Epoch 45/300 -- Iteration 43128 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.66 min) Epoch 45/300 -- Iteration 43137 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.66 min) Epoch 45/300 -- Iteration 43146 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.66 min) Epoch 45/300 -- Iteration 43155 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.67 min) Epoch 45/300 -- Iteration 43164 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.67 min) Epoch 45/300 -- Iteration 43173 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.68 min) Epoch 45/300 -- Iteration 43182 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.68 min) Epoch 45/300 -- Iteration 43191 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.69 min) Epoch 45/300 -- Iteration 43200 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.69 min) Epoch 45/300 -- Iteration 43209 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.70 min) Epoch 45/300 -- Iteration 43218 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.70 min) Epoch 45/300 -- Iteration 43227 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.71 min) Epoch 45/300 -- Iteration 43236 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.71 min) Epoch 45/300 -- Iteration 43245 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.71 min) Epoch 45/300 -- Iteration 43254 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.72 min) Epoch 45/300 -- Iteration 43263 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.72 min) Epoch 45/300 -- Iteration 43272 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.73 min) Epoch 45/300 -- Iteration 43281 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.73 min) Epoch 45/300 -- Iteration 43290 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.74 min) Epoch 45/300 -- Iteration 43299 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.74 min) Epoch 45/300 -- Iteration 43308 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.75 min) Epoch 45/300 -- Iteration 43317 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.75 min) Epoch 45/300 -- Iteration 43326 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(21.75 min) Epoch 45/300 -- Iteration 43335 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021 - Val acc: -0.0000\n",
      "(21.76 min) Epoch 46/300 -- Iteration 43344 - Batch 9/963 - Train loss: 0.0027  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.76 min) Epoch 46/300 -- Iteration 43353 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.77 min) Epoch 46/300 -- Iteration 43362 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.77 min) Epoch 46/300 -- Iteration 43371 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.78 min) Epoch 46/300 -- Iteration 43380 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.78 min) Epoch 46/300 -- Iteration 43389 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.79 min) Epoch 46/300 -- Iteration 43398 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.79 min) Epoch 46/300 -- Iteration 43407 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.80 min) Epoch 46/300 -- Iteration 43416 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.80 min) Epoch 46/300 -- Iteration 43425 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.80 min) Epoch 46/300 -- Iteration 43434 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.81 min) Epoch 46/300 -- Iteration 43443 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.81 min) Epoch 46/300 -- Iteration 43452 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.82 min) Epoch 46/300 -- Iteration 43461 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.82 min) Epoch 46/300 -- Iteration 43470 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.83 min) Epoch 46/300 -- Iteration 43479 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.83 min) Epoch 46/300 -- Iteration 43488 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.84 min) Epoch 46/300 -- Iteration 43497 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.84 min) Epoch 46/300 -- Iteration 43506 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.84 min) Epoch 46/300 -- Iteration 43515 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.85 min) Epoch 46/300 -- Iteration 43524 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.85 min) Epoch 46/300 -- Iteration 43533 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.86 min) Epoch 46/300 -- Iteration 43542 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.86 min) Epoch 46/300 -- Iteration 43551 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.87 min) Epoch 46/300 -- Iteration 43560 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.87 min) Epoch 46/300 -- Iteration 43569 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.88 min) Epoch 46/300 -- Iteration 43578 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.88 min) Epoch 46/300 -- Iteration 43587 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.88 min) Epoch 46/300 -- Iteration 43596 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.89 min) Epoch 46/300 -- Iteration 43605 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.89 min) Epoch 46/300 -- Iteration 43614 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.90 min) Epoch 46/300 -- Iteration 43623 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.90 min) Epoch 46/300 -- Iteration 43632 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.91 min) Epoch 46/300 -- Iteration 43641 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.91 min) Epoch 46/300 -- Iteration 43650 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.92 min) Epoch 46/300 -- Iteration 43659 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.92 min) Epoch 46/300 -- Iteration 43668 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.92 min) Epoch 46/300 -- Iteration 43677 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.93 min) Epoch 46/300 -- Iteration 43686 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.93 min) Epoch 46/300 -- Iteration 43695 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.94 min) Epoch 46/300 -- Iteration 43704 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.94 min) Epoch 46/300 -- Iteration 43713 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.95 min) Epoch 46/300 -- Iteration 43722 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.95 min) Epoch 46/300 -- Iteration 43731 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.96 min) Epoch 46/300 -- Iteration 43740 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.96 min) Epoch 46/300 -- Iteration 43749 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.96 min) Epoch 46/300 -- Iteration 43758 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.97 min) Epoch 46/300 -- Iteration 43767 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.97 min) Epoch 46/300 -- Iteration 43776 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.98 min) Epoch 46/300 -- Iteration 43785 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.98 min) Epoch 46/300 -- Iteration 43794 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.99 min) Epoch 46/300 -- Iteration 43803 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(21.99 min) Epoch 46/300 -- Iteration 43812 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.00 min) Epoch 46/300 -- Iteration 43821 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.00 min) Epoch 46/300 -- Iteration 43830 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.01 min) Epoch 46/300 -- Iteration 43839 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.01 min) Epoch 46/300 -- Iteration 43848 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.01 min) Epoch 46/300 -- Iteration 43857 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.02 min) Epoch 46/300 -- Iteration 43866 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.02 min) Epoch 46/300 -- Iteration 43875 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.03 min) Epoch 46/300 -- Iteration 43884 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.03 min) Epoch 46/300 -- Iteration 43893 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.04 min) Epoch 46/300 -- Iteration 43902 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.04 min) Epoch 46/300 -- Iteration 43911 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.05 min) Epoch 46/300 -- Iteration 43920 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.05 min) Epoch 46/300 -- Iteration 43929 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.05 min) Epoch 46/300 -- Iteration 43938 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.06 min) Epoch 46/300 -- Iteration 43947 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.06 min) Epoch 46/300 -- Iteration 43956 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.07 min) Epoch 46/300 -- Iteration 43965 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.07 min) Epoch 46/300 -- Iteration 43974 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.08 min) Epoch 46/300 -- Iteration 43983 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.08 min) Epoch 46/300 -- Iteration 43992 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.09 min) Epoch 46/300 -- Iteration 44001 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.09 min) Epoch 46/300 -- Iteration 44010 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.09 min) Epoch 46/300 -- Iteration 44019 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.10 min) Epoch 46/300 -- Iteration 44028 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.10 min) Epoch 46/300 -- Iteration 44037 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.11 min) Epoch 46/300 -- Iteration 44046 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.11 min) Epoch 46/300 -- Iteration 44055 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.12 min) Epoch 46/300 -- Iteration 44064 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.12 min) Epoch 46/300 -- Iteration 44073 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.13 min) Epoch 46/300 -- Iteration 44082 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.13 min) Epoch 46/300 -- Iteration 44091 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.13 min) Epoch 46/300 -- Iteration 44100 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.14 min) Epoch 46/300 -- Iteration 44109 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.14 min) Epoch 46/300 -- Iteration 44118 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.15 min) Epoch 46/300 -- Iteration 44127 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.15 min) Epoch 46/300 -- Iteration 44136 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.16 min) Epoch 46/300 -- Iteration 44145 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.16 min) Epoch 46/300 -- Iteration 44154 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.17 min) Epoch 46/300 -- Iteration 44163 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.17 min) Epoch 46/300 -- Iteration 44172 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.17 min) Epoch 46/300 -- Iteration 44181 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.18 min) Epoch 46/300 -- Iteration 44190 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.18 min) Epoch 46/300 -- Iteration 44199 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.19 min) Epoch 46/300 -- Iteration 44208 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.19 min) Epoch 46/300 -- Iteration 44217 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.20 min) Epoch 46/300 -- Iteration 44226 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.20 min) Epoch 46/300 -- Iteration 44235 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.21 min) Epoch 46/300 -- Iteration 44244 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.21 min) Epoch 46/300 -- Iteration 44253 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.21 min) Epoch 46/300 -- Iteration 44262 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.22 min) Epoch 46/300 -- Iteration 44271 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.22 min) Epoch 46/300 -- Iteration 44280 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.23 min) Epoch 46/300 -- Iteration 44289 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(22.23 min) Epoch 46/300 -- Iteration 44298 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022 - Val acc: -0.0000\n",
      "(22.24 min) Epoch 47/300 -- Iteration 44307 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.24 min) Epoch 47/300 -- Iteration 44316 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.25 min) Epoch 47/300 -- Iteration 44325 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.25 min) Epoch 47/300 -- Iteration 44334 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.26 min) Epoch 47/300 -- Iteration 44343 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.26 min) Epoch 47/300 -- Iteration 44352 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.26 min) Epoch 47/300 -- Iteration 44361 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.27 min) Epoch 47/300 -- Iteration 44370 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.27 min) Epoch 47/300 -- Iteration 44379 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.28 min) Epoch 47/300 -- Iteration 44388 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.28 min) Epoch 47/300 -- Iteration 44397 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.29 min) Epoch 47/300 -- Iteration 44406 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.29 min) Epoch 47/300 -- Iteration 44415 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.30 min) Epoch 47/300 -- Iteration 44424 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.30 min) Epoch 47/300 -- Iteration 44433 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.30 min) Epoch 47/300 -- Iteration 44442 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.31 min) Epoch 47/300 -- Iteration 44451 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.31 min) Epoch 47/300 -- Iteration 44460 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.32 min) Epoch 47/300 -- Iteration 44469 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.32 min) Epoch 47/300 -- Iteration 44478 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.33 min) Epoch 47/300 -- Iteration 44487 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.33 min) Epoch 47/300 -- Iteration 44496 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.34 min) Epoch 47/300 -- Iteration 44505 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.34 min) Epoch 47/300 -- Iteration 44514 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.34 min) Epoch 47/300 -- Iteration 44523 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.35 min) Epoch 47/300 -- Iteration 44532 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.35 min) Epoch 47/300 -- Iteration 44541 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.36 min) Epoch 47/300 -- Iteration 44550 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.36 min) Epoch 47/300 -- Iteration 44559 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.37 min) Epoch 47/300 -- Iteration 44568 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.37 min) Epoch 47/300 -- Iteration 44577 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.38 min) Epoch 47/300 -- Iteration 44586 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.38 min) Epoch 47/300 -- Iteration 44595 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.38 min) Epoch 47/300 -- Iteration 44604 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.39 min) Epoch 47/300 -- Iteration 44613 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.39 min) Epoch 47/300 -- Iteration 44622 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.40 min) Epoch 47/300 -- Iteration 44631 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.40 min) Epoch 47/300 -- Iteration 44640 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.41 min) Epoch 47/300 -- Iteration 44649 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.41 min) Epoch 47/300 -- Iteration 44658 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.42 min) Epoch 47/300 -- Iteration 44667 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.42 min) Epoch 47/300 -- Iteration 44676 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.42 min) Epoch 47/300 -- Iteration 44685 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.43 min) Epoch 47/300 -- Iteration 44694 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.43 min) Epoch 47/300 -- Iteration 44703 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.44 min) Epoch 47/300 -- Iteration 44712 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.44 min) Epoch 47/300 -- Iteration 44721 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.45 min) Epoch 47/300 -- Iteration 44730 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.45 min) Epoch 47/300 -- Iteration 44739 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.46 min) Epoch 47/300 -- Iteration 44748 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.46 min) Epoch 47/300 -- Iteration 44757 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.47 min) Epoch 47/300 -- Iteration 44766 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.47 min) Epoch 47/300 -- Iteration 44775 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.47 min) Epoch 47/300 -- Iteration 44784 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.48 min) Epoch 47/300 -- Iteration 44793 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.48 min) Epoch 47/300 -- Iteration 44802 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.49 min) Epoch 47/300 -- Iteration 44811 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.49 min) Epoch 47/300 -- Iteration 44820 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.50 min) Epoch 47/300 -- Iteration 44829 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.50 min) Epoch 47/300 -- Iteration 44838 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.51 min) Epoch 47/300 -- Iteration 44847 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.51 min) Epoch 47/300 -- Iteration 44856 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.51 min) Epoch 47/300 -- Iteration 44865 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.52 min) Epoch 47/300 -- Iteration 44874 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.52 min) Epoch 47/300 -- Iteration 44883 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.53 min) Epoch 47/300 -- Iteration 44892 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.53 min) Epoch 47/300 -- Iteration 44901 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.54 min) Epoch 47/300 -- Iteration 44910 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.54 min) Epoch 47/300 -- Iteration 44919 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.55 min) Epoch 47/300 -- Iteration 44928 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.55 min) Epoch 47/300 -- Iteration 44937 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.55 min) Epoch 47/300 -- Iteration 44946 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.56 min) Epoch 47/300 -- Iteration 44955 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.56 min) Epoch 47/300 -- Iteration 44964 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.57 min) Epoch 47/300 -- Iteration 44973 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.57 min) Epoch 47/300 -- Iteration 44982 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.58 min) Epoch 47/300 -- Iteration 44991 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.58 min) Epoch 47/300 -- Iteration 45000 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.59 min) Epoch 47/300 -- Iteration 45009 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.59 min) Epoch 47/300 -- Iteration 45018 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.59 min) Epoch 47/300 -- Iteration 45027 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.60 min) Epoch 47/300 -- Iteration 45036 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.60 min) Epoch 47/300 -- Iteration 45045 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.61 min) Epoch 47/300 -- Iteration 45054 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.61 min) Epoch 47/300 -- Iteration 45063 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.62 min) Epoch 47/300 -- Iteration 45072 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.62 min) Epoch 47/300 -- Iteration 45081 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.63 min) Epoch 47/300 -- Iteration 45090 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.63 min) Epoch 47/300 -- Iteration 45099 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.63 min) Epoch 47/300 -- Iteration 45108 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.64 min) Epoch 47/300 -- Iteration 45117 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.64 min) Epoch 47/300 -- Iteration 45126 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.65 min) Epoch 47/300 -- Iteration 45135 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.65 min) Epoch 47/300 -- Iteration 45144 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.66 min) Epoch 47/300 -- Iteration 45153 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.66 min) Epoch 47/300 -- Iteration 45162 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.67 min) Epoch 47/300 -- Iteration 45171 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.67 min) Epoch 47/300 -- Iteration 45180 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.68 min) Epoch 47/300 -- Iteration 45189 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.68 min) Epoch 47/300 -- Iteration 45198 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.68 min) Epoch 47/300 -- Iteration 45207 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.69 min) Epoch 47/300 -- Iteration 45216 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.69 min) Epoch 47/300 -- Iteration 45225 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.70 min) Epoch 47/300 -- Iteration 45234 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.70 min) Epoch 47/300 -- Iteration 45243 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.71 min) Epoch 47/300 -- Iteration 45252 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0022\n",
      "(22.71 min) Epoch 47/300 -- Iteration 45261 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(22.72 min) Epoch 48/300 -- Iteration 45270 - Batch 9/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.72 min) Epoch 48/300 -- Iteration 45279 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.72 min) Epoch 48/300 -- Iteration 45288 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.73 min) Epoch 48/300 -- Iteration 45297 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.73 min) Epoch 48/300 -- Iteration 45306 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.74 min) Epoch 48/300 -- Iteration 45315 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.74 min) Epoch 48/300 -- Iteration 45324 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.75 min) Epoch 48/300 -- Iteration 45333 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.75 min) Epoch 48/300 -- Iteration 45342 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.76 min) Epoch 48/300 -- Iteration 45351 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.76 min) Epoch 48/300 -- Iteration 45360 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.76 min) Epoch 48/300 -- Iteration 45369 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.77 min) Epoch 48/300 -- Iteration 45378 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.77 min) Epoch 48/300 -- Iteration 45387 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.78 min) Epoch 48/300 -- Iteration 45396 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.78 min) Epoch 48/300 -- Iteration 45405 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.79 min) Epoch 48/300 -- Iteration 45414 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.79 min) Epoch 48/300 -- Iteration 45423 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.80 min) Epoch 48/300 -- Iteration 45432 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.80 min) Epoch 48/300 -- Iteration 45441 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.80 min) Epoch 48/300 -- Iteration 45450 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.81 min) Epoch 48/300 -- Iteration 45459 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.81 min) Epoch 48/300 -- Iteration 45468 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.82 min) Epoch 48/300 -- Iteration 45477 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.82 min) Epoch 48/300 -- Iteration 45486 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.83 min) Epoch 48/300 -- Iteration 45495 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.83 min) Epoch 48/300 -- Iteration 45504 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.84 min) Epoch 48/300 -- Iteration 45513 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.84 min) Epoch 48/300 -- Iteration 45522 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.85 min) Epoch 48/300 -- Iteration 45531 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.85 min) Epoch 48/300 -- Iteration 45540 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.85 min) Epoch 48/300 -- Iteration 45549 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.86 min) Epoch 48/300 -- Iteration 45558 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.86 min) Epoch 48/300 -- Iteration 45567 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.87 min) Epoch 48/300 -- Iteration 45576 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.87 min) Epoch 48/300 -- Iteration 45585 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.88 min) Epoch 48/300 -- Iteration 45594 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.88 min) Epoch 48/300 -- Iteration 45603 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.89 min) Epoch 48/300 -- Iteration 45612 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.89 min) Epoch 48/300 -- Iteration 45621 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.89 min) Epoch 48/300 -- Iteration 45630 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.90 min) Epoch 48/300 -- Iteration 45639 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.90 min) Epoch 48/300 -- Iteration 45648 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.91 min) Epoch 48/300 -- Iteration 45657 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.91 min) Epoch 48/300 -- Iteration 45666 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.92 min) Epoch 48/300 -- Iteration 45675 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.92 min) Epoch 48/300 -- Iteration 45684 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.93 min) Epoch 48/300 -- Iteration 45693 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.93 min) Epoch 48/300 -- Iteration 45702 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.93 min) Epoch 48/300 -- Iteration 45711 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.94 min) Epoch 48/300 -- Iteration 45720 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.94 min) Epoch 48/300 -- Iteration 45729 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.95 min) Epoch 48/300 -- Iteration 45738 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.95 min) Epoch 48/300 -- Iteration 45747 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.96 min) Epoch 48/300 -- Iteration 45756 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.96 min) Epoch 48/300 -- Iteration 45765 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.97 min) Epoch 48/300 -- Iteration 45774 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.97 min) Epoch 48/300 -- Iteration 45783 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.97 min) Epoch 48/300 -- Iteration 45792 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.98 min) Epoch 48/300 -- Iteration 45801 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.98 min) Epoch 48/300 -- Iteration 45810 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.99 min) Epoch 48/300 -- Iteration 45819 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(22.99 min) Epoch 48/300 -- Iteration 45828 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.00 min) Epoch 48/300 -- Iteration 45837 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.00 min) Epoch 48/300 -- Iteration 45846 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.01 min) Epoch 48/300 -- Iteration 45855 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.01 min) Epoch 48/300 -- Iteration 45864 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.01 min) Epoch 48/300 -- Iteration 45873 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.02 min) Epoch 48/300 -- Iteration 45882 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.02 min) Epoch 48/300 -- Iteration 45891 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.03 min) Epoch 48/300 -- Iteration 45900 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.03 min) Epoch 48/300 -- Iteration 45909 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.04 min) Epoch 48/300 -- Iteration 45918 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.04 min) Epoch 48/300 -- Iteration 45927 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.05 min) Epoch 48/300 -- Iteration 45936 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.05 min) Epoch 48/300 -- Iteration 45945 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.05 min) Epoch 48/300 -- Iteration 45954 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.06 min) Epoch 48/300 -- Iteration 45963 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.06 min) Epoch 48/300 -- Iteration 45972 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.07 min) Epoch 48/300 -- Iteration 45981 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.07 min) Epoch 48/300 -- Iteration 45990 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.08 min) Epoch 48/300 -- Iteration 45999 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.08 min) Epoch 48/300 -- Iteration 46008 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.09 min) Epoch 48/300 -- Iteration 46017 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.09 min) Epoch 48/300 -- Iteration 46026 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.10 min) Epoch 48/300 -- Iteration 46035 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.10 min) Epoch 48/300 -- Iteration 46044 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.10 min) Epoch 48/300 -- Iteration 46053 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.11 min) Epoch 48/300 -- Iteration 46062 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.11 min) Epoch 48/300 -- Iteration 46071 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.12 min) Epoch 48/300 -- Iteration 46080 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.12 min) Epoch 48/300 -- Iteration 46089 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.13 min) Epoch 48/300 -- Iteration 46098 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.13 min) Epoch 48/300 -- Iteration 46107 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.14 min) Epoch 48/300 -- Iteration 46116 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.14 min) Epoch 48/300 -- Iteration 46125 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.14 min) Epoch 48/300 -- Iteration 46134 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.15 min) Epoch 48/300 -- Iteration 46143 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.15 min) Epoch 48/300 -- Iteration 46152 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.16 min) Epoch 48/300 -- Iteration 46161 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.16 min) Epoch 48/300 -- Iteration 46170 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.17 min) Epoch 48/300 -- Iteration 46179 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.17 min) Epoch 48/300 -- Iteration 46188 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.18 min) Epoch 48/300 -- Iteration 46197 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.18 min) Epoch 48/300 -- Iteration 46206 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.18 min) Epoch 48/300 -- Iteration 46215 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(23.19 min) Epoch 48/300 -- Iteration 46224 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021 - Val acc: -0.0000\n",
      "(23.19 min) Epoch 49/300 -- Iteration 46233 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.20 min) Epoch 49/300 -- Iteration 46242 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.20 min) Epoch 49/300 -- Iteration 46251 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.21 min) Epoch 49/300 -- Iteration 46260 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.21 min) Epoch 49/300 -- Iteration 46269 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.22 min) Epoch 49/300 -- Iteration 46278 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.22 min) Epoch 49/300 -- Iteration 46287 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.23 min) Epoch 49/300 -- Iteration 46296 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.23 min) Epoch 49/300 -- Iteration 46305 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.23 min) Epoch 49/300 -- Iteration 46314 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.24 min) Epoch 49/300 -- Iteration 46323 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.24 min) Epoch 49/300 -- Iteration 46332 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.25 min) Epoch 49/300 -- Iteration 46341 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.25 min) Epoch 49/300 -- Iteration 46350 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.26 min) Epoch 49/300 -- Iteration 46359 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.26 min) Epoch 49/300 -- Iteration 46368 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.27 min) Epoch 49/300 -- Iteration 46377 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.27 min) Epoch 49/300 -- Iteration 46386 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.27 min) Epoch 49/300 -- Iteration 46395 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.28 min) Epoch 49/300 -- Iteration 46404 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.28 min) Epoch 49/300 -- Iteration 46413 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.29 min) Epoch 49/300 -- Iteration 46422 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.29 min) Epoch 49/300 -- Iteration 46431 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.30 min) Epoch 49/300 -- Iteration 46440 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.30 min) Epoch 49/300 -- Iteration 46449 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.31 min) Epoch 49/300 -- Iteration 46458 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.31 min) Epoch 49/300 -- Iteration 46467 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.31 min) Epoch 49/300 -- Iteration 46476 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.32 min) Epoch 49/300 -- Iteration 46485 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.32 min) Epoch 49/300 -- Iteration 46494 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.33 min) Epoch 49/300 -- Iteration 46503 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.33 min) Epoch 49/300 -- Iteration 46512 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.34 min) Epoch 49/300 -- Iteration 46521 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.34 min) Epoch 49/300 -- Iteration 46530 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.35 min) Epoch 49/300 -- Iteration 46539 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.35 min) Epoch 49/300 -- Iteration 46548 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.36 min) Epoch 49/300 -- Iteration 46557 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.36 min) Epoch 49/300 -- Iteration 46566 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.36 min) Epoch 49/300 -- Iteration 46575 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.37 min) Epoch 49/300 -- Iteration 46584 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.37 min) Epoch 49/300 -- Iteration 46593 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.38 min) Epoch 49/300 -- Iteration 46602 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.38 min) Epoch 49/300 -- Iteration 46611 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.39 min) Epoch 49/300 -- Iteration 46620 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.39 min) Epoch 49/300 -- Iteration 46629 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.40 min) Epoch 49/300 -- Iteration 46638 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.40 min) Epoch 49/300 -- Iteration 46647 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.40 min) Epoch 49/300 -- Iteration 46656 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.41 min) Epoch 49/300 -- Iteration 46665 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.41 min) Epoch 49/300 -- Iteration 46674 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.42 min) Epoch 49/300 -- Iteration 46683 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.42 min) Epoch 49/300 -- Iteration 46692 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.43 min) Epoch 49/300 -- Iteration 46701 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.43 min) Epoch 49/300 -- Iteration 46710 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.44 min) Epoch 49/300 -- Iteration 46719 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.44 min) Epoch 49/300 -- Iteration 46728 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.44 min) Epoch 49/300 -- Iteration 46737 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.45 min) Epoch 49/300 -- Iteration 46746 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.45 min) Epoch 49/300 -- Iteration 46755 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.46 min) Epoch 49/300 -- Iteration 46764 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.46 min) Epoch 49/300 -- Iteration 46773 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.47 min) Epoch 49/300 -- Iteration 46782 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.47 min) Epoch 49/300 -- Iteration 46791 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.48 min) Epoch 49/300 -- Iteration 46800 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.48 min) Epoch 49/300 -- Iteration 46809 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.48 min) Epoch 49/300 -- Iteration 46818 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.49 min) Epoch 49/300 -- Iteration 46827 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.49 min) Epoch 49/300 -- Iteration 46836 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.50 min) Epoch 49/300 -- Iteration 46845 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.50 min) Epoch 49/300 -- Iteration 46854 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.51 min) Epoch 49/300 -- Iteration 46863 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.51 min) Epoch 49/300 -- Iteration 46872 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.52 min) Epoch 49/300 -- Iteration 46881 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.52 min) Epoch 49/300 -- Iteration 46890 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.52 min) Epoch 49/300 -- Iteration 46899 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.53 min) Epoch 49/300 -- Iteration 46908 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.53 min) Epoch 49/300 -- Iteration 46917 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.54 min) Epoch 49/300 -- Iteration 46926 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.54 min) Epoch 49/300 -- Iteration 46935 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.55 min) Epoch 49/300 -- Iteration 46944 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.55 min) Epoch 49/300 -- Iteration 46953 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.56 min) Epoch 49/300 -- Iteration 46962 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.56 min) Epoch 49/300 -- Iteration 46971 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.56 min) Epoch 49/300 -- Iteration 46980 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.57 min) Epoch 49/300 -- Iteration 46989 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.57 min) Epoch 49/300 -- Iteration 46998 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.58 min) Epoch 49/300 -- Iteration 47007 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.58 min) Epoch 49/300 -- Iteration 47016 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.59 min) Epoch 49/300 -- Iteration 47025 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.59 min) Epoch 49/300 -- Iteration 47034 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.60 min) Epoch 49/300 -- Iteration 47043 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.60 min) Epoch 49/300 -- Iteration 47052 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.60 min) Epoch 49/300 -- Iteration 47061 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.61 min) Epoch 49/300 -- Iteration 47070 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.61 min) Epoch 49/300 -- Iteration 47079 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.62 min) Epoch 49/300 -- Iteration 47088 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.62 min) Epoch 49/300 -- Iteration 47097 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.63 min) Epoch 49/300 -- Iteration 47106 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.63 min) Epoch 49/300 -- Iteration 47115 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.64 min) Epoch 49/300 -- Iteration 47124 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.64 min) Epoch 49/300 -- Iteration 47133 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.64 min) Epoch 49/300 -- Iteration 47142 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.65 min) Epoch 49/300 -- Iteration 47151 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.65 min) Epoch 49/300 -- Iteration 47160 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.66 min) Epoch 49/300 -- Iteration 47169 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.66 min) Epoch 49/300 -- Iteration 47178 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(23.67 min) Epoch 49/300 -- Iteration 47187 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(23.67 min) Epoch 50/300 -- Iteration 47196 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.68 min) Epoch 50/300 -- Iteration 47205 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.68 min) Epoch 50/300 -- Iteration 47214 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.69 min) Epoch 50/300 -- Iteration 47223 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.69 min) Epoch 50/300 -- Iteration 47232 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.69 min) Epoch 50/300 -- Iteration 47241 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.70 min) Epoch 50/300 -- Iteration 47250 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.70 min) Epoch 50/300 -- Iteration 47259 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.71 min) Epoch 50/300 -- Iteration 47268 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.71 min) Epoch 50/300 -- Iteration 47277 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.72 min) Epoch 50/300 -- Iteration 47286 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.72 min) Epoch 50/300 -- Iteration 47295 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.73 min) Epoch 50/300 -- Iteration 47304 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.73 min) Epoch 50/300 -- Iteration 47313 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.73 min) Epoch 50/300 -- Iteration 47322 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.74 min) Epoch 50/300 -- Iteration 47331 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.74 min) Epoch 50/300 -- Iteration 47340 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.75 min) Epoch 50/300 -- Iteration 47349 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.75 min) Epoch 50/300 -- Iteration 47358 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.76 min) Epoch 50/300 -- Iteration 47367 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.76 min) Epoch 50/300 -- Iteration 47376 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.77 min) Epoch 50/300 -- Iteration 47385 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.77 min) Epoch 50/300 -- Iteration 47394 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.78 min) Epoch 50/300 -- Iteration 47403 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.78 min) Epoch 50/300 -- Iteration 47412 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.78 min) Epoch 50/300 -- Iteration 47421 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.79 min) Epoch 50/300 -- Iteration 47430 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.79 min) Epoch 50/300 -- Iteration 47439 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.80 min) Epoch 50/300 -- Iteration 47448 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.80 min) Epoch 50/300 -- Iteration 47457 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.81 min) Epoch 50/300 -- Iteration 47466 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.81 min) Epoch 50/300 -- Iteration 47475 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.81 min) Epoch 50/300 -- Iteration 47484 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.82 min) Epoch 50/300 -- Iteration 47493 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.82 min) Epoch 50/300 -- Iteration 47502 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.83 min) Epoch 50/300 -- Iteration 47511 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.83 min) Epoch 50/300 -- Iteration 47520 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.84 min) Epoch 50/300 -- Iteration 47529 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.84 min) Epoch 50/300 -- Iteration 47538 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.85 min) Epoch 50/300 -- Iteration 47547 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.85 min) Epoch 50/300 -- Iteration 47556 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.86 min) Epoch 50/300 -- Iteration 47565 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.86 min) Epoch 50/300 -- Iteration 47574 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.86 min) Epoch 50/300 -- Iteration 47583 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.87 min) Epoch 50/300 -- Iteration 47592 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.87 min) Epoch 50/300 -- Iteration 47601 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.88 min) Epoch 50/300 -- Iteration 47610 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.88 min) Epoch 50/300 -- Iteration 47619 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.89 min) Epoch 50/300 -- Iteration 47628 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.89 min) Epoch 50/300 -- Iteration 47637 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.90 min) Epoch 50/300 -- Iteration 47646 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.90 min) Epoch 50/300 -- Iteration 47655 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.90 min) Epoch 50/300 -- Iteration 47664 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.91 min) Epoch 50/300 -- Iteration 47673 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.91 min) Epoch 50/300 -- Iteration 47682 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.92 min) Epoch 50/300 -- Iteration 47691 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.92 min) Epoch 50/300 -- Iteration 47700 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.93 min) Epoch 50/300 -- Iteration 47709 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.93 min) Epoch 50/300 -- Iteration 47718 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.94 min) Epoch 50/300 -- Iteration 47727 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.94 min) Epoch 50/300 -- Iteration 47736 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.94 min) Epoch 50/300 -- Iteration 47745 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.95 min) Epoch 50/300 -- Iteration 47754 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.95 min) Epoch 50/300 -- Iteration 47763 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.96 min) Epoch 50/300 -- Iteration 47772 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.96 min) Epoch 50/300 -- Iteration 47781 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.97 min) Epoch 50/300 -- Iteration 47790 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.97 min) Epoch 50/300 -- Iteration 47799 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.98 min) Epoch 50/300 -- Iteration 47808 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.98 min) Epoch 50/300 -- Iteration 47817 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.98 min) Epoch 50/300 -- Iteration 47826 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.99 min) Epoch 50/300 -- Iteration 47835 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(23.99 min) Epoch 50/300 -- Iteration 47844 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.00 min) Epoch 50/300 -- Iteration 47853 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.00 min) Epoch 50/300 -- Iteration 47862 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.01 min) Epoch 50/300 -- Iteration 47871 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.01 min) Epoch 50/300 -- Iteration 47880 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.02 min) Epoch 50/300 -- Iteration 47889 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.02 min) Epoch 50/300 -- Iteration 47898 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.02 min) Epoch 50/300 -- Iteration 47907 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.03 min) Epoch 50/300 -- Iteration 47916 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.03 min) Epoch 50/300 -- Iteration 47925 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.04 min) Epoch 50/300 -- Iteration 47934 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.04 min) Epoch 50/300 -- Iteration 47943 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.05 min) Epoch 50/300 -- Iteration 47952 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.05 min) Epoch 50/300 -- Iteration 47961 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.06 min) Epoch 50/300 -- Iteration 47970 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.06 min) Epoch 50/300 -- Iteration 47979 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.06 min) Epoch 50/300 -- Iteration 47988 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.07 min) Epoch 50/300 -- Iteration 47997 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.07 min) Epoch 50/300 -- Iteration 48006 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.08 min) Epoch 50/300 -- Iteration 48015 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.08 min) Epoch 50/300 -- Iteration 48024 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.09 min) Epoch 50/300 -- Iteration 48033 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.09 min) Epoch 50/300 -- Iteration 48042 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.10 min) Epoch 50/300 -- Iteration 48051 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.10 min) Epoch 50/300 -- Iteration 48060 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.10 min) Epoch 50/300 -- Iteration 48069 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.11 min) Epoch 50/300 -- Iteration 48078 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.11 min) Epoch 50/300 -- Iteration 48087 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.12 min) Epoch 50/300 -- Iteration 48096 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.12 min) Epoch 50/300 -- Iteration 48105 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.13 min) Epoch 50/300 -- Iteration 48114 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.13 min) Epoch 50/300 -- Iteration 48123 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.14 min) Epoch 50/300 -- Iteration 48132 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.14 min) Epoch 50/300 -- Iteration 48141 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(24.14 min) Epoch 50/300 -- Iteration 48150 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018 - Val acc: -0.0000\n",
      "(24.15 min) Epoch 51/300 -- Iteration 48159 - Batch 9/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.15 min) Epoch 51/300 -- Iteration 48168 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.16 min) Epoch 51/300 -- Iteration 48177 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.16 min) Epoch 51/300 -- Iteration 48186 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.17 min) Epoch 51/300 -- Iteration 48195 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.17 min) Epoch 51/300 -- Iteration 48204 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.18 min) Epoch 51/300 -- Iteration 48213 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.18 min) Epoch 51/300 -- Iteration 48222 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.19 min) Epoch 51/300 -- Iteration 48231 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.19 min) Epoch 51/300 -- Iteration 48240 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.20 min) Epoch 51/300 -- Iteration 48249 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.20 min) Epoch 51/300 -- Iteration 48258 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.20 min) Epoch 51/300 -- Iteration 48267 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.21 min) Epoch 51/300 -- Iteration 48276 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.21 min) Epoch 51/300 -- Iteration 48285 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.22 min) Epoch 51/300 -- Iteration 48294 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.22 min) Epoch 51/300 -- Iteration 48303 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.23 min) Epoch 51/300 -- Iteration 48312 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.23 min) Epoch 51/300 -- Iteration 48321 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.24 min) Epoch 51/300 -- Iteration 48330 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.24 min) Epoch 51/300 -- Iteration 48339 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.24 min) Epoch 51/300 -- Iteration 48348 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.25 min) Epoch 51/300 -- Iteration 48357 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.25 min) Epoch 51/300 -- Iteration 48366 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.26 min) Epoch 51/300 -- Iteration 48375 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.26 min) Epoch 51/300 -- Iteration 48384 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.27 min) Epoch 51/300 -- Iteration 48393 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.27 min) Epoch 51/300 -- Iteration 48402 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.28 min) Epoch 51/300 -- Iteration 48411 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.28 min) Epoch 51/300 -- Iteration 48420 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.28 min) Epoch 51/300 -- Iteration 48429 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.29 min) Epoch 51/300 -- Iteration 48438 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.29 min) Epoch 51/300 -- Iteration 48447 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.30 min) Epoch 51/300 -- Iteration 48456 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.30 min) Epoch 51/300 -- Iteration 48465 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.31 min) Epoch 51/300 -- Iteration 48474 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.31 min) Epoch 51/300 -- Iteration 48483 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.32 min) Epoch 51/300 -- Iteration 48492 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.32 min) Epoch 51/300 -- Iteration 48501 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.33 min) Epoch 51/300 -- Iteration 48510 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.33 min) Epoch 51/300 -- Iteration 48519 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.33 min) Epoch 51/300 -- Iteration 48528 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.34 min) Epoch 51/300 -- Iteration 48537 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.34 min) Epoch 51/300 -- Iteration 48546 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.35 min) Epoch 51/300 -- Iteration 48555 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.35 min) Epoch 51/300 -- Iteration 48564 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.36 min) Epoch 51/300 -- Iteration 48573 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.36 min) Epoch 51/300 -- Iteration 48582 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.37 min) Epoch 51/300 -- Iteration 48591 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.37 min) Epoch 51/300 -- Iteration 48600 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.37 min) Epoch 51/300 -- Iteration 48609 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.38 min) Epoch 51/300 -- Iteration 48618 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.38 min) Epoch 51/300 -- Iteration 48627 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.39 min) Epoch 51/300 -- Iteration 48636 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.39 min) Epoch 51/300 -- Iteration 48645 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.40 min) Epoch 51/300 -- Iteration 48654 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.40 min) Epoch 51/300 -- Iteration 48663 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.41 min) Epoch 51/300 -- Iteration 48672 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.41 min) Epoch 51/300 -- Iteration 48681 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.41 min) Epoch 51/300 -- Iteration 48690 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.42 min) Epoch 51/300 -- Iteration 48699 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.42 min) Epoch 51/300 -- Iteration 48708 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.43 min) Epoch 51/300 -- Iteration 48717 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.43 min) Epoch 51/300 -- Iteration 48726 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.44 min) Epoch 51/300 -- Iteration 48735 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.44 min) Epoch 51/300 -- Iteration 48744 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.45 min) Epoch 51/300 -- Iteration 48753 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.45 min) Epoch 51/300 -- Iteration 48762 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.45 min) Epoch 51/300 -- Iteration 48771 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.46 min) Epoch 51/300 -- Iteration 48780 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.46 min) Epoch 51/300 -- Iteration 48789 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.47 min) Epoch 51/300 -- Iteration 48798 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.47 min) Epoch 51/300 -- Iteration 48807 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.48 min) Epoch 51/300 -- Iteration 48816 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.48 min) Epoch 51/300 -- Iteration 48825 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.49 min) Epoch 51/300 -- Iteration 48834 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.49 min) Epoch 51/300 -- Iteration 48843 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.49 min) Epoch 51/300 -- Iteration 48852 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.50 min) Epoch 51/300 -- Iteration 48861 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.50 min) Epoch 51/300 -- Iteration 48870 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.51 min) Epoch 51/300 -- Iteration 48879 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.51 min) Epoch 51/300 -- Iteration 48888 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.52 min) Epoch 51/300 -- Iteration 48897 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.52 min) Epoch 51/300 -- Iteration 48906 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.53 min) Epoch 51/300 -- Iteration 48915 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.53 min) Epoch 51/300 -- Iteration 48924 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.54 min) Epoch 51/300 -- Iteration 48933 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.54 min) Epoch 51/300 -- Iteration 48942 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.54 min) Epoch 51/300 -- Iteration 48951 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.55 min) Epoch 51/300 -- Iteration 48960 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.55 min) Epoch 51/300 -- Iteration 48969 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.56 min) Epoch 51/300 -- Iteration 48978 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.56 min) Epoch 51/300 -- Iteration 48987 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.57 min) Epoch 51/300 -- Iteration 48996 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.57 min) Epoch 51/300 -- Iteration 49005 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.58 min) Epoch 51/300 -- Iteration 49014 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.58 min) Epoch 51/300 -- Iteration 49023 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.58 min) Epoch 51/300 -- Iteration 49032 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.59 min) Epoch 51/300 -- Iteration 49041 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.59 min) Epoch 51/300 -- Iteration 49050 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.60 min) Epoch 51/300 -- Iteration 49059 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.60 min) Epoch 51/300 -- Iteration 49068 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.61 min) Epoch 51/300 -- Iteration 49077 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.61 min) Epoch 51/300 -- Iteration 49086 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.62 min) Epoch 51/300 -- Iteration 49095 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.62 min) Epoch 51/300 -- Iteration 49104 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(24.62 min) Epoch 51/300 -- Iteration 49113 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021 - Val acc: -0.0000\n",
      "(24.63 min) Epoch 52/300 -- Iteration 49122 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.63 min) Epoch 52/300 -- Iteration 49131 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.64 min) Epoch 52/300 -- Iteration 49140 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.64 min) Epoch 52/300 -- Iteration 49149 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.65 min) Epoch 52/300 -- Iteration 49158 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.65 min) Epoch 52/300 -- Iteration 49167 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.66 min) Epoch 52/300 -- Iteration 49176 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.66 min) Epoch 52/300 -- Iteration 49185 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.67 min) Epoch 52/300 -- Iteration 49194 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.67 min) Epoch 52/300 -- Iteration 49203 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.67 min) Epoch 52/300 -- Iteration 49212 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.68 min) Epoch 52/300 -- Iteration 49221 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.68 min) Epoch 52/300 -- Iteration 49230 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.69 min) Epoch 52/300 -- Iteration 49239 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.69 min) Epoch 52/300 -- Iteration 49248 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.70 min) Epoch 52/300 -- Iteration 49257 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.70 min) Epoch 52/300 -- Iteration 49266 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.71 min) Epoch 52/300 -- Iteration 49275 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.71 min) Epoch 52/300 -- Iteration 49284 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.71 min) Epoch 52/300 -- Iteration 49293 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.72 min) Epoch 52/300 -- Iteration 49302 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.72 min) Epoch 52/300 -- Iteration 49311 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.73 min) Epoch 52/300 -- Iteration 49320 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.73 min) Epoch 52/300 -- Iteration 49329 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.74 min) Epoch 52/300 -- Iteration 49338 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.74 min) Epoch 52/300 -- Iteration 49347 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.75 min) Epoch 52/300 -- Iteration 49356 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.75 min) Epoch 52/300 -- Iteration 49365 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.75 min) Epoch 52/300 -- Iteration 49374 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.76 min) Epoch 52/300 -- Iteration 49383 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.76 min) Epoch 52/300 -- Iteration 49392 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.77 min) Epoch 52/300 -- Iteration 49401 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.77 min) Epoch 52/300 -- Iteration 49410 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.78 min) Epoch 52/300 -- Iteration 49419 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.78 min) Epoch 52/300 -- Iteration 49428 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.79 min) Epoch 52/300 -- Iteration 49437 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.79 min) Epoch 52/300 -- Iteration 49446 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.79 min) Epoch 52/300 -- Iteration 49455 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.80 min) Epoch 52/300 -- Iteration 49464 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.80 min) Epoch 52/300 -- Iteration 49473 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.81 min) Epoch 52/300 -- Iteration 49482 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.81 min) Epoch 52/300 -- Iteration 49491 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.82 min) Epoch 52/300 -- Iteration 49500 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.82 min) Epoch 52/300 -- Iteration 49509 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.83 min) Epoch 52/300 -- Iteration 49518 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.83 min) Epoch 52/300 -- Iteration 49527 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.84 min) Epoch 52/300 -- Iteration 49536 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.84 min) Epoch 52/300 -- Iteration 49545 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.84 min) Epoch 52/300 -- Iteration 49554 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.85 min) Epoch 52/300 -- Iteration 49563 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.85 min) Epoch 52/300 -- Iteration 49572 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.86 min) Epoch 52/300 -- Iteration 49581 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.86 min) Epoch 52/300 -- Iteration 49590 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.87 min) Epoch 52/300 -- Iteration 49599 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.87 min) Epoch 52/300 -- Iteration 49608 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.88 min) Epoch 52/300 -- Iteration 49617 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.88 min) Epoch 52/300 -- Iteration 49626 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.88 min) Epoch 52/300 -- Iteration 49635 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.89 min) Epoch 52/300 -- Iteration 49644 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.89 min) Epoch 52/300 -- Iteration 49653 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.90 min) Epoch 52/300 -- Iteration 49662 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.90 min) Epoch 52/300 -- Iteration 49671 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.91 min) Epoch 52/300 -- Iteration 49680 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.91 min) Epoch 52/300 -- Iteration 49689 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.92 min) Epoch 52/300 -- Iteration 49698 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.92 min) Epoch 52/300 -- Iteration 49707 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.92 min) Epoch 52/300 -- Iteration 49716 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.93 min) Epoch 52/300 -- Iteration 49725 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.93 min) Epoch 52/300 -- Iteration 49734 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.94 min) Epoch 52/300 -- Iteration 49743 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.94 min) Epoch 52/300 -- Iteration 49752 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.95 min) Epoch 52/300 -- Iteration 49761 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.95 min) Epoch 52/300 -- Iteration 49770 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.96 min) Epoch 52/300 -- Iteration 49779 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.96 min) Epoch 52/300 -- Iteration 49788 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.96 min) Epoch 52/300 -- Iteration 49797 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.97 min) Epoch 52/300 -- Iteration 49806 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.97 min) Epoch 52/300 -- Iteration 49815 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.98 min) Epoch 52/300 -- Iteration 49824 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.98 min) Epoch 52/300 -- Iteration 49833 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.99 min) Epoch 52/300 -- Iteration 49842 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(24.99 min) Epoch 52/300 -- Iteration 49851 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.00 min) Epoch 52/300 -- Iteration 49860 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.00 min) Epoch 52/300 -- Iteration 49869 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.01 min) Epoch 52/300 -- Iteration 49878 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.01 min) Epoch 52/300 -- Iteration 49887 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.01 min) Epoch 52/300 -- Iteration 49896 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.02 min) Epoch 52/300 -- Iteration 49905 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.02 min) Epoch 52/300 -- Iteration 49914 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.03 min) Epoch 52/300 -- Iteration 49923 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.03 min) Epoch 52/300 -- Iteration 49932 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.04 min) Epoch 52/300 -- Iteration 49941 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.04 min) Epoch 52/300 -- Iteration 49950 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.05 min) Epoch 52/300 -- Iteration 49959 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.05 min) Epoch 52/300 -- Iteration 49968 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.05 min) Epoch 52/300 -- Iteration 49977 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.06 min) Epoch 52/300 -- Iteration 49986 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.06 min) Epoch 52/300 -- Iteration 49995 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.07 min) Epoch 52/300 -- Iteration 50004 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.07 min) Epoch 52/300 -- Iteration 50013 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.08 min) Epoch 52/300 -- Iteration 50022 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.08 min) Epoch 52/300 -- Iteration 50031 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.09 min) Epoch 52/300 -- Iteration 50040 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.09 min) Epoch 52/300 -- Iteration 50049 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.09 min) Epoch 52/300 -- Iteration 50058 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.10 min) Epoch 52/300 -- Iteration 50067 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0021\n",
      "(25.10 min) Epoch 52/300 -- Iteration 50076 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(25.11 min) Epoch 53/300 -- Iteration 50085 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.11 min) Epoch 53/300 -- Iteration 50094 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.12 min) Epoch 53/300 -- Iteration 50103 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.12 min) Epoch 53/300 -- Iteration 50112 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.13 min) Epoch 53/300 -- Iteration 50121 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.13 min) Epoch 53/300 -- Iteration 50130 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.14 min) Epoch 53/300 -- Iteration 50139 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.14 min) Epoch 53/300 -- Iteration 50148 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.14 min) Epoch 53/300 -- Iteration 50157 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.15 min) Epoch 53/300 -- Iteration 50166 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.15 min) Epoch 53/300 -- Iteration 50175 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.16 min) Epoch 53/300 -- Iteration 50184 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.16 min) Epoch 53/300 -- Iteration 50193 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.17 min) Epoch 53/300 -- Iteration 50202 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.17 min) Epoch 53/300 -- Iteration 50211 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.18 min) Epoch 53/300 -- Iteration 50220 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.18 min) Epoch 53/300 -- Iteration 50229 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.18 min) Epoch 53/300 -- Iteration 50238 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.19 min) Epoch 53/300 -- Iteration 50247 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.19 min) Epoch 53/300 -- Iteration 50256 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.20 min) Epoch 53/300 -- Iteration 50265 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.20 min) Epoch 53/300 -- Iteration 50274 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.21 min) Epoch 53/300 -- Iteration 50283 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.21 min) Epoch 53/300 -- Iteration 50292 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.22 min) Epoch 53/300 -- Iteration 50301 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.22 min) Epoch 53/300 -- Iteration 50310 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.22 min) Epoch 53/300 -- Iteration 50319 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.23 min) Epoch 53/300 -- Iteration 50328 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.23 min) Epoch 53/300 -- Iteration 50337 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.24 min) Epoch 53/300 -- Iteration 50346 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.24 min) Epoch 53/300 -- Iteration 50355 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.25 min) Epoch 53/300 -- Iteration 50364 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.25 min) Epoch 53/300 -- Iteration 50373 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.26 min) Epoch 53/300 -- Iteration 50382 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.26 min) Epoch 53/300 -- Iteration 50391 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.26 min) Epoch 53/300 -- Iteration 50400 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.27 min) Epoch 53/300 -- Iteration 50409 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.27 min) Epoch 53/300 -- Iteration 50418 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.28 min) Epoch 53/300 -- Iteration 50427 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.28 min) Epoch 53/300 -- Iteration 50436 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.29 min) Epoch 53/300 -- Iteration 50445 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.29 min) Epoch 53/300 -- Iteration 50454 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.30 min) Epoch 53/300 -- Iteration 50463 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.30 min) Epoch 53/300 -- Iteration 50472 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.31 min) Epoch 53/300 -- Iteration 50481 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.31 min) Epoch 53/300 -- Iteration 50490 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.31 min) Epoch 53/300 -- Iteration 50499 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.32 min) Epoch 53/300 -- Iteration 50508 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.32 min) Epoch 53/300 -- Iteration 50517 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.33 min) Epoch 53/300 -- Iteration 50526 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.33 min) Epoch 53/300 -- Iteration 50535 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.34 min) Epoch 53/300 -- Iteration 50544 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.34 min) Epoch 53/300 -- Iteration 50553 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.35 min) Epoch 53/300 -- Iteration 50562 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.35 min) Epoch 53/300 -- Iteration 50571 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.35 min) Epoch 53/300 -- Iteration 50580 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.36 min) Epoch 53/300 -- Iteration 50589 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.36 min) Epoch 53/300 -- Iteration 50598 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.37 min) Epoch 53/300 -- Iteration 50607 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.37 min) Epoch 53/300 -- Iteration 50616 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.38 min) Epoch 53/300 -- Iteration 50625 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.38 min) Epoch 53/300 -- Iteration 50634 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.39 min) Epoch 53/300 -- Iteration 50643 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.39 min) Epoch 53/300 -- Iteration 50652 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.39 min) Epoch 53/300 -- Iteration 50661 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.40 min) Epoch 53/300 -- Iteration 50670 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.40 min) Epoch 53/300 -- Iteration 50679 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.41 min) Epoch 53/300 -- Iteration 50688 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.41 min) Epoch 53/300 -- Iteration 50697 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.42 min) Epoch 53/300 -- Iteration 50706 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.42 min) Epoch 53/300 -- Iteration 50715 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.43 min) Epoch 53/300 -- Iteration 50724 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.43 min) Epoch 53/300 -- Iteration 50733 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.43 min) Epoch 53/300 -- Iteration 50742 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.44 min) Epoch 53/300 -- Iteration 50751 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.44 min) Epoch 53/300 -- Iteration 50760 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.45 min) Epoch 53/300 -- Iteration 50769 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.45 min) Epoch 53/300 -- Iteration 50778 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.46 min) Epoch 53/300 -- Iteration 50787 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.46 min) Epoch 53/300 -- Iteration 50796 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.47 min) Epoch 53/300 -- Iteration 50805 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.47 min) Epoch 53/300 -- Iteration 50814 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.47 min) Epoch 53/300 -- Iteration 50823 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.48 min) Epoch 53/300 -- Iteration 50832 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.48 min) Epoch 53/300 -- Iteration 50841 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.49 min) Epoch 53/300 -- Iteration 50850 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.49 min) Epoch 53/300 -- Iteration 50859 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.50 min) Epoch 53/300 -- Iteration 50868 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.50 min) Epoch 53/300 -- Iteration 50877 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.51 min) Epoch 53/300 -- Iteration 50886 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.51 min) Epoch 53/300 -- Iteration 50895 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.52 min) Epoch 53/300 -- Iteration 50904 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.52 min) Epoch 53/300 -- Iteration 50913 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.52 min) Epoch 53/300 -- Iteration 50922 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.53 min) Epoch 53/300 -- Iteration 50931 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.53 min) Epoch 53/300 -- Iteration 50940 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.54 min) Epoch 53/300 -- Iteration 50949 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.54 min) Epoch 53/300 -- Iteration 50958 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.55 min) Epoch 53/300 -- Iteration 50967 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.55 min) Epoch 53/300 -- Iteration 50976 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.56 min) Epoch 53/300 -- Iteration 50985 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.56 min) Epoch 53/300 -- Iteration 50994 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.56 min) Epoch 53/300 -- Iteration 51003 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.57 min) Epoch 53/300 -- Iteration 51012 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.57 min) Epoch 53/300 -- Iteration 51021 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.58 min) Epoch 53/300 -- Iteration 51030 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(25.58 min) Epoch 53/300 -- Iteration 51039 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(25.59 min) Epoch 54/300 -- Iteration 51048 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.59 min) Epoch 54/300 -- Iteration 51057 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.60 min) Epoch 54/300 -- Iteration 51066 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.60 min) Epoch 54/300 -- Iteration 51075 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.61 min) Epoch 54/300 -- Iteration 51084 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.61 min) Epoch 54/300 -- Iteration 51093 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.61 min) Epoch 54/300 -- Iteration 51102 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.62 min) Epoch 54/300 -- Iteration 51111 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.62 min) Epoch 54/300 -- Iteration 51120 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.63 min) Epoch 54/300 -- Iteration 51129 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.63 min) Epoch 54/300 -- Iteration 51138 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.64 min) Epoch 54/300 -- Iteration 51147 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.64 min) Epoch 54/300 -- Iteration 51156 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.65 min) Epoch 54/300 -- Iteration 51165 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.65 min) Epoch 54/300 -- Iteration 51174 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.65 min) Epoch 54/300 -- Iteration 51183 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.66 min) Epoch 54/300 -- Iteration 51192 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.66 min) Epoch 54/300 -- Iteration 51201 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.67 min) Epoch 54/300 -- Iteration 51210 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.67 min) Epoch 54/300 -- Iteration 51219 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.68 min) Epoch 54/300 -- Iteration 51228 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.68 min) Epoch 54/300 -- Iteration 51237 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.69 min) Epoch 54/300 -- Iteration 51246 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.69 min) Epoch 54/300 -- Iteration 51255 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.69 min) Epoch 54/300 -- Iteration 51264 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.70 min) Epoch 54/300 -- Iteration 51273 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.70 min) Epoch 54/300 -- Iteration 51282 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.71 min) Epoch 54/300 -- Iteration 51291 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.71 min) Epoch 54/300 -- Iteration 51300 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.72 min) Epoch 54/300 -- Iteration 51309 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.72 min) Epoch 54/300 -- Iteration 51318 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.73 min) Epoch 54/300 -- Iteration 51327 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.73 min) Epoch 54/300 -- Iteration 51336 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.73 min) Epoch 54/300 -- Iteration 51345 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.74 min) Epoch 54/300 -- Iteration 51354 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.74 min) Epoch 54/300 -- Iteration 51363 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.75 min) Epoch 54/300 -- Iteration 51372 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.75 min) Epoch 54/300 -- Iteration 51381 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.76 min) Epoch 54/300 -- Iteration 51390 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.76 min) Epoch 54/300 -- Iteration 51399 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.77 min) Epoch 54/300 -- Iteration 51408 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.77 min) Epoch 54/300 -- Iteration 51417 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.78 min) Epoch 54/300 -- Iteration 51426 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.78 min) Epoch 54/300 -- Iteration 51435 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.78 min) Epoch 54/300 -- Iteration 51444 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.79 min) Epoch 54/300 -- Iteration 51453 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.79 min) Epoch 54/300 -- Iteration 51462 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.80 min) Epoch 54/300 -- Iteration 51471 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.80 min) Epoch 54/300 -- Iteration 51480 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.81 min) Epoch 54/300 -- Iteration 51489 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.81 min) Epoch 54/300 -- Iteration 51498 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.82 min) Epoch 54/300 -- Iteration 51507 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.82 min) Epoch 54/300 -- Iteration 51516 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.82 min) Epoch 54/300 -- Iteration 51525 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.83 min) Epoch 54/300 -- Iteration 51534 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.83 min) Epoch 54/300 -- Iteration 51543 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.84 min) Epoch 54/300 -- Iteration 51552 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.84 min) Epoch 54/300 -- Iteration 51561 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.85 min) Epoch 54/300 -- Iteration 51570 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.85 min) Epoch 54/300 -- Iteration 51579 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.86 min) Epoch 54/300 -- Iteration 51588 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.86 min) Epoch 54/300 -- Iteration 51597 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.87 min) Epoch 54/300 -- Iteration 51606 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.87 min) Epoch 54/300 -- Iteration 51615 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.87 min) Epoch 54/300 -- Iteration 51624 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.88 min) Epoch 54/300 -- Iteration 51633 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.88 min) Epoch 54/300 -- Iteration 51642 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.89 min) Epoch 54/300 -- Iteration 51651 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.89 min) Epoch 54/300 -- Iteration 51660 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.90 min) Epoch 54/300 -- Iteration 51669 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.90 min) Epoch 54/300 -- Iteration 51678 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.91 min) Epoch 54/300 -- Iteration 51687 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.91 min) Epoch 54/300 -- Iteration 51696 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.91 min) Epoch 54/300 -- Iteration 51705 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.92 min) Epoch 54/300 -- Iteration 51714 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.92 min) Epoch 54/300 -- Iteration 51723 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.93 min) Epoch 54/300 -- Iteration 51732 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.93 min) Epoch 54/300 -- Iteration 51741 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.94 min) Epoch 54/300 -- Iteration 51750 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.94 min) Epoch 54/300 -- Iteration 51759 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.95 min) Epoch 54/300 -- Iteration 51768 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.95 min) Epoch 54/300 -- Iteration 51777 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.95 min) Epoch 54/300 -- Iteration 51786 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.96 min) Epoch 54/300 -- Iteration 51795 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.96 min) Epoch 54/300 -- Iteration 51804 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.97 min) Epoch 54/300 -- Iteration 51813 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.97 min) Epoch 54/300 -- Iteration 51822 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.98 min) Epoch 54/300 -- Iteration 51831 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.98 min) Epoch 54/300 -- Iteration 51840 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.99 min) Epoch 54/300 -- Iteration 51849 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(25.99 min) Epoch 54/300 -- Iteration 51858 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.00 min) Epoch 54/300 -- Iteration 51867 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.00 min) Epoch 54/300 -- Iteration 51876 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.00 min) Epoch 54/300 -- Iteration 51885 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.01 min) Epoch 54/300 -- Iteration 51894 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.01 min) Epoch 54/300 -- Iteration 51903 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.02 min) Epoch 54/300 -- Iteration 51912 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.02 min) Epoch 54/300 -- Iteration 51921 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.03 min) Epoch 54/300 -- Iteration 51930 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.03 min) Epoch 54/300 -- Iteration 51939 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.03 min) Epoch 54/300 -- Iteration 51948 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.04 min) Epoch 54/300 -- Iteration 51957 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.04 min) Epoch 54/300 -- Iteration 51966 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.05 min) Epoch 54/300 -- Iteration 51975 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.05 min) Epoch 54/300 -- Iteration 51984 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.06 min) Epoch 54/300 -- Iteration 51993 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.06 min) Epoch 54/300 -- Iteration 52002 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(26.07 min) Epoch 55/300 -- Iteration 52011 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.07 min) Epoch 55/300 -- Iteration 52020 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.08 min) Epoch 55/300 -- Iteration 52029 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.08 min) Epoch 55/300 -- Iteration 52038 - Batch 36/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.09 min) Epoch 55/300 -- Iteration 52047 - Batch 45/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.09 min) Epoch 55/300 -- Iteration 52056 - Batch 54/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.09 min) Epoch 55/300 -- Iteration 52065 - Batch 63/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.10 min) Epoch 55/300 -- Iteration 52074 - Batch 72/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.10 min) Epoch 55/300 -- Iteration 52083 - Batch 81/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.11 min) Epoch 55/300 -- Iteration 52092 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.11 min) Epoch 55/300 -- Iteration 52101 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.12 min) Epoch 55/300 -- Iteration 52110 - Batch 108/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.12 min) Epoch 55/300 -- Iteration 52119 - Batch 117/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.13 min) Epoch 55/300 -- Iteration 52128 - Batch 126/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.13 min) Epoch 55/300 -- Iteration 52137 - Batch 135/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.13 min) Epoch 55/300 -- Iteration 52146 - Batch 144/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.14 min) Epoch 55/300 -- Iteration 52155 - Batch 153/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.14 min) Epoch 55/300 -- Iteration 52164 - Batch 162/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.15 min) Epoch 55/300 -- Iteration 52173 - Batch 171/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.15 min) Epoch 55/300 -- Iteration 52182 - Batch 180/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.16 min) Epoch 55/300 -- Iteration 52191 - Batch 189/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.16 min) Epoch 55/300 -- Iteration 52200 - Batch 198/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.17 min) Epoch 55/300 -- Iteration 52209 - Batch 207/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.17 min) Epoch 55/300 -- Iteration 52218 - Batch 216/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.17 min) Epoch 55/300 -- Iteration 52227 - Batch 225/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.18 min) Epoch 55/300 -- Iteration 52236 - Batch 234/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.18 min) Epoch 55/300 -- Iteration 52245 - Batch 243/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.19 min) Epoch 55/300 -- Iteration 52254 - Batch 252/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.19 min) Epoch 55/300 -- Iteration 52263 - Batch 261/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.20 min) Epoch 55/300 -- Iteration 52272 - Batch 270/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.20 min) Epoch 55/300 -- Iteration 52281 - Batch 279/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.21 min) Epoch 55/300 -- Iteration 52290 - Batch 288/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.21 min) Epoch 55/300 -- Iteration 52299 - Batch 297/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.21 min) Epoch 55/300 -- Iteration 52308 - Batch 306/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.22 min) Epoch 55/300 -- Iteration 52317 - Batch 315/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.22 min) Epoch 55/300 -- Iteration 52326 - Batch 324/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.23 min) Epoch 55/300 -- Iteration 52335 - Batch 333/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.23 min) Epoch 55/300 -- Iteration 52344 - Batch 342/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.24 min) Epoch 55/300 -- Iteration 52353 - Batch 351/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.24 min) Epoch 55/300 -- Iteration 52362 - Batch 360/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.25 min) Epoch 55/300 -- Iteration 52371 - Batch 369/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.25 min) Epoch 55/300 -- Iteration 52380 - Batch 378/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.25 min) Epoch 55/300 -- Iteration 52389 - Batch 387/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.26 min) Epoch 55/300 -- Iteration 52398 - Batch 396/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.26 min) Epoch 55/300 -- Iteration 52407 - Batch 405/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.27 min) Epoch 55/300 -- Iteration 52416 - Batch 414/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.27 min) Epoch 55/300 -- Iteration 52425 - Batch 423/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.28 min) Epoch 55/300 -- Iteration 52434 - Batch 432/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.28 min) Epoch 55/300 -- Iteration 52443 - Batch 441/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.29 min) Epoch 55/300 -- Iteration 52452 - Batch 450/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.29 min) Epoch 55/300 -- Iteration 52461 - Batch 459/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.29 min) Epoch 55/300 -- Iteration 52470 - Batch 468/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.30 min) Epoch 55/300 -- Iteration 52479 - Batch 477/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.30 min) Epoch 55/300 -- Iteration 52488 - Batch 486/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.31 min) Epoch 55/300 -- Iteration 52497 - Batch 495/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.31 min) Epoch 55/300 -- Iteration 52506 - Batch 504/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.32 min) Epoch 55/300 -- Iteration 52515 - Batch 513/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.32 min) Epoch 55/300 -- Iteration 52524 - Batch 522/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.33 min) Epoch 55/300 -- Iteration 52533 - Batch 531/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.33 min) Epoch 55/300 -- Iteration 52542 - Batch 540/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.33 min) Epoch 55/300 -- Iteration 52551 - Batch 549/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.34 min) Epoch 55/300 -- Iteration 52560 - Batch 558/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.34 min) Epoch 55/300 -- Iteration 52569 - Batch 567/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.35 min) Epoch 55/300 -- Iteration 52578 - Batch 576/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.35 min) Epoch 55/300 -- Iteration 52587 - Batch 585/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.36 min) Epoch 55/300 -- Iteration 52596 - Batch 594/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.36 min) Epoch 55/300 -- Iteration 52605 - Batch 603/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.37 min) Epoch 55/300 -- Iteration 52614 - Batch 612/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.37 min) Epoch 55/300 -- Iteration 52623 - Batch 621/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.37 min) Epoch 55/300 -- Iteration 52632 - Batch 630/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.38 min) Epoch 55/300 -- Iteration 52641 - Batch 639/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.38 min) Epoch 55/300 -- Iteration 52650 - Batch 648/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.39 min) Epoch 55/300 -- Iteration 52659 - Batch 657/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.39 min) Epoch 55/300 -- Iteration 52668 - Batch 666/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.40 min) Epoch 55/300 -- Iteration 52677 - Batch 675/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.40 min) Epoch 55/300 -- Iteration 52686 - Batch 684/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.41 min) Epoch 55/300 -- Iteration 52695 - Batch 693/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.41 min) Epoch 55/300 -- Iteration 52704 - Batch 702/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.42 min) Epoch 55/300 -- Iteration 52713 - Batch 711/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.42 min) Epoch 55/300 -- Iteration 52722 - Batch 720/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.42 min) Epoch 55/300 -- Iteration 52731 - Batch 729/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.43 min) Epoch 55/300 -- Iteration 52740 - Batch 738/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.43 min) Epoch 55/300 -- Iteration 52749 - Batch 747/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.44 min) Epoch 55/300 -- Iteration 52758 - Batch 756/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.44 min) Epoch 55/300 -- Iteration 52767 - Batch 765/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.45 min) Epoch 55/300 -- Iteration 52776 - Batch 774/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.45 min) Epoch 55/300 -- Iteration 52785 - Batch 783/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.46 min) Epoch 55/300 -- Iteration 52794 - Batch 792/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.46 min) Epoch 55/300 -- Iteration 52803 - Batch 801/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.46 min) Epoch 55/300 -- Iteration 52812 - Batch 810/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.47 min) Epoch 55/300 -- Iteration 52821 - Batch 819/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.47 min) Epoch 55/300 -- Iteration 52830 - Batch 828/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.48 min) Epoch 55/300 -- Iteration 52839 - Batch 837/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.48 min) Epoch 55/300 -- Iteration 52848 - Batch 846/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.49 min) Epoch 55/300 -- Iteration 52857 - Batch 855/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.49 min) Epoch 55/300 -- Iteration 52866 - Batch 864/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.50 min) Epoch 55/300 -- Iteration 52875 - Batch 873/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.50 min) Epoch 55/300 -- Iteration 52884 - Batch 882/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.50 min) Epoch 55/300 -- Iteration 52893 - Batch 891/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.51 min) Epoch 55/300 -- Iteration 52902 - Batch 900/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.51 min) Epoch 55/300 -- Iteration 52911 - Batch 909/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.52 min) Epoch 55/300 -- Iteration 52920 - Batch 918/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.52 min) Epoch 55/300 -- Iteration 52929 - Batch 927/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.53 min) Epoch 55/300 -- Iteration 52938 - Batch 936/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.53 min) Epoch 55/300 -- Iteration 52947 - Batch 945/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.54 min) Epoch 55/300 -- Iteration 52956 - Batch 954/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(26.54 min) Epoch 55/300 -- Iteration 52965 - Batch 962/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(26.55 min) Epoch 56/300 -- Iteration 52974 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.55 min) Epoch 56/300 -- Iteration 52983 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.55 min) Epoch 56/300 -- Iteration 52992 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.56 min) Epoch 56/300 -- Iteration 53001 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.56 min) Epoch 56/300 -- Iteration 53010 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.57 min) Epoch 56/300 -- Iteration 53019 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.57 min) Epoch 56/300 -- Iteration 53028 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.58 min) Epoch 56/300 -- Iteration 53037 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.58 min) Epoch 56/300 -- Iteration 53046 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.59 min) Epoch 56/300 -- Iteration 53055 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.59 min) Epoch 56/300 -- Iteration 53064 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.59 min) Epoch 56/300 -- Iteration 53073 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.60 min) Epoch 56/300 -- Iteration 53082 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.60 min) Epoch 56/300 -- Iteration 53091 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.61 min) Epoch 56/300 -- Iteration 53100 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.61 min) Epoch 56/300 -- Iteration 53109 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.62 min) Epoch 56/300 -- Iteration 53118 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.62 min) Epoch 56/300 -- Iteration 53127 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.63 min) Epoch 56/300 -- Iteration 53136 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.63 min) Epoch 56/300 -- Iteration 53145 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.63 min) Epoch 56/300 -- Iteration 53154 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.64 min) Epoch 56/300 -- Iteration 53163 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.64 min) Epoch 56/300 -- Iteration 53172 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.65 min) Epoch 56/300 -- Iteration 53181 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.65 min) Epoch 56/300 -- Iteration 53190 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.66 min) Epoch 56/300 -- Iteration 53199 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.66 min) Epoch 56/300 -- Iteration 53208 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.67 min) Epoch 56/300 -- Iteration 53217 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.67 min) Epoch 56/300 -- Iteration 53226 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.67 min) Epoch 56/300 -- Iteration 53235 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.68 min) Epoch 56/300 -- Iteration 53244 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.68 min) Epoch 56/300 -- Iteration 53253 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.69 min) Epoch 56/300 -- Iteration 53262 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.69 min) Epoch 56/300 -- Iteration 53271 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.70 min) Epoch 56/300 -- Iteration 53280 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.70 min) Epoch 56/300 -- Iteration 53289 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.71 min) Epoch 56/300 -- Iteration 53298 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.71 min) Epoch 56/300 -- Iteration 53307 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.71 min) Epoch 56/300 -- Iteration 53316 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.72 min) Epoch 56/300 -- Iteration 53325 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.72 min) Epoch 56/300 -- Iteration 53334 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.73 min) Epoch 56/300 -- Iteration 53343 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.73 min) Epoch 56/300 -- Iteration 53352 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.74 min) Epoch 56/300 -- Iteration 53361 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.74 min) Epoch 56/300 -- Iteration 53370 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.75 min) Epoch 56/300 -- Iteration 53379 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.75 min) Epoch 56/300 -- Iteration 53388 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.75 min) Epoch 56/300 -- Iteration 53397 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.76 min) Epoch 56/300 -- Iteration 53406 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.76 min) Epoch 56/300 -- Iteration 53415 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.77 min) Epoch 56/300 -- Iteration 53424 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.77 min) Epoch 56/300 -- Iteration 53433 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.78 min) Epoch 56/300 -- Iteration 53442 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.78 min) Epoch 56/300 -- Iteration 53451 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.79 min) Epoch 56/300 -- Iteration 53460 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.79 min) Epoch 56/300 -- Iteration 53469 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.80 min) Epoch 56/300 -- Iteration 53478 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.80 min) Epoch 56/300 -- Iteration 53487 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.80 min) Epoch 56/300 -- Iteration 53496 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.81 min) Epoch 56/300 -- Iteration 53505 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.81 min) Epoch 56/300 -- Iteration 53514 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.82 min) Epoch 56/300 -- Iteration 53523 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.82 min) Epoch 56/300 -- Iteration 53532 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.83 min) Epoch 56/300 -- Iteration 53541 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.83 min) Epoch 56/300 -- Iteration 53550 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.84 min) Epoch 56/300 -- Iteration 53559 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.84 min) Epoch 56/300 -- Iteration 53568 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.84 min) Epoch 56/300 -- Iteration 53577 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.85 min) Epoch 56/300 -- Iteration 53586 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.85 min) Epoch 56/300 -- Iteration 53595 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.86 min) Epoch 56/300 -- Iteration 53604 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.86 min) Epoch 56/300 -- Iteration 53613 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.87 min) Epoch 56/300 -- Iteration 53622 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.87 min) Epoch 56/300 -- Iteration 53631 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.88 min) Epoch 56/300 -- Iteration 53640 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.88 min) Epoch 56/300 -- Iteration 53649 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.88 min) Epoch 56/300 -- Iteration 53658 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.89 min) Epoch 56/300 -- Iteration 53667 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.89 min) Epoch 56/300 -- Iteration 53676 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.90 min) Epoch 56/300 -- Iteration 53685 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.90 min) Epoch 56/300 -- Iteration 53694 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.91 min) Epoch 56/300 -- Iteration 53703 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.91 min) Epoch 56/300 -- Iteration 53712 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.92 min) Epoch 56/300 -- Iteration 53721 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.92 min) Epoch 56/300 -- Iteration 53730 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.92 min) Epoch 56/300 -- Iteration 53739 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.93 min) Epoch 56/300 -- Iteration 53748 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.93 min) Epoch 56/300 -- Iteration 53757 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.94 min) Epoch 56/300 -- Iteration 53766 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.94 min) Epoch 56/300 -- Iteration 53775 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.95 min) Epoch 56/300 -- Iteration 53784 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.95 min) Epoch 56/300 -- Iteration 53793 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.96 min) Epoch 56/300 -- Iteration 53802 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.96 min) Epoch 56/300 -- Iteration 53811 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.97 min) Epoch 56/300 -- Iteration 53820 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.97 min) Epoch 56/300 -- Iteration 53829 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.97 min) Epoch 56/300 -- Iteration 53838 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.98 min) Epoch 56/300 -- Iteration 53847 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.98 min) Epoch 56/300 -- Iteration 53856 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.99 min) Epoch 56/300 -- Iteration 53865 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(26.99 min) Epoch 56/300 -- Iteration 53874 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(27.00 min) Epoch 56/300 -- Iteration 53883 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(27.00 min) Epoch 56/300 -- Iteration 53892 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(27.01 min) Epoch 56/300 -- Iteration 53901 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(27.01 min) Epoch 56/300 -- Iteration 53910 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(27.01 min) Epoch 56/300 -- Iteration 53919 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(27.02 min) Epoch 56/300 -- Iteration 53928 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(27.02 min) Epoch 57/300 -- Iteration 53937 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.03 min) Epoch 57/300 -- Iteration 53946 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.03 min) Epoch 57/300 -- Iteration 53955 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.04 min) Epoch 57/300 -- Iteration 53964 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.04 min) Epoch 57/300 -- Iteration 53973 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.05 min) Epoch 57/300 -- Iteration 53982 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.05 min) Epoch 57/300 -- Iteration 53991 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.06 min) Epoch 57/300 -- Iteration 54000 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.06 min) Epoch 57/300 -- Iteration 54009 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.06 min) Epoch 57/300 -- Iteration 54018 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.07 min) Epoch 57/300 -- Iteration 54027 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.07 min) Epoch 57/300 -- Iteration 54036 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.08 min) Epoch 57/300 -- Iteration 54045 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.08 min) Epoch 57/300 -- Iteration 54054 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.09 min) Epoch 57/300 -- Iteration 54063 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.09 min) Epoch 57/300 -- Iteration 54072 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.10 min) Epoch 57/300 -- Iteration 54081 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.10 min) Epoch 57/300 -- Iteration 54090 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.10 min) Epoch 57/300 -- Iteration 54099 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.11 min) Epoch 57/300 -- Iteration 54108 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.11 min) Epoch 57/300 -- Iteration 54117 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.12 min) Epoch 57/300 -- Iteration 54126 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.12 min) Epoch 57/300 -- Iteration 54135 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.13 min) Epoch 57/300 -- Iteration 54144 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.13 min) Epoch 57/300 -- Iteration 54153 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.14 min) Epoch 57/300 -- Iteration 54162 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.14 min) Epoch 57/300 -- Iteration 54171 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.14 min) Epoch 57/300 -- Iteration 54180 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.15 min) Epoch 57/300 -- Iteration 54189 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.15 min) Epoch 57/300 -- Iteration 54198 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.16 min) Epoch 57/300 -- Iteration 54207 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.16 min) Epoch 57/300 -- Iteration 54216 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.17 min) Epoch 57/300 -- Iteration 54225 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.17 min) Epoch 57/300 -- Iteration 54234 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.18 min) Epoch 57/300 -- Iteration 54243 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.18 min) Epoch 57/300 -- Iteration 54252 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.18 min) Epoch 57/300 -- Iteration 54261 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.19 min) Epoch 57/300 -- Iteration 54270 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.19 min) Epoch 57/300 -- Iteration 54279 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.20 min) Epoch 57/300 -- Iteration 54288 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.20 min) Epoch 57/300 -- Iteration 54297 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.21 min) Epoch 57/300 -- Iteration 54306 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.21 min) Epoch 57/300 -- Iteration 54315 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.22 min) Epoch 57/300 -- Iteration 54324 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.22 min) Epoch 57/300 -- Iteration 54333 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.22 min) Epoch 57/300 -- Iteration 54342 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.23 min) Epoch 57/300 -- Iteration 54351 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.23 min) Epoch 57/300 -- Iteration 54360 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.24 min) Epoch 57/300 -- Iteration 54369 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.24 min) Epoch 57/300 -- Iteration 54378 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.25 min) Epoch 57/300 -- Iteration 54387 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.25 min) Epoch 57/300 -- Iteration 54396 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.26 min) Epoch 57/300 -- Iteration 54405 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.26 min) Epoch 57/300 -- Iteration 54414 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.27 min) Epoch 57/300 -- Iteration 54423 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.27 min) Epoch 57/300 -- Iteration 54432 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.27 min) Epoch 57/300 -- Iteration 54441 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.28 min) Epoch 57/300 -- Iteration 54450 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.28 min) Epoch 57/300 -- Iteration 54459 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.29 min) Epoch 57/300 -- Iteration 54468 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.29 min) Epoch 57/300 -- Iteration 54477 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.30 min) Epoch 57/300 -- Iteration 54486 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.30 min) Epoch 57/300 -- Iteration 54495 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.31 min) Epoch 57/300 -- Iteration 54504 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.31 min) Epoch 57/300 -- Iteration 54513 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.31 min) Epoch 57/300 -- Iteration 54522 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.32 min) Epoch 57/300 -- Iteration 54531 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.32 min) Epoch 57/300 -- Iteration 54540 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.33 min) Epoch 57/300 -- Iteration 54549 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.33 min) Epoch 57/300 -- Iteration 54558 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.34 min) Epoch 57/300 -- Iteration 54567 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.34 min) Epoch 57/300 -- Iteration 54576 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.35 min) Epoch 57/300 -- Iteration 54585 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.35 min) Epoch 57/300 -- Iteration 54594 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.35 min) Epoch 57/300 -- Iteration 54603 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.36 min) Epoch 57/300 -- Iteration 54612 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.36 min) Epoch 57/300 -- Iteration 54621 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.37 min) Epoch 57/300 -- Iteration 54630 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.37 min) Epoch 57/300 -- Iteration 54639 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.38 min) Epoch 57/300 -- Iteration 54648 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.38 min) Epoch 57/300 -- Iteration 54657 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.39 min) Epoch 57/300 -- Iteration 54666 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.39 min) Epoch 57/300 -- Iteration 54675 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.39 min) Epoch 57/300 -- Iteration 54684 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.40 min) Epoch 57/300 -- Iteration 54693 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.40 min) Epoch 57/300 -- Iteration 54702 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.41 min) Epoch 57/300 -- Iteration 54711 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.41 min) Epoch 57/300 -- Iteration 54720 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.42 min) Epoch 57/300 -- Iteration 54729 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.42 min) Epoch 57/300 -- Iteration 54738 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.43 min) Epoch 57/300 -- Iteration 54747 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.43 min) Epoch 57/300 -- Iteration 54756 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.43 min) Epoch 57/300 -- Iteration 54765 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.44 min) Epoch 57/300 -- Iteration 54774 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.44 min) Epoch 57/300 -- Iteration 54783 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.45 min) Epoch 57/300 -- Iteration 54792 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.45 min) Epoch 57/300 -- Iteration 54801 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.46 min) Epoch 57/300 -- Iteration 54810 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.46 min) Epoch 57/300 -- Iteration 54819 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.47 min) Epoch 57/300 -- Iteration 54828 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.47 min) Epoch 57/300 -- Iteration 54837 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.47 min) Epoch 57/300 -- Iteration 54846 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.48 min) Epoch 57/300 -- Iteration 54855 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.48 min) Epoch 57/300 -- Iteration 54864 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.49 min) Epoch 57/300 -- Iteration 54873 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.49 min) Epoch 57/300 -- Iteration 54882 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.50 min) Epoch 57/300 -- Iteration 54891 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(27.50 min) Epoch 58/300 -- Iteration 54900 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.51 min) Epoch 58/300 -- Iteration 54909 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.51 min) Epoch 58/300 -- Iteration 54918 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.52 min) Epoch 58/300 -- Iteration 54927 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.52 min) Epoch 58/300 -- Iteration 54936 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.52 min) Epoch 58/300 -- Iteration 54945 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.53 min) Epoch 58/300 -- Iteration 54954 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.53 min) Epoch 58/300 -- Iteration 54963 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.54 min) Epoch 58/300 -- Iteration 54972 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.54 min) Epoch 58/300 -- Iteration 54981 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.55 min) Epoch 58/300 -- Iteration 54990 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.55 min) Epoch 58/300 -- Iteration 54999 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.56 min) Epoch 58/300 -- Iteration 55008 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.56 min) Epoch 58/300 -- Iteration 55017 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.57 min) Epoch 58/300 -- Iteration 55026 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.57 min) Epoch 58/300 -- Iteration 55035 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.57 min) Epoch 58/300 -- Iteration 55044 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.58 min) Epoch 58/300 -- Iteration 55053 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.58 min) Epoch 58/300 -- Iteration 55062 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.59 min) Epoch 58/300 -- Iteration 55071 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.59 min) Epoch 58/300 -- Iteration 55080 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.60 min) Epoch 58/300 -- Iteration 55089 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.60 min) Epoch 58/300 -- Iteration 55098 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.61 min) Epoch 58/300 -- Iteration 55107 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.61 min) Epoch 58/300 -- Iteration 55116 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.61 min) Epoch 58/300 -- Iteration 55125 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.62 min) Epoch 58/300 -- Iteration 55134 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.62 min) Epoch 58/300 -- Iteration 55143 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.63 min) Epoch 58/300 -- Iteration 55152 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.63 min) Epoch 58/300 -- Iteration 55161 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.64 min) Epoch 58/300 -- Iteration 55170 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.64 min) Epoch 58/300 -- Iteration 55179 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.65 min) Epoch 58/300 -- Iteration 55188 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.65 min) Epoch 58/300 -- Iteration 55197 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.65 min) Epoch 58/300 -- Iteration 55206 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.66 min) Epoch 58/300 -- Iteration 55215 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.66 min) Epoch 58/300 -- Iteration 55224 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.67 min) Epoch 58/300 -- Iteration 55233 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.67 min) Epoch 58/300 -- Iteration 55242 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.68 min) Epoch 58/300 -- Iteration 55251 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.68 min) Epoch 58/300 -- Iteration 55260 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.69 min) Epoch 58/300 -- Iteration 55269 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.69 min) Epoch 58/300 -- Iteration 55278 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.69 min) Epoch 58/300 -- Iteration 55287 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.70 min) Epoch 58/300 -- Iteration 55296 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.70 min) Epoch 58/300 -- Iteration 55305 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.71 min) Epoch 58/300 -- Iteration 55314 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.71 min) Epoch 58/300 -- Iteration 55323 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.72 min) Epoch 58/300 -- Iteration 55332 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.72 min) Epoch 58/300 -- Iteration 55341 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.73 min) Epoch 58/300 -- Iteration 55350 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.73 min) Epoch 58/300 -- Iteration 55359 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.73 min) Epoch 58/300 -- Iteration 55368 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.74 min) Epoch 58/300 -- Iteration 55377 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.74 min) Epoch 58/300 -- Iteration 55386 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.75 min) Epoch 58/300 -- Iteration 55395 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.75 min) Epoch 58/300 -- Iteration 55404 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.76 min) Epoch 58/300 -- Iteration 55413 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.76 min) Epoch 58/300 -- Iteration 55422 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.77 min) Epoch 58/300 -- Iteration 55431 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.77 min) Epoch 58/300 -- Iteration 55440 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.77 min) Epoch 58/300 -- Iteration 55449 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.78 min) Epoch 58/300 -- Iteration 55458 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.78 min) Epoch 58/300 -- Iteration 55467 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.79 min) Epoch 58/300 -- Iteration 55476 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.79 min) Epoch 58/300 -- Iteration 55485 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.80 min) Epoch 58/300 -- Iteration 55494 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.80 min) Epoch 58/300 -- Iteration 55503 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.81 min) Epoch 58/300 -- Iteration 55512 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.81 min) Epoch 58/300 -- Iteration 55521 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.81 min) Epoch 58/300 -- Iteration 55530 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.82 min) Epoch 58/300 -- Iteration 55539 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.82 min) Epoch 58/300 -- Iteration 55548 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.83 min) Epoch 58/300 -- Iteration 55557 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.83 min) Epoch 58/300 -- Iteration 55566 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.84 min) Epoch 58/300 -- Iteration 55575 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.84 min) Epoch 58/300 -- Iteration 55584 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.85 min) Epoch 58/300 -- Iteration 55593 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.85 min) Epoch 58/300 -- Iteration 55602 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.85 min) Epoch 58/300 -- Iteration 55611 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.86 min) Epoch 58/300 -- Iteration 55620 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.86 min) Epoch 58/300 -- Iteration 55629 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.87 min) Epoch 58/300 -- Iteration 55638 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.87 min) Epoch 58/300 -- Iteration 55647 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.88 min) Epoch 58/300 -- Iteration 55656 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.88 min) Epoch 58/300 -- Iteration 55665 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.89 min) Epoch 58/300 -- Iteration 55674 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.89 min) Epoch 58/300 -- Iteration 55683 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.89 min) Epoch 58/300 -- Iteration 55692 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.90 min) Epoch 58/300 -- Iteration 55701 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.90 min) Epoch 58/300 -- Iteration 55710 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.91 min) Epoch 58/300 -- Iteration 55719 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.91 min) Epoch 58/300 -- Iteration 55728 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.92 min) Epoch 58/300 -- Iteration 55737 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.92 min) Epoch 58/300 -- Iteration 55746 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.93 min) Epoch 58/300 -- Iteration 55755 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.93 min) Epoch 58/300 -- Iteration 55764 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.93 min) Epoch 58/300 -- Iteration 55773 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.94 min) Epoch 58/300 -- Iteration 55782 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.94 min) Epoch 58/300 -- Iteration 55791 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.95 min) Epoch 58/300 -- Iteration 55800 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.95 min) Epoch 58/300 -- Iteration 55809 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.96 min) Epoch 58/300 -- Iteration 55818 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.96 min) Epoch 58/300 -- Iteration 55827 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.97 min) Epoch 58/300 -- Iteration 55836 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.97 min) Epoch 58/300 -- Iteration 55845 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.97 min) Epoch 58/300 -- Iteration 55854 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(27.98 min) Epoch 59/300 -- Iteration 55863 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.98 min) Epoch 59/300 -- Iteration 55872 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.99 min) Epoch 59/300 -- Iteration 55881 - Batch 27/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(27.99 min) Epoch 59/300 -- Iteration 55890 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.00 min) Epoch 59/300 -- Iteration 55899 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.00 min) Epoch 59/300 -- Iteration 55908 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.01 min) Epoch 59/300 -- Iteration 55917 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.01 min) Epoch 59/300 -- Iteration 55926 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.02 min) Epoch 59/300 -- Iteration 55935 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.02 min) Epoch 59/300 -- Iteration 55944 - Batch 90/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.02 min) Epoch 59/300 -- Iteration 55953 - Batch 99/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.03 min) Epoch 59/300 -- Iteration 55962 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.03 min) Epoch 59/300 -- Iteration 55971 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.04 min) Epoch 59/300 -- Iteration 55980 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.04 min) Epoch 59/300 -- Iteration 55989 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.05 min) Epoch 59/300 -- Iteration 55998 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.05 min) Epoch 59/300 -- Iteration 56007 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.06 min) Epoch 59/300 -- Iteration 56016 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.06 min) Epoch 59/300 -- Iteration 56025 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.07 min) Epoch 59/300 -- Iteration 56034 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.07 min) Epoch 59/300 -- Iteration 56043 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.07 min) Epoch 59/300 -- Iteration 56052 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.08 min) Epoch 59/300 -- Iteration 56061 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.08 min) Epoch 59/300 -- Iteration 56070 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.09 min) Epoch 59/300 -- Iteration 56079 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.09 min) Epoch 59/300 -- Iteration 56088 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.10 min) Epoch 59/300 -- Iteration 56097 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.10 min) Epoch 59/300 -- Iteration 56106 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.11 min) Epoch 59/300 -- Iteration 56115 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.11 min) Epoch 59/300 -- Iteration 56124 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.11 min) Epoch 59/300 -- Iteration 56133 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.12 min) Epoch 59/300 -- Iteration 56142 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.12 min) Epoch 59/300 -- Iteration 56151 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.13 min) Epoch 59/300 -- Iteration 56160 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.13 min) Epoch 59/300 -- Iteration 56169 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.14 min) Epoch 59/300 -- Iteration 56178 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.14 min) Epoch 59/300 -- Iteration 56187 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.15 min) Epoch 59/300 -- Iteration 56196 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.15 min) Epoch 59/300 -- Iteration 56205 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.15 min) Epoch 59/300 -- Iteration 56214 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.16 min) Epoch 59/300 -- Iteration 56223 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.16 min) Epoch 59/300 -- Iteration 56232 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.17 min) Epoch 59/300 -- Iteration 56241 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.17 min) Epoch 59/300 -- Iteration 56250 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.18 min) Epoch 59/300 -- Iteration 56259 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.18 min) Epoch 59/300 -- Iteration 56268 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.19 min) Epoch 59/300 -- Iteration 56277 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.19 min) Epoch 59/300 -- Iteration 56286 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.19 min) Epoch 59/300 -- Iteration 56295 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.20 min) Epoch 59/300 -- Iteration 56304 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.20 min) Epoch 59/300 -- Iteration 56313 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.21 min) Epoch 59/300 -- Iteration 56322 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.21 min) Epoch 59/300 -- Iteration 56331 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.22 min) Epoch 59/300 -- Iteration 56340 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.22 min) Epoch 59/300 -- Iteration 56349 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.23 min) Epoch 59/300 -- Iteration 56358 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.23 min) Epoch 59/300 -- Iteration 56367 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.23 min) Epoch 59/300 -- Iteration 56376 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.24 min) Epoch 59/300 -- Iteration 56385 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.24 min) Epoch 59/300 -- Iteration 56394 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.25 min) Epoch 59/300 -- Iteration 56403 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.25 min) Epoch 59/300 -- Iteration 56412 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.26 min) Epoch 59/300 -- Iteration 56421 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.26 min) Epoch 59/300 -- Iteration 56430 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.27 min) Epoch 59/300 -- Iteration 56439 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.27 min) Epoch 59/300 -- Iteration 56448 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.27 min) Epoch 59/300 -- Iteration 56457 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.28 min) Epoch 59/300 -- Iteration 56466 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.28 min) Epoch 59/300 -- Iteration 56475 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.29 min) Epoch 59/300 -- Iteration 56484 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.29 min) Epoch 59/300 -- Iteration 56493 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.30 min) Epoch 59/300 -- Iteration 56502 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.30 min) Epoch 59/300 -- Iteration 56511 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.31 min) Epoch 59/300 -- Iteration 56520 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.31 min) Epoch 59/300 -- Iteration 56529 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.31 min) Epoch 59/300 -- Iteration 56538 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.32 min) Epoch 59/300 -- Iteration 56547 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.32 min) Epoch 59/300 -- Iteration 56556 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.33 min) Epoch 59/300 -- Iteration 56565 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.33 min) Epoch 59/300 -- Iteration 56574 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.34 min) Epoch 59/300 -- Iteration 56583 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.34 min) Epoch 59/300 -- Iteration 56592 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.35 min) Epoch 59/300 -- Iteration 56601 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.35 min) Epoch 59/300 -- Iteration 56610 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.36 min) Epoch 59/300 -- Iteration 56619 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.36 min) Epoch 59/300 -- Iteration 56628 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.36 min) Epoch 59/300 -- Iteration 56637 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.37 min) Epoch 59/300 -- Iteration 56646 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.37 min) Epoch 59/300 -- Iteration 56655 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.38 min) Epoch 59/300 -- Iteration 56664 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.38 min) Epoch 59/300 -- Iteration 56673 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.39 min) Epoch 59/300 -- Iteration 56682 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.39 min) Epoch 59/300 -- Iteration 56691 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.39 min) Epoch 59/300 -- Iteration 56700 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.40 min) Epoch 59/300 -- Iteration 56709 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.40 min) Epoch 59/300 -- Iteration 56718 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.41 min) Epoch 59/300 -- Iteration 56727 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.41 min) Epoch 59/300 -- Iteration 56736 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.42 min) Epoch 59/300 -- Iteration 56745 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.42 min) Epoch 59/300 -- Iteration 56754 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.43 min) Epoch 59/300 -- Iteration 56763 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.43 min) Epoch 59/300 -- Iteration 56772 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.44 min) Epoch 59/300 -- Iteration 56781 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.44 min) Epoch 59/300 -- Iteration 56790 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.44 min) Epoch 59/300 -- Iteration 56799 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.45 min) Epoch 59/300 -- Iteration 56808 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.45 min) Epoch 59/300 -- Iteration 56817 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(28.46 min) Epoch 60/300 -- Iteration 56826 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.46 min) Epoch 60/300 -- Iteration 56835 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.47 min) Epoch 60/300 -- Iteration 56844 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.47 min) Epoch 60/300 -- Iteration 56853 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.48 min) Epoch 60/300 -- Iteration 56862 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.48 min) Epoch 60/300 -- Iteration 56871 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.49 min) Epoch 60/300 -- Iteration 56880 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.49 min) Epoch 60/300 -- Iteration 56889 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.49 min) Epoch 60/300 -- Iteration 56898 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.50 min) Epoch 60/300 -- Iteration 56907 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.50 min) Epoch 60/300 -- Iteration 56916 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.51 min) Epoch 60/300 -- Iteration 56925 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.51 min) Epoch 60/300 -- Iteration 56934 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.52 min) Epoch 60/300 -- Iteration 56943 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.52 min) Epoch 60/300 -- Iteration 56952 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.53 min) Epoch 60/300 -- Iteration 56961 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.53 min) Epoch 60/300 -- Iteration 56970 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.53 min) Epoch 60/300 -- Iteration 56979 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.54 min) Epoch 60/300 -- Iteration 56988 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.54 min) Epoch 60/300 -- Iteration 56997 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.55 min) Epoch 60/300 -- Iteration 57006 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.55 min) Epoch 60/300 -- Iteration 57015 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.56 min) Epoch 60/300 -- Iteration 57024 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.56 min) Epoch 60/300 -- Iteration 57033 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.57 min) Epoch 60/300 -- Iteration 57042 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.57 min) Epoch 60/300 -- Iteration 57051 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.57 min) Epoch 60/300 -- Iteration 57060 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.58 min) Epoch 60/300 -- Iteration 57069 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.58 min) Epoch 60/300 -- Iteration 57078 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.59 min) Epoch 60/300 -- Iteration 57087 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.59 min) Epoch 60/300 -- Iteration 57096 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.60 min) Epoch 60/300 -- Iteration 57105 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.60 min) Epoch 60/300 -- Iteration 57114 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.61 min) Epoch 60/300 -- Iteration 57123 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.61 min) Epoch 60/300 -- Iteration 57132 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.61 min) Epoch 60/300 -- Iteration 57141 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.62 min) Epoch 60/300 -- Iteration 57150 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.62 min) Epoch 60/300 -- Iteration 57159 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.63 min) Epoch 60/300 -- Iteration 57168 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.63 min) Epoch 60/300 -- Iteration 57177 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.64 min) Epoch 60/300 -- Iteration 57186 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.64 min) Epoch 60/300 -- Iteration 57195 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.65 min) Epoch 60/300 -- Iteration 57204 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.65 min) Epoch 60/300 -- Iteration 57213 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.65 min) Epoch 60/300 -- Iteration 57222 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.66 min) Epoch 60/300 -- Iteration 57231 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.66 min) Epoch 60/300 -- Iteration 57240 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.67 min) Epoch 60/300 -- Iteration 57249 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.67 min) Epoch 60/300 -- Iteration 57258 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.68 min) Epoch 60/300 -- Iteration 57267 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.68 min) Epoch 60/300 -- Iteration 57276 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.69 min) Epoch 60/300 -- Iteration 57285 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.69 min) Epoch 60/300 -- Iteration 57294 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.69 min) Epoch 60/300 -- Iteration 57303 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.70 min) Epoch 60/300 -- Iteration 57312 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.70 min) Epoch 60/300 -- Iteration 57321 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.71 min) Epoch 60/300 -- Iteration 57330 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.71 min) Epoch 60/300 -- Iteration 57339 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.72 min) Epoch 60/300 -- Iteration 57348 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.72 min) Epoch 60/300 -- Iteration 57357 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.73 min) Epoch 60/300 -- Iteration 57366 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.73 min) Epoch 60/300 -- Iteration 57375 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.74 min) Epoch 60/300 -- Iteration 57384 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.74 min) Epoch 60/300 -- Iteration 57393 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.74 min) Epoch 60/300 -- Iteration 57402 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.75 min) Epoch 60/300 -- Iteration 57411 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.75 min) Epoch 60/300 -- Iteration 57420 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.76 min) Epoch 60/300 -- Iteration 57429 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.76 min) Epoch 60/300 -- Iteration 57438 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.77 min) Epoch 60/300 -- Iteration 57447 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.77 min) Epoch 60/300 -- Iteration 57456 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.78 min) Epoch 60/300 -- Iteration 57465 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.78 min) Epoch 60/300 -- Iteration 57474 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.78 min) Epoch 60/300 -- Iteration 57483 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.79 min) Epoch 60/300 -- Iteration 57492 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.79 min) Epoch 60/300 -- Iteration 57501 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.80 min) Epoch 60/300 -- Iteration 57510 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.80 min) Epoch 60/300 -- Iteration 57519 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.81 min) Epoch 60/300 -- Iteration 57528 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.81 min) Epoch 60/300 -- Iteration 57537 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.82 min) Epoch 60/300 -- Iteration 57546 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.82 min) Epoch 60/300 -- Iteration 57555 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.82 min) Epoch 60/300 -- Iteration 57564 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.83 min) Epoch 60/300 -- Iteration 57573 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.83 min) Epoch 60/300 -- Iteration 57582 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.84 min) Epoch 60/300 -- Iteration 57591 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.84 min) Epoch 60/300 -- Iteration 57600 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.85 min) Epoch 60/300 -- Iteration 57609 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.85 min) Epoch 60/300 -- Iteration 57618 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.86 min) Epoch 60/300 -- Iteration 57627 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.86 min) Epoch 60/300 -- Iteration 57636 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.86 min) Epoch 60/300 -- Iteration 57645 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.87 min) Epoch 60/300 -- Iteration 57654 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.87 min) Epoch 60/300 -- Iteration 57663 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.88 min) Epoch 60/300 -- Iteration 57672 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.88 min) Epoch 60/300 -- Iteration 57681 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.89 min) Epoch 60/300 -- Iteration 57690 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.89 min) Epoch 60/300 -- Iteration 57699 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.90 min) Epoch 60/300 -- Iteration 57708 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.90 min) Epoch 60/300 -- Iteration 57717 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.90 min) Epoch 60/300 -- Iteration 57726 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.91 min) Epoch 60/300 -- Iteration 57735 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.91 min) Epoch 60/300 -- Iteration 57744 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.92 min) Epoch 60/300 -- Iteration 57753 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.92 min) Epoch 60/300 -- Iteration 57762 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.93 min) Epoch 60/300 -- Iteration 57771 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(28.93 min) Epoch 60/300 -- Iteration 57780 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(28.94 min) Epoch 61/300 -- Iteration 57789 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.94 min) Epoch 61/300 -- Iteration 57798 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.95 min) Epoch 61/300 -- Iteration 57807 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.95 min) Epoch 61/300 -- Iteration 57816 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.95 min) Epoch 61/300 -- Iteration 57825 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.96 min) Epoch 61/300 -- Iteration 57834 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.96 min) Epoch 61/300 -- Iteration 57843 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.97 min) Epoch 61/300 -- Iteration 57852 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.97 min) Epoch 61/300 -- Iteration 57861 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.98 min) Epoch 61/300 -- Iteration 57870 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.98 min) Epoch 61/300 -- Iteration 57879 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.99 min) Epoch 61/300 -- Iteration 57888 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.99 min) Epoch 61/300 -- Iteration 57897 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(28.99 min) Epoch 61/300 -- Iteration 57906 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.00 min) Epoch 61/300 -- Iteration 57915 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.00 min) Epoch 61/300 -- Iteration 57924 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.01 min) Epoch 61/300 -- Iteration 57933 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.01 min) Epoch 61/300 -- Iteration 57942 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.02 min) Epoch 61/300 -- Iteration 57951 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.02 min) Epoch 61/300 -- Iteration 57960 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.03 min) Epoch 61/300 -- Iteration 57969 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.03 min) Epoch 61/300 -- Iteration 57978 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.03 min) Epoch 61/300 -- Iteration 57987 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.04 min) Epoch 61/300 -- Iteration 57996 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.04 min) Epoch 61/300 -- Iteration 58005 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.05 min) Epoch 61/300 -- Iteration 58014 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.05 min) Epoch 61/300 -- Iteration 58023 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.06 min) Epoch 61/300 -- Iteration 58032 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.06 min) Epoch 61/300 -- Iteration 58041 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.07 min) Epoch 61/300 -- Iteration 58050 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.07 min) Epoch 61/300 -- Iteration 58059 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.08 min) Epoch 61/300 -- Iteration 58068 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.08 min) Epoch 61/300 -- Iteration 58077 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.08 min) Epoch 61/300 -- Iteration 58086 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.09 min) Epoch 61/300 -- Iteration 58095 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.09 min) Epoch 61/300 -- Iteration 58104 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.10 min) Epoch 61/300 -- Iteration 58113 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.10 min) Epoch 61/300 -- Iteration 58122 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.11 min) Epoch 61/300 -- Iteration 58131 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.11 min) Epoch 61/300 -- Iteration 58140 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.12 min) Epoch 61/300 -- Iteration 58149 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.12 min) Epoch 61/300 -- Iteration 58158 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.12 min) Epoch 61/300 -- Iteration 58167 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.13 min) Epoch 61/300 -- Iteration 58176 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.13 min) Epoch 61/300 -- Iteration 58185 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.14 min) Epoch 61/300 -- Iteration 58194 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.14 min) Epoch 61/300 -- Iteration 58203 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.15 min) Epoch 61/300 -- Iteration 58212 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.15 min) Epoch 61/300 -- Iteration 58221 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.16 min) Epoch 61/300 -- Iteration 58230 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.16 min) Epoch 61/300 -- Iteration 58239 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.16 min) Epoch 61/300 -- Iteration 58248 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.17 min) Epoch 61/300 -- Iteration 58257 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.17 min) Epoch 61/300 -- Iteration 58266 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.18 min) Epoch 61/300 -- Iteration 58275 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.18 min) Epoch 61/300 -- Iteration 58284 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.19 min) Epoch 61/300 -- Iteration 58293 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.19 min) Epoch 61/300 -- Iteration 58302 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.20 min) Epoch 61/300 -- Iteration 58311 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.20 min) Epoch 61/300 -- Iteration 58320 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.20 min) Epoch 61/300 -- Iteration 58329 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.21 min) Epoch 61/300 -- Iteration 58338 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.21 min) Epoch 61/300 -- Iteration 58347 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.22 min) Epoch 61/300 -- Iteration 58356 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.22 min) Epoch 61/300 -- Iteration 58365 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.23 min) Epoch 61/300 -- Iteration 58374 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.23 min) Epoch 61/300 -- Iteration 58383 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.24 min) Epoch 61/300 -- Iteration 58392 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.24 min) Epoch 61/300 -- Iteration 58401 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.24 min) Epoch 61/300 -- Iteration 58410 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.25 min) Epoch 61/300 -- Iteration 58419 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.25 min) Epoch 61/300 -- Iteration 58428 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.26 min) Epoch 61/300 -- Iteration 58437 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.26 min) Epoch 61/300 -- Iteration 58446 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.27 min) Epoch 61/300 -- Iteration 58455 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.27 min) Epoch 61/300 -- Iteration 58464 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.28 min) Epoch 61/300 -- Iteration 58473 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.28 min) Epoch 61/300 -- Iteration 58482 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.28 min) Epoch 61/300 -- Iteration 58491 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.29 min) Epoch 61/300 -- Iteration 58500 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.29 min) Epoch 61/300 -- Iteration 58509 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.30 min) Epoch 61/300 -- Iteration 58518 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.30 min) Epoch 61/300 -- Iteration 58527 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.31 min) Epoch 61/300 -- Iteration 58536 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.31 min) Epoch 61/300 -- Iteration 58545 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.32 min) Epoch 61/300 -- Iteration 58554 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.32 min) Epoch 61/300 -- Iteration 58563 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.32 min) Epoch 61/300 -- Iteration 58572 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.33 min) Epoch 61/300 -- Iteration 58581 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.33 min) Epoch 61/300 -- Iteration 58590 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.34 min) Epoch 61/300 -- Iteration 58599 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.34 min) Epoch 61/300 -- Iteration 58608 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.35 min) Epoch 61/300 -- Iteration 58617 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.35 min) Epoch 61/300 -- Iteration 58626 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.36 min) Epoch 61/300 -- Iteration 58635 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.36 min) Epoch 61/300 -- Iteration 58644 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.37 min) Epoch 61/300 -- Iteration 58653 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.37 min) Epoch 61/300 -- Iteration 58662 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.37 min) Epoch 61/300 -- Iteration 58671 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.38 min) Epoch 61/300 -- Iteration 58680 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.38 min) Epoch 61/300 -- Iteration 58689 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.39 min) Epoch 61/300 -- Iteration 58698 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.39 min) Epoch 61/300 -- Iteration 58707 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.40 min) Epoch 61/300 -- Iteration 58716 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.40 min) Epoch 61/300 -- Iteration 58725 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.41 min) Epoch 61/300 -- Iteration 58734 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(29.41 min) Epoch 61/300 -- Iteration 58743 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018 - Val acc: -0.0000\n",
      "(29.42 min) Epoch 62/300 -- Iteration 58752 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.42 min) Epoch 62/300 -- Iteration 58761 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.42 min) Epoch 62/300 -- Iteration 58770 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.43 min) Epoch 62/300 -- Iteration 58779 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.43 min) Epoch 62/300 -- Iteration 58788 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.44 min) Epoch 62/300 -- Iteration 58797 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.44 min) Epoch 62/300 -- Iteration 58806 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.45 min) Epoch 62/300 -- Iteration 58815 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.45 min) Epoch 62/300 -- Iteration 58824 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.46 min) Epoch 62/300 -- Iteration 58833 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.46 min) Epoch 62/300 -- Iteration 58842 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.46 min) Epoch 62/300 -- Iteration 58851 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.47 min) Epoch 62/300 -- Iteration 58860 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.47 min) Epoch 62/300 -- Iteration 58869 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.48 min) Epoch 62/300 -- Iteration 58878 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.48 min) Epoch 62/300 -- Iteration 58887 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.49 min) Epoch 62/300 -- Iteration 58896 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.49 min) Epoch 62/300 -- Iteration 58905 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.50 min) Epoch 62/300 -- Iteration 58914 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.50 min) Epoch 62/300 -- Iteration 58923 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.50 min) Epoch 62/300 -- Iteration 58932 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.51 min) Epoch 62/300 -- Iteration 58941 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.51 min) Epoch 62/300 -- Iteration 58950 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.52 min) Epoch 62/300 -- Iteration 58959 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.52 min) Epoch 62/300 -- Iteration 58968 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.53 min) Epoch 62/300 -- Iteration 58977 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.53 min) Epoch 62/300 -- Iteration 58986 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.54 min) Epoch 62/300 -- Iteration 58995 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.54 min) Epoch 62/300 -- Iteration 59004 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.54 min) Epoch 62/300 -- Iteration 59013 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.55 min) Epoch 62/300 -- Iteration 59022 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.55 min) Epoch 62/300 -- Iteration 59031 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.56 min) Epoch 62/300 -- Iteration 59040 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.56 min) Epoch 62/300 -- Iteration 59049 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.57 min) Epoch 62/300 -- Iteration 59058 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.57 min) Epoch 62/300 -- Iteration 59067 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.58 min) Epoch 62/300 -- Iteration 59076 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.58 min) Epoch 62/300 -- Iteration 59085 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.58 min) Epoch 62/300 -- Iteration 59094 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.59 min) Epoch 62/300 -- Iteration 59103 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.59 min) Epoch 62/300 -- Iteration 59112 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.60 min) Epoch 62/300 -- Iteration 59121 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.60 min) Epoch 62/300 -- Iteration 59130 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.61 min) Epoch 62/300 -- Iteration 59139 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.61 min) Epoch 62/300 -- Iteration 59148 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.62 min) Epoch 62/300 -- Iteration 59157 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.62 min) Epoch 62/300 -- Iteration 59166 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.62 min) Epoch 62/300 -- Iteration 59175 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.63 min) Epoch 62/300 -- Iteration 59184 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.63 min) Epoch 62/300 -- Iteration 59193 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.64 min) Epoch 62/300 -- Iteration 59202 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.64 min) Epoch 62/300 -- Iteration 59211 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.65 min) Epoch 62/300 -- Iteration 59220 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.65 min) Epoch 62/300 -- Iteration 59229 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.66 min) Epoch 62/300 -- Iteration 59238 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.66 min) Epoch 62/300 -- Iteration 59247 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.66 min) Epoch 62/300 -- Iteration 59256 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.67 min) Epoch 62/300 -- Iteration 59265 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.67 min) Epoch 62/300 -- Iteration 59274 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.68 min) Epoch 62/300 -- Iteration 59283 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.68 min) Epoch 62/300 -- Iteration 59292 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.69 min) Epoch 62/300 -- Iteration 59301 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.69 min) Epoch 62/300 -- Iteration 59310 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.70 min) Epoch 62/300 -- Iteration 59319 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.70 min) Epoch 62/300 -- Iteration 59328 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.71 min) Epoch 62/300 -- Iteration 59337 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.71 min) Epoch 62/300 -- Iteration 59346 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.71 min) Epoch 62/300 -- Iteration 59355 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.72 min) Epoch 62/300 -- Iteration 59364 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.72 min) Epoch 62/300 -- Iteration 59373 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.73 min) Epoch 62/300 -- Iteration 59382 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.73 min) Epoch 62/300 -- Iteration 59391 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.74 min) Epoch 62/300 -- Iteration 59400 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.74 min) Epoch 62/300 -- Iteration 59409 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.75 min) Epoch 62/300 -- Iteration 59418 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.75 min) Epoch 62/300 -- Iteration 59427 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.75 min) Epoch 62/300 -- Iteration 59436 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.76 min) Epoch 62/300 -- Iteration 59445 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.76 min) Epoch 62/300 -- Iteration 59454 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.77 min) Epoch 62/300 -- Iteration 59463 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.77 min) Epoch 62/300 -- Iteration 59472 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.78 min) Epoch 62/300 -- Iteration 59481 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.78 min) Epoch 62/300 -- Iteration 59490 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.79 min) Epoch 62/300 -- Iteration 59499 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.79 min) Epoch 62/300 -- Iteration 59508 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.79 min) Epoch 62/300 -- Iteration 59517 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.80 min) Epoch 62/300 -- Iteration 59526 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.80 min) Epoch 62/300 -- Iteration 59535 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.81 min) Epoch 62/300 -- Iteration 59544 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.81 min) Epoch 62/300 -- Iteration 59553 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.82 min) Epoch 62/300 -- Iteration 59562 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.82 min) Epoch 62/300 -- Iteration 59571 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.83 min) Epoch 62/300 -- Iteration 59580 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.83 min) Epoch 62/300 -- Iteration 59589 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.83 min) Epoch 62/300 -- Iteration 59598 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.84 min) Epoch 62/300 -- Iteration 59607 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.84 min) Epoch 62/300 -- Iteration 59616 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.85 min) Epoch 62/300 -- Iteration 59625 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.85 min) Epoch 62/300 -- Iteration 59634 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.86 min) Epoch 62/300 -- Iteration 59643 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.86 min) Epoch 62/300 -- Iteration 59652 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.87 min) Epoch 62/300 -- Iteration 59661 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.87 min) Epoch 62/300 -- Iteration 59670 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.87 min) Epoch 62/300 -- Iteration 59679 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.88 min) Epoch 62/300 -- Iteration 59688 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.88 min) Epoch 62/300 -- Iteration 59697 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(29.89 min) Epoch 62/300 -- Iteration 59706 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(29.89 min) Epoch 63/300 -- Iteration 59715 - Batch 9/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.90 min) Epoch 63/300 -- Iteration 59724 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.90 min) Epoch 63/300 -- Iteration 59733 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.91 min) Epoch 63/300 -- Iteration 59742 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.91 min) Epoch 63/300 -- Iteration 59751 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.92 min) Epoch 63/300 -- Iteration 59760 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.92 min) Epoch 63/300 -- Iteration 59769 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.92 min) Epoch 63/300 -- Iteration 59778 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.93 min) Epoch 63/300 -- Iteration 59787 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.93 min) Epoch 63/300 -- Iteration 59796 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.94 min) Epoch 63/300 -- Iteration 59805 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.94 min) Epoch 63/300 -- Iteration 59814 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.95 min) Epoch 63/300 -- Iteration 59823 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.95 min) Epoch 63/300 -- Iteration 59832 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.96 min) Epoch 63/300 -- Iteration 59841 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.96 min) Epoch 63/300 -- Iteration 59850 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.97 min) Epoch 63/300 -- Iteration 59859 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.97 min) Epoch 63/300 -- Iteration 59868 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.97 min) Epoch 63/300 -- Iteration 59877 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.98 min) Epoch 63/300 -- Iteration 59886 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.98 min) Epoch 63/300 -- Iteration 59895 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.99 min) Epoch 63/300 -- Iteration 59904 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(29.99 min) Epoch 63/300 -- Iteration 59913 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.00 min) Epoch 63/300 -- Iteration 59922 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.00 min) Epoch 63/300 -- Iteration 59931 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.01 min) Epoch 63/300 -- Iteration 59940 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.01 min) Epoch 63/300 -- Iteration 59949 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.01 min) Epoch 63/300 -- Iteration 59958 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.02 min) Epoch 63/300 -- Iteration 59967 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.02 min) Epoch 63/300 -- Iteration 59976 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.03 min) Epoch 63/300 -- Iteration 59985 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.03 min) Epoch 63/300 -- Iteration 59994 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.04 min) Epoch 63/300 -- Iteration 60003 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.04 min) Epoch 63/300 -- Iteration 60012 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.05 min) Epoch 63/300 -- Iteration 60021 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.05 min) Epoch 63/300 -- Iteration 60030 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.05 min) Epoch 63/300 -- Iteration 60039 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.06 min) Epoch 63/300 -- Iteration 60048 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.06 min) Epoch 63/300 -- Iteration 60057 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.07 min) Epoch 63/300 -- Iteration 60066 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.07 min) Epoch 63/300 -- Iteration 60075 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.08 min) Epoch 63/300 -- Iteration 60084 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.08 min) Epoch 63/300 -- Iteration 60093 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.09 min) Epoch 63/300 -- Iteration 60102 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.09 min) Epoch 63/300 -- Iteration 60111 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.09 min) Epoch 63/300 -- Iteration 60120 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.10 min) Epoch 63/300 -- Iteration 60129 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.10 min) Epoch 63/300 -- Iteration 60138 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.11 min) Epoch 63/300 -- Iteration 60147 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.11 min) Epoch 63/300 -- Iteration 60156 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.12 min) Epoch 63/300 -- Iteration 60165 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.12 min) Epoch 63/300 -- Iteration 60174 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.13 min) Epoch 63/300 -- Iteration 60183 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.13 min) Epoch 63/300 -- Iteration 60192 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.14 min) Epoch 63/300 -- Iteration 60201 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.14 min) Epoch 63/300 -- Iteration 60210 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.14 min) Epoch 63/300 -- Iteration 60219 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.15 min) Epoch 63/300 -- Iteration 60228 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.15 min) Epoch 63/300 -- Iteration 60237 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.16 min) Epoch 63/300 -- Iteration 60246 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.16 min) Epoch 63/300 -- Iteration 60255 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.17 min) Epoch 63/300 -- Iteration 60264 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.17 min) Epoch 63/300 -- Iteration 60273 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.17 min) Epoch 63/300 -- Iteration 60282 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.18 min) Epoch 63/300 -- Iteration 60291 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.18 min) Epoch 63/300 -- Iteration 60300 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.19 min) Epoch 63/300 -- Iteration 60309 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.19 min) Epoch 63/300 -- Iteration 60318 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.20 min) Epoch 63/300 -- Iteration 60327 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.20 min) Epoch 63/300 -- Iteration 60336 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.21 min) Epoch 63/300 -- Iteration 60345 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.21 min) Epoch 63/300 -- Iteration 60354 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.22 min) Epoch 63/300 -- Iteration 60363 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.22 min) Epoch 63/300 -- Iteration 60372 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.22 min) Epoch 63/300 -- Iteration 60381 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.23 min) Epoch 63/300 -- Iteration 60390 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.23 min) Epoch 63/300 -- Iteration 60399 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.24 min) Epoch 63/300 -- Iteration 60408 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.24 min) Epoch 63/300 -- Iteration 60417 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.25 min) Epoch 63/300 -- Iteration 60426 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.25 min) Epoch 63/300 -- Iteration 60435 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.26 min) Epoch 63/300 -- Iteration 60444 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.26 min) Epoch 63/300 -- Iteration 60453 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.26 min) Epoch 63/300 -- Iteration 60462 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.27 min) Epoch 63/300 -- Iteration 60471 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.27 min) Epoch 63/300 -- Iteration 60480 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.28 min) Epoch 63/300 -- Iteration 60489 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.28 min) Epoch 63/300 -- Iteration 60498 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.29 min) Epoch 63/300 -- Iteration 60507 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.29 min) Epoch 63/300 -- Iteration 60516 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.30 min) Epoch 63/300 -- Iteration 60525 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.30 min) Epoch 63/300 -- Iteration 60534 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.30 min) Epoch 63/300 -- Iteration 60543 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.31 min) Epoch 63/300 -- Iteration 60552 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.31 min) Epoch 63/300 -- Iteration 60561 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.32 min) Epoch 63/300 -- Iteration 60570 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.32 min) Epoch 63/300 -- Iteration 60579 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.33 min) Epoch 63/300 -- Iteration 60588 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.33 min) Epoch 63/300 -- Iteration 60597 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.34 min) Epoch 63/300 -- Iteration 60606 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.34 min) Epoch 63/300 -- Iteration 60615 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.34 min) Epoch 63/300 -- Iteration 60624 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.35 min) Epoch 63/300 -- Iteration 60633 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.35 min) Epoch 63/300 -- Iteration 60642 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.36 min) Epoch 63/300 -- Iteration 60651 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.36 min) Epoch 63/300 -- Iteration 60660 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(30.37 min) Epoch 63/300 -- Iteration 60669 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(30.37 min) Epoch 64/300 -- Iteration 60678 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.38 min) Epoch 64/300 -- Iteration 60687 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.38 min) Epoch 64/300 -- Iteration 60696 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.39 min) Epoch 64/300 -- Iteration 60705 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.39 min) Epoch 64/300 -- Iteration 60714 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.39 min) Epoch 64/300 -- Iteration 60723 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.40 min) Epoch 64/300 -- Iteration 60732 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.40 min) Epoch 64/300 -- Iteration 60741 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.41 min) Epoch 64/300 -- Iteration 60750 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.41 min) Epoch 64/300 -- Iteration 60759 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.42 min) Epoch 64/300 -- Iteration 60768 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.42 min) Epoch 64/300 -- Iteration 60777 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.43 min) Epoch 64/300 -- Iteration 60786 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.43 min) Epoch 64/300 -- Iteration 60795 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.43 min) Epoch 64/300 -- Iteration 60804 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.44 min) Epoch 64/300 -- Iteration 60813 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.44 min) Epoch 64/300 -- Iteration 60822 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.45 min) Epoch 64/300 -- Iteration 60831 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.45 min) Epoch 64/300 -- Iteration 60840 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.46 min) Epoch 64/300 -- Iteration 60849 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.46 min) Epoch 64/300 -- Iteration 60858 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.47 min) Epoch 64/300 -- Iteration 60867 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.47 min) Epoch 64/300 -- Iteration 60876 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.47 min) Epoch 64/300 -- Iteration 60885 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.48 min) Epoch 64/300 -- Iteration 60894 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.48 min) Epoch 64/300 -- Iteration 60903 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.49 min) Epoch 64/300 -- Iteration 60912 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.49 min) Epoch 64/300 -- Iteration 60921 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.50 min) Epoch 64/300 -- Iteration 60930 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.50 min) Epoch 64/300 -- Iteration 60939 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.51 min) Epoch 64/300 -- Iteration 60948 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.51 min) Epoch 64/300 -- Iteration 60957 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.51 min) Epoch 64/300 -- Iteration 60966 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.52 min) Epoch 64/300 -- Iteration 60975 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.52 min) Epoch 64/300 -- Iteration 60984 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.53 min) Epoch 64/300 -- Iteration 60993 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.53 min) Epoch 64/300 -- Iteration 61002 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.54 min) Epoch 64/300 -- Iteration 61011 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.54 min) Epoch 64/300 -- Iteration 61020 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.55 min) Epoch 64/300 -- Iteration 61029 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.55 min) Epoch 64/300 -- Iteration 61038 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.55 min) Epoch 64/300 -- Iteration 61047 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.56 min) Epoch 64/300 -- Iteration 61056 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.56 min) Epoch 64/300 -- Iteration 61065 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.57 min) Epoch 64/300 -- Iteration 61074 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.57 min) Epoch 64/300 -- Iteration 61083 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.58 min) Epoch 64/300 -- Iteration 61092 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.58 min) Epoch 64/300 -- Iteration 61101 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.59 min) Epoch 64/300 -- Iteration 61110 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.59 min) Epoch 64/300 -- Iteration 61119 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.59 min) Epoch 64/300 -- Iteration 61128 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.60 min) Epoch 64/300 -- Iteration 61137 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.60 min) Epoch 64/300 -- Iteration 61146 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.61 min) Epoch 64/300 -- Iteration 61155 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.61 min) Epoch 64/300 -- Iteration 61164 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.62 min) Epoch 64/300 -- Iteration 61173 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.62 min) Epoch 64/300 -- Iteration 61182 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.63 min) Epoch 64/300 -- Iteration 61191 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.63 min) Epoch 64/300 -- Iteration 61200 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.63 min) Epoch 64/300 -- Iteration 61209 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.64 min) Epoch 64/300 -- Iteration 61218 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.64 min) Epoch 64/300 -- Iteration 61227 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.65 min) Epoch 64/300 -- Iteration 61236 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.65 min) Epoch 64/300 -- Iteration 61245 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.66 min) Epoch 64/300 -- Iteration 61254 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.66 min) Epoch 64/300 -- Iteration 61263 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.67 min) Epoch 64/300 -- Iteration 61272 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.67 min) Epoch 64/300 -- Iteration 61281 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.67 min) Epoch 64/300 -- Iteration 61290 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.68 min) Epoch 64/300 -- Iteration 61299 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.68 min) Epoch 64/300 -- Iteration 61308 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.69 min) Epoch 64/300 -- Iteration 61317 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.69 min) Epoch 64/300 -- Iteration 61326 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.70 min) Epoch 64/300 -- Iteration 61335 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.70 min) Epoch 64/300 -- Iteration 61344 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.71 min) Epoch 64/300 -- Iteration 61353 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.71 min) Epoch 64/300 -- Iteration 61362 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.71 min) Epoch 64/300 -- Iteration 61371 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.72 min) Epoch 64/300 -- Iteration 61380 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.72 min) Epoch 64/300 -- Iteration 61389 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.73 min) Epoch 64/300 -- Iteration 61398 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.73 min) Epoch 64/300 -- Iteration 61407 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.74 min) Epoch 64/300 -- Iteration 61416 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.74 min) Epoch 64/300 -- Iteration 61425 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.75 min) Epoch 64/300 -- Iteration 61434 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.75 min) Epoch 64/300 -- Iteration 61443 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.75 min) Epoch 64/300 -- Iteration 61452 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.76 min) Epoch 64/300 -- Iteration 61461 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.76 min) Epoch 64/300 -- Iteration 61470 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.77 min) Epoch 64/300 -- Iteration 61479 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.77 min) Epoch 64/300 -- Iteration 61488 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.78 min) Epoch 64/300 -- Iteration 61497 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.78 min) Epoch 64/300 -- Iteration 61506 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.79 min) Epoch 64/300 -- Iteration 61515 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.79 min) Epoch 64/300 -- Iteration 61524 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.80 min) Epoch 64/300 -- Iteration 61533 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.80 min) Epoch 64/300 -- Iteration 61542 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.80 min) Epoch 64/300 -- Iteration 61551 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.81 min) Epoch 64/300 -- Iteration 61560 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.81 min) Epoch 64/300 -- Iteration 61569 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.82 min) Epoch 64/300 -- Iteration 61578 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.82 min) Epoch 64/300 -- Iteration 61587 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.83 min) Epoch 64/300 -- Iteration 61596 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.83 min) Epoch 64/300 -- Iteration 61605 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.84 min) Epoch 64/300 -- Iteration 61614 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.84 min) Epoch 64/300 -- Iteration 61623 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.84 min) Epoch 64/300 -- Iteration 61632 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(30.85 min) Epoch 65/300 -- Iteration 61641 - Batch 9/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.85 min) Epoch 65/300 -- Iteration 61650 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.86 min) Epoch 65/300 -- Iteration 61659 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.86 min) Epoch 65/300 -- Iteration 61668 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.87 min) Epoch 65/300 -- Iteration 61677 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.87 min) Epoch 65/300 -- Iteration 61686 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.88 min) Epoch 65/300 -- Iteration 61695 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.88 min) Epoch 65/300 -- Iteration 61704 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.89 min) Epoch 65/300 -- Iteration 61713 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.89 min) Epoch 65/300 -- Iteration 61722 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.89 min) Epoch 65/300 -- Iteration 61731 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.90 min) Epoch 65/300 -- Iteration 61740 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.90 min) Epoch 65/300 -- Iteration 61749 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.91 min) Epoch 65/300 -- Iteration 61758 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.91 min) Epoch 65/300 -- Iteration 61767 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.92 min) Epoch 65/300 -- Iteration 61776 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.92 min) Epoch 65/300 -- Iteration 61785 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.93 min) Epoch 65/300 -- Iteration 61794 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.93 min) Epoch 65/300 -- Iteration 61803 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.93 min) Epoch 65/300 -- Iteration 61812 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.94 min) Epoch 65/300 -- Iteration 61821 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.94 min) Epoch 65/300 -- Iteration 61830 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.95 min) Epoch 65/300 -- Iteration 61839 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.95 min) Epoch 65/300 -- Iteration 61848 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.96 min) Epoch 65/300 -- Iteration 61857 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.96 min) Epoch 65/300 -- Iteration 61866 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.97 min) Epoch 65/300 -- Iteration 61875 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.97 min) Epoch 65/300 -- Iteration 61884 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.97 min) Epoch 65/300 -- Iteration 61893 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.98 min) Epoch 65/300 -- Iteration 61902 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.98 min) Epoch 65/300 -- Iteration 61911 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.99 min) Epoch 65/300 -- Iteration 61920 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(30.99 min) Epoch 65/300 -- Iteration 61929 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.00 min) Epoch 65/300 -- Iteration 61938 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.00 min) Epoch 65/300 -- Iteration 61947 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.01 min) Epoch 65/300 -- Iteration 61956 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.01 min) Epoch 65/300 -- Iteration 61965 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.01 min) Epoch 65/300 -- Iteration 61974 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.02 min) Epoch 65/300 -- Iteration 61983 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.02 min) Epoch 65/300 -- Iteration 61992 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.03 min) Epoch 65/300 -- Iteration 62001 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.03 min) Epoch 65/300 -- Iteration 62010 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.04 min) Epoch 65/300 -- Iteration 62019 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.04 min) Epoch 65/300 -- Iteration 62028 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.05 min) Epoch 65/300 -- Iteration 62037 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.05 min) Epoch 65/300 -- Iteration 62046 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.05 min) Epoch 65/300 -- Iteration 62055 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.06 min) Epoch 65/300 -- Iteration 62064 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.06 min) Epoch 65/300 -- Iteration 62073 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.07 min) Epoch 65/300 -- Iteration 62082 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.07 min) Epoch 65/300 -- Iteration 62091 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.08 min) Epoch 65/300 -- Iteration 62100 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.08 min) Epoch 65/300 -- Iteration 62109 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.09 min) Epoch 65/300 -- Iteration 62118 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.09 min) Epoch 65/300 -- Iteration 62127 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.09 min) Epoch 65/300 -- Iteration 62136 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.10 min) Epoch 65/300 -- Iteration 62145 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.10 min) Epoch 65/300 -- Iteration 62154 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.11 min) Epoch 65/300 -- Iteration 62163 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.11 min) Epoch 65/300 -- Iteration 62172 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.12 min) Epoch 65/300 -- Iteration 62181 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.12 min) Epoch 65/300 -- Iteration 62190 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.13 min) Epoch 65/300 -- Iteration 62199 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.13 min) Epoch 65/300 -- Iteration 62208 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.13 min) Epoch 65/300 -- Iteration 62217 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.14 min) Epoch 65/300 -- Iteration 62226 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.14 min) Epoch 65/300 -- Iteration 62235 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.15 min) Epoch 65/300 -- Iteration 62244 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.15 min) Epoch 65/300 -- Iteration 62253 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.16 min) Epoch 65/300 -- Iteration 62262 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.16 min) Epoch 65/300 -- Iteration 62271 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.17 min) Epoch 65/300 -- Iteration 62280 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.17 min) Epoch 65/300 -- Iteration 62289 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.17 min) Epoch 65/300 -- Iteration 62298 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.18 min) Epoch 65/300 -- Iteration 62307 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.18 min) Epoch 65/300 -- Iteration 62316 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.19 min) Epoch 65/300 -- Iteration 62325 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.19 min) Epoch 65/300 -- Iteration 62334 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.20 min) Epoch 65/300 -- Iteration 62343 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.20 min) Epoch 65/300 -- Iteration 62352 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.21 min) Epoch 65/300 -- Iteration 62361 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.21 min) Epoch 65/300 -- Iteration 62370 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.21 min) Epoch 65/300 -- Iteration 62379 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.22 min) Epoch 65/300 -- Iteration 62388 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.22 min) Epoch 65/300 -- Iteration 62397 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.23 min) Epoch 65/300 -- Iteration 62406 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.23 min) Epoch 65/300 -- Iteration 62415 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.24 min) Epoch 65/300 -- Iteration 62424 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.24 min) Epoch 65/300 -- Iteration 62433 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.25 min) Epoch 65/300 -- Iteration 62442 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.25 min) Epoch 65/300 -- Iteration 62451 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.25 min) Epoch 65/300 -- Iteration 62460 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.26 min) Epoch 65/300 -- Iteration 62469 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.26 min) Epoch 65/300 -- Iteration 62478 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.27 min) Epoch 65/300 -- Iteration 62487 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.27 min) Epoch 65/300 -- Iteration 62496 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.28 min) Epoch 65/300 -- Iteration 62505 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.28 min) Epoch 65/300 -- Iteration 62514 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.29 min) Epoch 65/300 -- Iteration 62523 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.29 min) Epoch 65/300 -- Iteration 62532 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.30 min) Epoch 65/300 -- Iteration 62541 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.30 min) Epoch 65/300 -- Iteration 62550 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.30 min) Epoch 65/300 -- Iteration 62559 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.31 min) Epoch 65/300 -- Iteration 62568 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.31 min) Epoch 65/300 -- Iteration 62577 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.32 min) Epoch 65/300 -- Iteration 62586 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.32 min) Epoch 65/300 -- Iteration 62595 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(31.33 min) Epoch 66/300 -- Iteration 62604 - Batch 9/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.33 min) Epoch 66/300 -- Iteration 62613 - Batch 18/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.34 min) Epoch 66/300 -- Iteration 62622 - Batch 27/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.34 min) Epoch 66/300 -- Iteration 62631 - Batch 36/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.34 min) Epoch 66/300 -- Iteration 62640 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.35 min) Epoch 66/300 -- Iteration 62649 - Batch 54/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.35 min) Epoch 66/300 -- Iteration 62658 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.36 min) Epoch 66/300 -- Iteration 62667 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.36 min) Epoch 66/300 -- Iteration 62676 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.37 min) Epoch 66/300 -- Iteration 62685 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.37 min) Epoch 66/300 -- Iteration 62694 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.38 min) Epoch 66/300 -- Iteration 62703 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.38 min) Epoch 66/300 -- Iteration 62712 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.38 min) Epoch 66/300 -- Iteration 62721 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.39 min) Epoch 66/300 -- Iteration 62730 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.39 min) Epoch 66/300 -- Iteration 62739 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.40 min) Epoch 66/300 -- Iteration 62748 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.40 min) Epoch 66/300 -- Iteration 62757 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.41 min) Epoch 66/300 -- Iteration 62766 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.41 min) Epoch 66/300 -- Iteration 62775 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.42 min) Epoch 66/300 -- Iteration 62784 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.42 min) Epoch 66/300 -- Iteration 62793 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.42 min) Epoch 66/300 -- Iteration 62802 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.43 min) Epoch 66/300 -- Iteration 62811 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.43 min) Epoch 66/300 -- Iteration 62820 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.44 min) Epoch 66/300 -- Iteration 62829 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.44 min) Epoch 66/300 -- Iteration 62838 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.45 min) Epoch 66/300 -- Iteration 62847 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.45 min) Epoch 66/300 -- Iteration 62856 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.46 min) Epoch 66/300 -- Iteration 62865 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.46 min) Epoch 66/300 -- Iteration 62874 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.46 min) Epoch 66/300 -- Iteration 62883 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.47 min) Epoch 66/300 -- Iteration 62892 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.47 min) Epoch 66/300 -- Iteration 62901 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.48 min) Epoch 66/300 -- Iteration 62910 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.48 min) Epoch 66/300 -- Iteration 62919 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.49 min) Epoch 66/300 -- Iteration 62928 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.49 min) Epoch 66/300 -- Iteration 62937 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.50 min) Epoch 66/300 -- Iteration 62946 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.50 min) Epoch 66/300 -- Iteration 62955 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.50 min) Epoch 66/300 -- Iteration 62964 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.51 min) Epoch 66/300 -- Iteration 62973 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.51 min) Epoch 66/300 -- Iteration 62982 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.52 min) Epoch 66/300 -- Iteration 62991 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.52 min) Epoch 66/300 -- Iteration 63000 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.53 min) Epoch 66/300 -- Iteration 63009 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.53 min) Epoch 66/300 -- Iteration 63018 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.54 min) Epoch 66/300 -- Iteration 63027 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.54 min) Epoch 66/300 -- Iteration 63036 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.55 min) Epoch 66/300 -- Iteration 63045 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.55 min) Epoch 66/300 -- Iteration 63054 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.55 min) Epoch 66/300 -- Iteration 63063 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.56 min) Epoch 66/300 -- Iteration 63072 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.56 min) Epoch 66/300 -- Iteration 63081 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.57 min) Epoch 66/300 -- Iteration 63090 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.57 min) Epoch 66/300 -- Iteration 63099 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.58 min) Epoch 66/300 -- Iteration 63108 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.58 min) Epoch 66/300 -- Iteration 63117 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.59 min) Epoch 66/300 -- Iteration 63126 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.59 min) Epoch 66/300 -- Iteration 63135 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.59 min) Epoch 66/300 -- Iteration 63144 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.60 min) Epoch 66/300 -- Iteration 63153 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.60 min) Epoch 66/300 -- Iteration 63162 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.61 min) Epoch 66/300 -- Iteration 63171 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.61 min) Epoch 66/300 -- Iteration 63180 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.62 min) Epoch 66/300 -- Iteration 63189 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.62 min) Epoch 66/300 -- Iteration 63198 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.63 min) Epoch 66/300 -- Iteration 63207 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.63 min) Epoch 66/300 -- Iteration 63216 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.63 min) Epoch 66/300 -- Iteration 63225 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.64 min) Epoch 66/300 -- Iteration 63234 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.64 min) Epoch 66/300 -- Iteration 63243 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.65 min) Epoch 66/300 -- Iteration 63252 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.65 min) Epoch 66/300 -- Iteration 63261 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.66 min) Epoch 66/300 -- Iteration 63270 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.66 min) Epoch 66/300 -- Iteration 63279 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.67 min) Epoch 66/300 -- Iteration 63288 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.67 min) Epoch 66/300 -- Iteration 63297 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.67 min) Epoch 66/300 -- Iteration 63306 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.68 min) Epoch 66/300 -- Iteration 63315 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.68 min) Epoch 66/300 -- Iteration 63324 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.69 min) Epoch 66/300 -- Iteration 63333 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.69 min) Epoch 66/300 -- Iteration 63342 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.70 min) Epoch 66/300 -- Iteration 63351 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.70 min) Epoch 66/300 -- Iteration 63360 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.71 min) Epoch 66/300 -- Iteration 63369 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.71 min) Epoch 66/300 -- Iteration 63378 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.71 min) Epoch 66/300 -- Iteration 63387 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.72 min) Epoch 66/300 -- Iteration 63396 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.72 min) Epoch 66/300 -- Iteration 63405 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.73 min) Epoch 66/300 -- Iteration 63414 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.73 min) Epoch 66/300 -- Iteration 63423 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.74 min) Epoch 66/300 -- Iteration 63432 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.74 min) Epoch 66/300 -- Iteration 63441 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.75 min) Epoch 66/300 -- Iteration 63450 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.75 min) Epoch 66/300 -- Iteration 63459 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.75 min) Epoch 66/300 -- Iteration 63468 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.76 min) Epoch 66/300 -- Iteration 63477 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.76 min) Epoch 66/300 -- Iteration 63486 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.77 min) Epoch 66/300 -- Iteration 63495 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.77 min) Epoch 66/300 -- Iteration 63504 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.78 min) Epoch 66/300 -- Iteration 63513 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.78 min) Epoch 66/300 -- Iteration 63522 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.79 min) Epoch 66/300 -- Iteration 63531 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.79 min) Epoch 66/300 -- Iteration 63540 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.80 min) Epoch 66/300 -- Iteration 63549 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(31.80 min) Epoch 66/300 -- Iteration 63558 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018 - Val acc: -0.0000\n",
      "(31.80 min) Epoch 67/300 -- Iteration 63567 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.81 min) Epoch 67/300 -- Iteration 63576 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.81 min) Epoch 67/300 -- Iteration 63585 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.82 min) Epoch 67/300 -- Iteration 63594 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.82 min) Epoch 67/300 -- Iteration 63603 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.83 min) Epoch 67/300 -- Iteration 63612 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.83 min) Epoch 67/300 -- Iteration 63621 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.84 min) Epoch 67/300 -- Iteration 63630 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.84 min) Epoch 67/300 -- Iteration 63639 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.84 min) Epoch 67/300 -- Iteration 63648 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.85 min) Epoch 67/300 -- Iteration 63657 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.85 min) Epoch 67/300 -- Iteration 63666 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.86 min) Epoch 67/300 -- Iteration 63675 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.86 min) Epoch 67/300 -- Iteration 63684 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.87 min) Epoch 67/300 -- Iteration 63693 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.87 min) Epoch 67/300 -- Iteration 63702 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.88 min) Epoch 67/300 -- Iteration 63711 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.88 min) Epoch 67/300 -- Iteration 63720 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.88 min) Epoch 67/300 -- Iteration 63729 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.89 min) Epoch 67/300 -- Iteration 63738 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.89 min) Epoch 67/300 -- Iteration 63747 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.90 min) Epoch 67/300 -- Iteration 63756 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.90 min) Epoch 67/300 -- Iteration 63765 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.91 min) Epoch 67/300 -- Iteration 63774 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.91 min) Epoch 67/300 -- Iteration 63783 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.92 min) Epoch 67/300 -- Iteration 63792 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.92 min) Epoch 67/300 -- Iteration 63801 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.92 min) Epoch 67/300 -- Iteration 63810 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.93 min) Epoch 67/300 -- Iteration 63819 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.93 min) Epoch 67/300 -- Iteration 63828 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.94 min) Epoch 67/300 -- Iteration 63837 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.94 min) Epoch 67/300 -- Iteration 63846 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.95 min) Epoch 67/300 -- Iteration 63855 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.95 min) Epoch 67/300 -- Iteration 63864 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.96 min) Epoch 67/300 -- Iteration 63873 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.96 min) Epoch 67/300 -- Iteration 63882 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.97 min) Epoch 67/300 -- Iteration 63891 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.97 min) Epoch 67/300 -- Iteration 63900 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.97 min) Epoch 67/300 -- Iteration 63909 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.98 min) Epoch 67/300 -- Iteration 63918 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.98 min) Epoch 67/300 -- Iteration 63927 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.99 min) Epoch 67/300 -- Iteration 63936 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(31.99 min) Epoch 67/300 -- Iteration 63945 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.00 min) Epoch 67/300 -- Iteration 63954 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.00 min) Epoch 67/300 -- Iteration 63963 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.01 min) Epoch 67/300 -- Iteration 63972 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.01 min) Epoch 67/300 -- Iteration 63981 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.01 min) Epoch 67/300 -- Iteration 63990 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.02 min) Epoch 67/300 -- Iteration 63999 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.02 min) Epoch 67/300 -- Iteration 64008 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.03 min) Epoch 67/300 -- Iteration 64017 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.03 min) Epoch 67/300 -- Iteration 64026 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.04 min) Epoch 67/300 -- Iteration 64035 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.04 min) Epoch 67/300 -- Iteration 64044 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.05 min) Epoch 67/300 -- Iteration 64053 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.05 min) Epoch 67/300 -- Iteration 64062 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.05 min) Epoch 67/300 -- Iteration 64071 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.06 min) Epoch 67/300 -- Iteration 64080 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.06 min) Epoch 67/300 -- Iteration 64089 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.07 min) Epoch 67/300 -- Iteration 64098 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.07 min) Epoch 67/300 -- Iteration 64107 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.08 min) Epoch 67/300 -- Iteration 64116 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.08 min) Epoch 67/300 -- Iteration 64125 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.09 min) Epoch 67/300 -- Iteration 64134 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.09 min) Epoch 67/300 -- Iteration 64143 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.09 min) Epoch 67/300 -- Iteration 64152 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.10 min) Epoch 67/300 -- Iteration 64161 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.10 min) Epoch 67/300 -- Iteration 64170 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.11 min) Epoch 67/300 -- Iteration 64179 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.11 min) Epoch 67/300 -- Iteration 64188 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.12 min) Epoch 67/300 -- Iteration 64197 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.12 min) Epoch 67/300 -- Iteration 64206 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.13 min) Epoch 67/300 -- Iteration 64215 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.13 min) Epoch 67/300 -- Iteration 64224 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.13 min) Epoch 67/300 -- Iteration 64233 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.14 min) Epoch 67/300 -- Iteration 64242 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.14 min) Epoch 67/300 -- Iteration 64251 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.15 min) Epoch 67/300 -- Iteration 64260 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.15 min) Epoch 67/300 -- Iteration 64269 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.16 min) Epoch 67/300 -- Iteration 64278 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.16 min) Epoch 67/300 -- Iteration 64287 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.17 min) Epoch 67/300 -- Iteration 64296 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.17 min) Epoch 67/300 -- Iteration 64305 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.17 min) Epoch 67/300 -- Iteration 64314 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.18 min) Epoch 67/300 -- Iteration 64323 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.18 min) Epoch 67/300 -- Iteration 64332 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.19 min) Epoch 67/300 -- Iteration 64341 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.19 min) Epoch 67/300 -- Iteration 64350 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.20 min) Epoch 67/300 -- Iteration 64359 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.20 min) Epoch 67/300 -- Iteration 64368 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.21 min) Epoch 67/300 -- Iteration 64377 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.21 min) Epoch 67/300 -- Iteration 64386 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.22 min) Epoch 67/300 -- Iteration 64395 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.22 min) Epoch 67/300 -- Iteration 64404 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.23 min) Epoch 67/300 -- Iteration 64413 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.23 min) Epoch 67/300 -- Iteration 64422 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.23 min) Epoch 67/300 -- Iteration 64431 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.24 min) Epoch 67/300 -- Iteration 64440 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.24 min) Epoch 67/300 -- Iteration 64449 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.25 min) Epoch 67/300 -- Iteration 64458 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.25 min) Epoch 67/300 -- Iteration 64467 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.26 min) Epoch 67/300 -- Iteration 64476 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.26 min) Epoch 67/300 -- Iteration 64485 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.27 min) Epoch 67/300 -- Iteration 64494 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.27 min) Epoch 67/300 -- Iteration 64503 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.28 min) Epoch 67/300 -- Iteration 64512 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(32.28 min) Epoch 67/300 -- Iteration 64521 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(32.29 min) Epoch 68/300 -- Iteration 64530 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.29 min) Epoch 68/300 -- Iteration 64539 - Batch 18/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.30 min) Epoch 68/300 -- Iteration 64548 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.30 min) Epoch 68/300 -- Iteration 64557 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.31 min) Epoch 68/300 -- Iteration 64566 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.31 min) Epoch 68/300 -- Iteration 64575 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.32 min) Epoch 68/300 -- Iteration 64584 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.32 min) Epoch 68/300 -- Iteration 64593 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.33 min) Epoch 68/300 -- Iteration 64602 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.33 min) Epoch 68/300 -- Iteration 64611 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.34 min) Epoch 68/300 -- Iteration 64620 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.34 min) Epoch 68/300 -- Iteration 64629 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.34 min) Epoch 68/300 -- Iteration 64638 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.35 min) Epoch 68/300 -- Iteration 64647 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.35 min) Epoch 68/300 -- Iteration 64656 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.36 min) Epoch 68/300 -- Iteration 64665 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.36 min) Epoch 68/300 -- Iteration 64674 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.37 min) Epoch 68/300 -- Iteration 64683 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.37 min) Epoch 68/300 -- Iteration 64692 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.38 min) Epoch 68/300 -- Iteration 64701 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.38 min) Epoch 68/300 -- Iteration 64710 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.39 min) Epoch 68/300 -- Iteration 64719 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.39 min) Epoch 68/300 -- Iteration 64728 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.40 min) Epoch 68/300 -- Iteration 64737 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.40 min) Epoch 68/300 -- Iteration 64746 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.41 min) Epoch 68/300 -- Iteration 64755 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.41 min) Epoch 68/300 -- Iteration 64764 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.41 min) Epoch 68/300 -- Iteration 64773 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.42 min) Epoch 68/300 -- Iteration 64782 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.42 min) Epoch 68/300 -- Iteration 64791 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.43 min) Epoch 68/300 -- Iteration 64800 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.43 min) Epoch 68/300 -- Iteration 64809 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.44 min) Epoch 68/300 -- Iteration 64818 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.44 min) Epoch 68/300 -- Iteration 64827 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.45 min) Epoch 68/300 -- Iteration 64836 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.45 min) Epoch 68/300 -- Iteration 64845 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.46 min) Epoch 68/300 -- Iteration 64854 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.46 min) Epoch 68/300 -- Iteration 64863 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.46 min) Epoch 68/300 -- Iteration 64872 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.47 min) Epoch 68/300 -- Iteration 64881 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.47 min) Epoch 68/300 -- Iteration 64890 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.48 min) Epoch 68/300 -- Iteration 64899 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.48 min) Epoch 68/300 -- Iteration 64908 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.49 min) Epoch 68/300 -- Iteration 64917 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.49 min) Epoch 68/300 -- Iteration 64926 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.50 min) Epoch 68/300 -- Iteration 64935 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.50 min) Epoch 68/300 -- Iteration 64944 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.51 min) Epoch 68/300 -- Iteration 64953 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.51 min) Epoch 68/300 -- Iteration 64962 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.51 min) Epoch 68/300 -- Iteration 64971 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.52 min) Epoch 68/300 -- Iteration 64980 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.52 min) Epoch 68/300 -- Iteration 64989 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.53 min) Epoch 68/300 -- Iteration 64998 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.53 min) Epoch 68/300 -- Iteration 65007 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.54 min) Epoch 68/300 -- Iteration 65016 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.54 min) Epoch 68/300 -- Iteration 65025 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.55 min) Epoch 68/300 -- Iteration 65034 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.55 min) Epoch 68/300 -- Iteration 65043 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.56 min) Epoch 68/300 -- Iteration 65052 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.56 min) Epoch 68/300 -- Iteration 65061 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.57 min) Epoch 68/300 -- Iteration 65070 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.57 min) Epoch 68/300 -- Iteration 65079 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.58 min) Epoch 68/300 -- Iteration 65088 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.58 min) Epoch 68/300 -- Iteration 65097 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.58 min) Epoch 68/300 -- Iteration 65106 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.59 min) Epoch 68/300 -- Iteration 65115 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.59 min) Epoch 68/300 -- Iteration 65124 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.60 min) Epoch 68/300 -- Iteration 65133 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.60 min) Epoch 68/300 -- Iteration 65142 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.61 min) Epoch 68/300 -- Iteration 65151 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.61 min) Epoch 68/300 -- Iteration 65160 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.62 min) Epoch 68/300 -- Iteration 65169 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.62 min) Epoch 68/300 -- Iteration 65178 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.62 min) Epoch 68/300 -- Iteration 65187 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.63 min) Epoch 68/300 -- Iteration 65196 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.63 min) Epoch 68/300 -- Iteration 65205 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.64 min) Epoch 68/300 -- Iteration 65214 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.64 min) Epoch 68/300 -- Iteration 65223 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.65 min) Epoch 68/300 -- Iteration 65232 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.65 min) Epoch 68/300 -- Iteration 65241 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.66 min) Epoch 68/300 -- Iteration 65250 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.66 min) Epoch 68/300 -- Iteration 65259 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.67 min) Epoch 68/300 -- Iteration 65268 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.67 min) Epoch 68/300 -- Iteration 65277 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.68 min) Epoch 68/300 -- Iteration 65286 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.68 min) Epoch 68/300 -- Iteration 65295 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.69 min) Epoch 68/300 -- Iteration 65304 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.69 min) Epoch 68/300 -- Iteration 65313 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.69 min) Epoch 68/300 -- Iteration 65322 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.70 min) Epoch 68/300 -- Iteration 65331 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.70 min) Epoch 68/300 -- Iteration 65340 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.71 min) Epoch 68/300 -- Iteration 65349 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.71 min) Epoch 68/300 -- Iteration 65358 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.72 min) Epoch 68/300 -- Iteration 65367 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.72 min) Epoch 68/300 -- Iteration 65376 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.73 min) Epoch 68/300 -- Iteration 65385 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.73 min) Epoch 68/300 -- Iteration 65394 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.74 min) Epoch 68/300 -- Iteration 65403 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.74 min) Epoch 68/300 -- Iteration 65412 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.74 min) Epoch 68/300 -- Iteration 65421 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.75 min) Epoch 68/300 -- Iteration 65430 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.75 min) Epoch 68/300 -- Iteration 65439 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.76 min) Epoch 68/300 -- Iteration 65448 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.76 min) Epoch 68/300 -- Iteration 65457 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.77 min) Epoch 68/300 -- Iteration 65466 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.77 min) Epoch 68/300 -- Iteration 65475 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.78 min) Epoch 68/300 -- Iteration 65484 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(32.78 min) Epoch 69/300 -- Iteration 65493 - Batch 9/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.79 min) Epoch 69/300 -- Iteration 65502 - Batch 18/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.79 min) Epoch 69/300 -- Iteration 65511 - Batch 27/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.80 min) Epoch 69/300 -- Iteration 65520 - Batch 36/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.80 min) Epoch 69/300 -- Iteration 65529 - Batch 45/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.81 min) Epoch 69/300 -- Iteration 65538 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.81 min) Epoch 69/300 -- Iteration 65547 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.82 min) Epoch 69/300 -- Iteration 65556 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.82 min) Epoch 69/300 -- Iteration 65565 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.82 min) Epoch 69/300 -- Iteration 65574 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.83 min) Epoch 69/300 -- Iteration 65583 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.83 min) Epoch 69/300 -- Iteration 65592 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.84 min) Epoch 69/300 -- Iteration 65601 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.84 min) Epoch 69/300 -- Iteration 65610 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.85 min) Epoch 69/300 -- Iteration 65619 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.85 min) Epoch 69/300 -- Iteration 65628 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.86 min) Epoch 69/300 -- Iteration 65637 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.86 min) Epoch 69/300 -- Iteration 65646 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.87 min) Epoch 69/300 -- Iteration 65655 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.87 min) Epoch 69/300 -- Iteration 65664 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.88 min) Epoch 69/300 -- Iteration 65673 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.88 min) Epoch 69/300 -- Iteration 65682 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.89 min) Epoch 69/300 -- Iteration 65691 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.89 min) Epoch 69/300 -- Iteration 65700 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.90 min) Epoch 69/300 -- Iteration 65709 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.90 min) Epoch 69/300 -- Iteration 65718 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.91 min) Epoch 69/300 -- Iteration 65727 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.91 min) Epoch 69/300 -- Iteration 65736 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.91 min) Epoch 69/300 -- Iteration 65745 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.92 min) Epoch 69/300 -- Iteration 65754 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.92 min) Epoch 69/300 -- Iteration 65763 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.93 min) Epoch 69/300 -- Iteration 65772 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.93 min) Epoch 69/300 -- Iteration 65781 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.94 min) Epoch 69/300 -- Iteration 65790 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.94 min) Epoch 69/300 -- Iteration 65799 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.95 min) Epoch 69/300 -- Iteration 65808 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.95 min) Epoch 69/300 -- Iteration 65817 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.96 min) Epoch 69/300 -- Iteration 65826 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.96 min) Epoch 69/300 -- Iteration 65835 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.97 min) Epoch 69/300 -- Iteration 65844 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.97 min) Epoch 69/300 -- Iteration 65853 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.97 min) Epoch 69/300 -- Iteration 65862 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.98 min) Epoch 69/300 -- Iteration 65871 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.98 min) Epoch 69/300 -- Iteration 65880 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.99 min) Epoch 69/300 -- Iteration 65889 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(32.99 min) Epoch 69/300 -- Iteration 65898 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.00 min) Epoch 69/300 -- Iteration 65907 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.00 min) Epoch 69/300 -- Iteration 65916 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.01 min) Epoch 69/300 -- Iteration 65925 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.01 min) Epoch 69/300 -- Iteration 65934 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.02 min) Epoch 69/300 -- Iteration 65943 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.02 min) Epoch 69/300 -- Iteration 65952 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.03 min) Epoch 69/300 -- Iteration 65961 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.03 min) Epoch 69/300 -- Iteration 65970 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.04 min) Epoch 69/300 -- Iteration 65979 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.04 min) Epoch 69/300 -- Iteration 65988 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.05 min) Epoch 69/300 -- Iteration 65997 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.05 min) Epoch 69/300 -- Iteration 66006 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.05 min) Epoch 69/300 -- Iteration 66015 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.06 min) Epoch 69/300 -- Iteration 66024 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.06 min) Epoch 69/300 -- Iteration 66033 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.07 min) Epoch 69/300 -- Iteration 66042 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.07 min) Epoch 69/300 -- Iteration 66051 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.08 min) Epoch 69/300 -- Iteration 66060 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.08 min) Epoch 69/300 -- Iteration 66069 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.09 min) Epoch 69/300 -- Iteration 66078 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.09 min) Epoch 69/300 -- Iteration 66087 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.09 min) Epoch 69/300 -- Iteration 66096 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.10 min) Epoch 69/300 -- Iteration 66105 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.10 min) Epoch 69/300 -- Iteration 66114 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.11 min) Epoch 69/300 -- Iteration 66123 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.11 min) Epoch 69/300 -- Iteration 66132 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.12 min) Epoch 69/300 -- Iteration 66141 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.12 min) Epoch 69/300 -- Iteration 66150 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.13 min) Epoch 69/300 -- Iteration 66159 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.13 min) Epoch 69/300 -- Iteration 66168 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.13 min) Epoch 69/300 -- Iteration 66177 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.14 min) Epoch 69/300 -- Iteration 66186 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.14 min) Epoch 69/300 -- Iteration 66195 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.15 min) Epoch 69/300 -- Iteration 66204 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.15 min) Epoch 69/300 -- Iteration 66213 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.16 min) Epoch 69/300 -- Iteration 66222 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.16 min) Epoch 69/300 -- Iteration 66231 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.17 min) Epoch 69/300 -- Iteration 66240 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.17 min) Epoch 69/300 -- Iteration 66249 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.17 min) Epoch 69/300 -- Iteration 66258 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.18 min) Epoch 69/300 -- Iteration 66267 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.18 min) Epoch 69/300 -- Iteration 66276 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.19 min) Epoch 69/300 -- Iteration 66285 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.19 min) Epoch 69/300 -- Iteration 66294 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.20 min) Epoch 69/300 -- Iteration 66303 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.20 min) Epoch 69/300 -- Iteration 66312 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.21 min) Epoch 69/300 -- Iteration 66321 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.21 min) Epoch 69/300 -- Iteration 66330 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.21 min) Epoch 69/300 -- Iteration 66339 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.22 min) Epoch 69/300 -- Iteration 66348 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.22 min) Epoch 69/300 -- Iteration 66357 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.23 min) Epoch 69/300 -- Iteration 66366 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.23 min) Epoch 69/300 -- Iteration 66375 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.24 min) Epoch 69/300 -- Iteration 66384 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.24 min) Epoch 69/300 -- Iteration 66393 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.25 min) Epoch 69/300 -- Iteration 66402 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.25 min) Epoch 69/300 -- Iteration 66411 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.25 min) Epoch 69/300 -- Iteration 66420 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.26 min) Epoch 69/300 -- Iteration 66429 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.26 min) Epoch 69/300 -- Iteration 66438 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.27 min) Epoch 69/300 -- Iteration 66447 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(33.27 min) Epoch 70/300 -- Iteration 66456 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.28 min) Epoch 70/300 -- Iteration 66465 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.28 min) Epoch 70/300 -- Iteration 66474 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.29 min) Epoch 70/300 -- Iteration 66483 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.29 min) Epoch 70/300 -- Iteration 66492 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.30 min) Epoch 70/300 -- Iteration 66501 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.30 min) Epoch 70/300 -- Iteration 66510 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.30 min) Epoch 70/300 -- Iteration 66519 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.31 min) Epoch 70/300 -- Iteration 66528 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.31 min) Epoch 70/300 -- Iteration 66537 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.32 min) Epoch 70/300 -- Iteration 66546 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.32 min) Epoch 70/300 -- Iteration 66555 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.33 min) Epoch 70/300 -- Iteration 66564 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.33 min) Epoch 70/300 -- Iteration 66573 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.34 min) Epoch 70/300 -- Iteration 66582 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.34 min) Epoch 70/300 -- Iteration 66591 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.34 min) Epoch 70/300 -- Iteration 66600 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.35 min) Epoch 70/300 -- Iteration 66609 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.35 min) Epoch 70/300 -- Iteration 66618 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.36 min) Epoch 70/300 -- Iteration 66627 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.36 min) Epoch 70/300 -- Iteration 66636 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.37 min) Epoch 70/300 -- Iteration 66645 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.37 min) Epoch 70/300 -- Iteration 66654 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.38 min) Epoch 70/300 -- Iteration 66663 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.38 min) Epoch 70/300 -- Iteration 66672 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.38 min) Epoch 70/300 -- Iteration 66681 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.39 min) Epoch 70/300 -- Iteration 66690 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.39 min) Epoch 70/300 -- Iteration 66699 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.40 min) Epoch 70/300 -- Iteration 66708 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.40 min) Epoch 70/300 -- Iteration 66717 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.41 min) Epoch 70/300 -- Iteration 66726 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.41 min) Epoch 70/300 -- Iteration 66735 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.42 min) Epoch 70/300 -- Iteration 66744 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.42 min) Epoch 70/300 -- Iteration 66753 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.42 min) Epoch 70/300 -- Iteration 66762 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.43 min) Epoch 70/300 -- Iteration 66771 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.43 min) Epoch 70/300 -- Iteration 66780 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.44 min) Epoch 70/300 -- Iteration 66789 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.44 min) Epoch 70/300 -- Iteration 66798 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.45 min) Epoch 70/300 -- Iteration 66807 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.45 min) Epoch 70/300 -- Iteration 66816 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.46 min) Epoch 70/300 -- Iteration 66825 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.46 min) Epoch 70/300 -- Iteration 66834 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.46 min) Epoch 70/300 -- Iteration 66843 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.47 min) Epoch 70/300 -- Iteration 66852 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.47 min) Epoch 70/300 -- Iteration 66861 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.48 min) Epoch 70/300 -- Iteration 66870 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.48 min) Epoch 70/300 -- Iteration 66879 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.49 min) Epoch 70/300 -- Iteration 66888 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.49 min) Epoch 70/300 -- Iteration 66897 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.49 min) Epoch 70/300 -- Iteration 66906 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.50 min) Epoch 70/300 -- Iteration 66915 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.50 min) Epoch 70/300 -- Iteration 66924 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.51 min) Epoch 70/300 -- Iteration 66933 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.51 min) Epoch 70/300 -- Iteration 66942 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.52 min) Epoch 70/300 -- Iteration 66951 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.52 min) Epoch 70/300 -- Iteration 66960 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.53 min) Epoch 70/300 -- Iteration 66969 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.53 min) Epoch 70/300 -- Iteration 66978 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.53 min) Epoch 70/300 -- Iteration 66987 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.54 min) Epoch 70/300 -- Iteration 66996 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.54 min) Epoch 70/300 -- Iteration 67005 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.55 min) Epoch 70/300 -- Iteration 67014 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.55 min) Epoch 70/300 -- Iteration 67023 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.56 min) Epoch 70/300 -- Iteration 67032 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.56 min) Epoch 70/300 -- Iteration 67041 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.57 min) Epoch 70/300 -- Iteration 67050 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.57 min) Epoch 70/300 -- Iteration 67059 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.57 min) Epoch 70/300 -- Iteration 67068 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.58 min) Epoch 70/300 -- Iteration 67077 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.58 min) Epoch 70/300 -- Iteration 67086 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.59 min) Epoch 70/300 -- Iteration 67095 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.59 min) Epoch 70/300 -- Iteration 67104 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.60 min) Epoch 70/300 -- Iteration 67113 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.60 min) Epoch 70/300 -- Iteration 67122 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.61 min) Epoch 70/300 -- Iteration 67131 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.61 min) Epoch 70/300 -- Iteration 67140 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.61 min) Epoch 70/300 -- Iteration 67149 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.62 min) Epoch 70/300 -- Iteration 67158 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.62 min) Epoch 70/300 -- Iteration 67167 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.63 min) Epoch 70/300 -- Iteration 67176 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.63 min) Epoch 70/300 -- Iteration 67185 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.64 min) Epoch 70/300 -- Iteration 67194 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.64 min) Epoch 70/300 -- Iteration 67203 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.65 min) Epoch 70/300 -- Iteration 67212 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.65 min) Epoch 70/300 -- Iteration 67221 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.65 min) Epoch 70/300 -- Iteration 67230 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.66 min) Epoch 70/300 -- Iteration 67239 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.66 min) Epoch 70/300 -- Iteration 67248 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.67 min) Epoch 70/300 -- Iteration 67257 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.67 min) Epoch 70/300 -- Iteration 67266 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.68 min) Epoch 70/300 -- Iteration 67275 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.68 min) Epoch 70/300 -- Iteration 67284 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.69 min) Epoch 70/300 -- Iteration 67293 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.69 min) Epoch 70/300 -- Iteration 67302 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.69 min) Epoch 70/300 -- Iteration 67311 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.70 min) Epoch 70/300 -- Iteration 67320 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.70 min) Epoch 70/300 -- Iteration 67329 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.71 min) Epoch 70/300 -- Iteration 67338 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.71 min) Epoch 70/300 -- Iteration 67347 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.72 min) Epoch 70/300 -- Iteration 67356 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.72 min) Epoch 70/300 -- Iteration 67365 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.73 min) Epoch 70/300 -- Iteration 67374 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.73 min) Epoch 70/300 -- Iteration 67383 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.73 min) Epoch 70/300 -- Iteration 67392 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.74 min) Epoch 70/300 -- Iteration 67401 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.74 min) Epoch 70/300 -- Iteration 67410 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(33.75 min) Epoch 71/300 -- Iteration 67419 - Batch 9/963 - Train loss: 0.0026  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.75 min) Epoch 71/300 -- Iteration 67428 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.76 min) Epoch 71/300 -- Iteration 67437 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.76 min) Epoch 71/300 -- Iteration 67446 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.77 min) Epoch 71/300 -- Iteration 67455 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.77 min) Epoch 71/300 -- Iteration 67464 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.78 min) Epoch 71/300 -- Iteration 67473 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.78 min) Epoch 71/300 -- Iteration 67482 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.78 min) Epoch 71/300 -- Iteration 67491 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.79 min) Epoch 71/300 -- Iteration 67500 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.79 min) Epoch 71/300 -- Iteration 67509 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.80 min) Epoch 71/300 -- Iteration 67518 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.80 min) Epoch 71/300 -- Iteration 67527 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.81 min) Epoch 71/300 -- Iteration 67536 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.81 min) Epoch 71/300 -- Iteration 67545 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.82 min) Epoch 71/300 -- Iteration 67554 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.82 min) Epoch 71/300 -- Iteration 67563 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.82 min) Epoch 71/300 -- Iteration 67572 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.83 min) Epoch 71/300 -- Iteration 67581 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.83 min) Epoch 71/300 -- Iteration 67590 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.84 min) Epoch 71/300 -- Iteration 67599 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.84 min) Epoch 71/300 -- Iteration 67608 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.85 min) Epoch 71/300 -- Iteration 67617 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.85 min) Epoch 71/300 -- Iteration 67626 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.86 min) Epoch 71/300 -- Iteration 67635 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.86 min) Epoch 71/300 -- Iteration 67644 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.86 min) Epoch 71/300 -- Iteration 67653 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.87 min) Epoch 71/300 -- Iteration 67662 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.87 min) Epoch 71/300 -- Iteration 67671 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.88 min) Epoch 71/300 -- Iteration 67680 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.88 min) Epoch 71/300 -- Iteration 67689 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.89 min) Epoch 71/300 -- Iteration 67698 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.89 min) Epoch 71/300 -- Iteration 67707 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.89 min) Epoch 71/300 -- Iteration 67716 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.90 min) Epoch 71/300 -- Iteration 67725 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.90 min) Epoch 71/300 -- Iteration 67734 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.91 min) Epoch 71/300 -- Iteration 67743 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.91 min) Epoch 71/300 -- Iteration 67752 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.92 min) Epoch 71/300 -- Iteration 67761 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.92 min) Epoch 71/300 -- Iteration 67770 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.93 min) Epoch 71/300 -- Iteration 67779 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.93 min) Epoch 71/300 -- Iteration 67788 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.93 min) Epoch 71/300 -- Iteration 67797 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.94 min) Epoch 71/300 -- Iteration 67806 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.94 min) Epoch 71/300 -- Iteration 67815 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.95 min) Epoch 71/300 -- Iteration 67824 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.95 min) Epoch 71/300 -- Iteration 67833 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.96 min) Epoch 71/300 -- Iteration 67842 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.96 min) Epoch 71/300 -- Iteration 67851 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.97 min) Epoch 71/300 -- Iteration 67860 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.97 min) Epoch 71/300 -- Iteration 67869 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.97 min) Epoch 71/300 -- Iteration 67878 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.98 min) Epoch 71/300 -- Iteration 67887 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.98 min) Epoch 71/300 -- Iteration 67896 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.99 min) Epoch 71/300 -- Iteration 67905 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(33.99 min) Epoch 71/300 -- Iteration 67914 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.00 min) Epoch 71/300 -- Iteration 67923 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.00 min) Epoch 71/300 -- Iteration 67932 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.01 min) Epoch 71/300 -- Iteration 67941 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.01 min) Epoch 71/300 -- Iteration 67950 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.01 min) Epoch 71/300 -- Iteration 67959 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.02 min) Epoch 71/300 -- Iteration 67968 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.02 min) Epoch 71/300 -- Iteration 67977 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.03 min) Epoch 71/300 -- Iteration 67986 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.03 min) Epoch 71/300 -- Iteration 67995 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.04 min) Epoch 71/300 -- Iteration 68004 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.04 min) Epoch 71/300 -- Iteration 68013 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.05 min) Epoch 71/300 -- Iteration 68022 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.05 min) Epoch 71/300 -- Iteration 68031 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.05 min) Epoch 71/300 -- Iteration 68040 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.06 min) Epoch 71/300 -- Iteration 68049 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.06 min) Epoch 71/300 -- Iteration 68058 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.07 min) Epoch 71/300 -- Iteration 68067 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.07 min) Epoch 71/300 -- Iteration 68076 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.08 min) Epoch 71/300 -- Iteration 68085 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.08 min) Epoch 71/300 -- Iteration 68094 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.09 min) Epoch 71/300 -- Iteration 68103 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.09 min) Epoch 71/300 -- Iteration 68112 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.09 min) Epoch 71/300 -- Iteration 68121 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.10 min) Epoch 71/300 -- Iteration 68130 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.10 min) Epoch 71/300 -- Iteration 68139 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.11 min) Epoch 71/300 -- Iteration 68148 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.11 min) Epoch 71/300 -- Iteration 68157 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.12 min) Epoch 71/300 -- Iteration 68166 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.12 min) Epoch 71/300 -- Iteration 68175 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.13 min) Epoch 71/300 -- Iteration 68184 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.13 min) Epoch 71/300 -- Iteration 68193 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.13 min) Epoch 71/300 -- Iteration 68202 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.14 min) Epoch 71/300 -- Iteration 68211 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.14 min) Epoch 71/300 -- Iteration 68220 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.15 min) Epoch 71/300 -- Iteration 68229 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.15 min) Epoch 71/300 -- Iteration 68238 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.16 min) Epoch 71/300 -- Iteration 68247 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.16 min) Epoch 71/300 -- Iteration 68256 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.17 min) Epoch 71/300 -- Iteration 68265 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.17 min) Epoch 71/300 -- Iteration 68274 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.17 min) Epoch 71/300 -- Iteration 68283 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.18 min) Epoch 71/300 -- Iteration 68292 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.18 min) Epoch 71/300 -- Iteration 68301 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.19 min) Epoch 71/300 -- Iteration 68310 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.19 min) Epoch 71/300 -- Iteration 68319 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.20 min) Epoch 71/300 -- Iteration 68328 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.20 min) Epoch 71/300 -- Iteration 68337 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.21 min) Epoch 71/300 -- Iteration 68346 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.21 min) Epoch 71/300 -- Iteration 68355 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.21 min) Epoch 71/300 -- Iteration 68364 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.22 min) Epoch 71/300 -- Iteration 68373 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(34.22 min) Epoch 72/300 -- Iteration 68382 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.23 min) Epoch 72/300 -- Iteration 68391 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.23 min) Epoch 72/300 -- Iteration 68400 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.24 min) Epoch 72/300 -- Iteration 68409 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.24 min) Epoch 72/300 -- Iteration 68418 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.25 min) Epoch 72/300 -- Iteration 68427 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.25 min) Epoch 72/300 -- Iteration 68436 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.26 min) Epoch 72/300 -- Iteration 68445 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.26 min) Epoch 72/300 -- Iteration 68454 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.26 min) Epoch 72/300 -- Iteration 68463 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.27 min) Epoch 72/300 -- Iteration 68472 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.27 min) Epoch 72/300 -- Iteration 68481 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.28 min) Epoch 72/300 -- Iteration 68490 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.28 min) Epoch 72/300 -- Iteration 68499 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.29 min) Epoch 72/300 -- Iteration 68508 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.29 min) Epoch 72/300 -- Iteration 68517 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.30 min) Epoch 72/300 -- Iteration 68526 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.30 min) Epoch 72/300 -- Iteration 68535 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.30 min) Epoch 72/300 -- Iteration 68544 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.31 min) Epoch 72/300 -- Iteration 68553 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.31 min) Epoch 72/300 -- Iteration 68562 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.32 min) Epoch 72/300 -- Iteration 68571 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.32 min) Epoch 72/300 -- Iteration 68580 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.33 min) Epoch 72/300 -- Iteration 68589 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.33 min) Epoch 72/300 -- Iteration 68598 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.33 min) Epoch 72/300 -- Iteration 68607 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.34 min) Epoch 72/300 -- Iteration 68616 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.34 min) Epoch 72/300 -- Iteration 68625 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.35 min) Epoch 72/300 -- Iteration 68634 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.35 min) Epoch 72/300 -- Iteration 68643 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.36 min) Epoch 72/300 -- Iteration 68652 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.36 min) Epoch 72/300 -- Iteration 68661 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.37 min) Epoch 72/300 -- Iteration 68670 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.37 min) Epoch 72/300 -- Iteration 68679 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.37 min) Epoch 72/300 -- Iteration 68688 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.38 min) Epoch 72/300 -- Iteration 68697 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.38 min) Epoch 72/300 -- Iteration 68706 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.39 min) Epoch 72/300 -- Iteration 68715 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.39 min) Epoch 72/300 -- Iteration 68724 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.40 min) Epoch 72/300 -- Iteration 68733 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.40 min) Epoch 72/300 -- Iteration 68742 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.41 min) Epoch 72/300 -- Iteration 68751 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.41 min) Epoch 72/300 -- Iteration 68760 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.42 min) Epoch 72/300 -- Iteration 68769 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.42 min) Epoch 72/300 -- Iteration 68778 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.42 min) Epoch 72/300 -- Iteration 68787 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.43 min) Epoch 72/300 -- Iteration 68796 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.43 min) Epoch 72/300 -- Iteration 68805 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.44 min) Epoch 72/300 -- Iteration 68814 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.44 min) Epoch 72/300 -- Iteration 68823 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.45 min) Epoch 72/300 -- Iteration 68832 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.45 min) Epoch 72/300 -- Iteration 68841 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.46 min) Epoch 72/300 -- Iteration 68850 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.46 min) Epoch 72/300 -- Iteration 68859 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.46 min) Epoch 72/300 -- Iteration 68868 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.47 min) Epoch 72/300 -- Iteration 68877 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.47 min) Epoch 72/300 -- Iteration 68886 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.48 min) Epoch 72/300 -- Iteration 68895 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.48 min) Epoch 72/300 -- Iteration 68904 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.49 min) Epoch 72/300 -- Iteration 68913 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.49 min) Epoch 72/300 -- Iteration 68922 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.49 min) Epoch 72/300 -- Iteration 68931 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.50 min) Epoch 72/300 -- Iteration 68940 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.50 min) Epoch 72/300 -- Iteration 68949 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.51 min) Epoch 72/300 -- Iteration 68958 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.51 min) Epoch 72/300 -- Iteration 68967 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.52 min) Epoch 72/300 -- Iteration 68976 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.52 min) Epoch 72/300 -- Iteration 68985 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.53 min) Epoch 72/300 -- Iteration 68994 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.53 min) Epoch 72/300 -- Iteration 69003 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.54 min) Epoch 72/300 -- Iteration 69012 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.54 min) Epoch 72/300 -- Iteration 69021 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.54 min) Epoch 72/300 -- Iteration 69030 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.55 min) Epoch 72/300 -- Iteration 69039 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.55 min) Epoch 72/300 -- Iteration 69048 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.56 min) Epoch 72/300 -- Iteration 69057 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.56 min) Epoch 72/300 -- Iteration 69066 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.57 min) Epoch 72/300 -- Iteration 69075 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.57 min) Epoch 72/300 -- Iteration 69084 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.57 min) Epoch 72/300 -- Iteration 69093 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.58 min) Epoch 72/300 -- Iteration 69102 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.58 min) Epoch 72/300 -- Iteration 69111 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.59 min) Epoch 72/300 -- Iteration 69120 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.59 min) Epoch 72/300 -- Iteration 69129 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.60 min) Epoch 72/300 -- Iteration 69138 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.60 min) Epoch 72/300 -- Iteration 69147 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.61 min) Epoch 72/300 -- Iteration 69156 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.61 min) Epoch 72/300 -- Iteration 69165 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.62 min) Epoch 72/300 -- Iteration 69174 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.62 min) Epoch 72/300 -- Iteration 69183 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.62 min) Epoch 72/300 -- Iteration 69192 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.63 min) Epoch 72/300 -- Iteration 69201 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.63 min) Epoch 72/300 -- Iteration 69210 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.64 min) Epoch 72/300 -- Iteration 69219 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.64 min) Epoch 72/300 -- Iteration 69228 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.65 min) Epoch 72/300 -- Iteration 69237 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.65 min) Epoch 72/300 -- Iteration 69246 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.66 min) Epoch 72/300 -- Iteration 69255 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.66 min) Epoch 72/300 -- Iteration 69264 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.66 min) Epoch 72/300 -- Iteration 69273 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.67 min) Epoch 72/300 -- Iteration 69282 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.67 min) Epoch 72/300 -- Iteration 69291 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.68 min) Epoch 72/300 -- Iteration 69300 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.68 min) Epoch 72/300 -- Iteration 69309 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.69 min) Epoch 72/300 -- Iteration 69318 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.69 min) Epoch 72/300 -- Iteration 69327 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.69 min) Epoch 72/300 -- Iteration 69336 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(34.70 min) Epoch 73/300 -- Iteration 69345 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.71 min) Epoch 73/300 -- Iteration 69354 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.71 min) Epoch 73/300 -- Iteration 69363 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.71 min) Epoch 73/300 -- Iteration 69372 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.72 min) Epoch 73/300 -- Iteration 69381 - Batch 45/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.72 min) Epoch 73/300 -- Iteration 69390 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.73 min) Epoch 73/300 -- Iteration 69399 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.73 min) Epoch 73/300 -- Iteration 69408 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.74 min) Epoch 73/300 -- Iteration 69417 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.74 min) Epoch 73/300 -- Iteration 69426 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.74 min) Epoch 73/300 -- Iteration 69435 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.75 min) Epoch 73/300 -- Iteration 69444 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.75 min) Epoch 73/300 -- Iteration 69453 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.76 min) Epoch 73/300 -- Iteration 69462 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.76 min) Epoch 73/300 -- Iteration 69471 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.77 min) Epoch 73/300 -- Iteration 69480 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.77 min) Epoch 73/300 -- Iteration 69489 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.78 min) Epoch 73/300 -- Iteration 69498 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.78 min) Epoch 73/300 -- Iteration 69507 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.79 min) Epoch 73/300 -- Iteration 69516 - Batch 180/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.79 min) Epoch 73/300 -- Iteration 69525 - Batch 189/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.79 min) Epoch 73/300 -- Iteration 69534 - Batch 198/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.80 min) Epoch 73/300 -- Iteration 69543 - Batch 207/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.80 min) Epoch 73/300 -- Iteration 69552 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.81 min) Epoch 73/300 -- Iteration 69561 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.81 min) Epoch 73/300 -- Iteration 69570 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.82 min) Epoch 73/300 -- Iteration 69579 - Batch 243/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.82 min) Epoch 73/300 -- Iteration 69588 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.83 min) Epoch 73/300 -- Iteration 69597 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.83 min) Epoch 73/300 -- Iteration 69606 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.83 min) Epoch 73/300 -- Iteration 69615 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.84 min) Epoch 73/300 -- Iteration 69624 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.84 min) Epoch 73/300 -- Iteration 69633 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.85 min) Epoch 73/300 -- Iteration 69642 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.85 min) Epoch 73/300 -- Iteration 69651 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.86 min) Epoch 73/300 -- Iteration 69660 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.86 min) Epoch 73/300 -- Iteration 69669 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.86 min) Epoch 73/300 -- Iteration 69678 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.87 min) Epoch 73/300 -- Iteration 69687 - Batch 351/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.87 min) Epoch 73/300 -- Iteration 69696 - Batch 360/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.88 min) Epoch 73/300 -- Iteration 69705 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.88 min) Epoch 73/300 -- Iteration 69714 - Batch 378/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.89 min) Epoch 73/300 -- Iteration 69723 - Batch 387/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.89 min) Epoch 73/300 -- Iteration 69732 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.90 min) Epoch 73/300 -- Iteration 69741 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.90 min) Epoch 73/300 -- Iteration 69750 - Batch 414/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.91 min) Epoch 73/300 -- Iteration 69759 - Batch 423/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.91 min) Epoch 73/300 -- Iteration 69768 - Batch 432/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.91 min) Epoch 73/300 -- Iteration 69777 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.92 min) Epoch 73/300 -- Iteration 69786 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.92 min) Epoch 73/300 -- Iteration 69795 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.93 min) Epoch 73/300 -- Iteration 69804 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.93 min) Epoch 73/300 -- Iteration 69813 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.94 min) Epoch 73/300 -- Iteration 69822 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.94 min) Epoch 73/300 -- Iteration 69831 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.95 min) Epoch 73/300 -- Iteration 69840 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.95 min) Epoch 73/300 -- Iteration 69849 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.95 min) Epoch 73/300 -- Iteration 69858 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.96 min) Epoch 73/300 -- Iteration 69867 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.96 min) Epoch 73/300 -- Iteration 69876 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.97 min) Epoch 73/300 -- Iteration 69885 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.97 min) Epoch 73/300 -- Iteration 69894 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.98 min) Epoch 73/300 -- Iteration 69903 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.98 min) Epoch 73/300 -- Iteration 69912 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.99 min) Epoch 73/300 -- Iteration 69921 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.99 min) Epoch 73/300 -- Iteration 69930 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(34.99 min) Epoch 73/300 -- Iteration 69939 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.00 min) Epoch 73/300 -- Iteration 69948 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.00 min) Epoch 73/300 -- Iteration 69957 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.01 min) Epoch 73/300 -- Iteration 69966 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.01 min) Epoch 73/300 -- Iteration 69975 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.02 min) Epoch 73/300 -- Iteration 69984 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.02 min) Epoch 73/300 -- Iteration 69993 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.03 min) Epoch 73/300 -- Iteration 70002 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.03 min) Epoch 73/300 -- Iteration 70011 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.03 min) Epoch 73/300 -- Iteration 70020 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.04 min) Epoch 73/300 -- Iteration 70029 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.04 min) Epoch 73/300 -- Iteration 70038 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.05 min) Epoch 73/300 -- Iteration 70047 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.05 min) Epoch 73/300 -- Iteration 70056 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.06 min) Epoch 73/300 -- Iteration 70065 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.06 min) Epoch 73/300 -- Iteration 70074 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.07 min) Epoch 73/300 -- Iteration 70083 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.07 min) Epoch 73/300 -- Iteration 70092 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.07 min) Epoch 73/300 -- Iteration 70101 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.08 min) Epoch 73/300 -- Iteration 70110 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.08 min) Epoch 73/300 -- Iteration 70119 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.09 min) Epoch 73/300 -- Iteration 70128 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.09 min) Epoch 73/300 -- Iteration 70137 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.10 min) Epoch 73/300 -- Iteration 70146 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.10 min) Epoch 73/300 -- Iteration 70155 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.11 min) Epoch 73/300 -- Iteration 70164 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.11 min) Epoch 73/300 -- Iteration 70173 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.11 min) Epoch 73/300 -- Iteration 70182 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.12 min) Epoch 73/300 -- Iteration 70191 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.12 min) Epoch 73/300 -- Iteration 70200 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.13 min) Epoch 73/300 -- Iteration 70209 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.13 min) Epoch 73/300 -- Iteration 70218 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.14 min) Epoch 73/300 -- Iteration 70227 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.14 min) Epoch 73/300 -- Iteration 70236 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.15 min) Epoch 73/300 -- Iteration 70245 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.15 min) Epoch 73/300 -- Iteration 70254 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.15 min) Epoch 73/300 -- Iteration 70263 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.16 min) Epoch 73/300 -- Iteration 70272 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.16 min) Epoch 73/300 -- Iteration 70281 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.17 min) Epoch 73/300 -- Iteration 70290 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(35.17 min) Epoch 73/300 -- Iteration 70299 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018 - Val acc: -0.0000\n",
      "(35.18 min) Epoch 74/300 -- Iteration 70308 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.18 min) Epoch 74/300 -- Iteration 70317 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.19 min) Epoch 74/300 -- Iteration 70326 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.19 min) Epoch 74/300 -- Iteration 70335 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.20 min) Epoch 74/300 -- Iteration 70344 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.20 min) Epoch 74/300 -- Iteration 70353 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.20 min) Epoch 74/300 -- Iteration 70362 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.21 min) Epoch 74/300 -- Iteration 70371 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.21 min) Epoch 74/300 -- Iteration 70380 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.22 min) Epoch 74/300 -- Iteration 70389 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.22 min) Epoch 74/300 -- Iteration 70398 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.23 min) Epoch 74/300 -- Iteration 70407 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.23 min) Epoch 74/300 -- Iteration 70416 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.24 min) Epoch 74/300 -- Iteration 70425 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.24 min) Epoch 74/300 -- Iteration 70434 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.24 min) Epoch 74/300 -- Iteration 70443 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.25 min) Epoch 74/300 -- Iteration 70452 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.25 min) Epoch 74/300 -- Iteration 70461 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.26 min) Epoch 74/300 -- Iteration 70470 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.26 min) Epoch 74/300 -- Iteration 70479 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.27 min) Epoch 74/300 -- Iteration 70488 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.27 min) Epoch 74/300 -- Iteration 70497 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.28 min) Epoch 74/300 -- Iteration 70506 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.28 min) Epoch 74/300 -- Iteration 70515 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.28 min) Epoch 74/300 -- Iteration 70524 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.29 min) Epoch 74/300 -- Iteration 70533 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.29 min) Epoch 74/300 -- Iteration 70542 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.30 min) Epoch 74/300 -- Iteration 70551 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.30 min) Epoch 74/300 -- Iteration 70560 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.31 min) Epoch 74/300 -- Iteration 70569 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.31 min) Epoch 74/300 -- Iteration 70578 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.32 min) Epoch 74/300 -- Iteration 70587 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.32 min) Epoch 74/300 -- Iteration 70596 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.32 min) Epoch 74/300 -- Iteration 70605 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.33 min) Epoch 74/300 -- Iteration 70614 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.33 min) Epoch 74/300 -- Iteration 70623 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.34 min) Epoch 74/300 -- Iteration 70632 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.34 min) Epoch 74/300 -- Iteration 70641 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.35 min) Epoch 74/300 -- Iteration 70650 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.35 min) Epoch 74/300 -- Iteration 70659 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.36 min) Epoch 74/300 -- Iteration 70668 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.36 min) Epoch 74/300 -- Iteration 70677 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.36 min) Epoch 74/300 -- Iteration 70686 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.37 min) Epoch 74/300 -- Iteration 70695 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.37 min) Epoch 74/300 -- Iteration 70704 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.38 min) Epoch 74/300 -- Iteration 70713 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.38 min) Epoch 74/300 -- Iteration 70722 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.39 min) Epoch 74/300 -- Iteration 70731 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.39 min) Epoch 74/300 -- Iteration 70740 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.40 min) Epoch 74/300 -- Iteration 70749 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.40 min) Epoch 74/300 -- Iteration 70758 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.40 min) Epoch 74/300 -- Iteration 70767 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.41 min) Epoch 74/300 -- Iteration 70776 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.41 min) Epoch 74/300 -- Iteration 70785 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.42 min) Epoch 74/300 -- Iteration 70794 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.42 min) Epoch 74/300 -- Iteration 70803 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.43 min) Epoch 74/300 -- Iteration 70812 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.43 min) Epoch 74/300 -- Iteration 70821 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.43 min) Epoch 74/300 -- Iteration 70830 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.44 min) Epoch 74/300 -- Iteration 70839 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.44 min) Epoch 74/300 -- Iteration 70848 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.45 min) Epoch 74/300 -- Iteration 70857 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.45 min) Epoch 74/300 -- Iteration 70866 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.46 min) Epoch 74/300 -- Iteration 70875 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.46 min) Epoch 74/300 -- Iteration 70884 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.47 min) Epoch 74/300 -- Iteration 70893 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.47 min) Epoch 74/300 -- Iteration 70902 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.47 min) Epoch 74/300 -- Iteration 70911 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.48 min) Epoch 74/300 -- Iteration 70920 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.48 min) Epoch 74/300 -- Iteration 70929 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.49 min) Epoch 74/300 -- Iteration 70938 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.49 min) Epoch 74/300 -- Iteration 70947 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.50 min) Epoch 74/300 -- Iteration 70956 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.50 min) Epoch 74/300 -- Iteration 70965 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.51 min) Epoch 74/300 -- Iteration 70974 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.51 min) Epoch 74/300 -- Iteration 70983 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.51 min) Epoch 74/300 -- Iteration 70992 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.52 min) Epoch 74/300 -- Iteration 71001 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.52 min) Epoch 74/300 -- Iteration 71010 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.53 min) Epoch 74/300 -- Iteration 71019 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.53 min) Epoch 74/300 -- Iteration 71028 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.54 min) Epoch 74/300 -- Iteration 71037 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.54 min) Epoch 74/300 -- Iteration 71046 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.55 min) Epoch 74/300 -- Iteration 71055 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.55 min) Epoch 74/300 -- Iteration 71064 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.55 min) Epoch 74/300 -- Iteration 71073 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.56 min) Epoch 74/300 -- Iteration 71082 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.56 min) Epoch 74/300 -- Iteration 71091 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.57 min) Epoch 74/300 -- Iteration 71100 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.57 min) Epoch 74/300 -- Iteration 71109 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.58 min) Epoch 74/300 -- Iteration 71118 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.58 min) Epoch 74/300 -- Iteration 71127 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.59 min) Epoch 74/300 -- Iteration 71136 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.59 min) Epoch 74/300 -- Iteration 71145 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.59 min) Epoch 74/300 -- Iteration 71154 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.60 min) Epoch 74/300 -- Iteration 71163 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.60 min) Epoch 74/300 -- Iteration 71172 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.61 min) Epoch 74/300 -- Iteration 71181 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.61 min) Epoch 74/300 -- Iteration 71190 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.62 min) Epoch 74/300 -- Iteration 71199 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.62 min) Epoch 74/300 -- Iteration 71208 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.63 min) Epoch 74/300 -- Iteration 71217 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.63 min) Epoch 74/300 -- Iteration 71226 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.63 min) Epoch 74/300 -- Iteration 71235 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.64 min) Epoch 74/300 -- Iteration 71244 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.64 min) Epoch 74/300 -- Iteration 71253 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.65 min) Epoch 74/300 -- Iteration 71262 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018 - Val acc: -0.0000\n",
      "(35.65 min) Epoch 75/300 -- Iteration 71271 - Batch 9/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.66 min) Epoch 75/300 -- Iteration 71280 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.66 min) Epoch 75/300 -- Iteration 71289 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.67 min) Epoch 75/300 -- Iteration 71298 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.67 min) Epoch 75/300 -- Iteration 71307 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.68 min) Epoch 75/300 -- Iteration 71316 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.68 min) Epoch 75/300 -- Iteration 71325 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.68 min) Epoch 75/300 -- Iteration 71334 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.69 min) Epoch 75/300 -- Iteration 71343 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.69 min) Epoch 75/300 -- Iteration 71352 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.70 min) Epoch 75/300 -- Iteration 71361 - Batch 99/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.70 min) Epoch 75/300 -- Iteration 71370 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.71 min) Epoch 75/300 -- Iteration 71379 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.71 min) Epoch 75/300 -- Iteration 71388 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.72 min) Epoch 75/300 -- Iteration 71397 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.72 min) Epoch 75/300 -- Iteration 71406 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.72 min) Epoch 75/300 -- Iteration 71415 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.73 min) Epoch 75/300 -- Iteration 71424 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.73 min) Epoch 75/300 -- Iteration 71433 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.74 min) Epoch 75/300 -- Iteration 71442 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.74 min) Epoch 75/300 -- Iteration 71451 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.75 min) Epoch 75/300 -- Iteration 71460 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.75 min) Epoch 75/300 -- Iteration 71469 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.76 min) Epoch 75/300 -- Iteration 71478 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.76 min) Epoch 75/300 -- Iteration 71487 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.76 min) Epoch 75/300 -- Iteration 71496 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.77 min) Epoch 75/300 -- Iteration 71505 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.77 min) Epoch 75/300 -- Iteration 71514 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.78 min) Epoch 75/300 -- Iteration 71523 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.78 min) Epoch 75/300 -- Iteration 71532 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.79 min) Epoch 75/300 -- Iteration 71541 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.79 min) Epoch 75/300 -- Iteration 71550 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.80 min) Epoch 75/300 -- Iteration 71559 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.80 min) Epoch 75/300 -- Iteration 71568 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.80 min) Epoch 75/300 -- Iteration 71577 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.81 min) Epoch 75/300 -- Iteration 71586 - Batch 324/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.81 min) Epoch 75/300 -- Iteration 71595 - Batch 333/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.82 min) Epoch 75/300 -- Iteration 71604 - Batch 342/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.82 min) Epoch 75/300 -- Iteration 71613 - Batch 351/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.83 min) Epoch 75/300 -- Iteration 71622 - Batch 360/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.83 min) Epoch 75/300 -- Iteration 71631 - Batch 369/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.84 min) Epoch 75/300 -- Iteration 71640 - Batch 378/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.84 min) Epoch 75/300 -- Iteration 71649 - Batch 387/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.84 min) Epoch 75/300 -- Iteration 71658 - Batch 396/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.85 min) Epoch 75/300 -- Iteration 71667 - Batch 405/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.85 min) Epoch 75/300 -- Iteration 71676 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.86 min) Epoch 75/300 -- Iteration 71685 - Batch 423/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.86 min) Epoch 75/300 -- Iteration 71694 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.87 min) Epoch 75/300 -- Iteration 71703 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.87 min) Epoch 75/300 -- Iteration 71712 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.88 min) Epoch 75/300 -- Iteration 71721 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.88 min) Epoch 75/300 -- Iteration 71730 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.88 min) Epoch 75/300 -- Iteration 71739 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.89 min) Epoch 75/300 -- Iteration 71748 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.89 min) Epoch 75/300 -- Iteration 71757 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.90 min) Epoch 75/300 -- Iteration 71766 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.90 min) Epoch 75/300 -- Iteration 71775 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.91 min) Epoch 75/300 -- Iteration 71784 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.91 min) Epoch 75/300 -- Iteration 71793 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.92 min) Epoch 75/300 -- Iteration 71802 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.92 min) Epoch 75/300 -- Iteration 71811 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.92 min) Epoch 75/300 -- Iteration 71820 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.93 min) Epoch 75/300 -- Iteration 71829 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.93 min) Epoch 75/300 -- Iteration 71838 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.94 min) Epoch 75/300 -- Iteration 71847 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.94 min) Epoch 75/300 -- Iteration 71856 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.95 min) Epoch 75/300 -- Iteration 71865 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.95 min) Epoch 75/300 -- Iteration 71874 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.96 min) Epoch 75/300 -- Iteration 71883 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.96 min) Epoch 75/300 -- Iteration 71892 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.96 min) Epoch 75/300 -- Iteration 71901 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.97 min) Epoch 75/300 -- Iteration 71910 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.97 min) Epoch 75/300 -- Iteration 71919 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.98 min) Epoch 75/300 -- Iteration 71928 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.98 min) Epoch 75/300 -- Iteration 71937 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.99 min) Epoch 75/300 -- Iteration 71946 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(35.99 min) Epoch 75/300 -- Iteration 71955 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.00 min) Epoch 75/300 -- Iteration 71964 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.00 min) Epoch 75/300 -- Iteration 71973 - Batch 711/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.00 min) Epoch 75/300 -- Iteration 71982 - Batch 720/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.01 min) Epoch 75/300 -- Iteration 71991 - Batch 729/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.01 min) Epoch 75/300 -- Iteration 72000 - Batch 738/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.02 min) Epoch 75/300 -- Iteration 72009 - Batch 747/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.02 min) Epoch 75/300 -- Iteration 72018 - Batch 756/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.03 min) Epoch 75/300 -- Iteration 72027 - Batch 765/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.03 min) Epoch 75/300 -- Iteration 72036 - Batch 774/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.04 min) Epoch 75/300 -- Iteration 72045 - Batch 783/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.04 min) Epoch 75/300 -- Iteration 72054 - Batch 792/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.04 min) Epoch 75/300 -- Iteration 72063 - Batch 801/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.05 min) Epoch 75/300 -- Iteration 72072 - Batch 810/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.05 min) Epoch 75/300 -- Iteration 72081 - Batch 819/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.06 min) Epoch 75/300 -- Iteration 72090 - Batch 828/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.06 min) Epoch 75/300 -- Iteration 72099 - Batch 837/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.07 min) Epoch 75/300 -- Iteration 72108 - Batch 846/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.07 min) Epoch 75/300 -- Iteration 72117 - Batch 855/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.08 min) Epoch 75/300 -- Iteration 72126 - Batch 864/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.08 min) Epoch 75/300 -- Iteration 72135 - Batch 873/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.08 min) Epoch 75/300 -- Iteration 72144 - Batch 882/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.09 min) Epoch 75/300 -- Iteration 72153 - Batch 891/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.09 min) Epoch 75/300 -- Iteration 72162 - Batch 900/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.10 min) Epoch 75/300 -- Iteration 72171 - Batch 909/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.10 min) Epoch 75/300 -- Iteration 72180 - Batch 918/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.11 min) Epoch 75/300 -- Iteration 72189 - Batch 927/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.11 min) Epoch 75/300 -- Iteration 72198 - Batch 936/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.12 min) Epoch 75/300 -- Iteration 72207 - Batch 945/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.12 min) Epoch 75/300 -- Iteration 72216 - Batch 954/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0018\n",
      "(36.12 min) Epoch 75/300 -- Iteration 72225 - Batch 962/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(36.13 min) Epoch 76/300 -- Iteration 72234 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.13 min) Epoch 76/300 -- Iteration 72243 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.14 min) Epoch 76/300 -- Iteration 72252 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.14 min) Epoch 76/300 -- Iteration 72261 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.15 min) Epoch 76/300 -- Iteration 72270 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.15 min) Epoch 76/300 -- Iteration 72279 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.16 min) Epoch 76/300 -- Iteration 72288 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.16 min) Epoch 76/300 -- Iteration 72297 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.17 min) Epoch 76/300 -- Iteration 72306 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.17 min) Epoch 76/300 -- Iteration 72315 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.17 min) Epoch 76/300 -- Iteration 72324 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.18 min) Epoch 76/300 -- Iteration 72333 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.18 min) Epoch 76/300 -- Iteration 72342 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.19 min) Epoch 76/300 -- Iteration 72351 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.19 min) Epoch 76/300 -- Iteration 72360 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.20 min) Epoch 76/300 -- Iteration 72369 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.20 min) Epoch 76/300 -- Iteration 72378 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.21 min) Epoch 76/300 -- Iteration 72387 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.21 min) Epoch 76/300 -- Iteration 72396 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.21 min) Epoch 76/300 -- Iteration 72405 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.22 min) Epoch 76/300 -- Iteration 72414 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.22 min) Epoch 76/300 -- Iteration 72423 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.23 min) Epoch 76/300 -- Iteration 72432 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.23 min) Epoch 76/300 -- Iteration 72441 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.24 min) Epoch 76/300 -- Iteration 72450 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.24 min) Epoch 76/300 -- Iteration 72459 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.24 min) Epoch 76/300 -- Iteration 72468 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.25 min) Epoch 76/300 -- Iteration 72477 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.25 min) Epoch 76/300 -- Iteration 72486 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.26 min) Epoch 76/300 -- Iteration 72495 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.26 min) Epoch 76/300 -- Iteration 72504 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.27 min) Epoch 76/300 -- Iteration 72513 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.27 min) Epoch 76/300 -- Iteration 72522 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.28 min) Epoch 76/300 -- Iteration 72531 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.28 min) Epoch 76/300 -- Iteration 72540 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.29 min) Epoch 76/300 -- Iteration 72549 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.29 min) Epoch 76/300 -- Iteration 72558 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.29 min) Epoch 76/300 -- Iteration 72567 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.30 min) Epoch 76/300 -- Iteration 72576 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.30 min) Epoch 76/300 -- Iteration 72585 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.31 min) Epoch 76/300 -- Iteration 72594 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.31 min) Epoch 76/300 -- Iteration 72603 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.32 min) Epoch 76/300 -- Iteration 72612 - Batch 387/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.32 min) Epoch 76/300 -- Iteration 72621 - Batch 396/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.33 min) Epoch 76/300 -- Iteration 72630 - Batch 405/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.33 min) Epoch 76/300 -- Iteration 72639 - Batch 414/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.33 min) Epoch 76/300 -- Iteration 72648 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.34 min) Epoch 76/300 -- Iteration 72657 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.34 min) Epoch 76/300 -- Iteration 72666 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.35 min) Epoch 76/300 -- Iteration 72675 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.35 min) Epoch 76/300 -- Iteration 72684 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.36 min) Epoch 76/300 -- Iteration 72693 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.36 min) Epoch 76/300 -- Iteration 72702 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.36 min) Epoch 76/300 -- Iteration 72711 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.37 min) Epoch 76/300 -- Iteration 72720 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.37 min) Epoch 76/300 -- Iteration 72729 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.38 min) Epoch 76/300 -- Iteration 72738 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.38 min) Epoch 76/300 -- Iteration 72747 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.39 min) Epoch 76/300 -- Iteration 72756 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.39 min) Epoch 76/300 -- Iteration 72765 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.40 min) Epoch 76/300 -- Iteration 72774 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.40 min) Epoch 76/300 -- Iteration 72783 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.40 min) Epoch 76/300 -- Iteration 72792 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.41 min) Epoch 76/300 -- Iteration 72801 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.41 min) Epoch 76/300 -- Iteration 72810 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.42 min) Epoch 76/300 -- Iteration 72819 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.42 min) Epoch 76/300 -- Iteration 72828 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.43 min) Epoch 76/300 -- Iteration 72837 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.43 min) Epoch 76/300 -- Iteration 72846 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.44 min) Epoch 76/300 -- Iteration 72855 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.44 min) Epoch 76/300 -- Iteration 72864 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.44 min) Epoch 76/300 -- Iteration 72873 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.45 min) Epoch 76/300 -- Iteration 72882 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.45 min) Epoch 76/300 -- Iteration 72891 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.46 min) Epoch 76/300 -- Iteration 72900 - Batch 675/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.46 min) Epoch 76/300 -- Iteration 72909 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.47 min) Epoch 76/300 -- Iteration 72918 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.47 min) Epoch 76/300 -- Iteration 72927 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.48 min) Epoch 76/300 -- Iteration 72936 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.48 min) Epoch 76/300 -- Iteration 72945 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.48 min) Epoch 76/300 -- Iteration 72954 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.49 min) Epoch 76/300 -- Iteration 72963 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.49 min) Epoch 76/300 -- Iteration 72972 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.50 min) Epoch 76/300 -- Iteration 72981 - Batch 756/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.50 min) Epoch 76/300 -- Iteration 72990 - Batch 765/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.51 min) Epoch 76/300 -- Iteration 72999 - Batch 774/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.51 min) Epoch 76/300 -- Iteration 73008 - Batch 783/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.52 min) Epoch 76/300 -- Iteration 73017 - Batch 792/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.52 min) Epoch 76/300 -- Iteration 73026 - Batch 801/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.52 min) Epoch 76/300 -- Iteration 73035 - Batch 810/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.53 min) Epoch 76/300 -- Iteration 73044 - Batch 819/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.53 min) Epoch 76/300 -- Iteration 73053 - Batch 828/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.54 min) Epoch 76/300 -- Iteration 73062 - Batch 837/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.54 min) Epoch 76/300 -- Iteration 73071 - Batch 846/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.55 min) Epoch 76/300 -- Iteration 73080 - Batch 855/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.55 min) Epoch 76/300 -- Iteration 73089 - Batch 864/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.56 min) Epoch 76/300 -- Iteration 73098 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.56 min) Epoch 76/300 -- Iteration 73107 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.56 min) Epoch 76/300 -- Iteration 73116 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.57 min) Epoch 76/300 -- Iteration 73125 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.57 min) Epoch 76/300 -- Iteration 73134 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.58 min) Epoch 76/300 -- Iteration 73143 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.58 min) Epoch 76/300 -- Iteration 73152 - Batch 927/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.59 min) Epoch 76/300 -- Iteration 73161 - Batch 936/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.59 min) Epoch 76/300 -- Iteration 73170 - Batch 945/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.60 min) Epoch 76/300 -- Iteration 73179 - Batch 954/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.60 min) Epoch 76/300 -- Iteration 73188 - Batch 962/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(36.61 min) Epoch 77/300 -- Iteration 73197 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.61 min) Epoch 77/300 -- Iteration 73206 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.61 min) Epoch 77/300 -- Iteration 73215 - Batch 27/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.62 min) Epoch 77/300 -- Iteration 73224 - Batch 36/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.62 min) Epoch 77/300 -- Iteration 73233 - Batch 45/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.63 min) Epoch 77/300 -- Iteration 73242 - Batch 54/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.63 min) Epoch 77/300 -- Iteration 73251 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.64 min) Epoch 77/300 -- Iteration 73260 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.64 min) Epoch 77/300 -- Iteration 73269 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.65 min) Epoch 77/300 -- Iteration 73278 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.65 min) Epoch 77/300 -- Iteration 73287 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.65 min) Epoch 77/300 -- Iteration 73296 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.66 min) Epoch 77/300 -- Iteration 73305 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.66 min) Epoch 77/300 -- Iteration 73314 - Batch 126/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.67 min) Epoch 77/300 -- Iteration 73323 - Batch 135/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.67 min) Epoch 77/300 -- Iteration 73332 - Batch 144/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.68 min) Epoch 77/300 -- Iteration 73341 - Batch 153/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.68 min) Epoch 77/300 -- Iteration 73350 - Batch 162/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.69 min) Epoch 77/300 -- Iteration 73359 - Batch 171/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.69 min) Epoch 77/300 -- Iteration 73368 - Batch 180/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.69 min) Epoch 77/300 -- Iteration 73377 - Batch 189/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.70 min) Epoch 77/300 -- Iteration 73386 - Batch 198/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.70 min) Epoch 77/300 -- Iteration 73395 - Batch 207/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.71 min) Epoch 77/300 -- Iteration 73404 - Batch 216/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.71 min) Epoch 77/300 -- Iteration 73413 - Batch 225/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.72 min) Epoch 77/300 -- Iteration 73422 - Batch 234/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.72 min) Epoch 77/300 -- Iteration 73431 - Batch 243/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.73 min) Epoch 77/300 -- Iteration 73440 - Batch 252/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.73 min) Epoch 77/300 -- Iteration 73449 - Batch 261/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.73 min) Epoch 77/300 -- Iteration 73458 - Batch 270/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.74 min) Epoch 77/300 -- Iteration 73467 - Batch 279/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.74 min) Epoch 77/300 -- Iteration 73476 - Batch 288/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.75 min) Epoch 77/300 -- Iteration 73485 - Batch 297/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.75 min) Epoch 77/300 -- Iteration 73494 - Batch 306/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.76 min) Epoch 77/300 -- Iteration 73503 - Batch 315/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.76 min) Epoch 77/300 -- Iteration 73512 - Batch 324/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.77 min) Epoch 77/300 -- Iteration 73521 - Batch 333/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.77 min) Epoch 77/300 -- Iteration 73530 - Batch 342/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.77 min) Epoch 77/300 -- Iteration 73539 - Batch 351/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.78 min) Epoch 77/300 -- Iteration 73548 - Batch 360/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.78 min) Epoch 77/300 -- Iteration 73557 - Batch 369/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.79 min) Epoch 77/300 -- Iteration 73566 - Batch 378/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.79 min) Epoch 77/300 -- Iteration 73575 - Batch 387/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.80 min) Epoch 77/300 -- Iteration 73584 - Batch 396/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.80 min) Epoch 77/300 -- Iteration 73593 - Batch 405/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.81 min) Epoch 77/300 -- Iteration 73602 - Batch 414/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.81 min) Epoch 77/300 -- Iteration 73611 - Batch 423/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.81 min) Epoch 77/300 -- Iteration 73620 - Batch 432/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.82 min) Epoch 77/300 -- Iteration 73629 - Batch 441/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.82 min) Epoch 77/300 -- Iteration 73638 - Batch 450/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.83 min) Epoch 77/300 -- Iteration 73647 - Batch 459/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.83 min) Epoch 77/300 -- Iteration 73656 - Batch 468/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.84 min) Epoch 77/300 -- Iteration 73665 - Batch 477/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.84 min) Epoch 77/300 -- Iteration 73674 - Batch 486/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.84 min) Epoch 77/300 -- Iteration 73683 - Batch 495/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.85 min) Epoch 77/300 -- Iteration 73692 - Batch 504/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.85 min) Epoch 77/300 -- Iteration 73701 - Batch 513/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.86 min) Epoch 77/300 -- Iteration 73710 - Batch 522/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.86 min) Epoch 77/300 -- Iteration 73719 - Batch 531/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.87 min) Epoch 77/300 -- Iteration 73728 - Batch 540/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.87 min) Epoch 77/300 -- Iteration 73737 - Batch 549/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.88 min) Epoch 77/300 -- Iteration 73746 - Batch 558/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.88 min) Epoch 77/300 -- Iteration 73755 - Batch 567/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.88 min) Epoch 77/300 -- Iteration 73764 - Batch 576/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.89 min) Epoch 77/300 -- Iteration 73773 - Batch 585/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.89 min) Epoch 77/300 -- Iteration 73782 - Batch 594/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.90 min) Epoch 77/300 -- Iteration 73791 - Batch 603/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.90 min) Epoch 77/300 -- Iteration 73800 - Batch 612/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.91 min) Epoch 77/300 -- Iteration 73809 - Batch 621/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.91 min) Epoch 77/300 -- Iteration 73818 - Batch 630/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.92 min) Epoch 77/300 -- Iteration 73827 - Batch 639/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.92 min) Epoch 77/300 -- Iteration 73836 - Batch 648/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.92 min) Epoch 77/300 -- Iteration 73845 - Batch 657/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.93 min) Epoch 77/300 -- Iteration 73854 - Batch 666/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.93 min) Epoch 77/300 -- Iteration 73863 - Batch 675/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.94 min) Epoch 77/300 -- Iteration 73872 - Batch 684/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.94 min) Epoch 77/300 -- Iteration 73881 - Batch 693/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.95 min) Epoch 77/300 -- Iteration 73890 - Batch 702/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.95 min) Epoch 77/300 -- Iteration 73899 - Batch 711/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.96 min) Epoch 77/300 -- Iteration 73908 - Batch 720/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.96 min) Epoch 77/300 -- Iteration 73917 - Batch 729/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.96 min) Epoch 77/300 -- Iteration 73926 - Batch 738/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.97 min) Epoch 77/300 -- Iteration 73935 - Batch 747/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.97 min) Epoch 77/300 -- Iteration 73944 - Batch 756/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.98 min) Epoch 77/300 -- Iteration 73953 - Batch 765/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.98 min) Epoch 77/300 -- Iteration 73962 - Batch 774/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.99 min) Epoch 77/300 -- Iteration 73971 - Batch 783/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(36.99 min) Epoch 77/300 -- Iteration 73980 - Batch 792/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.00 min) Epoch 77/300 -- Iteration 73989 - Batch 801/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.00 min) Epoch 77/300 -- Iteration 73998 - Batch 810/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.00 min) Epoch 77/300 -- Iteration 74007 - Batch 819/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.01 min) Epoch 77/300 -- Iteration 74016 - Batch 828/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.01 min) Epoch 77/300 -- Iteration 74025 - Batch 837/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.02 min) Epoch 77/300 -- Iteration 74034 - Batch 846/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.02 min) Epoch 77/300 -- Iteration 74043 - Batch 855/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.03 min) Epoch 77/300 -- Iteration 74052 - Batch 864/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.03 min) Epoch 77/300 -- Iteration 74061 - Batch 873/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.04 min) Epoch 77/300 -- Iteration 74070 - Batch 882/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.04 min) Epoch 77/300 -- Iteration 74079 - Batch 891/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.04 min) Epoch 77/300 -- Iteration 74088 - Batch 900/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.05 min) Epoch 77/300 -- Iteration 74097 - Batch 909/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.05 min) Epoch 77/300 -- Iteration 74106 - Batch 918/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.06 min) Epoch 77/300 -- Iteration 74115 - Batch 927/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.06 min) Epoch 77/300 -- Iteration 74124 - Batch 936/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.07 min) Epoch 77/300 -- Iteration 74133 - Batch 945/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.07 min) Epoch 77/300 -- Iteration 74142 - Batch 954/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(37.08 min) Epoch 77/300 -- Iteration 74151 - Batch 962/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(37.08 min) Epoch 78/300 -- Iteration 74160 - Batch 9/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.09 min) Epoch 78/300 -- Iteration 74169 - Batch 18/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.09 min) Epoch 78/300 -- Iteration 74178 - Batch 27/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.09 min) Epoch 78/300 -- Iteration 74187 - Batch 36/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.10 min) Epoch 78/300 -- Iteration 74196 - Batch 45/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.10 min) Epoch 78/300 -- Iteration 74205 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.11 min) Epoch 78/300 -- Iteration 74214 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.11 min) Epoch 78/300 -- Iteration 74223 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.12 min) Epoch 78/300 -- Iteration 74232 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.12 min) Epoch 78/300 -- Iteration 74241 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.13 min) Epoch 78/300 -- Iteration 74250 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.13 min) Epoch 78/300 -- Iteration 74259 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.13 min) Epoch 78/300 -- Iteration 74268 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.14 min) Epoch 78/300 -- Iteration 74277 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.14 min) Epoch 78/300 -- Iteration 74286 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.15 min) Epoch 78/300 -- Iteration 74295 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.15 min) Epoch 78/300 -- Iteration 74304 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.16 min) Epoch 78/300 -- Iteration 74313 - Batch 162/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.16 min) Epoch 78/300 -- Iteration 74322 - Batch 171/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.17 min) Epoch 78/300 -- Iteration 74331 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.17 min) Epoch 78/300 -- Iteration 74340 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.17 min) Epoch 78/300 -- Iteration 74349 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.18 min) Epoch 78/300 -- Iteration 74358 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.18 min) Epoch 78/300 -- Iteration 74367 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.19 min) Epoch 78/300 -- Iteration 74376 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.19 min) Epoch 78/300 -- Iteration 74385 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.20 min) Epoch 78/300 -- Iteration 74394 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.20 min) Epoch 78/300 -- Iteration 74403 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.21 min) Epoch 78/300 -- Iteration 74412 - Batch 261/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.21 min) Epoch 78/300 -- Iteration 74421 - Batch 270/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.21 min) Epoch 78/300 -- Iteration 74430 - Batch 279/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.22 min) Epoch 78/300 -- Iteration 74439 - Batch 288/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.22 min) Epoch 78/300 -- Iteration 74448 - Batch 297/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.23 min) Epoch 78/300 -- Iteration 74457 - Batch 306/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.23 min) Epoch 78/300 -- Iteration 74466 - Batch 315/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.24 min) Epoch 78/300 -- Iteration 74475 - Batch 324/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.24 min) Epoch 78/300 -- Iteration 74484 - Batch 333/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.25 min) Epoch 78/300 -- Iteration 74493 - Batch 342/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.25 min) Epoch 78/300 -- Iteration 74502 - Batch 351/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.25 min) Epoch 78/300 -- Iteration 74511 - Batch 360/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.26 min) Epoch 78/300 -- Iteration 74520 - Batch 369/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.26 min) Epoch 78/300 -- Iteration 74529 - Batch 378/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.27 min) Epoch 78/300 -- Iteration 74538 - Batch 387/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.27 min) Epoch 78/300 -- Iteration 74547 - Batch 396/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.28 min) Epoch 78/300 -- Iteration 74556 - Batch 405/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.28 min) Epoch 78/300 -- Iteration 74565 - Batch 414/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.29 min) Epoch 78/300 -- Iteration 74574 - Batch 423/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.29 min) Epoch 78/300 -- Iteration 74583 - Batch 432/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.29 min) Epoch 78/300 -- Iteration 74592 - Batch 441/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.30 min) Epoch 78/300 -- Iteration 74601 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.30 min) Epoch 78/300 -- Iteration 74610 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.31 min) Epoch 78/300 -- Iteration 74619 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.31 min) Epoch 78/300 -- Iteration 74628 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.32 min) Epoch 78/300 -- Iteration 74637 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.32 min) Epoch 78/300 -- Iteration 74646 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.33 min) Epoch 78/300 -- Iteration 74655 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.33 min) Epoch 78/300 -- Iteration 74664 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.33 min) Epoch 78/300 -- Iteration 74673 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.34 min) Epoch 78/300 -- Iteration 74682 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.34 min) Epoch 78/300 -- Iteration 74691 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.35 min) Epoch 78/300 -- Iteration 74700 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.35 min) Epoch 78/300 -- Iteration 74709 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.36 min) Epoch 78/300 -- Iteration 74718 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.36 min) Epoch 78/300 -- Iteration 74727 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.37 min) Epoch 78/300 -- Iteration 74736 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.37 min) Epoch 78/300 -- Iteration 74745 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.37 min) Epoch 78/300 -- Iteration 74754 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.38 min) Epoch 78/300 -- Iteration 74763 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.38 min) Epoch 78/300 -- Iteration 74772 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.39 min) Epoch 78/300 -- Iteration 74781 - Batch 630/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.39 min) Epoch 78/300 -- Iteration 74790 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.40 min) Epoch 78/300 -- Iteration 74799 - Batch 648/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.40 min) Epoch 78/300 -- Iteration 74808 - Batch 657/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.41 min) Epoch 78/300 -- Iteration 74817 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.41 min) Epoch 78/300 -- Iteration 74826 - Batch 675/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.41 min) Epoch 78/300 -- Iteration 74835 - Batch 684/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.42 min) Epoch 78/300 -- Iteration 74844 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.42 min) Epoch 78/300 -- Iteration 74853 - Batch 702/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.43 min) Epoch 78/300 -- Iteration 74862 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.43 min) Epoch 78/300 -- Iteration 74871 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.44 min) Epoch 78/300 -- Iteration 74880 - Batch 729/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.44 min) Epoch 78/300 -- Iteration 74889 - Batch 738/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.45 min) Epoch 78/300 -- Iteration 74898 - Batch 747/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.45 min) Epoch 78/300 -- Iteration 74907 - Batch 756/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.45 min) Epoch 78/300 -- Iteration 74916 - Batch 765/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.46 min) Epoch 78/300 -- Iteration 74925 - Batch 774/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.46 min) Epoch 78/300 -- Iteration 74934 - Batch 783/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.47 min) Epoch 78/300 -- Iteration 74943 - Batch 792/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.47 min) Epoch 78/300 -- Iteration 74952 - Batch 801/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.48 min) Epoch 78/300 -- Iteration 74961 - Batch 810/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.48 min) Epoch 78/300 -- Iteration 74970 - Batch 819/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.49 min) Epoch 78/300 -- Iteration 74979 - Batch 828/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.49 min) Epoch 78/300 -- Iteration 74988 - Batch 837/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.49 min) Epoch 78/300 -- Iteration 74997 - Batch 846/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.50 min) Epoch 78/300 -- Iteration 75006 - Batch 855/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.50 min) Epoch 78/300 -- Iteration 75015 - Batch 864/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.51 min) Epoch 78/300 -- Iteration 75024 - Batch 873/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.51 min) Epoch 78/300 -- Iteration 75033 - Batch 882/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.52 min) Epoch 78/300 -- Iteration 75042 - Batch 891/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.52 min) Epoch 78/300 -- Iteration 75051 - Batch 900/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.53 min) Epoch 78/300 -- Iteration 75060 - Batch 909/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.53 min) Epoch 78/300 -- Iteration 75069 - Batch 918/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.54 min) Epoch 78/300 -- Iteration 75078 - Batch 927/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.54 min) Epoch 78/300 -- Iteration 75087 - Batch 936/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.55 min) Epoch 78/300 -- Iteration 75096 - Batch 945/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.55 min) Epoch 78/300 -- Iteration 75105 - Batch 954/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.56 min) Epoch 78/300 -- Iteration 75114 - Batch 962/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(37.56 min) Epoch 79/300 -- Iteration 75123 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.57 min) Epoch 79/300 -- Iteration 75132 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.57 min) Epoch 79/300 -- Iteration 75141 - Batch 27/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.58 min) Epoch 79/300 -- Iteration 75150 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.58 min) Epoch 79/300 -- Iteration 75159 - Batch 45/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.59 min) Epoch 79/300 -- Iteration 75168 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.59 min) Epoch 79/300 -- Iteration 75177 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.59 min) Epoch 79/300 -- Iteration 75186 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.60 min) Epoch 79/300 -- Iteration 75195 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.60 min) Epoch 79/300 -- Iteration 75204 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.61 min) Epoch 79/300 -- Iteration 75213 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.61 min) Epoch 79/300 -- Iteration 75222 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.62 min) Epoch 79/300 -- Iteration 75231 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.62 min) Epoch 79/300 -- Iteration 75240 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.63 min) Epoch 79/300 -- Iteration 75249 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.63 min) Epoch 79/300 -- Iteration 75258 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.64 min) Epoch 79/300 -- Iteration 75267 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.64 min) Epoch 79/300 -- Iteration 75276 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.65 min) Epoch 79/300 -- Iteration 75285 - Batch 171/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.65 min) Epoch 79/300 -- Iteration 75294 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.66 min) Epoch 79/300 -- Iteration 75303 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.66 min) Epoch 79/300 -- Iteration 75312 - Batch 198/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.66 min) Epoch 79/300 -- Iteration 75321 - Batch 207/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.67 min) Epoch 79/300 -- Iteration 75330 - Batch 216/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.67 min) Epoch 79/300 -- Iteration 75339 - Batch 225/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.68 min) Epoch 79/300 -- Iteration 75348 - Batch 234/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.68 min) Epoch 79/300 -- Iteration 75357 - Batch 243/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.69 min) Epoch 79/300 -- Iteration 75366 - Batch 252/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.69 min) Epoch 79/300 -- Iteration 75375 - Batch 261/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.70 min) Epoch 79/300 -- Iteration 75384 - Batch 270/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.70 min) Epoch 79/300 -- Iteration 75393 - Batch 279/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.71 min) Epoch 79/300 -- Iteration 75402 - Batch 288/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.71 min) Epoch 79/300 -- Iteration 75411 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.71 min) Epoch 79/300 -- Iteration 75420 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.72 min) Epoch 79/300 -- Iteration 75429 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.72 min) Epoch 79/300 -- Iteration 75438 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.73 min) Epoch 79/300 -- Iteration 75447 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.73 min) Epoch 79/300 -- Iteration 75456 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.74 min) Epoch 79/300 -- Iteration 75465 - Batch 351/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.74 min) Epoch 79/300 -- Iteration 75474 - Batch 360/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.75 min) Epoch 79/300 -- Iteration 75483 - Batch 369/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.75 min) Epoch 79/300 -- Iteration 75492 - Batch 378/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.75 min) Epoch 79/300 -- Iteration 75501 - Batch 387/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.76 min) Epoch 79/300 -- Iteration 75510 - Batch 396/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.76 min) Epoch 79/300 -- Iteration 75519 - Batch 405/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.77 min) Epoch 79/300 -- Iteration 75528 - Batch 414/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.77 min) Epoch 79/300 -- Iteration 75537 - Batch 423/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.78 min) Epoch 79/300 -- Iteration 75546 - Batch 432/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.78 min) Epoch 79/300 -- Iteration 75555 - Batch 441/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.79 min) Epoch 79/300 -- Iteration 75564 - Batch 450/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.79 min) Epoch 79/300 -- Iteration 75573 - Batch 459/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.79 min) Epoch 79/300 -- Iteration 75582 - Batch 468/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.80 min) Epoch 79/300 -- Iteration 75591 - Batch 477/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.80 min) Epoch 79/300 -- Iteration 75600 - Batch 486/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.81 min) Epoch 79/300 -- Iteration 75609 - Batch 495/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.81 min) Epoch 79/300 -- Iteration 75618 - Batch 504/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.82 min) Epoch 79/300 -- Iteration 75627 - Batch 513/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.82 min) Epoch 79/300 -- Iteration 75636 - Batch 522/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.83 min) Epoch 79/300 -- Iteration 75645 - Batch 531/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.83 min) Epoch 79/300 -- Iteration 75654 - Batch 540/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.83 min) Epoch 79/300 -- Iteration 75663 - Batch 549/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.84 min) Epoch 79/300 -- Iteration 75672 - Batch 558/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.84 min) Epoch 79/300 -- Iteration 75681 - Batch 567/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.85 min) Epoch 79/300 -- Iteration 75690 - Batch 576/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.85 min) Epoch 79/300 -- Iteration 75699 - Batch 585/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.86 min) Epoch 79/300 -- Iteration 75708 - Batch 594/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.86 min) Epoch 79/300 -- Iteration 75717 - Batch 603/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.87 min) Epoch 79/300 -- Iteration 75726 - Batch 612/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.87 min) Epoch 79/300 -- Iteration 75735 - Batch 621/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.87 min) Epoch 79/300 -- Iteration 75744 - Batch 630/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.88 min) Epoch 79/300 -- Iteration 75753 - Batch 639/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.88 min) Epoch 79/300 -- Iteration 75762 - Batch 648/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.89 min) Epoch 79/300 -- Iteration 75771 - Batch 657/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.89 min) Epoch 79/300 -- Iteration 75780 - Batch 666/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.90 min) Epoch 79/300 -- Iteration 75789 - Batch 675/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.90 min) Epoch 79/300 -- Iteration 75798 - Batch 684/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.91 min) Epoch 79/300 -- Iteration 75807 - Batch 693/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.91 min) Epoch 79/300 -- Iteration 75816 - Batch 702/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.91 min) Epoch 79/300 -- Iteration 75825 - Batch 711/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.92 min) Epoch 79/300 -- Iteration 75834 - Batch 720/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.92 min) Epoch 79/300 -- Iteration 75843 - Batch 729/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.93 min) Epoch 79/300 -- Iteration 75852 - Batch 738/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.93 min) Epoch 79/300 -- Iteration 75861 - Batch 747/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.94 min) Epoch 79/300 -- Iteration 75870 - Batch 756/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.94 min) Epoch 79/300 -- Iteration 75879 - Batch 765/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.95 min) Epoch 79/300 -- Iteration 75888 - Batch 774/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.95 min) Epoch 79/300 -- Iteration 75897 - Batch 783/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.95 min) Epoch 79/300 -- Iteration 75906 - Batch 792/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.96 min) Epoch 79/300 -- Iteration 75915 - Batch 801/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.96 min) Epoch 79/300 -- Iteration 75924 - Batch 810/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.97 min) Epoch 79/300 -- Iteration 75933 - Batch 819/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.97 min) Epoch 79/300 -- Iteration 75942 - Batch 828/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.98 min) Epoch 79/300 -- Iteration 75951 - Batch 837/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.98 min) Epoch 79/300 -- Iteration 75960 - Batch 846/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.99 min) Epoch 79/300 -- Iteration 75969 - Batch 855/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(37.99 min) Epoch 79/300 -- Iteration 75978 - Batch 864/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.00 min) Epoch 79/300 -- Iteration 75987 - Batch 873/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.00 min) Epoch 79/300 -- Iteration 75996 - Batch 882/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.00 min) Epoch 79/300 -- Iteration 76005 - Batch 891/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.01 min) Epoch 79/300 -- Iteration 76014 - Batch 900/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.01 min) Epoch 79/300 -- Iteration 76023 - Batch 909/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.02 min) Epoch 79/300 -- Iteration 76032 - Batch 918/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.02 min) Epoch 79/300 -- Iteration 76041 - Batch 927/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.03 min) Epoch 79/300 -- Iteration 76050 - Batch 936/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.03 min) Epoch 79/300 -- Iteration 76059 - Batch 945/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.04 min) Epoch 79/300 -- Iteration 76068 - Batch 954/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.04 min) Epoch 79/300 -- Iteration 76077 - Batch 962/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(38.05 min) Epoch 80/300 -- Iteration 76086 - Batch 9/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.05 min) Epoch 80/300 -- Iteration 76095 - Batch 18/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.06 min) Epoch 80/300 -- Iteration 76104 - Batch 27/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.06 min) Epoch 80/300 -- Iteration 76113 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.07 min) Epoch 80/300 -- Iteration 76122 - Batch 45/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.07 min) Epoch 80/300 -- Iteration 76131 - Batch 54/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.08 min) Epoch 80/300 -- Iteration 76140 - Batch 63/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.08 min) Epoch 80/300 -- Iteration 76149 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.09 min) Epoch 80/300 -- Iteration 76158 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.09 min) Epoch 80/300 -- Iteration 76167 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.09 min) Epoch 80/300 -- Iteration 76176 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.10 min) Epoch 80/300 -- Iteration 76185 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.10 min) Epoch 80/300 -- Iteration 76194 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.11 min) Epoch 80/300 -- Iteration 76203 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.11 min) Epoch 80/300 -- Iteration 76212 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.12 min) Epoch 80/300 -- Iteration 76221 - Batch 144/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.12 min) Epoch 80/300 -- Iteration 76230 - Batch 153/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.13 min) Epoch 80/300 -- Iteration 76239 - Batch 162/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.13 min) Epoch 80/300 -- Iteration 76248 - Batch 171/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.14 min) Epoch 80/300 -- Iteration 76257 - Batch 180/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.14 min) Epoch 80/300 -- Iteration 76266 - Batch 189/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.15 min) Epoch 80/300 -- Iteration 76275 - Batch 198/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.15 min) Epoch 80/300 -- Iteration 76284 - Batch 207/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.16 min) Epoch 80/300 -- Iteration 76293 - Batch 216/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.16 min) Epoch 80/300 -- Iteration 76302 - Batch 225/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.16 min) Epoch 80/300 -- Iteration 76311 - Batch 234/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.17 min) Epoch 80/300 -- Iteration 76320 - Batch 243/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.17 min) Epoch 80/300 -- Iteration 76329 - Batch 252/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.18 min) Epoch 80/300 -- Iteration 76338 - Batch 261/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.18 min) Epoch 80/300 -- Iteration 76347 - Batch 270/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.19 min) Epoch 80/300 -- Iteration 76356 - Batch 279/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.19 min) Epoch 80/300 -- Iteration 76365 - Batch 288/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.20 min) Epoch 80/300 -- Iteration 76374 - Batch 297/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.20 min) Epoch 80/300 -- Iteration 76383 - Batch 306/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.21 min) Epoch 80/300 -- Iteration 76392 - Batch 315/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.21 min) Epoch 80/300 -- Iteration 76401 - Batch 324/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.21 min) Epoch 80/300 -- Iteration 76410 - Batch 333/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.22 min) Epoch 80/300 -- Iteration 76419 - Batch 342/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.22 min) Epoch 80/300 -- Iteration 76428 - Batch 351/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.23 min) Epoch 80/300 -- Iteration 76437 - Batch 360/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.23 min) Epoch 80/300 -- Iteration 76446 - Batch 369/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.24 min) Epoch 80/300 -- Iteration 76455 - Batch 378/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.24 min) Epoch 80/300 -- Iteration 76464 - Batch 387/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.25 min) Epoch 80/300 -- Iteration 76473 - Batch 396/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.25 min) Epoch 80/300 -- Iteration 76482 - Batch 405/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.26 min) Epoch 80/300 -- Iteration 76491 - Batch 414/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.26 min) Epoch 80/300 -- Iteration 76500 - Batch 423/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.27 min) Epoch 80/300 -- Iteration 76509 - Batch 432/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.27 min) Epoch 80/300 -- Iteration 76518 - Batch 441/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.27 min) Epoch 80/300 -- Iteration 76527 - Batch 450/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.28 min) Epoch 80/300 -- Iteration 76536 - Batch 459/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.28 min) Epoch 80/300 -- Iteration 76545 - Batch 468/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.29 min) Epoch 80/300 -- Iteration 76554 - Batch 477/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.29 min) Epoch 80/300 -- Iteration 76563 - Batch 486/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.30 min) Epoch 80/300 -- Iteration 76572 - Batch 495/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.30 min) Epoch 80/300 -- Iteration 76581 - Batch 504/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.31 min) Epoch 80/300 -- Iteration 76590 - Batch 513/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.31 min) Epoch 80/300 -- Iteration 76599 - Batch 522/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.32 min) Epoch 80/300 -- Iteration 76608 - Batch 531/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.32 min) Epoch 80/300 -- Iteration 76617 - Batch 540/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.33 min) Epoch 80/300 -- Iteration 76626 - Batch 549/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.33 min) Epoch 80/300 -- Iteration 76635 - Batch 558/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.34 min) Epoch 80/300 -- Iteration 76644 - Batch 567/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.34 min) Epoch 80/300 -- Iteration 76653 - Batch 576/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.34 min) Epoch 80/300 -- Iteration 76662 - Batch 585/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.35 min) Epoch 80/300 -- Iteration 76671 - Batch 594/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.35 min) Epoch 80/300 -- Iteration 76680 - Batch 603/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.36 min) Epoch 80/300 -- Iteration 76689 - Batch 612/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.36 min) Epoch 80/300 -- Iteration 76698 - Batch 621/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.37 min) Epoch 80/300 -- Iteration 76707 - Batch 630/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.37 min) Epoch 80/300 -- Iteration 76716 - Batch 639/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.38 min) Epoch 80/300 -- Iteration 76725 - Batch 648/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.38 min) Epoch 80/300 -- Iteration 76734 - Batch 657/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.39 min) Epoch 80/300 -- Iteration 76743 - Batch 666/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.39 min) Epoch 80/300 -- Iteration 76752 - Batch 675/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.40 min) Epoch 80/300 -- Iteration 76761 - Batch 684/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.40 min) Epoch 80/300 -- Iteration 76770 - Batch 693/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.41 min) Epoch 80/300 -- Iteration 76779 - Batch 702/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.41 min) Epoch 80/300 -- Iteration 76788 - Batch 711/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.42 min) Epoch 80/300 -- Iteration 76797 - Batch 720/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.42 min) Epoch 80/300 -- Iteration 76806 - Batch 729/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.42 min) Epoch 80/300 -- Iteration 76815 - Batch 738/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.43 min) Epoch 80/300 -- Iteration 76824 - Batch 747/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.43 min) Epoch 80/300 -- Iteration 76833 - Batch 756/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.44 min) Epoch 80/300 -- Iteration 76842 - Batch 765/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.44 min) Epoch 80/300 -- Iteration 76851 - Batch 774/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.45 min) Epoch 80/300 -- Iteration 76860 - Batch 783/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.45 min) Epoch 80/300 -- Iteration 76869 - Batch 792/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.46 min) Epoch 80/300 -- Iteration 76878 - Batch 801/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.46 min) Epoch 80/300 -- Iteration 76887 - Batch 810/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.47 min) Epoch 80/300 -- Iteration 76896 - Batch 819/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.47 min) Epoch 80/300 -- Iteration 76905 - Batch 828/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.48 min) Epoch 80/300 -- Iteration 76914 - Batch 837/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.48 min) Epoch 80/300 -- Iteration 76923 - Batch 846/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.49 min) Epoch 80/300 -- Iteration 76932 - Batch 855/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.49 min) Epoch 80/300 -- Iteration 76941 - Batch 864/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.49 min) Epoch 80/300 -- Iteration 76950 - Batch 873/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.50 min) Epoch 80/300 -- Iteration 76959 - Batch 882/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.50 min) Epoch 80/300 -- Iteration 76968 - Batch 891/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.51 min) Epoch 80/300 -- Iteration 76977 - Batch 900/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.51 min) Epoch 80/300 -- Iteration 76986 - Batch 909/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.52 min) Epoch 80/300 -- Iteration 76995 - Batch 918/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.52 min) Epoch 80/300 -- Iteration 77004 - Batch 927/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.53 min) Epoch 80/300 -- Iteration 77013 - Batch 936/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.53 min) Epoch 80/300 -- Iteration 77022 - Batch 945/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.54 min) Epoch 80/300 -- Iteration 77031 - Batch 954/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020\n",
      "(38.54 min) Epoch 80/300 -- Iteration 77040 - Batch 962/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019 - Val acc: -0.0000\n",
      "(38.55 min) Epoch 81/300 -- Iteration 77049 - Batch 9/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.55 min) Epoch 81/300 -- Iteration 77058 - Batch 18/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.56 min) Epoch 81/300 -- Iteration 77067 - Batch 27/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.56 min) Epoch 81/300 -- Iteration 77076 - Batch 36/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.56 min) Epoch 81/300 -- Iteration 77085 - Batch 45/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.57 min) Epoch 81/300 -- Iteration 77094 - Batch 54/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.57 min) Epoch 81/300 -- Iteration 77103 - Batch 63/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.58 min) Epoch 81/300 -- Iteration 77112 - Batch 72/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.58 min) Epoch 81/300 -- Iteration 77121 - Batch 81/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.59 min) Epoch 81/300 -- Iteration 77130 - Batch 90/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.59 min) Epoch 81/300 -- Iteration 77139 - Batch 99/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.60 min) Epoch 81/300 -- Iteration 77148 - Batch 108/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.60 min) Epoch 81/300 -- Iteration 77157 - Batch 117/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.61 min) Epoch 81/300 -- Iteration 77166 - Batch 126/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.61 min) Epoch 81/300 -- Iteration 77175 - Batch 135/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.62 min) Epoch 81/300 -- Iteration 77184 - Batch 144/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.62 min) Epoch 81/300 -- Iteration 77193 - Batch 153/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.62 min) Epoch 81/300 -- Iteration 77202 - Batch 162/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.63 min) Epoch 81/300 -- Iteration 77211 - Batch 171/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.63 min) Epoch 81/300 -- Iteration 77220 - Batch 180/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.64 min) Epoch 81/300 -- Iteration 77229 - Batch 189/963 - Train loss: 0.0025  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.64 min) Epoch 81/300 -- Iteration 77238 - Batch 198/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.65 min) Epoch 81/300 -- Iteration 77247 - Batch 207/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.65 min) Epoch 81/300 -- Iteration 77256 - Batch 216/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.66 min) Epoch 81/300 -- Iteration 77265 - Batch 225/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.66 min) Epoch 81/300 -- Iteration 77274 - Batch 234/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.66 min) Epoch 81/300 -- Iteration 77283 - Batch 243/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.67 min) Epoch 81/300 -- Iteration 77292 - Batch 252/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.67 min) Epoch 81/300 -- Iteration 77301 - Batch 261/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.68 min) Epoch 81/300 -- Iteration 77310 - Batch 270/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.68 min) Epoch 81/300 -- Iteration 77319 - Batch 279/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.69 min) Epoch 81/300 -- Iteration 77328 - Batch 288/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.69 min) Epoch 81/300 -- Iteration 77337 - Batch 297/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.70 min) Epoch 81/300 -- Iteration 77346 - Batch 306/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.70 min) Epoch 81/300 -- Iteration 77355 - Batch 315/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.70 min) Epoch 81/300 -- Iteration 77364 - Batch 324/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.71 min) Epoch 81/300 -- Iteration 77373 - Batch 333/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.71 min) Epoch 81/300 -- Iteration 77382 - Batch 342/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.72 min) Epoch 81/300 -- Iteration 77391 - Batch 351/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.72 min) Epoch 81/300 -- Iteration 77400 - Batch 360/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.73 min) Epoch 81/300 -- Iteration 77409 - Batch 369/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.73 min) Epoch 81/300 -- Iteration 77418 - Batch 378/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.74 min) Epoch 81/300 -- Iteration 77427 - Batch 387/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.74 min) Epoch 81/300 -- Iteration 77436 - Batch 396/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.75 min) Epoch 81/300 -- Iteration 77445 - Batch 405/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.75 min) Epoch 81/300 -- Iteration 77454 - Batch 414/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.75 min) Epoch 81/300 -- Iteration 77463 - Batch 423/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.76 min) Epoch 81/300 -- Iteration 77472 - Batch 432/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.76 min) Epoch 81/300 -- Iteration 77481 - Batch 441/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.77 min) Epoch 81/300 -- Iteration 77490 - Batch 450/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.77 min) Epoch 81/300 -- Iteration 77499 - Batch 459/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.78 min) Epoch 81/300 -- Iteration 77508 - Batch 468/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.78 min) Epoch 81/300 -- Iteration 77517 - Batch 477/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.78 min) Epoch 81/300 -- Iteration 77526 - Batch 486/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.79 min) Epoch 81/300 -- Iteration 77535 - Batch 495/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.79 min) Epoch 81/300 -- Iteration 77544 - Batch 504/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.80 min) Epoch 81/300 -- Iteration 77553 - Batch 513/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.80 min) Epoch 81/300 -- Iteration 77562 - Batch 522/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.81 min) Epoch 81/300 -- Iteration 77571 - Batch 531/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.81 min) Epoch 81/300 -- Iteration 77580 - Batch 540/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.82 min) Epoch 81/300 -- Iteration 77589 - Batch 549/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.82 min) Epoch 81/300 -- Iteration 77598 - Batch 558/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.82 min) Epoch 81/300 -- Iteration 77607 - Batch 567/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.83 min) Epoch 81/300 -- Iteration 77616 - Batch 576/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.83 min) Epoch 81/300 -- Iteration 77625 - Batch 585/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.84 min) Epoch 81/300 -- Iteration 77634 - Batch 594/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.84 min) Epoch 81/300 -- Iteration 77643 - Batch 603/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.85 min) Epoch 81/300 -- Iteration 77652 - Batch 612/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.85 min) Epoch 81/300 -- Iteration 77661 - Batch 621/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.86 min) Epoch 81/300 -- Iteration 77670 - Batch 630/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.86 min) Epoch 81/300 -- Iteration 77679 - Batch 639/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.87 min) Epoch 81/300 -- Iteration 77688 - Batch 648/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.87 min) Epoch 81/300 -- Iteration 77697 - Batch 657/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.87 min) Epoch 81/300 -- Iteration 77706 - Batch 666/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.88 min) Epoch 81/300 -- Iteration 77715 - Batch 675/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.88 min) Epoch 81/300 -- Iteration 77724 - Batch 684/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.89 min) Epoch 81/300 -- Iteration 77733 - Batch 693/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.89 min) Epoch 81/300 -- Iteration 77742 - Batch 702/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.90 min) Epoch 81/300 -- Iteration 77751 - Batch 711/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.90 min) Epoch 81/300 -- Iteration 77760 - Batch 720/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.91 min) Epoch 81/300 -- Iteration 77769 - Batch 729/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.91 min) Epoch 81/300 -- Iteration 77778 - Batch 738/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.91 min) Epoch 81/300 -- Iteration 77787 - Batch 747/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.92 min) Epoch 81/300 -- Iteration 77796 - Batch 756/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.92 min) Epoch 81/300 -- Iteration 77805 - Batch 765/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.93 min) Epoch 81/300 -- Iteration 77814 - Batch 774/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.93 min) Epoch 81/300 -- Iteration 77823 - Batch 783/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.94 min) Epoch 81/300 -- Iteration 77832 - Batch 792/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.94 min) Epoch 81/300 -- Iteration 77841 - Batch 801/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.95 min) Epoch 81/300 -- Iteration 77850 - Batch 810/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.95 min) Epoch 81/300 -- Iteration 77859 - Batch 819/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.95 min) Epoch 81/300 -- Iteration 77868 - Batch 828/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.96 min) Epoch 81/300 -- Iteration 77877 - Batch 837/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.96 min) Epoch 81/300 -- Iteration 77886 - Batch 846/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.97 min) Epoch 81/300 -- Iteration 77895 - Batch 855/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.97 min) Epoch 81/300 -- Iteration 77904 - Batch 864/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.98 min) Epoch 81/300 -- Iteration 77913 - Batch 873/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.98 min) Epoch 81/300 -- Iteration 77922 - Batch 882/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.98 min) Epoch 81/300 -- Iteration 77931 - Batch 891/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.99 min) Epoch 81/300 -- Iteration 77940 - Batch 900/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(38.99 min) Epoch 81/300 -- Iteration 77949 - Batch 909/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(39.00 min) Epoch 81/300 -- Iteration 77958 - Batch 918/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(39.00 min) Epoch 81/300 -- Iteration 77967 - Batch 927/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(39.01 min) Epoch 81/300 -- Iteration 77976 - Batch 936/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(39.01 min) Epoch 81/300 -- Iteration 77985 - Batch 945/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(39.02 min) Epoch 81/300 -- Iteration 77994 - Batch 954/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0019\n",
      "(39.02 min) Epoch 81/300 -- Iteration 78003 - Batch 962/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n",
      "(39.02 min) Epoch 81/300 -- Iteration 78003 - Batch 962/963 - Train loss: 0.0024  - Train acc: -0.0000 - Val loss: 0.0020 - Val acc: -0.0000\n"
     ]
    }
   ],
   "source": [
    "import src.final_model as fm\n",
    "import src.final_training as ft\n",
    "importlib.reload(fm)\n",
    "importlib.reload(ft)\n",
    "importlib.reload(src.final_training)\n",
    "importlib.reload(src.final_model)\n",
    "\n",
    "modelo_final = fm.FinalModel(latent_dim, n_channels, rnn_type, hidden_dim, num_layers, dropout_prob,  n_classes, name='2nd_iteration')\n",
    "\n",
    "early_stop = 20\n",
    "batch_size = 128\n",
    "\n",
    "stage = 'ae_rnn'\n",
    "curves = ft.train_final_model(modelo_final,\n",
    "                                              stage,\n",
    "                                                train_dataset,\n",
    "                                                validation_dataset,\n",
    "                                                ae_loss,\n",
    "                                                rnn_loss,\n",
    "                                                max_epochs,\n",
    "                                                max_time,\n",
    "                                                batch_size,\n",
    "                                                lr,\n",
    "                                                random_sampler,\n",
    "                                                only_classifier,\n",
    "                                                augmentation,\n",
    "                                                early_stop,\n",
    "                                                use_gpu,\n",
    "                                                num_cpu)\n",
    "\n",
    "torch.save(modelo_final.state_dict(), f'models/{modelo_final.name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmarc\\AppData\\Local\\Temp\\ipykernel_24488\\2805212558.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  modelo_final.load_state_dict(torch.load(f'models/aernn_final.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.final_model as fm\n",
    "import src.final_training as ft\n",
    "importlib.reload(fm)\n",
    "importlib.reload(ft)\n",
    "importlib.reload(src.final_training)\n",
    "importlib.reload(src.final_model)\n",
    "\n",
    "modelo_final = fm.FinalModel(latent_dim, n_channels, rnn_type, hidden_dim, num_layers, dropout_prob,  n_classes, name='2nd_iteration')\n",
    "modelo_final.load_state_dict(torch.load(f'models/aernn_final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1950x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.show_curves([curves], [modelo_final])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setup finishied. Starting training...\n",
      "(0.01 min) Epoch 1/300 -- Iteration 9 - Batch 9/963 - Train loss: 0.40260745  - Train acc: 0.8438 - Val loss: 0.00000000\n",
      "(0.01 min) Epoch 1/300 -- Iteration 18 - Batch 18/963 - Train loss: 0.38477126  - Train acc: 0.8569 - Val loss: 0.00000000\n",
      "(0.01 min) Epoch 1/300 -- Iteration 27 - Batch 27/963 - Train loss: 0.38449227  - Train acc: 0.8583 - Val loss: 0.00000000\n",
      "(0.01 min) Epoch 1/300 -- Iteration 36 - Batch 36/963 - Train loss: 0.38193089  - Train acc: 0.8592 - Val loss: 0.00000000\n",
      "(0.02 min) Epoch 1/300 -- Iteration 45 - Batch 45/963 - Train loss: 0.38622411  - Train acc: 0.8573 - Val loss: 0.00000000\n",
      "(0.02 min) Epoch 1/300 -- Iteration 54 - Batch 54/963 - Train loss: 0.38683633  - Train acc: 0.8564 - Val loss: 0.00000000\n",
      "(0.02 min) Epoch 1/300 -- Iteration 63 - Batch 63/963 - Train loss: 0.38334937  - Train acc: 0.8574 - Val loss: 0.00000000\n",
      "(0.02 min) Epoch 1/300 -- Iteration 72 - Batch 72/963 - Train loss: 0.38345871  - Train acc: 0.8573 - Val loss: 0.00000000\n",
      "(0.02 min) Epoch 1/300 -- Iteration 81 - Batch 81/963 - Train loss: 0.38398837  - Train acc: 0.8569 - Val loss: 0.00000000\n",
      "(0.03 min) Epoch 1/300 -- Iteration 90 - Batch 90/963 - Train loss: 0.38548396  - Train acc: 0.8560 - Val loss: 0.00000000\n",
      "(0.03 min) Epoch 1/300 -- Iteration 99 - Batch 99/963 - Train loss: 0.38514812  - Train acc: 0.8566 - Val loss: 0.00000000\n",
      "(0.03 min) Epoch 1/300 -- Iteration 108 - Batch 108/963 - Train loss: 0.38518331  - Train acc: 0.8554 - Val loss: 0.00000000\n",
      "(0.03 min) Epoch 1/300 -- Iteration 117 - Batch 117/963 - Train loss: 0.38514365  - Train acc: 0.8558 - Val loss: 0.00000000\n",
      "(0.03 min) Epoch 1/300 -- Iteration 126 - Batch 126/963 - Train loss: 0.38454037  - Train acc: 0.8557 - Val loss: 0.00000000\n",
      "(0.04 min) Epoch 1/300 -- Iteration 135 - Batch 135/963 - Train loss: 0.38530379  - Train acc: 0.8557 - Val loss: 0.00000000\n",
      "(0.04 min) Epoch 1/300 -- Iteration 144 - Batch 144/963 - Train loss: 0.38625577  - Train acc: 0.8550 - Val loss: 0.00000000\n",
      "(0.04 min) Epoch 1/300 -- Iteration 153 - Batch 153/963 - Train loss: 0.38639268  - Train acc: 0.8546 - Val loss: 0.00000000\n",
      "(0.04 min) Epoch 1/300 -- Iteration 162 - Batch 162/963 - Train loss: 0.38833464  - Train acc: 0.8534 - Val loss: 0.00000000\n",
      "(0.04 min) Epoch 1/300 -- Iteration 171 - Batch 171/963 - Train loss: 0.38874874  - Train acc: 0.8533 - Val loss: 0.00000000\n",
      "(0.05 min) Epoch 1/300 -- Iteration 180 - Batch 180/963 - Train loss: 0.38963190  - Train acc: 0.8532 - Val loss: 0.00000000\n",
      "(0.05 min) Epoch 1/300 -- Iteration 189 - Batch 189/963 - Train loss: 0.38923773  - Train acc: 0.8537 - Val loss: 0.00000000\n",
      "(0.05 min) Epoch 1/300 -- Iteration 198 - Batch 198/963 - Train loss: 0.38880017  - Train acc: 0.8542 - Val loss: 0.00000000\n",
      "(0.05 min) Epoch 1/300 -- Iteration 207 - Batch 207/963 - Train loss: 0.38842489  - Train acc: 0.8542 - Val loss: 0.00000000\n",
      "(0.05 min) Epoch 1/300 -- Iteration 216 - Batch 216/963 - Train loss: 0.38854433  - Train acc: 0.8542 - Val loss: 0.00000000\n",
      "(0.06 min) Epoch 1/300 -- Iteration 225 - Batch 225/963 - Train loss: 0.38929473  - Train acc: 0.8541 - Val loss: 0.00000000\n",
      "(0.06 min) Epoch 1/300 -- Iteration 234 - Batch 234/963 - Train loss: 0.38879509  - Train acc: 0.8540 - Val loss: 0.00000000\n",
      "(0.06 min) Epoch 1/300 -- Iteration 243 - Batch 243/963 - Train loss: 0.38808312  - Train acc: 0.8544 - Val loss: 0.00000000\n",
      "(0.06 min) Epoch 1/300 -- Iteration 252 - Batch 252/963 - Train loss: 0.38718693  - Train acc: 0.8548 - Val loss: 0.00000000\n",
      "(0.06 min) Epoch 1/300 -- Iteration 261 - Batch 261/963 - Train loss: 0.38788170  - Train acc: 0.8543 - Val loss: 0.00000000\n",
      "(0.07 min) Epoch 1/300 -- Iteration 270 - Batch 270/963 - Train loss: 0.38713628  - Train acc: 0.8547 - Val loss: 0.00000000\n",
      "(0.07 min) Epoch 1/300 -- Iteration 279 - Batch 279/963 - Train loss: 0.38662027  - Train acc: 0.8546 - Val loss: 0.00000000\n",
      "(0.07 min) Epoch 1/300 -- Iteration 288 - Batch 288/963 - Train loss: 0.38718572  - Train acc: 0.8546 - Val loss: 0.00000000\n",
      "(0.07 min) Epoch 1/300 -- Iteration 297 - Batch 297/963 - Train loss: 0.38646574  - Train acc: 0.8549 - Val loss: 0.00000000\n",
      "(0.07 min) Epoch 1/300 -- Iteration 306 - Batch 306/963 - Train loss: 0.38693903  - Train acc: 0.8546 - Val loss: 0.00000000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 315 - Batch 315/963 - Train loss: 0.38748161  - Train acc: 0.8544 - Val loss: 0.00000000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 324 - Batch 324/963 - Train loss: 0.38799462  - Train acc: 0.8543 - Val loss: 0.00000000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 333 - Batch 333/963 - Train loss: 0.38865923  - Train acc: 0.8539 - Val loss: 0.00000000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 342 - Batch 342/963 - Train loss: 0.39001245  - Train acc: 0.8532 - Val loss: 0.00000000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 351 - Batch 351/963 - Train loss: 0.38988247  - Train acc: 0.8530 - Val loss: 0.00000000\n",
      "(0.09 min) Epoch 1/300 -- Iteration 360 - Batch 360/963 - Train loss: 0.39032851  - Train acc: 0.8529 - Val loss: 0.00000000\n",
      "(0.09 min) Epoch 1/300 -- Iteration 369 - Batch 369/963 - Train loss: 0.39082275  - Train acc: 0.8526 - Val loss: 0.00000000\n",
      "(0.09 min) Epoch 1/300 -- Iteration 378 - Batch 378/963 - Train loss: 0.39107375  - Train acc: 0.8524 - Val loss: 0.00000000\n",
      "(0.09 min) Epoch 1/300 -- Iteration 387 - Batch 387/963 - Train loss: 0.39127669  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.09 min) Epoch 1/300 -- Iteration 396 - Batch 396/963 - Train loss: 0.39166342  - Train acc: 0.8521 - Val loss: 0.00000000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 405 - Batch 405/963 - Train loss: 0.39204352  - Train acc: 0.8519 - Val loss: 0.00000000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 414 - Batch 414/963 - Train loss: 0.39173224  - Train acc: 0.8518 - Val loss: 0.00000000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 423 - Batch 423/963 - Train loss: 0.39174493  - Train acc: 0.8519 - Val loss: 0.00000000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 432 - Batch 432/963 - Train loss: 0.39194963  - Train acc: 0.8518 - Val loss: 0.00000000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 441 - Batch 441/963 - Train loss: 0.39268580  - Train acc: 0.8516 - Val loss: 0.00000000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 450 - Batch 450/963 - Train loss: 0.39231604  - Train acc: 0.8519 - Val loss: 0.00000000\n",
      "(0.11 min) Epoch 1/300 -- Iteration 459 - Batch 459/963 - Train loss: 0.39247912  - Train acc: 0.8519 - Val loss: 0.00000000\n",
      "(0.11 min) Epoch 1/300 -- Iteration 468 - Batch 468/963 - Train loss: 0.39272100  - Train acc: 0.8519 - Val loss: 0.00000000\n",
      "(0.11 min) Epoch 1/300 -- Iteration 477 - Batch 477/963 - Train loss: 0.39216261  - Train acc: 0.8522 - Val loss: 0.00000000\n",
      "(0.11 min) Epoch 1/300 -- Iteration 486 - Batch 486/963 - Train loss: 0.39162638  - Train acc: 0.8525 - Val loss: 0.00000000\n",
      "(0.11 min) Epoch 1/300 -- Iteration 495 - Batch 495/963 - Train loss: 0.39191495  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.12 min) Epoch 1/300 -- Iteration 504 - Batch 504/963 - Train loss: 0.39156556  - Train acc: 0.8524 - Val loss: 0.00000000\n",
      "(0.12 min) Epoch 1/300 -- Iteration 513 - Batch 513/963 - Train loss: 0.39168681  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.12 min) Epoch 1/300 -- Iteration 522 - Batch 522/963 - Train loss: 0.39201881  - Train acc: 0.8520 - Val loss: 0.00000000\n",
      "(0.12 min) Epoch 1/300 -- Iteration 531 - Batch 531/963 - Train loss: 0.39211366  - Train acc: 0.8519 - Val loss: 0.00000000\n",
      "(0.12 min) Epoch 1/300 -- Iteration 540 - Batch 540/963 - Train loss: 0.39196756  - Train acc: 0.8520 - Val loss: 0.00000000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 549 - Batch 549/963 - Train loss: 0.39188271  - Train acc: 0.8521 - Val loss: 0.00000000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 558 - Batch 558/963 - Train loss: 0.39160470  - Train acc: 0.8521 - Val loss: 0.00000000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 567 - Batch 567/963 - Train loss: 0.39127127  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 576 - Batch 576/963 - Train loss: 0.39107312  - Train acc: 0.8522 - Val loss: 0.00000000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 585 - Batch 585/963 - Train loss: 0.39076804  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 594 - Batch 594/963 - Train loss: 0.39065017  - Train acc: 0.8525 - Val loss: 0.00000000\n",
      "(0.14 min) Epoch 1/300 -- Iteration 603 - Batch 603/963 - Train loss: 0.39062645  - Train acc: 0.8525 - Val loss: 0.00000000\n",
      "(0.14 min) Epoch 1/300 -- Iteration 612 - Batch 612/963 - Train loss: 0.39067418  - Train acc: 0.8524 - Val loss: 0.00000000\n",
      "(0.14 min) Epoch 1/300 -- Iteration 621 - Batch 621/963 - Train loss: 0.39095592  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.14 min) Epoch 1/300 -- Iteration 630 - Batch 630/963 - Train loss: 0.39137477  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.14 min) Epoch 1/300 -- Iteration 639 - Batch 639/963 - Train loss: 0.39156017  - Train acc: 0.8521 - Val loss: 0.00000000\n",
      "(0.15 min) Epoch 1/300 -- Iteration 648 - Batch 648/963 - Train loss: 0.39158346  - Train acc: 0.8521 - Val loss: 0.00000000\n",
      "(0.15 min) Epoch 1/300 -- Iteration 657 - Batch 657/963 - Train loss: 0.39126429  - Train acc: 0.8521 - Val loss: 0.00000000\n",
      "(0.15 min) Epoch 1/300 -- Iteration 666 - Batch 666/963 - Train loss: 0.39082578  - Train acc: 0.8522 - Val loss: 0.00000000\n",
      "(0.15 min) Epoch 1/300 -- Iteration 675 - Batch 675/963 - Train loss: 0.39070692  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.15 min) Epoch 1/300 -- Iteration 684 - Batch 684/963 - Train loss: 0.39103125  - Train acc: 0.8522 - Val loss: 0.00000000\n",
      "(0.16 min) Epoch 1/300 -- Iteration 693 - Batch 693/963 - Train loss: 0.39096716  - Train acc: 0.8522 - Val loss: 0.00000000\n",
      "(0.16 min) Epoch 1/300 -- Iteration 702 - Batch 702/963 - Train loss: 0.39106753  - Train acc: 0.8521 - Val loss: 0.00000000\n",
      "(0.16 min) Epoch 1/300 -- Iteration 711 - Batch 711/963 - Train loss: 0.39083641  - Train acc: 0.8522 - Val loss: 0.00000000\n",
      "(0.16 min) Epoch 1/300 -- Iteration 720 - Batch 720/963 - Train loss: 0.39062618  - Train acc: 0.8522 - Val loss: 0.00000000\n",
      "(0.16 min) Epoch 1/300 -- Iteration 729 - Batch 729/963 - Train loss: 0.39055302  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 738 - Batch 738/963 - Train loss: 0.39019635  - Train acc: 0.8524 - Val loss: 0.00000000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 747 - Batch 747/963 - Train loss: 0.39046931  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 756 - Batch 756/963 - Train loss: 0.39037439  - Train acc: 0.8523 - Val loss: 0.00000000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 765 - Batch 765/963 - Train loss: 0.39012164  - Train acc: 0.8524 - Val loss: 0.00000000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 774 - Batch 774/963 - Train loss: 0.39001749  - Train acc: 0.8525 - Val loss: 0.00000000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 783 - Batch 783/963 - Train loss: 0.38972884  - Train acc: 0.8525 - Val loss: 0.00000000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 792 - Batch 792/963 - Train loss: 0.38971225  - Train acc: 0.8525 - Val loss: 0.00000000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 801 - Batch 801/963 - Train loss: 0.38950110  - Train acc: 0.8525 - Val loss: 0.00000000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 810 - Batch 810/963 - Train loss: 0.38923000  - Train acc: 0.8526 - Val loss: 0.00000000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 819 - Batch 819/963 - Train loss: 0.38898524  - Train acc: 0.8527 - Val loss: 0.00000000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 828 - Batch 828/963 - Train loss: 0.38891451  - Train acc: 0.8528 - Val loss: 0.00000000\n",
      "(0.19 min) Epoch 1/300 -- Iteration 837 - Batch 837/963 - Train loss: 0.38850167  - Train acc: 0.8529 - Val loss: 0.00000000\n",
      "(0.19 min) Epoch 1/300 -- Iteration 846 - Batch 846/963 - Train loss: 0.38869545  - Train acc: 0.8528 - Val loss: 0.00000000\n",
      "(0.19 min) Epoch 1/300 -- Iteration 855 - Batch 855/963 - Train loss: 0.38857428  - Train acc: 0.8528 - Val loss: 0.00000000\n",
      "(0.19 min) Epoch 1/300 -- Iteration 864 - Batch 864/963 - Train loss: 0.38868411  - Train acc: 0.8528 - Val loss: 0.00000000\n",
      "(0.19 min) Epoch 1/300 -- Iteration 873 - Batch 873/963 - Train loss: 0.38884765  - Train acc: 0.8528 - Val loss: 0.00000000\n",
      "(0.20 min) Epoch 1/300 -- Iteration 882 - Batch 882/963 - Train loss: 0.38882914  - Train acc: 0.8528 - Val loss: 0.00000000\n",
      "(0.20 min) Epoch 1/300 -- Iteration 891 - Batch 891/963 - Train loss: 0.38908863  - Train acc: 0.8527 - Val loss: 0.00000000\n",
      "(0.20 min) Epoch 1/300 -- Iteration 900 - Batch 900/963 - Train loss: 0.38898915  - Train acc: 0.8529 - Val loss: 0.00000000\n",
      "(0.20 min) Epoch 1/300 -- Iteration 909 - Batch 909/963 - Train loss: 0.38873791  - Train acc: 0.8531 - Val loss: 0.00000000\n",
      "(0.20 min) Epoch 1/300 -- Iteration 918 - Batch 918/963 - Train loss: 0.38856556  - Train acc: 0.8531 - Val loss: 0.00000000\n",
      "(0.20 min) Epoch 1/300 -- Iteration 927 - Batch 927/963 - Train loss: 0.38860322  - Train acc: 0.8531 - Val loss: 0.00000000\n",
      "(0.21 min) Epoch 1/300 -- Iteration 936 - Batch 936/963 - Train loss: 0.38843871  - Train acc: 0.8531 - Val loss: 0.00000000\n",
      "(0.21 min) Epoch 1/300 -- Iteration 945 - Batch 945/963 - Train loss: 0.38843456  - Train acc: 0.8531 - Val loss: 0.00000000\n",
      "(0.21 min) Epoch 1/300 -- Iteration 954 - Batch 954/963 - Train loss: 0.38795832  - Train acc: 0.8533 - Val loss: 0.00000000\n",
      "(0.21 min) Epoch 1/300 -- Iteration 963 - Batch 962/963 - Train loss: 0.38806872  - Train acc: 0.8532 - Val loss: 1.31299937 - Val acc: 0.6017\n",
      "(0.21 min) Epoch 2/300 -- Iteration 972 - Batch 9/963 - Train loss: 0.37522862  - Train acc: 0.8492 - Val loss: 1.31299937\n",
      "(0.22 min) Epoch 2/300 -- Iteration 981 - Batch 18/963 - Train loss: 0.36580732  - Train acc: 0.8590 - Val loss: 1.31299937\n",
      "(0.22 min) Epoch 2/300 -- Iteration 990 - Batch 27/963 - Train loss: 0.37461491  - Train acc: 0.8527 - Val loss: 1.31299937\n",
      "(0.22 min) Epoch 2/300 -- Iteration 999 - Batch 36/963 - Train loss: 0.37121732  - Train acc: 0.8541 - Val loss: 1.31299937\n",
      "(0.22 min) Epoch 2/300 -- Iteration 1008 - Batch 45/963 - Train loss: 0.37197991  - Train acc: 0.8560 - Val loss: 1.31299937\n",
      "(0.22 min) Epoch 2/300 -- Iteration 1017 - Batch 54/963 - Train loss: 0.37158038  - Train acc: 0.8565 - Val loss: 1.31299937\n",
      "(0.23 min) Epoch 2/300 -- Iteration 1026 - Batch 63/963 - Train loss: 0.37235988  - Train acc: 0.8580 - Val loss: 1.31299937\n",
      "(0.23 min) Epoch 2/300 -- Iteration 1035 - Batch 72/963 - Train loss: 0.37516297  - Train acc: 0.8578 - Val loss: 1.31299937\n",
      "(0.23 min) Epoch 2/300 -- Iteration 1044 - Batch 81/963 - Train loss: 0.37554686  - Train acc: 0.8574 - Val loss: 1.31299937\n",
      "(0.23 min) Epoch 2/300 -- Iteration 1053 - Batch 90/963 - Train loss: 0.37755523  - Train acc: 0.8565 - Val loss: 1.31299937\n",
      "(0.23 min) Epoch 2/300 -- Iteration 1062 - Batch 99/963 - Train loss: 0.37800515  - Train acc: 0.8562 - Val loss: 1.31299937\n",
      "(0.24 min) Epoch 2/300 -- Iteration 1071 - Batch 108/963 - Train loss: 0.37689576  - Train acc: 0.8569 - Val loss: 1.31299937\n",
      "(0.24 min) Epoch 2/300 -- Iteration 1080 - Batch 117/963 - Train loss: 0.37939927  - Train acc: 0.8559 - Val loss: 1.31299937\n",
      "(0.24 min) Epoch 2/300 -- Iteration 1089 - Batch 126/963 - Train loss: 0.37905914  - Train acc: 0.8559 - Val loss: 1.31299937\n",
      "(0.24 min) Epoch 2/300 -- Iteration 1098 - Batch 135/963 - Train loss: 0.37917508  - Train acc: 0.8559 - Val loss: 1.31299937\n",
      "(0.24 min) Epoch 2/300 -- Iteration 1107 - Batch 144/963 - Train loss: 0.37795786  - Train acc: 0.8566 - Val loss: 1.31299937\n",
      "(0.25 min) Epoch 2/300 -- Iteration 1116 - Batch 153/963 - Train loss: 0.37901841  - Train acc: 0.8567 - Val loss: 1.31299937\n",
      "(0.25 min) Epoch 2/300 -- Iteration 1125 - Batch 162/963 - Train loss: 0.38027007  - Train acc: 0.8559 - Val loss: 1.31299937\n",
      "(0.25 min) Epoch 2/300 -- Iteration 1134 - Batch 171/963 - Train loss: 0.38019287  - Train acc: 0.8560 - Val loss: 1.31299937\n",
      "(0.25 min) Epoch 2/300 -- Iteration 1143 - Batch 180/963 - Train loss: 0.38130206  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.25 min) Epoch 2/300 -- Iteration 1152 - Batch 189/963 - Train loss: 0.38176003  - Train acc: 0.8551 - Val loss: 1.31299937\n",
      "(0.25 min) Epoch 2/300 -- Iteration 1161 - Batch 198/963 - Train loss: 0.38139172  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.26 min) Epoch 2/300 -- Iteration 1170 - Batch 207/963 - Train loss: 0.38217178  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.26 min) Epoch 2/300 -- Iteration 1179 - Batch 216/963 - Train loss: 0.38230871  - Train acc: 0.8551 - Val loss: 1.31299937\n",
      "(0.26 min) Epoch 2/300 -- Iteration 1188 - Batch 225/963 - Train loss: 0.38220622  - Train acc: 0.8548 - Val loss: 1.31299937\n",
      "(0.26 min) Epoch 2/300 -- Iteration 1197 - Batch 234/963 - Train loss: 0.38218879  - Train acc: 0.8548 - Val loss: 1.31299937\n",
      "(0.26 min) Epoch 2/300 -- Iteration 1206 - Batch 243/963 - Train loss: 0.38189509  - Train acc: 0.8552 - Val loss: 1.31299937\n",
      "(0.27 min) Epoch 2/300 -- Iteration 1215 - Batch 252/963 - Train loss: 0.38266983  - Train acc: 0.8547 - Val loss: 1.31299937\n",
      "(0.27 min) Epoch 2/300 -- Iteration 1224 - Batch 261/963 - Train loss: 0.38197810  - Train acc: 0.8549 - Val loss: 1.31299937\n",
      "(0.27 min) Epoch 2/300 -- Iteration 1233 - Batch 270/963 - Train loss: 0.38168079  - Train acc: 0.8550 - Val loss: 1.31299937\n",
      "(0.27 min) Epoch 2/300 -- Iteration 1242 - Batch 279/963 - Train loss: 0.38087329  - Train acc: 0.8553 - Val loss: 1.31299937\n",
      "(0.27 min) Epoch 2/300 -- Iteration 1251 - Batch 288/963 - Train loss: 0.38053850  - Train acc: 0.8552 - Val loss: 1.31299937\n",
      "(0.27 min) Epoch 2/300 -- Iteration 1260 - Batch 297/963 - Train loss: 0.38145833  - Train acc: 0.8547 - Val loss: 1.31299937\n",
      "(0.28 min) Epoch 2/300 -- Iteration 1269 - Batch 306/963 - Train loss: 0.38165868  - Train acc: 0.8546 - Val loss: 1.31299937\n",
      "(0.28 min) Epoch 2/300 -- Iteration 1278 - Batch 315/963 - Train loss: 0.38259756  - Train acc: 0.8544 - Val loss: 1.31299937\n",
      "(0.28 min) Epoch 2/300 -- Iteration 1287 - Batch 324/963 - Train loss: 0.38255162  - Train acc: 0.8545 - Val loss: 1.31299937\n",
      "(0.28 min) Epoch 2/300 -- Iteration 1296 - Batch 333/963 - Train loss: 0.38259467  - Train acc: 0.8544 - Val loss: 1.31299937\n",
      "(0.28 min) Epoch 2/300 -- Iteration 1305 - Batch 342/963 - Train loss: 0.38247586  - Train acc: 0.8545 - Val loss: 1.31299937\n",
      "(0.29 min) Epoch 2/300 -- Iteration 1314 - Batch 351/963 - Train loss: 0.38236600  - Train acc: 0.8544 - Val loss: 1.31299937\n",
      "(0.29 min) Epoch 2/300 -- Iteration 1323 - Batch 360/963 - Train loss: 0.38204188  - Train acc: 0.8545 - Val loss: 1.31299937\n",
      "(0.29 min) Epoch 2/300 -- Iteration 1332 - Batch 369/963 - Train loss: 0.38200955  - Train acc: 0.8548 - Val loss: 1.31299937\n",
      "(0.29 min) Epoch 2/300 -- Iteration 1341 - Batch 378/963 - Train loss: 0.38227595  - Train acc: 0.8544 - Val loss: 1.31299937\n",
      "(0.29 min) Epoch 2/300 -- Iteration 1350 - Batch 387/963 - Train loss: 0.38224638  - Train acc: 0.8544 - Val loss: 1.31299937\n",
      "(0.29 min) Epoch 2/300 -- Iteration 1359 - Batch 396/963 - Train loss: 0.38308488  - Train acc: 0.8541 - Val loss: 1.31299937\n",
      "(0.30 min) Epoch 2/300 -- Iteration 1368 - Batch 405/963 - Train loss: 0.38264268  - Train acc: 0.8543 - Val loss: 1.31299937\n",
      "(0.30 min) Epoch 2/300 -- Iteration 1377 - Batch 414/963 - Train loss: 0.38273327  - Train acc: 0.8543 - Val loss: 1.31299937\n",
      "(0.30 min) Epoch 2/300 -- Iteration 1386 - Batch 423/963 - Train loss: 0.38267397  - Train acc: 0.8543 - Val loss: 1.31299937\n",
      "(0.30 min) Epoch 2/300 -- Iteration 1395 - Batch 432/963 - Train loss: 0.38251305  - Train acc: 0.8543 - Val loss: 1.31299937\n",
      "(0.30 min) Epoch 2/300 -- Iteration 1404 - Batch 441/963 - Train loss: 0.38276413  - Train acc: 0.8543 - Val loss: 1.31299937\n",
      "(0.31 min) Epoch 2/300 -- Iteration 1413 - Batch 450/963 - Train loss: 0.38244345  - Train acc: 0.8546 - Val loss: 1.31299937\n",
      "(0.31 min) Epoch 2/300 -- Iteration 1422 - Batch 459/963 - Train loss: 0.38265426  - Train acc: 0.8546 - Val loss: 1.31299937\n",
      "(0.31 min) Epoch 2/300 -- Iteration 1431 - Batch 468/963 - Train loss: 0.38240828  - Train acc: 0.8548 - Val loss: 1.31299937\n",
      "(0.31 min) Epoch 2/300 -- Iteration 1440 - Batch 477/963 - Train loss: 0.38185987  - Train acc: 0.8551 - Val loss: 1.31299937\n",
      "(0.31 min) Epoch 2/300 -- Iteration 1449 - Batch 486/963 - Train loss: 0.38200024  - Train acc: 0.8549 - Val loss: 1.31299937\n",
      "(0.31 min) Epoch 2/300 -- Iteration 1458 - Batch 495/963 - Train loss: 0.38215763  - Train acc: 0.8549 - Val loss: 1.31299937\n",
      "(0.32 min) Epoch 2/300 -- Iteration 1467 - Batch 504/963 - Train loss: 0.38236074  - Train acc: 0.8548 - Val loss: 1.31299937\n",
      "(0.32 min) Epoch 2/300 -- Iteration 1476 - Batch 513/963 - Train loss: 0.38227487  - Train acc: 0.8548 - Val loss: 1.31299937\n",
      "(0.32 min) Epoch 2/300 -- Iteration 1485 - Batch 522/963 - Train loss: 0.38304072  - Train acc: 0.8544 - Val loss: 1.31299937\n",
      "(0.32 min) Epoch 2/300 -- Iteration 1494 - Batch 531/963 - Train loss: 0.38297640  - Train acc: 0.8543 - Val loss: 1.31299937\n",
      "(0.32 min) Epoch 2/300 -- Iteration 1503 - Batch 540/963 - Train loss: 0.38237322  - Train acc: 0.8547 - Val loss: 1.31299937\n",
      "(0.33 min) Epoch 2/300 -- Iteration 1512 - Batch 549/963 - Train loss: 0.38232492  - Train acc: 0.8548 - Val loss: 1.31299937\n",
      "(0.33 min) Epoch 2/300 -- Iteration 1521 - Batch 558/963 - Train loss: 0.38191613  - Train acc: 0.8550 - Val loss: 1.31299937\n",
      "(0.33 min) Epoch 2/300 -- Iteration 1530 - Batch 567/963 - Train loss: 0.38224461  - Train acc: 0.8550 - Val loss: 1.31299937\n",
      "(0.33 min) Epoch 2/300 -- Iteration 1539 - Batch 576/963 - Train loss: 0.38222311  - Train acc: 0.8550 - Val loss: 1.31299937\n",
      "(0.33 min) Epoch 2/300 -- Iteration 1548 - Batch 585/963 - Train loss: 0.38204088  - Train acc: 0.8552 - Val loss: 1.31299937\n",
      "(0.33 min) Epoch 2/300 -- Iteration 1557 - Batch 594/963 - Train loss: 0.38176769  - Train acc: 0.8552 - Val loss: 1.31299937\n",
      "(0.34 min) Epoch 2/300 -- Iteration 1566 - Batch 603/963 - Train loss: 0.38152430  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.34 min) Epoch 2/300 -- Iteration 1575 - Batch 612/963 - Train loss: 0.38153387  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.34 min) Epoch 2/300 -- Iteration 1584 - Batch 621/963 - Train loss: 0.38163328  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.34 min) Epoch 2/300 -- Iteration 1593 - Batch 630/963 - Train loss: 0.38168280  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.34 min) Epoch 2/300 -- Iteration 1602 - Batch 639/963 - Train loss: 0.38205602  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.35 min) Epoch 2/300 -- Iteration 1611 - Batch 648/963 - Train loss: 0.38193071  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.35 min) Epoch 2/300 -- Iteration 1620 - Batch 657/963 - Train loss: 0.38175033  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.35 min) Epoch 2/300 -- Iteration 1629 - Batch 666/963 - Train loss: 0.38143893  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.35 min) Epoch 2/300 -- Iteration 1638 - Batch 675/963 - Train loss: 0.38148978  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.35 min) Epoch 2/300 -- Iteration 1647 - Batch 684/963 - Train loss: 0.38166088  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.35 min) Epoch 2/300 -- Iteration 1656 - Batch 693/963 - Train loss: 0.38192010  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.36 min) Epoch 2/300 -- Iteration 1665 - Batch 702/963 - Train loss: 0.38133375  - Train acc: 0.8558 - Val loss: 1.31299937\n",
      "(0.36 min) Epoch 2/300 -- Iteration 1674 - Batch 711/963 - Train loss: 0.38170051  - Train acc: 0.8558 - Val loss: 1.31299937\n",
      "(0.36 min) Epoch 2/300 -- Iteration 1683 - Batch 720/963 - Train loss: 0.38187358  - Train acc: 0.8558 - Val loss: 1.31299937\n",
      "(0.36 min) Epoch 2/300 -- Iteration 1692 - Batch 729/963 - Train loss: 0.38237907  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.36 min) Epoch 2/300 -- Iteration 1701 - Batch 738/963 - Train loss: 0.38228729  - Train acc: 0.8557 - Val loss: 1.31299937\n",
      "(0.37 min) Epoch 2/300 -- Iteration 1710 - Batch 747/963 - Train loss: 0.38258413  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.37 min) Epoch 2/300 -- Iteration 1719 - Batch 756/963 - Train loss: 0.38280601  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.37 min) Epoch 2/300 -- Iteration 1728 - Batch 765/963 - Train loss: 0.38260260  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.37 min) Epoch 2/300 -- Iteration 1737 - Batch 774/963 - Train loss: 0.38249597  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.37 min) Epoch 2/300 -- Iteration 1746 - Batch 783/963 - Train loss: 0.38247839  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.37 min) Epoch 2/300 -- Iteration 1755 - Batch 792/963 - Train loss: 0.38270700  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.38 min) Epoch 2/300 -- Iteration 1764 - Batch 801/963 - Train loss: 0.38253231  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.38 min) Epoch 2/300 -- Iteration 1773 - Batch 810/963 - Train loss: 0.38276705  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.38 min) Epoch 2/300 -- Iteration 1782 - Batch 819/963 - Train loss: 0.38271157  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.38 min) Epoch 2/300 -- Iteration 1791 - Batch 828/963 - Train loss: 0.38261555  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.38 min) Epoch 2/300 -- Iteration 1800 - Batch 837/963 - Train loss: 0.38246392  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.39 min) Epoch 2/300 -- Iteration 1809 - Batch 846/963 - Train loss: 0.38250486  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.39 min) Epoch 2/300 -- Iteration 1818 - Batch 855/963 - Train loss: 0.38260292  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.39 min) Epoch 2/300 -- Iteration 1827 - Batch 864/963 - Train loss: 0.38248951  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.39 min) Epoch 2/300 -- Iteration 1836 - Batch 873/963 - Train loss: 0.38249055  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.39 min) Epoch 2/300 -- Iteration 1845 - Batch 882/963 - Train loss: 0.38229475  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.39 min) Epoch 2/300 -- Iteration 1854 - Batch 891/963 - Train loss: 0.38216326  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.40 min) Epoch 2/300 -- Iteration 1863 - Batch 900/963 - Train loss: 0.38203893  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.40 min) Epoch 2/300 -- Iteration 1872 - Batch 909/963 - Train loss: 0.38222702  - Train acc: 0.8552 - Val loss: 1.31299937\n",
      "(0.40 min) Epoch 2/300 -- Iteration 1881 - Batch 918/963 - Train loss: 0.38201404  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.40 min) Epoch 2/300 -- Iteration 1890 - Batch 927/963 - Train loss: 0.38222419  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.40 min) Epoch 2/300 -- Iteration 1899 - Batch 936/963 - Train loss: 0.38226874  - Train acc: 0.8554 - Val loss: 1.31299937\n",
      "(0.41 min) Epoch 2/300 -- Iteration 1908 - Batch 945/963 - Train loss: 0.38206102  - Train acc: 0.8555 - Val loss: 1.31299937\n",
      "(0.41 min) Epoch 2/300 -- Iteration 1917 - Batch 954/963 - Train loss: 0.38217086  - Train acc: 0.8556 - Val loss: 1.31299937\n",
      "(0.41 min) Epoch 2/300 -- Iteration 1926 - Batch 962/963 - Train loss: 0.38192789  - Train acc: 0.8556 - Val loss: 1.29812074 - Val acc: 0.6083\n",
      "(0.41 min) Epoch 3/300 -- Iteration 1935 - Batch 9/963 - Train loss: 0.36976851  - Train acc: 0.8508 - Val loss: 1.29812074\n",
      "(0.41 min) Epoch 3/300 -- Iteration 1944 - Batch 18/963 - Train loss: 0.37053461  - Train acc: 0.8540 - Val loss: 1.29812074\n",
      "(0.41 min) Epoch 3/300 -- Iteration 1953 - Batch 27/963 - Train loss: 0.36881346  - Train acc: 0.8546 - Val loss: 1.29812074\n",
      "(0.42 min) Epoch 3/300 -- Iteration 1962 - Batch 36/963 - Train loss: 0.37582393  - Train acc: 0.8541 - Val loss: 1.29812074\n",
      "(0.42 min) Epoch 3/300 -- Iteration 1971 - Batch 45/963 - Train loss: 0.37956222  - Train acc: 0.8541 - Val loss: 1.29812074\n",
      "(0.42 min) Epoch 3/300 -- Iteration 1980 - Batch 54/963 - Train loss: 0.37743167  - Train acc: 0.8554 - Val loss: 1.29812074\n",
      "(0.42 min) Epoch 3/300 -- Iteration 1989 - Batch 63/963 - Train loss: 0.38132803  - Train acc: 0.8535 - Val loss: 1.29812074\n",
      "(0.42 min) Epoch 3/300 -- Iteration 1998 - Batch 72/963 - Train loss: 0.38323813  - Train acc: 0.8533 - Val loss: 1.29812074\n",
      "(0.43 min) Epoch 3/300 -- Iteration 2007 - Batch 81/963 - Train loss: 0.38513954  - Train acc: 0.8518 - Val loss: 1.29812074\n",
      "(0.43 min) Epoch 3/300 -- Iteration 2016 - Batch 90/963 - Train loss: 0.38615060  - Train acc: 0.8521 - Val loss: 1.29812074\n",
      "(0.43 min) Epoch 3/300 -- Iteration 2025 - Batch 99/963 - Train loss: 0.38228019  - Train acc: 0.8544 - Val loss: 1.29812074\n",
      "(0.43 min) Epoch 3/300 -- Iteration 2034 - Batch 108/963 - Train loss: 0.38276379  - Train acc: 0.8537 - Val loss: 1.29812074\n",
      "(0.43 min) Epoch 3/300 -- Iteration 2043 - Batch 117/963 - Train loss: 0.38221158  - Train acc: 0.8540 - Val loss: 1.29812074\n",
      "(0.44 min) Epoch 3/300 -- Iteration 2052 - Batch 126/963 - Train loss: 0.38385058  - Train acc: 0.8537 - Val loss: 1.29812074\n",
      "(0.44 min) Epoch 3/300 -- Iteration 2061 - Batch 135/963 - Train loss: 0.38186305  - Train acc: 0.8552 - Val loss: 1.29812074\n",
      "(0.44 min) Epoch 3/300 -- Iteration 2070 - Batch 144/963 - Train loss: 0.38050014  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.44 min) Epoch 3/300 -- Iteration 2079 - Batch 153/963 - Train loss: 0.38005265  - Train acc: 0.8555 - Val loss: 1.29812074\n",
      "(0.44 min) Epoch 3/300 -- Iteration 2088 - Batch 162/963 - Train loss: 0.38265940  - Train acc: 0.8539 - Val loss: 1.29812074\n",
      "(0.44 min) Epoch 3/300 -- Iteration 2097 - Batch 171/963 - Train loss: 0.38245094  - Train acc: 0.8538 - Val loss: 1.29812074\n",
      "(0.45 min) Epoch 3/300 -- Iteration 2106 - Batch 180/963 - Train loss: 0.38284214  - Train acc: 0.8539 - Val loss: 1.29812074\n",
      "(0.45 min) Epoch 3/300 -- Iteration 2115 - Batch 189/963 - Train loss: 0.38294565  - Train acc: 0.8542 - Val loss: 1.29812074\n",
      "(0.45 min) Epoch 3/300 -- Iteration 2124 - Batch 198/963 - Train loss: 0.38140888  - Train acc: 0.8551 - Val loss: 1.29812074\n",
      "(0.45 min) Epoch 3/300 -- Iteration 2133 - Batch 207/963 - Train loss: 0.38137372  - Train acc: 0.8553 - Val loss: 1.29812074\n",
      "(0.45 min) Epoch 3/300 -- Iteration 2142 - Batch 216/963 - Train loss: 0.38158371  - Train acc: 0.8552 - Val loss: 1.29812074\n",
      "(0.46 min) Epoch 3/300 -- Iteration 2151 - Batch 225/963 - Train loss: 0.38123127  - Train acc: 0.8558 - Val loss: 1.29812074\n",
      "(0.46 min) Epoch 3/300 -- Iteration 2160 - Batch 234/963 - Train loss: 0.38103162  - Train acc: 0.8560 - Val loss: 1.29812074\n",
      "(0.46 min) Epoch 3/300 -- Iteration 2169 - Batch 243/963 - Train loss: 0.38047322  - Train acc: 0.8565 - Val loss: 1.29812074\n",
      "(0.46 min) Epoch 3/300 -- Iteration 2178 - Batch 252/963 - Train loss: 0.38058352  - Train acc: 0.8561 - Val loss: 1.29812074\n",
      "(0.46 min) Epoch 3/300 -- Iteration 2187 - Batch 261/963 - Train loss: 0.38080509  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.46 min) Epoch 3/300 -- Iteration 2196 - Batch 270/963 - Train loss: 0.38067952  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.47 min) Epoch 3/300 -- Iteration 2205 - Batch 279/963 - Train loss: 0.37998910  - Train acc: 0.8564 - Val loss: 1.29812074\n",
      "(0.47 min) Epoch 3/300 -- Iteration 2214 - Batch 288/963 - Train loss: 0.37958688  - Train acc: 0.8566 - Val loss: 1.29812074\n",
      "(0.47 min) Epoch 3/300 -- Iteration 2223 - Batch 297/963 - Train loss: 0.38038056  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.47 min) Epoch 3/300 -- Iteration 2232 - Batch 306/963 - Train loss: 0.37974516  - Train acc: 0.8565 - Val loss: 1.29812074\n",
      "(0.47 min) Epoch 3/300 -- Iteration 2241 - Batch 315/963 - Train loss: 0.38030173  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.48 min) Epoch 3/300 -- Iteration 2250 - Batch 324/963 - Train loss: 0.38116402  - Train acc: 0.8556 - Val loss: 1.29812074\n",
      "(0.48 min) Epoch 3/300 -- Iteration 2259 - Batch 333/963 - Train loss: 0.38120205  - Train acc: 0.8555 - Val loss: 1.29812074\n",
      "(0.48 min) Epoch 3/300 -- Iteration 2268 - Batch 342/963 - Train loss: 0.38073125  - Train acc: 0.8556 - Val loss: 1.29812074\n",
      "(0.48 min) Epoch 3/300 -- Iteration 2277 - Batch 351/963 - Train loss: 0.38042872  - Train acc: 0.8557 - Val loss: 1.29812074\n",
      "(0.48 min) Epoch 3/300 -- Iteration 2286 - Batch 360/963 - Train loss: 0.37969557  - Train acc: 0.8558 - Val loss: 1.29812074\n",
      "(0.48 min) Epoch 3/300 -- Iteration 2295 - Batch 369/963 - Train loss: 0.37880487  - Train acc: 0.8560 - Val loss: 1.29812074\n",
      "(0.49 min) Epoch 3/300 -- Iteration 2304 - Batch 378/963 - Train loss: 0.37876068  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.49 min) Epoch 3/300 -- Iteration 2313 - Batch 387/963 - Train loss: 0.37807481  - Train acc: 0.8564 - Val loss: 1.29812074\n",
      "(0.49 min) Epoch 3/300 -- Iteration 2322 - Batch 396/963 - Train loss: 0.37809662  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.49 min) Epoch 3/300 -- Iteration 2331 - Batch 405/963 - Train loss: 0.37756037  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.49 min) Epoch 3/300 -- Iteration 2340 - Batch 414/963 - Train loss: 0.37717290  - Train acc: 0.8565 - Val loss: 1.29812074\n",
      "(0.49 min) Epoch 3/300 -- Iteration 2349 - Batch 423/963 - Train loss: 0.37726910  - Train acc: 0.8565 - Val loss: 1.29812074\n",
      "(0.50 min) Epoch 3/300 -- Iteration 2358 - Batch 432/963 - Train loss: 0.37758866  - Train acc: 0.8565 - Val loss: 1.29812074\n",
      "(0.50 min) Epoch 3/300 -- Iteration 2367 - Batch 441/963 - Train loss: 0.37745058  - Train acc: 0.8565 - Val loss: 1.29812074\n",
      "(0.50 min) Epoch 3/300 -- Iteration 2376 - Batch 450/963 - Train loss: 0.37755724  - Train acc: 0.8564 - Val loss: 1.29812074\n",
      "(0.50 min) Epoch 3/300 -- Iteration 2385 - Batch 459/963 - Train loss: 0.37754950  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.50 min) Epoch 3/300 -- Iteration 2394 - Batch 468/963 - Train loss: 0.37785732  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.51 min) Epoch 3/300 -- Iteration 2403 - Batch 477/963 - Train loss: 0.37730836  - Train acc: 0.8561 - Val loss: 1.29812074\n",
      "(0.51 min) Epoch 3/300 -- Iteration 2412 - Batch 486/963 - Train loss: 0.37832110  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.51 min) Epoch 3/300 -- Iteration 2421 - Batch 495/963 - Train loss: 0.37827992  - Train acc: 0.8557 - Val loss: 1.29812074\n",
      "(0.51 min) Epoch 3/300 -- Iteration 2430 - Batch 504/963 - Train loss: 0.37808975  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.51 min) Epoch 3/300 -- Iteration 2439 - Batch 513/963 - Train loss: 0.37871207  - Train acc: 0.8557 - Val loss: 1.29812074\n",
      "(0.52 min) Epoch 3/300 -- Iteration 2448 - Batch 522/963 - Train loss: 0.37840057  - Train acc: 0.8556 - Val loss: 1.29812074\n",
      "(0.52 min) Epoch 3/300 -- Iteration 2457 - Batch 531/963 - Train loss: 0.37856653  - Train acc: 0.8555 - Val loss: 1.29812074\n",
      "(0.52 min) Epoch 3/300 -- Iteration 2466 - Batch 540/963 - Train loss: 0.37877544  - Train acc: 0.8553 - Val loss: 1.29812074\n",
      "(0.52 min) Epoch 3/300 -- Iteration 2475 - Batch 549/963 - Train loss: 0.37913887  - Train acc: 0.8550 - Val loss: 1.29812074\n",
      "(0.52 min) Epoch 3/300 -- Iteration 2484 - Batch 558/963 - Train loss: 0.37950338  - Train acc: 0.8547 - Val loss: 1.29812074\n",
      "(0.52 min) Epoch 3/300 -- Iteration 2493 - Batch 567/963 - Train loss: 0.37922271  - Train acc: 0.8548 - Val loss: 1.29812074\n",
      "(0.53 min) Epoch 3/300 -- Iteration 2502 - Batch 576/963 - Train loss: 0.37900137  - Train acc: 0.8549 - Val loss: 1.29812074\n",
      "(0.53 min) Epoch 3/300 -- Iteration 2511 - Batch 585/963 - Train loss: 0.37895567  - Train acc: 0.8550 - Val loss: 1.29812074\n",
      "(0.53 min) Epoch 3/300 -- Iteration 2520 - Batch 594/963 - Train loss: 0.37898247  - Train acc: 0.8550 - Val loss: 1.29812074\n",
      "(0.53 min) Epoch 3/300 -- Iteration 2529 - Batch 603/963 - Train loss: 0.37885160  - Train acc: 0.8552 - Val loss: 1.29812074\n",
      "(0.53 min) Epoch 3/300 -- Iteration 2538 - Batch 612/963 - Train loss: 0.37893244  - Train acc: 0.8553 - Val loss: 1.29812074\n",
      "(0.54 min) Epoch 3/300 -- Iteration 2547 - Batch 621/963 - Train loss: 0.37891595  - Train acc: 0.8555 - Val loss: 1.29812074\n",
      "(0.54 min) Epoch 3/300 -- Iteration 2556 - Batch 630/963 - Train loss: 0.37841556  - Train acc: 0.8558 - Val loss: 1.29812074\n",
      "(0.54 min) Epoch 3/300 -- Iteration 2565 - Batch 639/963 - Train loss: 0.37849557  - Train acc: 0.8557 - Val loss: 1.29812074\n",
      "(0.54 min) Epoch 3/300 -- Iteration 2574 - Batch 648/963 - Train loss: 0.37851973  - Train acc: 0.8558 - Val loss: 1.29812074\n",
      "(0.54 min) Epoch 3/300 -- Iteration 2583 - Batch 657/963 - Train loss: 0.37829343  - Train acc: 0.8560 - Val loss: 1.29812074\n",
      "(0.54 min) Epoch 3/300 -- Iteration 2592 - Batch 666/963 - Train loss: 0.37856614  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.55 min) Epoch 3/300 -- Iteration 2601 - Batch 675/963 - Train loss: 0.37847822  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.55 min) Epoch 3/300 -- Iteration 2610 - Batch 684/963 - Train loss: 0.37900121  - Train acc: 0.8558 - Val loss: 1.29812074\n",
      "(0.55 min) Epoch 3/300 -- Iteration 2619 - Batch 693/963 - Train loss: 0.37881671  - Train acc: 0.8561 - Val loss: 1.29812074\n",
      "(0.55 min) Epoch 3/300 -- Iteration 2628 - Batch 702/963 - Train loss: 0.37887843  - Train acc: 0.8561 - Val loss: 1.29812074\n",
      "(0.55 min) Epoch 3/300 -- Iteration 2637 - Batch 711/963 - Train loss: 0.37885010  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.56 min) Epoch 3/300 -- Iteration 2646 - Batch 720/963 - Train loss: 0.37915729  - Train acc: 0.8560 - Val loss: 1.29812074\n",
      "(0.56 min) Epoch 3/300 -- Iteration 2655 - Batch 729/963 - Train loss: 0.37920249  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.56 min) Epoch 3/300 -- Iteration 2664 - Batch 738/963 - Train loss: 0.37861305  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.56 min) Epoch 3/300 -- Iteration 2673 - Batch 747/963 - Train loss: 0.37898140  - Train acc: 0.8561 - Val loss: 1.29812074\n",
      "(0.56 min) Epoch 3/300 -- Iteration 2682 - Batch 756/963 - Train loss: 0.37881712  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.56 min) Epoch 3/300 -- Iteration 2691 - Batch 765/963 - Train loss: 0.37866754  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.57 min) Epoch 3/300 -- Iteration 2700 - Batch 774/963 - Train loss: 0.37849136  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.57 min) Epoch 3/300 -- Iteration 2709 - Batch 783/963 - Train loss: 0.37861833  - Train acc: 0.8561 - Val loss: 1.29812074\n",
      "(0.57 min) Epoch 3/300 -- Iteration 2718 - Batch 792/963 - Train loss: 0.37861631  - Train acc: 0.8561 - Val loss: 1.29812074\n",
      "(0.57 min) Epoch 3/300 -- Iteration 2727 - Batch 801/963 - Train loss: 0.37885759  - Train acc: 0.8560 - Val loss: 1.29812074\n",
      "(0.57 min) Epoch 3/300 -- Iteration 2736 - Batch 810/963 - Train loss: 0.37903684  - Train acc: 0.8558 - Val loss: 1.29812074\n",
      "(0.58 min) Epoch 3/300 -- Iteration 2745 - Batch 819/963 - Train loss: 0.37878090  - Train acc: 0.8559 - Val loss: 1.29812074\n",
      "(0.58 min) Epoch 3/300 -- Iteration 2754 - Batch 828/963 - Train loss: 0.37853454  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.58 min) Epoch 3/300 -- Iteration 2763 - Batch 837/963 - Train loss: 0.37816311  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.58 min) Epoch 3/300 -- Iteration 2772 - Batch 846/963 - Train loss: 0.37859577  - Train acc: 0.8561 - Val loss: 1.29812074\n",
      "(0.58 min) Epoch 3/300 -- Iteration 2781 - Batch 855/963 - Train loss: 0.37844854  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.58 min) Epoch 3/300 -- Iteration 2790 - Batch 864/963 - Train loss: 0.37828241  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.59 min) Epoch 3/300 -- Iteration 2799 - Batch 873/963 - Train loss: 0.37824652  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.59 min) Epoch 3/300 -- Iteration 2808 - Batch 882/963 - Train loss: 0.37836102  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.59 min) Epoch 3/300 -- Iteration 2817 - Batch 891/963 - Train loss: 0.37821620  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.59 min) Epoch 3/300 -- Iteration 2826 - Batch 900/963 - Train loss: 0.37807108  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.59 min) Epoch 3/300 -- Iteration 2835 - Batch 909/963 - Train loss: 0.37798980  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.60 min) Epoch 3/300 -- Iteration 2844 - Batch 918/963 - Train loss: 0.37823060  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.60 min) Epoch 3/300 -- Iteration 2853 - Batch 927/963 - Train loss: 0.37827209  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.60 min) Epoch 3/300 -- Iteration 2862 - Batch 936/963 - Train loss: 0.37851061  - Train acc: 0.8561 - Val loss: 1.29812074\n",
      "(0.60 min) Epoch 3/300 -- Iteration 2871 - Batch 945/963 - Train loss: 0.37839920  - Train acc: 0.8562 - Val loss: 1.29812074\n",
      "(0.60 min) Epoch 3/300 -- Iteration 2880 - Batch 954/963 - Train loss: 0.37811826  - Train acc: 0.8563 - Val loss: 1.29812074\n",
      "(0.60 min) Epoch 3/300 -- Iteration 2889 - Batch 962/963 - Train loss: 0.37794898  - Train acc: 0.8564 - Val loss: 1.28778195 - Val acc: 0.6083\n",
      "(0.61 min) Epoch 4/300 -- Iteration 2898 - Batch 9/963 - Train loss: 0.36919544  - Train acc: 0.8555 - Val loss: 1.28778195\n",
      "(0.61 min) Epoch 4/300 -- Iteration 2907 - Batch 18/963 - Train loss: 0.37384756  - Train acc: 0.8561 - Val loss: 1.28778195\n",
      "(0.61 min) Epoch 4/300 -- Iteration 2916 - Batch 27/963 - Train loss: 0.36551020  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.61 min) Epoch 4/300 -- Iteration 2925 - Batch 36/963 - Train loss: 0.36702362  - Train acc: 0.8592 - Val loss: 1.28778195\n",
      "(0.61 min) Epoch 4/300 -- Iteration 2934 - Batch 45/963 - Train loss: 0.36571797  - Train acc: 0.8601 - Val loss: 1.28778195\n",
      "(0.62 min) Epoch 4/300 -- Iteration 2943 - Batch 54/963 - Train loss: 0.36978779  - Train acc: 0.8592 - Val loss: 1.28778195\n",
      "(0.62 min) Epoch 4/300 -- Iteration 2952 - Batch 63/963 - Train loss: 0.37214493  - Train acc: 0.8588 - Val loss: 1.28778195\n",
      "(0.62 min) Epoch 4/300 -- Iteration 2961 - Batch 72/963 - Train loss: 0.37279811  - Train acc: 0.8584 - Val loss: 1.28778195\n",
      "(0.62 min) Epoch 4/300 -- Iteration 2970 - Batch 81/963 - Train loss: 0.37557967  - Train acc: 0.8577 - Val loss: 1.28778195\n",
      "(0.62 min) Epoch 4/300 -- Iteration 2979 - Batch 90/963 - Train loss: 0.37923341  - Train acc: 0.8574 - Val loss: 1.28778195\n",
      "(0.63 min) Epoch 4/300 -- Iteration 2988 - Batch 99/963 - Train loss: 0.37749562  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.63 min) Epoch 4/300 -- Iteration 2997 - Batch 108/963 - Train loss: 0.37756118  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.63 min) Epoch 4/300 -- Iteration 3006 - Batch 117/963 - Train loss: 0.38020392  - Train acc: 0.8572 - Val loss: 1.28778195\n",
      "(0.63 min) Epoch 4/300 -- Iteration 3015 - Batch 126/963 - Train loss: 0.38059331  - Train acc: 0.8569 - Val loss: 1.28778195\n",
      "(0.63 min) Epoch 4/300 -- Iteration 3024 - Batch 135/963 - Train loss: 0.37976423  - Train acc: 0.8572 - Val loss: 1.28778195\n",
      "(0.63 min) Epoch 4/300 -- Iteration 3033 - Batch 144/963 - Train loss: 0.37942746  - Train acc: 0.8573 - Val loss: 1.28778195\n",
      "(0.64 min) Epoch 4/300 -- Iteration 3042 - Batch 153/963 - Train loss: 0.37946356  - Train acc: 0.8575 - Val loss: 1.28778195\n",
      "(0.64 min) Epoch 4/300 -- Iteration 3051 - Batch 162/963 - Train loss: 0.37956299  - Train acc: 0.8574 - Val loss: 1.28778195\n",
      "(0.64 min) Epoch 4/300 -- Iteration 3060 - Batch 171/963 - Train loss: 0.37833975  - Train acc: 0.8578 - Val loss: 1.28778195\n",
      "(0.64 min) Epoch 4/300 -- Iteration 3069 - Batch 180/963 - Train loss: 0.37844294  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.64 min) Epoch 4/300 -- Iteration 3078 - Batch 189/963 - Train loss: 0.37955113  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.65 min) Epoch 4/300 -- Iteration 3087 - Batch 198/963 - Train loss: 0.38058867  - Train acc: 0.8573 - Val loss: 1.28778195\n",
      "(0.65 min) Epoch 4/300 -- Iteration 3096 - Batch 207/963 - Train loss: 0.37900984  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.65 min) Epoch 4/300 -- Iteration 3105 - Batch 216/963 - Train loss: 0.37841262  - Train acc: 0.8583 - Val loss: 1.28778195\n",
      "(0.65 min) Epoch 4/300 -- Iteration 3114 - Batch 225/963 - Train loss: 0.37843155  - Train acc: 0.8584 - Val loss: 1.28778195\n",
      "(0.65 min) Epoch 4/300 -- Iteration 3123 - Batch 234/963 - Train loss: 0.37887138  - Train acc: 0.8583 - Val loss: 1.28778195\n",
      "(0.65 min) Epoch 4/300 -- Iteration 3132 - Batch 243/963 - Train loss: 0.37969235  - Train acc: 0.8577 - Val loss: 1.28778195\n",
      "(0.66 min) Epoch 4/300 -- Iteration 3141 - Batch 252/963 - Train loss: 0.37914147  - Train acc: 0.8576 - Val loss: 1.28778195\n",
      "(0.66 min) Epoch 4/300 -- Iteration 3150 - Batch 261/963 - Train loss: 0.37856918  - Train acc: 0.8577 - Val loss: 1.28778195\n",
      "(0.66 min) Epoch 4/300 -- Iteration 3159 - Batch 270/963 - Train loss: 0.37812926  - Train acc: 0.8577 - Val loss: 1.28778195\n",
      "(0.66 min) Epoch 4/300 -- Iteration 3168 - Batch 279/963 - Train loss: 0.37841835  - Train acc: 0.8576 - Val loss: 1.28778195\n",
      "(0.66 min) Epoch 4/300 -- Iteration 3177 - Batch 288/963 - Train loss: 0.37830426  - Train acc: 0.8576 - Val loss: 1.28778195\n",
      "(0.67 min) Epoch 4/300 -- Iteration 3186 - Batch 297/963 - Train loss: 0.37833264  - Train acc: 0.8576 - Val loss: 1.28778195\n",
      "(0.67 min) Epoch 4/300 -- Iteration 3195 - Batch 306/963 - Train loss: 0.37773231  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.67 min) Epoch 4/300 -- Iteration 3204 - Batch 315/963 - Train loss: 0.37806297  - Train acc: 0.8578 - Val loss: 1.28778195\n",
      "(0.67 min) Epoch 4/300 -- Iteration 3213 - Batch 324/963 - Train loss: 0.37722108  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.67 min) Epoch 4/300 -- Iteration 3222 - Batch 333/963 - Train loss: 0.37711456  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.67 min) Epoch 4/300 -- Iteration 3231 - Batch 342/963 - Train loss: 0.37775277  - Train acc: 0.8578 - Val loss: 1.28778195\n",
      "(0.68 min) Epoch 4/300 -- Iteration 3240 - Batch 351/963 - Train loss: 0.37627965  - Train acc: 0.8586 - Val loss: 1.28778195\n",
      "(0.68 min) Epoch 4/300 -- Iteration 3249 - Batch 360/963 - Train loss: 0.37647001  - Train acc: 0.8585 - Val loss: 1.28778195\n",
      "(0.68 min) Epoch 4/300 -- Iteration 3258 - Batch 369/963 - Train loss: 0.37595589  - Train acc: 0.8587 - Val loss: 1.28778195\n",
      "(0.68 min) Epoch 4/300 -- Iteration 3267 - Batch 378/963 - Train loss: 0.37514064  - Train acc: 0.8589 - Val loss: 1.28778195\n",
      "(0.68 min) Epoch 4/300 -- Iteration 3276 - Batch 387/963 - Train loss: 0.37480550  - Train acc: 0.8589 - Val loss: 1.28778195\n",
      "(0.69 min) Epoch 4/300 -- Iteration 3285 - Batch 396/963 - Train loss: 0.37426270  - Train acc: 0.8590 - Val loss: 1.28778195\n",
      "(0.69 min) Epoch 4/300 -- Iteration 3294 - Batch 405/963 - Train loss: 0.37441218  - Train acc: 0.8588 - Val loss: 1.28778195\n",
      "(0.69 min) Epoch 4/300 -- Iteration 3303 - Batch 414/963 - Train loss: 0.37371245  - Train acc: 0.8591 - Val loss: 1.28778195\n",
      "(0.69 min) Epoch 4/300 -- Iteration 3312 - Batch 423/963 - Train loss: 0.37418181  - Train acc: 0.8586 - Val loss: 1.28778195\n",
      "(0.69 min) Epoch 4/300 -- Iteration 3321 - Batch 432/963 - Train loss: 0.37400205  - Train acc: 0.8587 - Val loss: 1.28778195\n",
      "(0.69 min) Epoch 4/300 -- Iteration 3330 - Batch 441/963 - Train loss: 0.37381402  - Train acc: 0.8589 - Val loss: 1.28778195\n",
      "(0.70 min) Epoch 4/300 -- Iteration 3339 - Batch 450/963 - Train loss: 0.37455118  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.70 min) Epoch 4/300 -- Iteration 3348 - Batch 459/963 - Train loss: 0.37512576  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.70 min) Epoch 4/300 -- Iteration 3357 - Batch 468/963 - Train loss: 0.37528435  - Train acc: 0.8578 - Val loss: 1.28778195\n",
      "(0.70 min) Epoch 4/300 -- Iteration 3366 - Batch 477/963 - Train loss: 0.37548753  - Train acc: 0.8578 - Val loss: 1.28778195\n",
      "(0.70 min) Epoch 4/300 -- Iteration 3375 - Batch 486/963 - Train loss: 0.37516395  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.71 min) Epoch 4/300 -- Iteration 3384 - Batch 495/963 - Train loss: 0.37442162  - Train acc: 0.8583 - Val loss: 1.28778195\n",
      "(0.71 min) Epoch 4/300 -- Iteration 3393 - Batch 504/963 - Train loss: 0.37456296  - Train acc: 0.8583 - Val loss: 1.28778195\n",
      "(0.71 min) Epoch 4/300 -- Iteration 3402 - Batch 513/963 - Train loss: 0.37420589  - Train acc: 0.8586 - Val loss: 1.28778195\n",
      "(0.71 min) Epoch 4/300 -- Iteration 3411 - Batch 522/963 - Train loss: 0.37440435  - Train acc: 0.8584 - Val loss: 1.28778195\n",
      "(0.71 min) Epoch 4/300 -- Iteration 3420 - Batch 531/963 - Train loss: 0.37429631  - Train acc: 0.8584 - Val loss: 1.28778195\n",
      "(0.71 min) Epoch 4/300 -- Iteration 3429 - Batch 540/963 - Train loss: 0.37454400  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.72 min) Epoch 4/300 -- Iteration 3438 - Batch 549/963 - Train loss: 0.37459589  - Train acc: 0.8583 - Val loss: 1.28778195\n",
      "(0.72 min) Epoch 4/300 -- Iteration 3447 - Batch 558/963 - Train loss: 0.37490767  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.72 min) Epoch 4/300 -- Iteration 3456 - Batch 567/963 - Train loss: 0.37534447  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.72 min) Epoch 4/300 -- Iteration 3465 - Batch 576/963 - Train loss: 0.37570021  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.72 min) Epoch 4/300 -- Iteration 3474 - Batch 585/963 - Train loss: 0.37602648  - Train acc: 0.8576 - Val loss: 1.28778195\n",
      "(0.73 min) Epoch 4/300 -- Iteration 3483 - Batch 594/963 - Train loss: 0.37606818  - Train acc: 0.8577 - Val loss: 1.28778195\n",
      "(0.73 min) Epoch 4/300 -- Iteration 3492 - Batch 603/963 - Train loss: 0.37601211  - Train acc: 0.8578 - Val loss: 1.28778195\n",
      "(0.73 min) Epoch 4/300 -- Iteration 3501 - Batch 612/963 - Train loss: 0.37559426  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.73 min) Epoch 4/300 -- Iteration 3510 - Batch 621/963 - Train loss: 0.37538608  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.73 min) Epoch 4/300 -- Iteration 3519 - Batch 630/963 - Train loss: 0.37544934  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.73 min) Epoch 4/300 -- Iteration 3528 - Batch 639/963 - Train loss: 0.37559447  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.74 min) Epoch 4/300 -- Iteration 3537 - Batch 648/963 - Train loss: 0.37572068  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.74 min) Epoch 4/300 -- Iteration 3546 - Batch 657/963 - Train loss: 0.37553482  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.74 min) Epoch 4/300 -- Iteration 3555 - Batch 666/963 - Train loss: 0.37508997  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.74 min) Epoch 4/300 -- Iteration 3564 - Batch 675/963 - Train loss: 0.37494056  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.74 min) Epoch 4/300 -- Iteration 3573 - Batch 684/963 - Train loss: 0.37527720  - Train acc: 0.8578 - Val loss: 1.28778195\n",
      "(0.75 min) Epoch 4/300 -- Iteration 3582 - Batch 693/963 - Train loss: 0.37500167  - Train acc: 0.8579 - Val loss: 1.28778195\n",
      "(0.75 min) Epoch 4/300 -- Iteration 3591 - Batch 702/963 - Train loss: 0.37484006  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.75 min) Epoch 4/300 -- Iteration 3600 - Batch 711/963 - Train loss: 0.37479267  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.75 min) Epoch 4/300 -- Iteration 3609 - Batch 720/963 - Train loss: 0.37493095  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.75 min) Epoch 4/300 -- Iteration 3618 - Batch 729/963 - Train loss: 0.37466335  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.75 min) Epoch 4/300 -- Iteration 3627 - Batch 738/963 - Train loss: 0.37469857  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.76 min) Epoch 4/300 -- Iteration 3636 - Batch 747/963 - Train loss: 0.37442593  - Train acc: 0.8583 - Val loss: 1.28778195\n",
      "(0.76 min) Epoch 4/300 -- Iteration 3645 - Batch 756/963 - Train loss: 0.37406162  - Train acc: 0.8584 - Val loss: 1.28778195\n",
      "(0.76 min) Epoch 4/300 -- Iteration 3654 - Batch 765/963 - Train loss: 0.37421776  - Train acc: 0.8584 - Val loss: 1.28778195\n",
      "(0.76 min) Epoch 4/300 -- Iteration 3663 - Batch 774/963 - Train loss: 0.37399156  - Train acc: 0.8585 - Val loss: 1.28778195\n",
      "(0.76 min) Epoch 4/300 -- Iteration 3672 - Batch 783/963 - Train loss: 0.37367816  - Train acc: 0.8587 - Val loss: 1.28778195\n",
      "(0.77 min) Epoch 4/300 -- Iteration 3681 - Batch 792/963 - Train loss: 0.37378320  - Train acc: 0.8585 - Val loss: 1.28778195\n",
      "(0.77 min) Epoch 4/300 -- Iteration 3690 - Batch 801/963 - Train loss: 0.37408483  - Train acc: 0.8584 - Val loss: 1.28778195\n",
      "(0.77 min) Epoch 4/300 -- Iteration 3699 - Batch 810/963 - Train loss: 0.37453690  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.77 min) Epoch 4/300 -- Iteration 3708 - Batch 819/963 - Train loss: 0.37467896  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.77 min) Epoch 4/300 -- Iteration 3717 - Batch 828/963 - Train loss: 0.37440447  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.77 min) Epoch 4/300 -- Iteration 3726 - Batch 837/963 - Train loss: 0.37409527  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.78 min) Epoch 4/300 -- Iteration 3735 - Batch 846/963 - Train loss: 0.37370643  - Train acc: 0.8584 - Val loss: 1.28778195\n",
      "(0.78 min) Epoch 4/300 -- Iteration 3744 - Batch 855/963 - Train loss: 0.37385337  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.78 min) Epoch 4/300 -- Iteration 3753 - Batch 864/963 - Train loss: 0.37410760  - Train acc: 0.8581 - Val loss: 1.28778195\n",
      "(0.78 min) Epoch 4/300 -- Iteration 3762 - Batch 873/963 - Train loss: 0.37439582  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.78 min) Epoch 4/300 -- Iteration 3771 - Batch 882/963 - Train loss: 0.37403595  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.79 min) Epoch 4/300 -- Iteration 3780 - Batch 891/963 - Train loss: 0.37374055  - Train acc: 0.8583 - Val loss: 1.28778195\n",
      "(0.79 min) Epoch 4/300 -- Iteration 3789 - Batch 900/963 - Train loss: 0.37375131  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.79 min) Epoch 4/300 -- Iteration 3798 - Batch 909/963 - Train loss: 0.37372001  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.79 min) Epoch 4/300 -- Iteration 3807 - Batch 918/963 - Train loss: 0.37375308  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.79 min) Epoch 4/300 -- Iteration 3816 - Batch 927/963 - Train loss: 0.37371274  - Train acc: 0.8582 - Val loss: 1.28778195\n",
      "(0.79 min) Epoch 4/300 -- Iteration 3825 - Batch 936/963 - Train loss: 0.37402539  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.80 min) Epoch 4/300 -- Iteration 3834 - Batch 945/963 - Train loss: 0.37416208  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.80 min) Epoch 4/300 -- Iteration 3843 - Batch 954/963 - Train loss: 0.37419169  - Train acc: 0.8580 - Val loss: 1.28778195\n",
      "(0.80 min) Epoch 4/300 -- Iteration 3852 - Batch 962/963 - Train loss: 0.37425784  - Train acc: 0.8580 - Val loss: 1.27371466 - Val acc: 0.6067\n",
      "(0.80 min) Epoch 5/300 -- Iteration 3861 - Batch 9/963 - Train loss: 0.36860805  - Train acc: 0.8570 - Val loss: 1.27371466\n",
      "(0.80 min) Epoch 5/300 -- Iteration 3870 - Batch 18/963 - Train loss: 0.36240887  - Train acc: 0.8590 - Val loss: 1.27371466\n",
      "(0.81 min) Epoch 5/300 -- Iteration 3879 - Batch 27/963 - Train loss: 0.36396974  - Train acc: 0.8627 - Val loss: 1.27371466\n",
      "(0.81 min) Epoch 5/300 -- Iteration 3888 - Batch 36/963 - Train loss: 0.36923058  - Train acc: 0.8609 - Val loss: 1.27371466\n",
      "(0.81 min) Epoch 5/300 -- Iteration 3897 - Batch 45/963 - Train loss: 0.36802769  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.81 min) Epoch 5/300 -- Iteration 3906 - Batch 54/963 - Train loss: 0.37193837  - Train acc: 0.8588 - Val loss: 1.27371466\n",
      "(0.81 min) Epoch 5/300 -- Iteration 3915 - Batch 63/963 - Train loss: 0.37352022  - Train acc: 0.8582 - Val loss: 1.27371466\n",
      "(0.82 min) Epoch 5/300 -- Iteration 3924 - Batch 72/963 - Train loss: 0.37682650  - Train acc: 0.8570 - Val loss: 1.27371466\n",
      "(0.82 min) Epoch 5/300 -- Iteration 3933 - Batch 81/963 - Train loss: 0.37090563  - Train acc: 0.8599 - Val loss: 1.27371466\n",
      "(0.82 min) Epoch 5/300 -- Iteration 3942 - Batch 90/963 - Train loss: 0.36939333  - Train acc: 0.8606 - Val loss: 1.27371466\n",
      "(0.82 min) Epoch 5/300 -- Iteration 3951 - Batch 99/963 - Train loss: 0.36805633  - Train acc: 0.8607 - Val loss: 1.27371466\n",
      "(0.82 min) Epoch 5/300 -- Iteration 3960 - Batch 108/963 - Train loss: 0.36956322  - Train acc: 0.8600 - Val loss: 1.27371466\n",
      "(0.82 min) Epoch 5/300 -- Iteration 3969 - Batch 117/963 - Train loss: 0.37011819  - Train acc: 0.8599 - Val loss: 1.27371466\n",
      "(0.83 min) Epoch 5/300 -- Iteration 3978 - Batch 126/963 - Train loss: 0.37094563  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.83 min) Epoch 5/300 -- Iteration 3987 - Batch 135/963 - Train loss: 0.37092484  - Train acc: 0.8591 - Val loss: 1.27371466\n",
      "(0.83 min) Epoch 5/300 -- Iteration 3996 - Batch 144/963 - Train loss: 0.36998678  - Train acc: 0.8597 - Val loss: 1.27371466\n",
      "(0.83 min) Epoch 5/300 -- Iteration 4005 - Batch 153/963 - Train loss: 0.37061075  - Train acc: 0.8589 - Val loss: 1.27371466\n",
      "(0.83 min) Epoch 5/300 -- Iteration 4014 - Batch 162/963 - Train loss: 0.37099933  - Train acc: 0.8590 - Val loss: 1.27371466\n",
      "(0.84 min) Epoch 5/300 -- Iteration 4023 - Batch 171/963 - Train loss: 0.37153217  - Train acc: 0.8588 - Val loss: 1.27371466\n",
      "(0.84 min) Epoch 5/300 -- Iteration 4032 - Batch 180/963 - Train loss: 0.37134215  - Train acc: 0.8592 - Val loss: 1.27371466\n",
      "(0.84 min) Epoch 5/300 -- Iteration 4041 - Batch 189/963 - Train loss: 0.37284870  - Train acc: 0.8585 - Val loss: 1.27371466\n",
      "(0.84 min) Epoch 5/300 -- Iteration 4050 - Batch 198/963 - Train loss: 0.37274182  - Train acc: 0.8585 - Val loss: 1.27371466\n",
      "(0.84 min) Epoch 5/300 -- Iteration 4059 - Batch 207/963 - Train loss: 0.37310171  - Train acc: 0.8588 - Val loss: 1.27371466\n",
      "(0.84 min) Epoch 5/300 -- Iteration 4068 - Batch 216/963 - Train loss: 0.37216531  - Train acc: 0.8595 - Val loss: 1.27371466\n",
      "(0.85 min) Epoch 5/300 -- Iteration 4077 - Batch 225/963 - Train loss: 0.37148289  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.85 min) Epoch 5/300 -- Iteration 4086 - Batch 234/963 - Train loss: 0.37152215  - Train acc: 0.8593 - Val loss: 1.27371466\n",
      "(0.85 min) Epoch 5/300 -- Iteration 4095 - Batch 243/963 - Train loss: 0.37128921  - Train acc: 0.8592 - Val loss: 1.27371466\n",
      "(0.85 min) Epoch 5/300 -- Iteration 4104 - Batch 252/963 - Train loss: 0.37116555  - Train acc: 0.8590 - Val loss: 1.27371466\n",
      "(0.85 min) Epoch 5/300 -- Iteration 4113 - Batch 261/963 - Train loss: 0.37099087  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.86 min) Epoch 5/300 -- Iteration 4122 - Batch 270/963 - Train loss: 0.37084416  - Train acc: 0.8595 - Val loss: 1.27371466\n",
      "(0.86 min) Epoch 5/300 -- Iteration 4131 - Batch 279/963 - Train loss: 0.37134907  - Train acc: 0.8592 - Val loss: 1.27371466\n",
      "(0.86 min) Epoch 5/300 -- Iteration 4140 - Batch 288/963 - Train loss: 0.37176854  - Train acc: 0.8591 - Val loss: 1.27371466\n",
      "(0.86 min) Epoch 5/300 -- Iteration 4149 - Batch 297/963 - Train loss: 0.37121998  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.86 min) Epoch 5/300 -- Iteration 4158 - Batch 306/963 - Train loss: 0.37118366  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.86 min) Epoch 5/300 -- Iteration 4167 - Batch 315/963 - Train loss: 0.37180155  - Train acc: 0.8593 - Val loss: 1.27371466\n",
      "(0.87 min) Epoch 5/300 -- Iteration 4176 - Batch 324/963 - Train loss: 0.37154030  - Train acc: 0.8593 - Val loss: 1.27371466\n",
      "(0.87 min) Epoch 5/300 -- Iteration 4185 - Batch 333/963 - Train loss: 0.37202361  - Train acc: 0.8592 - Val loss: 1.27371466\n",
      "(0.87 min) Epoch 5/300 -- Iteration 4194 - Batch 342/963 - Train loss: 0.37300475  - Train acc: 0.8585 - Val loss: 1.27371466\n",
      "(0.87 min) Epoch 5/300 -- Iteration 4203 - Batch 351/963 - Train loss: 0.37322237  - Train acc: 0.8582 - Val loss: 1.27371466\n",
      "(0.87 min) Epoch 5/300 -- Iteration 4212 - Batch 360/963 - Train loss: 0.37300604  - Train acc: 0.8584 - Val loss: 1.27371466\n",
      "(0.88 min) Epoch 5/300 -- Iteration 4221 - Batch 369/963 - Train loss: 0.37284123  - Train acc: 0.8584 - Val loss: 1.27371466\n",
      "(0.88 min) Epoch 5/300 -- Iteration 4230 - Batch 378/963 - Train loss: 0.37262144  - Train acc: 0.8584 - Val loss: 1.27371466\n",
      "(0.88 min) Epoch 5/300 -- Iteration 4239 - Batch 387/963 - Train loss: 0.37220127  - Train acc: 0.8586 - Val loss: 1.27371466\n",
      "(0.88 min) Epoch 5/300 -- Iteration 4248 - Batch 396/963 - Train loss: 0.37216798  - Train acc: 0.8582 - Val loss: 1.27371466\n",
      "(0.88 min) Epoch 5/300 -- Iteration 4257 - Batch 405/963 - Train loss: 0.37276654  - Train acc: 0.8581 - Val loss: 1.27371466\n",
      "(0.88 min) Epoch 5/300 -- Iteration 4266 - Batch 414/963 - Train loss: 0.37249766  - Train acc: 0.8584 - Val loss: 1.27371466\n",
      "(0.89 min) Epoch 5/300 -- Iteration 4275 - Batch 423/963 - Train loss: 0.37188839  - Train acc: 0.8585 - Val loss: 1.27371466\n",
      "(0.89 min) Epoch 5/300 -- Iteration 4284 - Batch 432/963 - Train loss: 0.37141060  - Train acc: 0.8589 - Val loss: 1.27371466\n",
      "(0.89 min) Epoch 5/300 -- Iteration 4293 - Batch 441/963 - Train loss: 0.37192617  - Train acc: 0.8588 - Val loss: 1.27371466\n",
      "(0.89 min) Epoch 5/300 -- Iteration 4302 - Batch 450/963 - Train loss: 0.37247194  - Train acc: 0.8586 - Val loss: 1.27371466\n",
      "(0.89 min) Epoch 5/300 -- Iteration 4311 - Batch 459/963 - Train loss: 0.37250168  - Train acc: 0.8586 - Val loss: 1.27371466\n",
      "(0.90 min) Epoch 5/300 -- Iteration 4320 - Batch 468/963 - Train loss: 0.37225834  - Train acc: 0.8588 - Val loss: 1.27371466\n",
      "(0.90 min) Epoch 5/300 -- Iteration 4329 - Batch 477/963 - Train loss: 0.37227145  - Train acc: 0.8588 - Val loss: 1.27371466\n",
      "(0.90 min) Epoch 5/300 -- Iteration 4338 - Batch 486/963 - Train loss: 0.37231842  - Train acc: 0.8587 - Val loss: 1.27371466\n",
      "(0.90 min) Epoch 5/300 -- Iteration 4347 - Batch 495/963 - Train loss: 0.37243328  - Train acc: 0.8586 - Val loss: 1.27371466\n",
      "(0.90 min) Epoch 5/300 -- Iteration 4356 - Batch 504/963 - Train loss: 0.37251338  - Train acc: 0.8585 - Val loss: 1.27371466\n",
      "(0.90 min) Epoch 5/300 -- Iteration 4365 - Batch 513/963 - Train loss: 0.37246211  - Train acc: 0.8586 - Val loss: 1.27371466\n",
      "(0.91 min) Epoch 5/300 -- Iteration 4374 - Batch 522/963 - Train loss: 0.37221485  - Train acc: 0.8588 - Val loss: 1.27371466\n",
      "(0.91 min) Epoch 5/300 -- Iteration 4383 - Batch 531/963 - Train loss: 0.37178877  - Train acc: 0.8590 - Val loss: 1.27371466\n",
      "(0.91 min) Epoch 5/300 -- Iteration 4392 - Batch 540/963 - Train loss: 0.37183896  - Train acc: 0.8592 - Val loss: 1.27371466\n",
      "(0.91 min) Epoch 5/300 -- Iteration 4401 - Batch 549/963 - Train loss: 0.37196127  - Train acc: 0.8592 - Val loss: 1.27371466\n",
      "(0.91 min) Epoch 5/300 -- Iteration 4410 - Batch 558/963 - Train loss: 0.37190199  - Train acc: 0.8592 - Val loss: 1.27371466\n",
      "(0.92 min) Epoch 5/300 -- Iteration 4419 - Batch 567/963 - Train loss: 0.37206068  - Train acc: 0.8590 - Val loss: 1.27371466\n",
      "(0.92 min) Epoch 5/300 -- Iteration 4428 - Batch 576/963 - Train loss: 0.37256862  - Train acc: 0.8588 - Val loss: 1.27371466\n",
      "(0.92 min) Epoch 5/300 -- Iteration 4437 - Batch 585/963 - Train loss: 0.37223044  - Train acc: 0.8591 - Val loss: 1.27371466\n",
      "(0.92 min) Epoch 5/300 -- Iteration 4446 - Batch 594/963 - Train loss: 0.37176517  - Train acc: 0.8592 - Val loss: 1.27371466\n",
      "(0.92 min) Epoch 5/300 -- Iteration 4455 - Batch 603/963 - Train loss: 0.37158722  - Train acc: 0.8592 - Val loss: 1.27371466\n",
      "(0.93 min) Epoch 5/300 -- Iteration 4464 - Batch 612/963 - Train loss: 0.37132098  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.93 min) Epoch 5/300 -- Iteration 4473 - Batch 621/963 - Train loss: 0.37165916  - Train acc: 0.8593 - Val loss: 1.27371466\n",
      "(0.93 min) Epoch 5/300 -- Iteration 4482 - Batch 630/963 - Train loss: 0.37171080  - Train acc: 0.8591 - Val loss: 1.27371466\n",
      "(0.93 min) Epoch 5/300 -- Iteration 4491 - Batch 639/963 - Train loss: 0.37183766  - Train acc: 0.8589 - Val loss: 1.27371466\n",
      "(0.93 min) Epoch 5/300 -- Iteration 4500 - Batch 648/963 - Train loss: 0.37162842  - Train acc: 0.8591 - Val loss: 1.27371466\n",
      "(0.93 min) Epoch 5/300 -- Iteration 4509 - Batch 657/963 - Train loss: 0.37174954  - Train acc: 0.8589 - Val loss: 1.27371466\n",
      "(0.94 min) Epoch 5/300 -- Iteration 4518 - Batch 666/963 - Train loss: 0.37169128  - Train acc: 0.8590 - Val loss: 1.27371466\n",
      "(0.94 min) Epoch 5/300 -- Iteration 4527 - Batch 675/963 - Train loss: 0.37184153  - Train acc: 0.8590 - Val loss: 1.27371466\n",
      "(0.94 min) Epoch 5/300 -- Iteration 4536 - Batch 684/963 - Train loss: 0.37126758  - Train acc: 0.8593 - Val loss: 1.27371466\n",
      "(0.94 min) Epoch 5/300 -- Iteration 4545 - Batch 693/963 - Train loss: 0.37121421  - Train acc: 0.8595 - Val loss: 1.27371466\n",
      "(0.94 min) Epoch 5/300 -- Iteration 4554 - Batch 702/963 - Train loss: 0.37118239  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.95 min) Epoch 5/300 -- Iteration 4563 - Batch 711/963 - Train loss: 0.37098037  - Train acc: 0.8595 - Val loss: 1.27371466\n",
      "(0.95 min) Epoch 5/300 -- Iteration 4572 - Batch 720/963 - Train loss: 0.37102472  - Train acc: 0.8595 - Val loss: 1.27371466\n",
      "(0.95 min) Epoch 5/300 -- Iteration 4581 - Batch 729/963 - Train loss: 0.37076600  - Train acc: 0.8595 - Val loss: 1.27371466\n",
      "(0.95 min) Epoch 5/300 -- Iteration 4590 - Batch 738/963 - Train loss: 0.37110916  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.95 min) Epoch 5/300 -- Iteration 4599 - Batch 747/963 - Train loss: 0.37092489  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.95 min) Epoch 5/300 -- Iteration 4608 - Batch 756/963 - Train loss: 0.37079682  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.96 min) Epoch 5/300 -- Iteration 4617 - Batch 765/963 - Train loss: 0.37074608  - Train acc: 0.8595 - Val loss: 1.27371466\n",
      "(0.96 min) Epoch 5/300 -- Iteration 4626 - Batch 774/963 - Train loss: 0.37071369  - Train acc: 0.8595 - Val loss: 1.27371466\n",
      "(0.96 min) Epoch 5/300 -- Iteration 4635 - Batch 783/963 - Train loss: 0.37057136  - Train acc: 0.8596 - Val loss: 1.27371466\n",
      "(0.96 min) Epoch 5/300 -- Iteration 4644 - Batch 792/963 - Train loss: 0.37054042  - Train acc: 0.8596 - Val loss: 1.27371466\n",
      "(0.96 min) Epoch 5/300 -- Iteration 4653 - Batch 801/963 - Train loss: 0.37029762  - Train acc: 0.8597 - Val loss: 1.27371466\n",
      "(0.97 min) Epoch 5/300 -- Iteration 4662 - Batch 810/963 - Train loss: 0.37032895  - Train acc: 0.8597 - Val loss: 1.27371466\n",
      "(0.97 min) Epoch 5/300 -- Iteration 4671 - Batch 819/963 - Train loss: 0.37043473  - Train acc: 0.8596 - Val loss: 1.27371466\n",
      "(0.97 min) Epoch 5/300 -- Iteration 4680 - Batch 828/963 - Train loss: 0.37087909  - Train acc: 0.8594 - Val loss: 1.27371466\n",
      "(0.97 min) Epoch 5/300 -- Iteration 4689 - Batch 837/963 - Train loss: 0.37061988  - Train acc: 0.8596 - Val loss: 1.27371466\n",
      "(0.97 min) Epoch 5/300 -- Iteration 4698 - Batch 846/963 - Train loss: 0.37034254  - Train acc: 0.8597 - Val loss: 1.27371466\n",
      "(0.97 min) Epoch 5/300 -- Iteration 4707 - Batch 855/963 - Train loss: 0.37036088  - Train acc: 0.8596 - Val loss: 1.27371466\n",
      "(0.98 min) Epoch 5/300 -- Iteration 4716 - Batch 864/963 - Train loss: 0.37037506  - Train acc: 0.8596 - Val loss: 1.27371466\n",
      "(0.98 min) Epoch 5/300 -- Iteration 4725 - Batch 873/963 - Train loss: 0.37031545  - Train acc: 0.8597 - Val loss: 1.27371466\n",
      "(0.98 min) Epoch 5/300 -- Iteration 4734 - Batch 882/963 - Train loss: 0.36996232  - Train acc: 0.8599 - Val loss: 1.27371466\n",
      "(0.98 min) Epoch 5/300 -- Iteration 4743 - Batch 891/963 - Train loss: 0.36947714  - Train acc: 0.8602 - Val loss: 1.27371466\n",
      "(0.98 min) Epoch 5/300 -- Iteration 4752 - Batch 900/963 - Train loss: 0.36961805  - Train acc: 0.8600 - Val loss: 1.27371466\n",
      "(0.99 min) Epoch 5/300 -- Iteration 4761 - Batch 909/963 - Train loss: 0.36952277  - Train acc: 0.8600 - Val loss: 1.27371466\n",
      "(0.99 min) Epoch 5/300 -- Iteration 4770 - Batch 918/963 - Train loss: 0.36921777  - Train acc: 0.8602 - Val loss: 1.27371466\n",
      "(0.99 min) Epoch 5/300 -- Iteration 4779 - Batch 927/963 - Train loss: 0.36905618  - Train acc: 0.8603 - Val loss: 1.27371466\n",
      "(0.99 min) Epoch 5/300 -- Iteration 4788 - Batch 936/963 - Train loss: 0.36869293  - Train acc: 0.8604 - Val loss: 1.27371466\n",
      "(0.99 min) Epoch 5/300 -- Iteration 4797 - Batch 945/963 - Train loss: 0.36881724  - Train acc: 0.8604 - Val loss: 1.27371466\n",
      "(0.99 min) Epoch 5/300 -- Iteration 4806 - Batch 954/963 - Train loss: 0.36870526  - Train acc: 0.8604 - Val loss: 1.27371466\n",
      "(1.00 min) Epoch 5/300 -- Iteration 4815 - Batch 962/963 - Train loss: 0.36865322  - Train acc: 0.8604 - Val loss: 1.26834238 - Val acc: 0.6083\n",
      "(1.00 min) Epoch 6/300 -- Iteration 4824 - Batch 9/963 - Train loss: 0.34341063  - Train acc: 0.8719 - Val loss: 1.26834238\n",
      "(1.00 min) Epoch 6/300 -- Iteration 4833 - Batch 18/963 - Train loss: 0.36958244  - Train acc: 0.8606 - Val loss: 1.26834238\n",
      "(1.00 min) Epoch 6/300 -- Iteration 4842 - Batch 27/963 - Train loss: 0.37025885  - Train acc: 0.8588 - Val loss: 1.26834238\n",
      "(1.00 min) Epoch 6/300 -- Iteration 4851 - Batch 36/963 - Train loss: 0.36949689  - Train acc: 0.8602 - Val loss: 1.26834238\n",
      "(1.01 min) Epoch 6/300 -- Iteration 4860 - Batch 45/963 - Train loss: 0.36897928  - Train acc: 0.8599 - Val loss: 1.26834238\n",
      "(1.01 min) Epoch 6/300 -- Iteration 4869 - Batch 54/963 - Train loss: 0.36863614  - Train acc: 0.8595 - Val loss: 1.26834238\n",
      "(1.01 min) Epoch 6/300 -- Iteration 4878 - Batch 63/963 - Train loss: 0.37268014  - Train acc: 0.8589 - Val loss: 1.26834238\n",
      "(1.01 min) Epoch 6/300 -- Iteration 4887 - Batch 72/963 - Train loss: 0.37102530  - Train acc: 0.8584 - Val loss: 1.26834238\n",
      "(1.01 min) Epoch 6/300 -- Iteration 4896 - Batch 81/963 - Train loss: 0.36981091  - Train acc: 0.8593 - Val loss: 1.26834238\n",
      "(1.01 min) Epoch 6/300 -- Iteration 4905 - Batch 90/963 - Train loss: 0.37154421  - Train acc: 0.8582 - Val loss: 1.26834238\n",
      "(1.02 min) Epoch 6/300 -- Iteration 4914 - Batch 99/963 - Train loss: 0.36842645  - Train acc: 0.8610 - Val loss: 1.26834238\n",
      "(1.02 min) Epoch 6/300 -- Iteration 4923 - Batch 108/963 - Train loss: 0.36645560  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.02 min) Epoch 6/300 -- Iteration 4932 - Batch 117/963 - Train loss: 0.36520432  - Train acc: 0.8624 - Val loss: 1.26834238\n",
      "(1.02 min) Epoch 6/300 -- Iteration 4941 - Batch 126/963 - Train loss: 0.36360870  - Train acc: 0.8628 - Val loss: 1.26834238\n",
      "(1.02 min) Epoch 6/300 -- Iteration 4950 - Batch 135/963 - Train loss: 0.36308699  - Train acc: 0.8635 - Val loss: 1.26834238\n",
      "(1.03 min) Epoch 6/300 -- Iteration 4959 - Batch 144/963 - Train loss: 0.36308950  - Train acc: 0.8636 - Val loss: 1.26834238\n",
      "(1.03 min) Epoch 6/300 -- Iteration 4968 - Batch 153/963 - Train loss: 0.36459416  - Train acc: 0.8631 - Val loss: 1.26834238\n",
      "(1.03 min) Epoch 6/300 -- Iteration 4977 - Batch 162/963 - Train loss: 0.36643044  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.03 min) Epoch 6/300 -- Iteration 4986 - Batch 171/963 - Train loss: 0.36779895  - Train acc: 0.8618 - Val loss: 1.26834238\n",
      "(1.03 min) Epoch 6/300 -- Iteration 4995 - Batch 180/963 - Train loss: 0.36745938  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.04 min) Epoch 6/300 -- Iteration 5004 - Batch 189/963 - Train loss: 0.36673122  - Train acc: 0.8618 - Val loss: 1.26834238\n",
      "(1.04 min) Epoch 6/300 -- Iteration 5013 - Batch 198/963 - Train loss: 0.36543033  - Train acc: 0.8624 - Val loss: 1.26834238\n",
      "(1.04 min) Epoch 6/300 -- Iteration 5022 - Batch 207/963 - Train loss: 0.36633807  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.04 min) Epoch 6/300 -- Iteration 5031 - Batch 216/963 - Train loss: 0.36643995  - Train acc: 0.8616 - Val loss: 1.26834238\n",
      "(1.04 min) Epoch 6/300 -- Iteration 5040 - Batch 225/963 - Train loss: 0.36708714  - Train acc: 0.8613 - Val loss: 1.26834238\n",
      "(1.04 min) Epoch 6/300 -- Iteration 5049 - Batch 234/963 - Train loss: 0.36769278  - Train acc: 0.8608 - Val loss: 1.26834238\n",
      "(1.05 min) Epoch 6/300 -- Iteration 5058 - Batch 243/963 - Train loss: 0.36809341  - Train acc: 0.8607 - Val loss: 1.26834238\n",
      "(1.05 min) Epoch 6/300 -- Iteration 5067 - Batch 252/963 - Train loss: 0.36820324  - Train acc: 0.8607 - Val loss: 1.26834238\n",
      "(1.05 min) Epoch 6/300 -- Iteration 5076 - Batch 261/963 - Train loss: 0.36634679  - Train acc: 0.8618 - Val loss: 1.26834238\n",
      "(1.05 min) Epoch 6/300 -- Iteration 5085 - Batch 270/963 - Train loss: 0.36648427  - Train acc: 0.8617 - Val loss: 1.26834238\n",
      "(1.05 min) Epoch 6/300 -- Iteration 5094 - Batch 279/963 - Train loss: 0.36652351  - Train acc: 0.8615 - Val loss: 1.26834238\n",
      "(1.06 min) Epoch 6/300 -- Iteration 5103 - Batch 288/963 - Train loss: 0.36665292  - Train acc: 0.8615 - Val loss: 1.26834238\n",
      "(1.06 min) Epoch 6/300 -- Iteration 5112 - Batch 297/963 - Train loss: 0.36570805  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.06 min) Epoch 6/300 -- Iteration 5121 - Batch 306/963 - Train loss: 0.36592089  - Train acc: 0.8617 - Val loss: 1.26834238\n",
      "(1.06 min) Epoch 6/300 -- Iteration 5130 - Batch 315/963 - Train loss: 0.36526597  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.06 min) Epoch 6/300 -- Iteration 5139 - Batch 324/963 - Train loss: 0.36501292  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.06 min) Epoch 6/300 -- Iteration 5148 - Batch 333/963 - Train loss: 0.36556025  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.07 min) Epoch 6/300 -- Iteration 5157 - Batch 342/963 - Train loss: 0.36572421  - Train acc: 0.8617 - Val loss: 1.26834238\n",
      "(1.07 min) Epoch 6/300 -- Iteration 5166 - Batch 351/963 - Train loss: 0.36613714  - Train acc: 0.8614 - Val loss: 1.26834238\n",
      "(1.07 min) Epoch 6/300 -- Iteration 5175 - Batch 360/963 - Train loss: 0.36556353  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.07 min) Epoch 6/300 -- Iteration 5184 - Batch 369/963 - Train loss: 0.36569029  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.07 min) Epoch 6/300 -- Iteration 5193 - Batch 378/963 - Train loss: 0.36585736  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.08 min) Epoch 6/300 -- Iteration 5202 - Batch 387/963 - Train loss: 0.36561738  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.08 min) Epoch 6/300 -- Iteration 5211 - Batch 396/963 - Train loss: 0.36495486  - Train acc: 0.8626 - Val loss: 1.26834238\n",
      "(1.08 min) Epoch 6/300 -- Iteration 5220 - Batch 405/963 - Train loss: 0.36523859  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.08 min) Epoch 6/300 -- Iteration 5229 - Batch 414/963 - Train loss: 0.36550719  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.08 min) Epoch 6/300 -- Iteration 5238 - Batch 423/963 - Train loss: 0.36585320  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.08 min) Epoch 6/300 -- Iteration 5247 - Batch 432/963 - Train loss: 0.36681622  - Train acc: 0.8615 - Val loss: 1.26834238\n",
      "(1.09 min) Epoch 6/300 -- Iteration 5256 - Batch 441/963 - Train loss: 0.36622309  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.09 min) Epoch 6/300 -- Iteration 5265 - Batch 450/963 - Train loss: 0.36589815  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.09 min) Epoch 6/300 -- Iteration 5274 - Batch 459/963 - Train loss: 0.36608183  - Train acc: 0.8617 - Val loss: 1.26834238\n",
      "(1.09 min) Epoch 6/300 -- Iteration 5283 - Batch 468/963 - Train loss: 0.36588416  - Train acc: 0.8618 - Val loss: 1.26834238\n",
      "(1.09 min) Epoch 6/300 -- Iteration 5292 - Batch 477/963 - Train loss: 0.36599953  - Train acc: 0.8617 - Val loss: 1.26834238\n",
      "(1.10 min) Epoch 6/300 -- Iteration 5301 - Batch 486/963 - Train loss: 0.36600676  - Train acc: 0.8617 - Val loss: 1.26834238\n",
      "(1.10 min) Epoch 6/300 -- Iteration 5310 - Batch 495/963 - Train loss: 0.36568780  - Train acc: 0.8617 - Val loss: 1.26834238\n",
      "(1.10 min) Epoch 6/300 -- Iteration 5319 - Batch 504/963 - Train loss: 0.36568707  - Train acc: 0.8616 - Val loss: 1.26834238\n",
      "(1.10 min) Epoch 6/300 -- Iteration 5328 - Batch 513/963 - Train loss: 0.36559692  - Train acc: 0.8618 - Val loss: 1.26834238\n",
      "(1.10 min) Epoch 6/300 -- Iteration 5337 - Batch 522/963 - Train loss: 0.36547765  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.10 min) Epoch 6/300 -- Iteration 5346 - Batch 531/963 - Train loss: 0.36549593  - Train acc: 0.8618 - Val loss: 1.26834238\n",
      "(1.11 min) Epoch 6/300 -- Iteration 5355 - Batch 540/963 - Train loss: 0.36552856  - Train acc: 0.8618 - Val loss: 1.26834238\n",
      "(1.11 min) Epoch 6/300 -- Iteration 5364 - Batch 549/963 - Train loss: 0.36478088  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.11 min) Epoch 6/300 -- Iteration 5373 - Batch 558/963 - Train loss: 0.36432531  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.11 min) Epoch 6/300 -- Iteration 5382 - Batch 567/963 - Train loss: 0.36433921  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.11 min) Epoch 6/300 -- Iteration 5391 - Batch 576/963 - Train loss: 0.36409961  - Train acc: 0.8624 - Val loss: 1.26834238\n",
      "(1.12 min) Epoch 6/300 -- Iteration 5400 - Batch 585/963 - Train loss: 0.36459317  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.12 min) Epoch 6/300 -- Iteration 5409 - Batch 594/963 - Train loss: 0.36471636  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.12 min) Epoch 6/300 -- Iteration 5418 - Batch 603/963 - Train loss: 0.36465094  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.12 min) Epoch 6/300 -- Iteration 5427 - Batch 612/963 - Train loss: 0.36414626  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.12 min) Epoch 6/300 -- Iteration 5436 - Batch 621/963 - Train loss: 0.36417288  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.12 min) Epoch 6/300 -- Iteration 5445 - Batch 630/963 - Train loss: 0.36388249  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.13 min) Epoch 6/300 -- Iteration 5454 - Batch 639/963 - Train loss: 0.36388273  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.13 min) Epoch 6/300 -- Iteration 5463 - Batch 648/963 - Train loss: 0.36380358  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.13 min) Epoch 6/300 -- Iteration 5472 - Batch 657/963 - Train loss: 0.36381648  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.13 min) Epoch 6/300 -- Iteration 5481 - Batch 666/963 - Train loss: 0.36433998  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.13 min) Epoch 6/300 -- Iteration 5490 - Batch 675/963 - Train loss: 0.36414992  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.14 min) Epoch 6/300 -- Iteration 5499 - Batch 684/963 - Train loss: 0.36381584  - Train acc: 0.8624 - Val loss: 1.26834238\n",
      "(1.14 min) Epoch 6/300 -- Iteration 5508 - Batch 693/963 - Train loss: 0.36354404  - Train acc: 0.8624 - Val loss: 1.26834238\n",
      "(1.14 min) Epoch 6/300 -- Iteration 5517 - Batch 702/963 - Train loss: 0.36381474  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.14 min) Epoch 6/300 -- Iteration 5526 - Batch 711/963 - Train loss: 0.36380512  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.14 min) Epoch 6/300 -- Iteration 5535 - Batch 720/963 - Train loss: 0.36354682  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.14 min) Epoch 6/300 -- Iteration 5544 - Batch 729/963 - Train loss: 0.36330561  - Train acc: 0.8625 - Val loss: 1.26834238\n",
      "(1.15 min) Epoch 6/300 -- Iteration 5553 - Batch 738/963 - Train loss: 0.36333441  - Train acc: 0.8625 - Val loss: 1.26834238\n",
      "(1.15 min) Epoch 6/300 -- Iteration 5562 - Batch 747/963 - Train loss: 0.36325318  - Train acc: 0.8627 - Val loss: 1.26834238\n",
      "(1.15 min) Epoch 6/300 -- Iteration 5571 - Batch 756/963 - Train loss: 0.36382403  - Train acc: 0.8625 - Val loss: 1.26834238\n",
      "(1.15 min) Epoch 6/300 -- Iteration 5580 - Batch 765/963 - Train loss: 0.36422413  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.15 min) Epoch 6/300 -- Iteration 5589 - Batch 774/963 - Train loss: 0.36399071  - Train acc: 0.8624 - Val loss: 1.26834238\n",
      "(1.16 min) Epoch 6/300 -- Iteration 5598 - Batch 783/963 - Train loss: 0.36419251  - Train acc: 0.8623 - Val loss: 1.26834238\n",
      "(1.16 min) Epoch 6/300 -- Iteration 5607 - Batch 792/963 - Train loss: 0.36436789  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.16 min) Epoch 6/300 -- Iteration 5616 - Batch 801/963 - Train loss: 0.36435956  - Train acc: 0.8621 - Val loss: 1.26834238\n",
      "(1.16 min) Epoch 6/300 -- Iteration 5625 - Batch 810/963 - Train loss: 0.36433961  - Train acc: 0.8621 - Val loss: 1.26834238\n",
      "(1.16 min) Epoch 6/300 -- Iteration 5634 - Batch 819/963 - Train loss: 0.36399288  - Train acc: 0.8622 - Val loss: 1.26834238\n",
      "(1.16 min) Epoch 6/300 -- Iteration 5643 - Batch 828/963 - Train loss: 0.36364836  - Train acc: 0.8624 - Val loss: 1.26834238\n",
      "(1.17 min) Epoch 6/300 -- Iteration 5652 - Batch 837/963 - Train loss: 0.36362218  - Train acc: 0.8624 - Val loss: 1.26834238\n",
      "(1.17 min) Epoch 6/300 -- Iteration 5661 - Batch 846/963 - Train loss: 0.36424561  - Train acc: 0.8621 - Val loss: 1.26834238\n",
      "(1.17 min) Epoch 6/300 -- Iteration 5670 - Batch 855/963 - Train loss: 0.36433056  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.17 min) Epoch 6/300 -- Iteration 5679 - Batch 864/963 - Train loss: 0.36413773  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.17 min) Epoch 6/300 -- Iteration 5688 - Batch 873/963 - Train loss: 0.36426929  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.18 min) Epoch 6/300 -- Iteration 5697 - Batch 882/963 - Train loss: 0.36459075  - Train acc: 0.8618 - Val loss: 1.26834238\n",
      "(1.18 min) Epoch 6/300 -- Iteration 5706 - Batch 891/963 - Train loss: 0.36461257  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.18 min) Epoch 6/300 -- Iteration 5715 - Batch 900/963 - Train loss: 0.36434423  - Train acc: 0.8620 - Val loss: 1.26834238\n",
      "(1.18 min) Epoch 6/300 -- Iteration 5724 - Batch 909/963 - Train loss: 0.36433613  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.18 min) Epoch 6/300 -- Iteration 5733 - Batch 918/963 - Train loss: 0.36425538  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.18 min) Epoch 6/300 -- Iteration 5742 - Batch 927/963 - Train loss: 0.36424326  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.19 min) Epoch 6/300 -- Iteration 5751 - Batch 936/963 - Train loss: 0.36423212  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.19 min) Epoch 6/300 -- Iteration 5760 - Batch 945/963 - Train loss: 0.36419994  - Train acc: 0.8619 - Val loss: 1.26834238\n",
      "(1.19 min) Epoch 6/300 -- Iteration 5769 - Batch 954/963 - Train loss: 0.36443269  - Train acc: 0.8618 - Val loss: 1.26834238\n",
      "(1.19 min) Epoch 6/300 -- Iteration 5778 - Batch 962/963 - Train loss: 0.36436323  - Train acc: 0.8618 - Val loss: 1.24915028 - Val acc: 0.6083\n",
      "(1.19 min) Epoch 7/300 -- Iteration 5787 - Batch 9/963 - Train loss: 0.39362370  - Train acc: 0.8461 - Val loss: 1.24915028\n",
      "(1.20 min) Epoch 7/300 -- Iteration 5796 - Batch 18/963 - Train loss: 0.37177113  - Train acc: 0.8586 - Val loss: 1.24915028\n",
      "(1.20 min) Epoch 7/300 -- Iteration 5805 - Batch 27/963 - Train loss: 0.37041000  - Train acc: 0.8602 - Val loss: 1.24915028\n",
      "(1.20 min) Epoch 7/300 -- Iteration 5814 - Batch 36/963 - Train loss: 0.37458107  - Train acc: 0.8592 - Val loss: 1.24915028\n",
      "(1.20 min) Epoch 7/300 -- Iteration 5823 - Batch 45/963 - Train loss: 0.37406709  - Train acc: 0.8584 - Val loss: 1.24915028\n",
      "(1.20 min) Epoch 7/300 -- Iteration 5832 - Batch 54/963 - Train loss: 0.36889682  - Train acc: 0.8614 - Val loss: 1.24915028\n",
      "(1.21 min) Epoch 7/300 -- Iteration 5841 - Batch 63/963 - Train loss: 0.37019017  - Train acc: 0.8612 - Val loss: 1.24915028\n",
      "(1.21 min) Epoch 7/300 -- Iteration 5850 - Batch 72/963 - Train loss: 0.36883232  - Train acc: 0.8613 - Val loss: 1.24915028\n",
      "(1.21 min) Epoch 7/300 -- Iteration 5859 - Batch 81/963 - Train loss: 0.37330222  - Train acc: 0.8588 - Val loss: 1.24915028\n",
      "(1.21 min) Epoch 7/300 -- Iteration 5868 - Batch 90/963 - Train loss: 0.37491737  - Train acc: 0.8576 - Val loss: 1.24915028\n",
      "(1.21 min) Epoch 7/300 -- Iteration 5877 - Batch 99/963 - Train loss: 0.37708618  - Train acc: 0.8570 - Val loss: 1.24915028\n",
      "(1.21 min) Epoch 7/300 -- Iteration 5886 - Batch 108/963 - Train loss: 0.37527233  - Train acc: 0.8574 - Val loss: 1.24915028\n",
      "(1.22 min) Epoch 7/300 -- Iteration 5895 - Batch 117/963 - Train loss: 0.37241976  - Train acc: 0.8589 - Val loss: 1.24915028\n",
      "(1.22 min) Epoch 7/300 -- Iteration 5904 - Batch 126/963 - Train loss: 0.37179358  - Train acc: 0.8591 - Val loss: 1.24915028\n",
      "(1.22 min) Epoch 7/300 -- Iteration 5913 - Batch 135/963 - Train loss: 0.37089102  - Train acc: 0.8597 - Val loss: 1.24915028\n",
      "(1.22 min) Epoch 7/300 -- Iteration 5922 - Batch 144/963 - Train loss: 0.37200254  - Train acc: 0.8594 - Val loss: 1.24915028\n",
      "(1.22 min) Epoch 7/300 -- Iteration 5931 - Batch 153/963 - Train loss: 0.37107928  - Train acc: 0.8592 - Val loss: 1.24915028\n",
      "(1.23 min) Epoch 7/300 -- Iteration 5940 - Batch 162/963 - Train loss: 0.37025121  - Train acc: 0.8600 - Val loss: 1.24915028\n",
      "(1.23 min) Epoch 7/300 -- Iteration 5949 - Batch 171/963 - Train loss: 0.37201984  - Train acc: 0.8587 - Val loss: 1.24915028\n",
      "(1.23 min) Epoch 7/300 -- Iteration 5958 - Batch 180/963 - Train loss: 0.37168057  - Train acc: 0.8591 - Val loss: 1.24915028\n",
      "(1.23 min) Epoch 7/300 -- Iteration 5967 - Batch 189/963 - Train loss: 0.37291779  - Train acc: 0.8586 - Val loss: 1.24915028\n",
      "(1.23 min) Epoch 7/300 -- Iteration 5976 - Batch 198/963 - Train loss: 0.37171215  - Train acc: 0.8596 - Val loss: 1.24915028\n",
      "(1.23 min) Epoch 7/300 -- Iteration 5985 - Batch 207/963 - Train loss: 0.37025621  - Train acc: 0.8605 - Val loss: 1.24915028\n",
      "(1.24 min) Epoch 7/300 -- Iteration 5994 - Batch 216/963 - Train loss: 0.36885606  - Train acc: 0.8610 - Val loss: 1.24915028\n",
      "(1.24 min) Epoch 7/300 -- Iteration 6003 - Batch 225/963 - Train loss: 0.36731533  - Train acc: 0.8617 - Val loss: 1.24915028\n",
      "(1.24 min) Epoch 7/300 -- Iteration 6012 - Batch 234/963 - Train loss: 0.36673666  - Train acc: 0.8618 - Val loss: 1.24915028\n",
      "(1.24 min) Epoch 7/300 -- Iteration 6021 - Batch 243/963 - Train loss: 0.36570785  - Train acc: 0.8624 - Val loss: 1.24915028\n",
      "(1.24 min) Epoch 7/300 -- Iteration 6030 - Batch 252/963 - Train loss: 0.36474309  - Train acc: 0.8625 - Val loss: 1.24915028\n",
      "(1.25 min) Epoch 7/300 -- Iteration 6039 - Batch 261/963 - Train loss: 0.36377109  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.25 min) Epoch 7/300 -- Iteration 6048 - Batch 270/963 - Train loss: 0.36323719  - Train acc: 0.8631 - Val loss: 1.24915028\n",
      "(1.25 min) Epoch 7/300 -- Iteration 6057 - Batch 279/963 - Train loss: 0.36264317  - Train acc: 0.8633 - Val loss: 1.24915028\n",
      "(1.25 min) Epoch 7/300 -- Iteration 6066 - Batch 288/963 - Train loss: 0.36258045  - Train acc: 0.8632 - Val loss: 1.24915028\n",
      "(1.25 min) Epoch 7/300 -- Iteration 6075 - Batch 297/963 - Train loss: 0.36339268  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.25 min) Epoch 7/300 -- Iteration 6084 - Batch 306/963 - Train loss: 0.36388337  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.26 min) Epoch 7/300 -- Iteration 6093 - Batch 315/963 - Train loss: 0.36351433  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.26 min) Epoch 7/300 -- Iteration 6102 - Batch 324/963 - Train loss: 0.36403588  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.26 min) Epoch 7/300 -- Iteration 6111 - Batch 333/963 - Train loss: 0.36378164  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.26 min) Epoch 7/300 -- Iteration 6120 - Batch 342/963 - Train loss: 0.36388196  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.26 min) Epoch 7/300 -- Iteration 6129 - Batch 351/963 - Train loss: 0.36374973  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.27 min) Epoch 7/300 -- Iteration 6138 - Batch 360/963 - Train loss: 0.36348024  - Train acc: 0.8625 - Val loss: 1.24915028\n",
      "(1.27 min) Epoch 7/300 -- Iteration 6147 - Batch 369/963 - Train loss: 0.36408364  - Train acc: 0.8624 - Val loss: 1.24915028\n",
      "(1.27 min) Epoch 7/300 -- Iteration 6156 - Batch 378/963 - Train loss: 0.36429004  - Train acc: 0.8620 - Val loss: 1.24915028\n",
      "(1.27 min) Epoch 7/300 -- Iteration 6165 - Batch 387/963 - Train loss: 0.36356037  - Train acc: 0.8621 - Val loss: 1.24915028\n",
      "(1.27 min) Epoch 7/300 -- Iteration 6174 - Batch 396/963 - Train loss: 0.36321516  - Train acc: 0.8623 - Val loss: 1.24915028\n",
      "(1.28 min) Epoch 7/300 -- Iteration 6183 - Batch 405/963 - Train loss: 0.36382341  - Train acc: 0.8620 - Val loss: 1.24915028\n",
      "(1.28 min) Epoch 7/300 -- Iteration 6192 - Batch 414/963 - Train loss: 0.36305646  - Train acc: 0.8626 - Val loss: 1.24915028\n",
      "(1.28 min) Epoch 7/300 -- Iteration 6201 - Batch 423/963 - Train loss: 0.36350462  - Train acc: 0.8625 - Val loss: 1.24915028\n",
      "(1.28 min) Epoch 7/300 -- Iteration 6210 - Batch 432/963 - Train loss: 0.36353292  - Train acc: 0.8624 - Val loss: 1.24915028\n",
      "(1.28 min) Epoch 7/300 -- Iteration 6219 - Batch 441/963 - Train loss: 0.36323214  - Train acc: 0.8626 - Val loss: 1.24915028\n",
      "(1.28 min) Epoch 7/300 -- Iteration 6228 - Batch 450/963 - Train loss: 0.36324506  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.29 min) Epoch 7/300 -- Iteration 6237 - Batch 459/963 - Train loss: 0.36294414  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.29 min) Epoch 7/300 -- Iteration 6246 - Batch 468/963 - Train loss: 0.36293008  - Train acc: 0.8630 - Val loss: 1.24915028\n",
      "(1.29 min) Epoch 7/300 -- Iteration 6255 - Batch 477/963 - Train loss: 0.36292962  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.29 min) Epoch 7/300 -- Iteration 6264 - Batch 486/963 - Train loss: 0.36244005  - Train acc: 0.8630 - Val loss: 1.24915028\n",
      "(1.29 min) Epoch 7/300 -- Iteration 6273 - Batch 495/963 - Train loss: 0.36239391  - Train acc: 0.8631 - Val loss: 1.24915028\n",
      "(1.30 min) Epoch 7/300 -- Iteration 6282 - Batch 504/963 - Train loss: 0.36248256  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.30 min) Epoch 7/300 -- Iteration 6291 - Batch 513/963 - Train loss: 0.36293062  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.30 min) Epoch 7/300 -- Iteration 6300 - Batch 522/963 - Train loss: 0.36236237  - Train acc: 0.8631 - Val loss: 1.24915028\n",
      "(1.30 min) Epoch 7/300 -- Iteration 6309 - Batch 531/963 - Train loss: 0.36258257  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.30 min) Epoch 7/300 -- Iteration 6318 - Batch 540/963 - Train loss: 0.36208650  - Train acc: 0.8633 - Val loss: 1.24915028\n",
      "(1.30 min) Epoch 7/300 -- Iteration 6327 - Batch 549/963 - Train loss: 0.36196507  - Train acc: 0.8632 - Val loss: 1.24915028\n",
      "(1.31 min) Epoch 7/300 -- Iteration 6336 - Batch 558/963 - Train loss: 0.36204241  - Train acc: 0.8631 - Val loss: 1.24915028\n",
      "(1.31 min) Epoch 7/300 -- Iteration 6345 - Batch 567/963 - Train loss: 0.36186747  - Train acc: 0.8632 - Val loss: 1.24915028\n",
      "(1.31 min) Epoch 7/300 -- Iteration 6354 - Batch 576/963 - Train loss: 0.36208851  - Train acc: 0.8630 - Val loss: 1.24915028\n",
      "(1.31 min) Epoch 7/300 -- Iteration 6363 - Batch 585/963 - Train loss: 0.36168445  - Train acc: 0.8632 - Val loss: 1.24915028\n",
      "(1.31 min) Epoch 7/300 -- Iteration 6372 - Batch 594/963 - Train loss: 0.36174371  - Train acc: 0.8632 - Val loss: 1.24915028\n",
      "(1.32 min) Epoch 7/300 -- Iteration 6381 - Batch 603/963 - Train loss: 0.36224217  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.32 min) Epoch 7/300 -- Iteration 6390 - Batch 612/963 - Train loss: 0.36249782  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.32 min) Epoch 7/300 -- Iteration 6399 - Batch 621/963 - Train loss: 0.36274506  - Train acc: 0.8626 - Val loss: 1.24915028\n",
      "(1.32 min) Epoch 7/300 -- Iteration 6408 - Batch 630/963 - Train loss: 0.36245112  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.32 min) Epoch 7/300 -- Iteration 6417 - Batch 639/963 - Train loss: 0.36286936  - Train acc: 0.8626 - Val loss: 1.24915028\n",
      "(1.32 min) Epoch 7/300 -- Iteration 6426 - Batch 648/963 - Train loss: 0.36271436  - Train acc: 0.8626 - Val loss: 1.24915028\n",
      "(1.33 min) Epoch 7/300 -- Iteration 6435 - Batch 657/963 - Train loss: 0.36270811  - Train acc: 0.8626 - Val loss: 1.24915028\n",
      "(1.33 min) Epoch 7/300 -- Iteration 6444 - Batch 666/963 - Train loss: 0.36302896  - Train acc: 0.8625 - Val loss: 1.24915028\n",
      "(1.33 min) Epoch 7/300 -- Iteration 6453 - Batch 675/963 - Train loss: 0.36337926  - Train acc: 0.8624 - Val loss: 1.24915028\n",
      "(1.33 min) Epoch 7/300 -- Iteration 6462 - Batch 684/963 - Train loss: 0.36312980  - Train acc: 0.8625 - Val loss: 1.24915028\n",
      "(1.33 min) Epoch 7/300 -- Iteration 6471 - Batch 693/963 - Train loss: 0.36270058  - Train acc: 0.8626 - Val loss: 1.24915028\n",
      "(1.34 min) Epoch 7/300 -- Iteration 6480 - Batch 702/963 - Train loss: 0.36238743  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.34 min) Epoch 7/300 -- Iteration 6489 - Batch 711/963 - Train loss: 0.36259800  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.34 min) Epoch 7/300 -- Iteration 6498 - Batch 720/963 - Train loss: 0.36239404  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.34 min) Epoch 7/300 -- Iteration 6507 - Batch 729/963 - Train loss: 0.36236260  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.34 min) Epoch 7/300 -- Iteration 6516 - Batch 738/963 - Train loss: 0.36257924  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.34 min) Epoch 7/300 -- Iteration 6525 - Batch 747/963 - Train loss: 0.36250877  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.35 min) Epoch 7/300 -- Iteration 6534 - Batch 756/963 - Train loss: 0.36227793  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.35 min) Epoch 7/300 -- Iteration 6543 - Batch 765/963 - Train loss: 0.36245393  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.35 min) Epoch 7/300 -- Iteration 6552 - Batch 774/963 - Train loss: 0.36262930  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.35 min) Epoch 7/300 -- Iteration 6561 - Batch 783/963 - Train loss: 0.36283994  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.35 min) Epoch 7/300 -- Iteration 6570 - Batch 792/963 - Train loss: 0.36269704  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.36 min) Epoch 7/300 -- Iteration 6579 - Batch 801/963 - Train loss: 0.36291253  - Train acc: 0.8626 - Val loss: 1.24915028\n",
      "(1.36 min) Epoch 7/300 -- Iteration 6588 - Batch 810/963 - Train loss: 0.36281406  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.36 min) Epoch 7/300 -- Iteration 6597 - Batch 819/963 - Train loss: 0.36250594  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.36 min) Epoch 7/300 -- Iteration 6606 - Batch 828/963 - Train loss: 0.36240977  - Train acc: 0.8630 - Val loss: 1.24915028\n",
      "(1.36 min) Epoch 7/300 -- Iteration 6615 - Batch 837/963 - Train loss: 0.36242269  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.36 min) Epoch 7/300 -- Iteration 6624 - Batch 846/963 - Train loss: 0.36265237  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.37 min) Epoch 7/300 -- Iteration 6633 - Batch 855/963 - Train loss: 0.36262571  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.37 min) Epoch 7/300 -- Iteration 6642 - Batch 864/963 - Train loss: 0.36263208  - Train acc: 0.8630 - Val loss: 1.24915028\n",
      "(1.37 min) Epoch 7/300 -- Iteration 6651 - Batch 873/963 - Train loss: 0.36254276  - Train acc: 0.8630 - Val loss: 1.24915028\n",
      "(1.37 min) Epoch 7/300 -- Iteration 6660 - Batch 882/963 - Train loss: 0.36258229  - Train acc: 0.8630 - Val loss: 1.24915028\n",
      "(1.37 min) Epoch 7/300 -- Iteration 6669 - Batch 891/963 - Train loss: 0.36291129  - Train acc: 0.8629 - Val loss: 1.24915028\n",
      "(1.38 min) Epoch 7/300 -- Iteration 6678 - Batch 900/963 - Train loss: 0.36301558  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.38 min) Epoch 7/300 -- Iteration 6687 - Batch 909/963 - Train loss: 0.36294409  - Train acc: 0.8628 - Val loss: 1.24915028\n",
      "(1.38 min) Epoch 7/300 -- Iteration 6696 - Batch 918/963 - Train loss: 0.36311806  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.38 min) Epoch 7/300 -- Iteration 6705 - Batch 927/963 - Train loss: 0.36341644  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.38 min) Epoch 7/300 -- Iteration 6714 - Batch 936/963 - Train loss: 0.36325733  - Train acc: 0.8627 - Val loss: 1.24915028\n",
      "(1.38 min) Epoch 7/300 -- Iteration 6723 - Batch 945/963 - Train loss: 0.36327535  - Train acc: 0.8626 - Val loss: 1.24915028\n",
      "(1.39 min) Epoch 7/300 -- Iteration 6732 - Batch 954/963 - Train loss: 0.36373009  - Train acc: 0.8624 - Val loss: 1.24915028\n",
      "(1.39 min) Epoch 7/300 -- Iteration 6741 - Batch 962/963 - Train loss: 0.36411911  - Train acc: 0.8622 - Val loss: 1.22620988 - Val acc: 0.6067\n",
      "(1.39 min) Epoch 8/300 -- Iteration 6750 - Batch 9/963 - Train loss: 0.34919322  - Train acc: 0.8594 - Val loss: 1.22620988\n",
      "(1.39 min) Epoch 8/300 -- Iteration 6759 - Batch 18/963 - Train loss: 0.35030656  - Train acc: 0.8660 - Val loss: 1.22620988\n",
      "(1.39 min) Epoch 8/300 -- Iteration 6768 - Batch 27/963 - Train loss: 0.36124208  - Train acc: 0.8602 - Val loss: 1.22620988\n",
      "(1.40 min) Epoch 8/300 -- Iteration 6777 - Batch 36/963 - Train loss: 0.36917740  - Train acc: 0.8545 - Val loss: 1.22620988\n",
      "(1.40 min) Epoch 8/300 -- Iteration 6786 - Batch 45/963 - Train loss: 0.36455223  - Train acc: 0.8587 - Val loss: 1.22620988\n",
      "(1.40 min) Epoch 8/300 -- Iteration 6795 - Batch 54/963 - Train loss: 0.35842428  - Train acc: 0.8619 - Val loss: 1.22620988\n",
      "(1.40 min) Epoch 8/300 -- Iteration 6804 - Batch 63/963 - Train loss: 0.36202134  - Train acc: 0.8597 - Val loss: 1.22620988\n",
      "(1.40 min) Epoch 8/300 -- Iteration 6813 - Batch 72/963 - Train loss: 0.36437900  - Train acc: 0.8588 - Val loss: 1.22620988\n",
      "(1.41 min) Epoch 8/300 -- Iteration 6822 - Batch 81/963 - Train loss: 0.36357244  - Train acc: 0.8600 - Val loss: 1.22620988\n",
      "(1.41 min) Epoch 8/300 -- Iteration 6831 - Batch 90/963 - Train loss: 0.35848051  - Train acc: 0.8623 - Val loss: 1.22620988\n",
      "(1.41 min) Epoch 8/300 -- Iteration 6840 - Batch 99/963 - Train loss: 0.35791396  - Train acc: 0.8629 - Val loss: 1.22620988\n",
      "(1.41 min) Epoch 8/300 -- Iteration 6849 - Batch 108/963 - Train loss: 0.35692968  - Train acc: 0.8632 - Val loss: 1.22620988\n",
      "(1.41 min) Epoch 8/300 -- Iteration 6858 - Batch 117/963 - Train loss: 0.35550090  - Train acc: 0.8642 - Val loss: 1.22620988\n",
      "(1.41 min) Epoch 8/300 -- Iteration 6867 - Batch 126/963 - Train loss: 0.35292596  - Train acc: 0.8655 - Val loss: 1.22620988\n",
      "(1.42 min) Epoch 8/300 -- Iteration 6876 - Batch 135/963 - Train loss: 0.35372641  - Train acc: 0.8646 - Val loss: 1.22620988\n",
      "(1.42 min) Epoch 8/300 -- Iteration 6885 - Batch 144/963 - Train loss: 0.35641008  - Train acc: 0.8630 - Val loss: 1.22620988\n",
      "(1.42 min) Epoch 8/300 -- Iteration 6894 - Batch 153/963 - Train loss: 0.35670955  - Train acc: 0.8626 - Val loss: 1.22620988\n",
      "(1.42 min) Epoch 8/300 -- Iteration 6903 - Batch 162/963 - Train loss: 0.35594140  - Train acc: 0.8635 - Val loss: 1.22620988\n",
      "(1.42 min) Epoch 8/300 -- Iteration 6912 - Batch 171/963 - Train loss: 0.35760344  - Train acc: 0.8629 - Val loss: 1.22620988\n",
      "(1.43 min) Epoch 8/300 -- Iteration 6921 - Batch 180/963 - Train loss: 0.35785333  - Train acc: 0.8629 - Val loss: 1.22620988\n",
      "(1.43 min) Epoch 8/300 -- Iteration 6930 - Batch 189/963 - Train loss: 0.35823691  - Train acc: 0.8632 - Val loss: 1.22620988\n",
      "(1.43 min) Epoch 8/300 -- Iteration 6939 - Batch 198/963 - Train loss: 0.35828271  - Train acc: 0.8631 - Val loss: 1.22620988\n",
      "(1.43 min) Epoch 8/300 -- Iteration 6948 - Batch 207/963 - Train loss: 0.35867680  - Train acc: 0.8631 - Val loss: 1.22620988\n",
      "(1.43 min) Epoch 8/300 -- Iteration 6957 - Batch 216/963 - Train loss: 0.35810709  - Train acc: 0.8632 - Val loss: 1.22620988\n",
      "(1.43 min) Epoch 8/300 -- Iteration 6966 - Batch 225/963 - Train loss: 0.35924433  - Train acc: 0.8627 - Val loss: 1.22620988\n",
      "(1.44 min) Epoch 8/300 -- Iteration 6975 - Batch 234/963 - Train loss: 0.35899472  - Train acc: 0.8627 - Val loss: 1.22620988\n",
      "(1.44 min) Epoch 8/300 -- Iteration 6984 - Batch 243/963 - Train loss: 0.35807776  - Train acc: 0.8628 - Val loss: 1.22620988\n",
      "(1.44 min) Epoch 8/300 -- Iteration 6993 - Batch 252/963 - Train loss: 0.35744219  - Train acc: 0.8633 - Val loss: 1.22620988\n",
      "(1.44 min) Epoch 8/300 -- Iteration 7002 - Batch 261/963 - Train loss: 0.35811596  - Train acc: 0.8629 - Val loss: 1.22620988\n",
      "(1.44 min) Epoch 8/300 -- Iteration 7011 - Batch 270/963 - Train loss: 0.35780674  - Train acc: 0.8630 - Val loss: 1.22620988\n",
      "(1.45 min) Epoch 8/300 -- Iteration 7020 - Batch 279/963 - Train loss: 0.35812234  - Train acc: 0.8633 - Val loss: 1.22620988\n",
      "(1.45 min) Epoch 8/300 -- Iteration 7029 - Batch 288/963 - Train loss: 0.35895412  - Train acc: 0.8632 - Val loss: 1.22620988\n",
      "(1.45 min) Epoch 8/300 -- Iteration 7038 - Batch 297/963 - Train loss: 0.35919712  - Train acc: 0.8633 - Val loss: 1.22620988\n",
      "(1.45 min) Epoch 8/300 -- Iteration 7047 - Batch 306/963 - Train loss: 0.35885511  - Train acc: 0.8634 - Val loss: 1.22620988\n",
      "(1.45 min) Epoch 8/300 -- Iteration 7056 - Batch 315/963 - Train loss: 0.35798200  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.45 min) Epoch 8/300 -- Iteration 7065 - Batch 324/963 - Train loss: 0.35738449  - Train acc: 0.8641 - Val loss: 1.22620988\n",
      "(1.46 min) Epoch 8/300 -- Iteration 7074 - Batch 333/963 - Train loss: 0.35803944  - Train acc: 0.8637 - Val loss: 1.22620988\n",
      "(1.46 min) Epoch 8/300 -- Iteration 7083 - Batch 342/963 - Train loss: 0.35734842  - Train acc: 0.8639 - Val loss: 1.22620988\n",
      "(1.46 min) Epoch 8/300 -- Iteration 7092 - Batch 351/963 - Train loss: 0.35797284  - Train acc: 0.8637 - Val loss: 1.22620988\n",
      "(1.46 min) Epoch 8/300 -- Iteration 7101 - Batch 360/963 - Train loss: 0.35837058  - Train acc: 0.8635 - Val loss: 1.22620988\n",
      "(1.46 min) Epoch 8/300 -- Iteration 7110 - Batch 369/963 - Train loss: 0.35873023  - Train acc: 0.8633 - Val loss: 1.22620988\n",
      "(1.47 min) Epoch 8/300 -- Iteration 7119 - Batch 378/963 - Train loss: 0.35849115  - Train acc: 0.8634 - Val loss: 1.22620988\n",
      "(1.47 min) Epoch 8/300 -- Iteration 7128 - Batch 387/963 - Train loss: 0.35881592  - Train acc: 0.8632 - Val loss: 1.22620988\n",
      "(1.47 min) Epoch 8/300 -- Iteration 7137 - Batch 396/963 - Train loss: 0.35945519  - Train acc: 0.8630 - Val loss: 1.22620988\n",
      "(1.47 min) Epoch 8/300 -- Iteration 7146 - Batch 405/963 - Train loss: 0.36037023  - Train acc: 0.8627 - Val loss: 1.22620988\n",
      "(1.47 min) Epoch 8/300 -- Iteration 7155 - Batch 414/963 - Train loss: 0.36035485  - Train acc: 0.8629 - Val loss: 1.22620988\n",
      "(1.47 min) Epoch 8/300 -- Iteration 7164 - Batch 423/963 - Train loss: 0.36047197  - Train acc: 0.8629 - Val loss: 1.22620988\n",
      "(1.48 min) Epoch 8/300 -- Iteration 7173 - Batch 432/963 - Train loss: 0.36109745  - Train acc: 0.8627 - Val loss: 1.22620988\n",
      "(1.48 min) Epoch 8/300 -- Iteration 7182 - Batch 441/963 - Train loss: 0.36055018  - Train acc: 0.8630 - Val loss: 1.22620988\n",
      "(1.48 min) Epoch 8/300 -- Iteration 7191 - Batch 450/963 - Train loss: 0.36138381  - Train acc: 0.8626 - Val loss: 1.22620988\n",
      "(1.48 min) Epoch 8/300 -- Iteration 7200 - Batch 459/963 - Train loss: 0.36155016  - Train acc: 0.8624 - Val loss: 1.22620988\n",
      "(1.48 min) Epoch 8/300 -- Iteration 7209 - Batch 468/963 - Train loss: 0.36145886  - Train acc: 0.8626 - Val loss: 1.22620988\n",
      "(1.49 min) Epoch 8/300 -- Iteration 7218 - Batch 477/963 - Train loss: 0.36134390  - Train acc: 0.8629 - Val loss: 1.22620988\n",
      "(1.49 min) Epoch 8/300 -- Iteration 7227 - Batch 486/963 - Train loss: 0.36100084  - Train acc: 0.8630 - Val loss: 1.22620988\n",
      "(1.49 min) Epoch 8/300 -- Iteration 7236 - Batch 495/963 - Train loss: 0.36046113  - Train acc: 0.8632 - Val loss: 1.22620988\n",
      "(1.49 min) Epoch 8/300 -- Iteration 7245 - Batch 504/963 - Train loss: 0.36010950  - Train acc: 0.8636 - Val loss: 1.22620988\n",
      "(1.49 min) Epoch 8/300 -- Iteration 7254 - Batch 513/963 - Train loss: 0.35959649  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.49 min) Epoch 8/300 -- Iteration 7263 - Batch 522/963 - Train loss: 0.35953871  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.50 min) Epoch 8/300 -- Iteration 7272 - Batch 531/963 - Train loss: 0.35937018  - Train acc: 0.8639 - Val loss: 1.22620988\n",
      "(1.50 min) Epoch 8/300 -- Iteration 7281 - Batch 540/963 - Train loss: 0.35895735  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.50 min) Epoch 8/300 -- Iteration 7290 - Batch 549/963 - Train loss: 0.35933225  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.50 min) Epoch 8/300 -- Iteration 7299 - Batch 558/963 - Train loss: 0.35897369  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.50 min) Epoch 8/300 -- Iteration 7308 - Batch 567/963 - Train loss: 0.35890112  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.51 min) Epoch 8/300 -- Iteration 7317 - Batch 576/963 - Train loss: 0.35857124  - Train acc: 0.8641 - Val loss: 1.22620988\n",
      "(1.51 min) Epoch 8/300 -- Iteration 7326 - Batch 585/963 - Train loss: 0.35866614  - Train acc: 0.8641 - Val loss: 1.22620988\n",
      "(1.51 min) Epoch 8/300 -- Iteration 7335 - Batch 594/963 - Train loss: 0.35882039  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.51 min) Epoch 8/300 -- Iteration 7344 - Batch 603/963 - Train loss: 0.35893303  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.51 min) Epoch 8/300 -- Iteration 7353 - Batch 612/963 - Train loss: 0.35852256  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.51 min) Epoch 8/300 -- Iteration 7362 - Batch 621/963 - Train loss: 0.35868442  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.52 min) Epoch 8/300 -- Iteration 7371 - Batch 630/963 - Train loss: 0.35882551  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.52 min) Epoch 8/300 -- Iteration 7380 - Batch 639/963 - Train loss: 0.35880559  - Train acc: 0.8641 - Val loss: 1.22620988\n",
      "(1.52 min) Epoch 8/300 -- Iteration 7389 - Batch 648/963 - Train loss: 0.35882574  - Train acc: 0.8642 - Val loss: 1.22620988\n",
      "(1.52 min) Epoch 8/300 -- Iteration 7398 - Batch 657/963 - Train loss: 0.35864865  - Train acc: 0.8642 - Val loss: 1.22620988\n",
      "(1.52 min) Epoch 8/300 -- Iteration 7407 - Batch 666/963 - Train loss: 0.35858866  - Train acc: 0.8642 - Val loss: 1.22620988\n",
      "(1.53 min) Epoch 8/300 -- Iteration 7416 - Batch 675/963 - Train loss: 0.35820228  - Train acc: 0.8643 - Val loss: 1.22620988\n",
      "(1.53 min) Epoch 8/300 -- Iteration 7425 - Batch 684/963 - Train loss: 0.35890754  - Train acc: 0.8641 - Val loss: 1.22620988\n",
      "(1.53 min) Epoch 8/300 -- Iteration 7434 - Batch 693/963 - Train loss: 0.35874687  - Train acc: 0.8641 - Val loss: 1.22620988\n",
      "(1.53 min) Epoch 8/300 -- Iteration 7443 - Batch 702/963 - Train loss: 0.35870037  - Train acc: 0.8641 - Val loss: 1.22620988\n",
      "(1.53 min) Epoch 8/300 -- Iteration 7452 - Batch 711/963 - Train loss: 0.35881707  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.53 min) Epoch 8/300 -- Iteration 7461 - Batch 720/963 - Train loss: 0.35898484  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.54 min) Epoch 8/300 -- Iteration 7470 - Batch 729/963 - Train loss: 0.35912195  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.54 min) Epoch 8/300 -- Iteration 7479 - Batch 738/963 - Train loss: 0.35906395  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.54 min) Epoch 8/300 -- Iteration 7488 - Batch 747/963 - Train loss: 0.35886571  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.54 min) Epoch 8/300 -- Iteration 7497 - Batch 756/963 - Train loss: 0.35891719  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.54 min) Epoch 8/300 -- Iteration 7506 - Batch 765/963 - Train loss: 0.35902033  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.55 min) Epoch 8/300 -- Iteration 7515 - Batch 774/963 - Train loss: 0.35914962  - Train acc: 0.8637 - Val loss: 1.22620988\n",
      "(1.55 min) Epoch 8/300 -- Iteration 7524 - Batch 783/963 - Train loss: 0.35898286  - Train acc: 0.8638 - Val loss: 1.22620988\n",
      "(1.55 min) Epoch 8/300 -- Iteration 7533 - Batch 792/963 - Train loss: 0.35870580  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.55 min) Epoch 8/300 -- Iteration 7542 - Batch 801/963 - Train loss: 0.35854168  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.55 min) Epoch 8/300 -- Iteration 7551 - Batch 810/963 - Train loss: 0.35842207  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.55 min) Epoch 8/300 -- Iteration 7560 - Batch 819/963 - Train loss: 0.35833488  - Train acc: 0.8641 - Val loss: 1.22620988\n",
      "(1.56 min) Epoch 8/300 -- Iteration 7569 - Batch 828/963 - Train loss: 0.35801698  - Train acc: 0.8643 - Val loss: 1.22620988\n",
      "(1.56 min) Epoch 8/300 -- Iteration 7578 - Batch 837/963 - Train loss: 0.35856257  - Train acc: 0.8640 - Val loss: 1.22620988\n",
      "(1.56 min) Epoch 8/300 -- Iteration 7587 - Batch 846/963 - Train loss: 0.35855767  - Train acc: 0.8643 - Val loss: 1.22620988\n",
      "(1.56 min) Epoch 8/300 -- Iteration 7596 - Batch 855/963 - Train loss: 0.35836571  - Train acc: 0.8643 - Val loss: 1.22620988\n",
      "(1.56 min) Epoch 8/300 -- Iteration 7605 - Batch 864/963 - Train loss: 0.35849611  - Train acc: 0.8643 - Val loss: 1.22620988\n",
      "(1.57 min) Epoch 8/300 -- Iteration 7614 - Batch 873/963 - Train loss: 0.35855912  - Train acc: 0.8642 - Val loss: 1.22620988\n",
      "(1.57 min) Epoch 8/300 -- Iteration 7623 - Batch 882/963 - Train loss: 0.35830378  - Train acc: 0.8644 - Val loss: 1.22620988\n",
      "(1.57 min) Epoch 8/300 -- Iteration 7632 - Batch 891/963 - Train loss: 0.35840272  - Train acc: 0.8643 - Val loss: 1.22620988\n",
      "(1.57 min) Epoch 8/300 -- Iteration 7641 - Batch 900/963 - Train loss: 0.35835113  - Train acc: 0.8643 - Val loss: 1.22620988\n",
      "(1.57 min) Epoch 8/300 -- Iteration 7650 - Batch 909/963 - Train loss: 0.35845895  - Train acc: 0.8642 - Val loss: 1.22620988\n",
      "(1.57 min) Epoch 8/300 -- Iteration 7659 - Batch 918/963 - Train loss: 0.35854648  - Train acc: 0.8642 - Val loss: 1.22620988\n",
      "(1.58 min) Epoch 8/300 -- Iteration 7668 - Batch 927/963 - Train loss: 0.35834341  - Train acc: 0.8644 - Val loss: 1.22620988\n",
      "(1.58 min) Epoch 8/300 -- Iteration 7677 - Batch 936/963 - Train loss: 0.35851480  - Train acc: 0.8644 - Val loss: 1.22620988\n",
      "(1.58 min) Epoch 8/300 -- Iteration 7686 - Batch 945/963 - Train loss: 0.35845383  - Train acc: 0.8644 - Val loss: 1.22620988\n",
      "(1.58 min) Epoch 8/300 -- Iteration 7695 - Batch 954/963 - Train loss: 0.35852906  - Train acc: 0.8643 - Val loss: 1.22620988\n",
      "(1.58 min) Epoch 8/300 -- Iteration 7704 - Batch 962/963 - Train loss: 0.35852923  - Train acc: 0.8643 - Val loss: 1.21623480 - Val acc: 0.6067\n",
      "(1.59 min) Epoch 9/300 -- Iteration 7713 - Batch 9/963 - Train loss: 0.36640236  - Train acc: 0.8547 - Val loss: 1.21623480\n",
      "(1.59 min) Epoch 9/300 -- Iteration 7722 - Batch 18/963 - Train loss: 0.35098881  - Train acc: 0.8618 - Val loss: 1.21623480\n",
      "(1.59 min) Epoch 9/300 -- Iteration 7731 - Batch 27/963 - Train loss: 0.34759780  - Train acc: 0.8644 - Val loss: 1.21623480\n",
      "(1.59 min) Epoch 9/300 -- Iteration 7740 - Batch 36/963 - Train loss: 0.34480681  - Train acc: 0.8674 - Val loss: 1.21623480\n",
      "(1.59 min) Epoch 9/300 -- Iteration 7749 - Batch 45/963 - Train loss: 0.34668223  - Train acc: 0.8689 - Val loss: 1.21623480\n",
      "(1.60 min) Epoch 9/300 -- Iteration 7758 - Batch 54/963 - Train loss: 0.34334442  - Train acc: 0.8710 - Val loss: 1.21623480\n",
      "(1.60 min) Epoch 9/300 -- Iteration 7767 - Batch 63/963 - Train loss: 0.34176840  - Train acc: 0.8717 - Val loss: 1.21623480\n",
      "(1.60 min) Epoch 9/300 -- Iteration 7776 - Batch 72/963 - Train loss: 0.34362296  - Train acc: 0.8699 - Val loss: 1.21623480\n",
      "(1.60 min) Epoch 9/300 -- Iteration 7785 - Batch 81/963 - Train loss: 0.34543942  - Train acc: 0.8695 - Val loss: 1.21623480\n",
      "(1.60 min) Epoch 9/300 -- Iteration 7794 - Batch 90/963 - Train loss: 0.34558220  - Train acc: 0.8698 - Val loss: 1.21623480\n",
      "(1.60 min) Epoch 9/300 -- Iteration 7803 - Batch 99/963 - Train loss: 0.34672870  - Train acc: 0.8683 - Val loss: 1.21623480\n",
      "(1.61 min) Epoch 9/300 -- Iteration 7812 - Batch 108/963 - Train loss: 0.34747999  - Train acc: 0.8681 - Val loss: 1.21623480\n",
      "(1.61 min) Epoch 9/300 -- Iteration 7821 - Batch 117/963 - Train loss: 0.34736429  - Train acc: 0.8686 - Val loss: 1.21623480\n",
      "(1.61 min) Epoch 9/300 -- Iteration 7830 - Batch 126/963 - Train loss: 0.34767170  - Train acc: 0.8687 - Val loss: 1.21623480\n",
      "(1.61 min) Epoch 9/300 -- Iteration 7839 - Batch 135/963 - Train loss: 0.34815355  - Train acc: 0.8686 - Val loss: 1.21623480\n",
      "(1.61 min) Epoch 9/300 -- Iteration 7848 - Batch 144/963 - Train loss: 0.34741290  - Train acc: 0.8689 - Val loss: 1.21623480\n",
      "(1.62 min) Epoch 9/300 -- Iteration 7857 - Batch 153/963 - Train loss: 0.34651174  - Train acc: 0.8694 - Val loss: 1.21623480\n",
      "(1.62 min) Epoch 9/300 -- Iteration 7866 - Batch 162/963 - Train loss: 0.34798479  - Train acc: 0.8686 - Val loss: 1.21623480\n",
      "(1.62 min) Epoch 9/300 -- Iteration 7875 - Batch 171/963 - Train loss: 0.34896916  - Train acc: 0.8678 - Val loss: 1.21623480\n",
      "(1.62 min) Epoch 9/300 -- Iteration 7884 - Batch 180/963 - Train loss: 0.34861067  - Train acc: 0.8681 - Val loss: 1.21623480\n",
      "(1.62 min) Epoch 9/300 -- Iteration 7893 - Batch 189/963 - Train loss: 0.34984802  - Train acc: 0.8672 - Val loss: 1.21623480\n",
      "(1.62 min) Epoch 9/300 -- Iteration 7902 - Batch 198/963 - Train loss: 0.35069740  - Train acc: 0.8671 - Val loss: 1.21623480\n",
      "(1.63 min) Epoch 9/300 -- Iteration 7911 - Batch 207/963 - Train loss: 0.35135054  - Train acc: 0.8668 - Val loss: 1.21623480\n",
      "(1.63 min) Epoch 9/300 -- Iteration 7920 - Batch 216/963 - Train loss: 0.35146358  - Train acc: 0.8668 - Val loss: 1.21623480\n",
      "(1.63 min) Epoch 9/300 -- Iteration 7929 - Batch 225/963 - Train loss: 0.35086193  - Train acc: 0.8675 - Val loss: 1.21623480\n",
      "(1.63 min) Epoch 9/300 -- Iteration 7938 - Batch 234/963 - Train loss: 0.35113253  - Train acc: 0.8673 - Val loss: 1.21623480\n",
      "(1.63 min) Epoch 9/300 -- Iteration 7947 - Batch 243/963 - Train loss: 0.34991721  - Train acc: 0.8680 - Val loss: 1.21623480\n",
      "(1.64 min) Epoch 9/300 -- Iteration 7956 - Batch 252/963 - Train loss: 0.35087304  - Train acc: 0.8673 - Val loss: 1.21623480\n",
      "(1.64 min) Epoch 9/300 -- Iteration 7965 - Batch 261/963 - Train loss: 0.35052698  - Train acc: 0.8673 - Val loss: 1.21623480\n",
      "(1.64 min) Epoch 9/300 -- Iteration 7974 - Batch 270/963 - Train loss: 0.35068783  - Train acc: 0.8675 - Val loss: 1.21623480\n",
      "(1.64 min) Epoch 9/300 -- Iteration 7983 - Batch 279/963 - Train loss: 0.35063722  - Train acc: 0.8677 - Val loss: 1.21623480\n",
      "(1.64 min) Epoch 9/300 -- Iteration 7992 - Batch 288/963 - Train loss: 0.35075291  - Train acc: 0.8672 - Val loss: 1.21623480\n",
      "(1.64 min) Epoch 9/300 -- Iteration 8001 - Batch 297/963 - Train loss: 0.35043412  - Train acc: 0.8676 - Val loss: 1.21623480\n",
      "(1.65 min) Epoch 9/300 -- Iteration 8010 - Batch 306/963 - Train loss: 0.35088320  - Train acc: 0.8674 - Val loss: 1.21623480\n",
      "(1.65 min) Epoch 9/300 -- Iteration 8019 - Batch 315/963 - Train loss: 0.35140770  - Train acc: 0.8671 - Val loss: 1.21623480\n",
      "(1.65 min) Epoch 9/300 -- Iteration 8028 - Batch 324/963 - Train loss: 0.35072920  - Train acc: 0.8675 - Val loss: 1.21623480\n",
      "(1.65 min) Epoch 9/300 -- Iteration 8037 - Batch 333/963 - Train loss: 0.35053969  - Train acc: 0.8676 - Val loss: 1.21623480\n",
      "(1.65 min) Epoch 9/300 -- Iteration 8046 - Batch 342/963 - Train loss: 0.35061175  - Train acc: 0.8674 - Val loss: 1.21623480\n",
      "(1.66 min) Epoch 9/300 -- Iteration 8055 - Batch 351/963 - Train loss: 0.35112395  - Train acc: 0.8671 - Val loss: 1.21623480\n",
      "(1.66 min) Epoch 9/300 -- Iteration 8064 - Batch 360/963 - Train loss: 0.35071499  - Train acc: 0.8673 - Val loss: 1.21623480\n",
      "(1.66 min) Epoch 9/300 -- Iteration 8073 - Batch 369/963 - Train loss: 0.35047943  - Train acc: 0.8675 - Val loss: 1.21623480\n",
      "(1.66 min) Epoch 9/300 -- Iteration 8082 - Batch 378/963 - Train loss: 0.35037260  - Train acc: 0.8676 - Val loss: 1.21623480\n",
      "(1.66 min) Epoch 9/300 -- Iteration 8091 - Batch 387/963 - Train loss: 0.35097496  - Train acc: 0.8672 - Val loss: 1.21623480\n",
      "(1.66 min) Epoch 9/300 -- Iteration 8100 - Batch 396/963 - Train loss: 0.35127257  - Train acc: 0.8671 - Val loss: 1.21623480\n",
      "(1.67 min) Epoch 9/300 -- Iteration 8109 - Batch 405/963 - Train loss: 0.35203400  - Train acc: 0.8668 - Val loss: 1.21623480\n",
      "(1.67 min) Epoch 9/300 -- Iteration 8118 - Batch 414/963 - Train loss: 0.35236440  - Train acc: 0.8667 - Val loss: 1.21623480\n",
      "(1.67 min) Epoch 9/300 -- Iteration 8127 - Batch 423/963 - Train loss: 0.35248467  - Train acc: 0.8667 - Val loss: 1.21623480\n",
      "(1.67 min) Epoch 9/300 -- Iteration 8136 - Batch 432/963 - Train loss: 0.35220617  - Train acc: 0.8670 - Val loss: 1.21623480\n",
      "(1.67 min) Epoch 9/300 -- Iteration 8145 - Batch 441/963 - Train loss: 0.35222388  - Train acc: 0.8668 - Val loss: 1.21623480\n",
      "(1.68 min) Epoch 9/300 -- Iteration 8154 - Batch 450/963 - Train loss: 0.35286938  - Train acc: 0.8668 - Val loss: 1.21623480\n",
      "(1.68 min) Epoch 9/300 -- Iteration 8163 - Batch 459/963 - Train loss: 0.35277296  - Train acc: 0.8669 - Val loss: 1.21623480\n",
      "(1.68 min) Epoch 9/300 -- Iteration 8172 - Batch 468/963 - Train loss: 0.35274140  - Train acc: 0.8669 - Val loss: 1.21623480\n",
      "(1.68 min) Epoch 9/300 -- Iteration 8181 - Batch 477/963 - Train loss: 0.35338299  - Train acc: 0.8666 - Val loss: 1.21623480\n",
      "(1.68 min) Epoch 9/300 -- Iteration 8190 - Batch 486/963 - Train loss: 0.35340438  - Train acc: 0.8665 - Val loss: 1.21623480\n",
      "(1.68 min) Epoch 9/300 -- Iteration 8199 - Batch 495/963 - Train loss: 0.35337667  - Train acc: 0.8664 - Val loss: 1.21623480\n",
      "(1.69 min) Epoch 9/300 -- Iteration 8208 - Batch 504/963 - Train loss: 0.35347452  - Train acc: 0.8665 - Val loss: 1.21623480\n",
      "(1.69 min) Epoch 9/300 -- Iteration 8217 - Batch 513/963 - Train loss: 0.35428611  - Train acc: 0.8661 - Val loss: 1.21623480\n",
      "(1.69 min) Epoch 9/300 -- Iteration 8226 - Batch 522/963 - Train loss: 0.35407483  - Train acc: 0.8662 - Val loss: 1.21623480\n",
      "(1.69 min) Epoch 9/300 -- Iteration 8235 - Batch 531/963 - Train loss: 0.35472809  - Train acc: 0.8659 - Val loss: 1.21623480\n",
      "(1.69 min) Epoch 9/300 -- Iteration 8244 - Batch 540/963 - Train loss: 0.35461035  - Train acc: 0.8659 - Val loss: 1.21623480\n",
      "(1.70 min) Epoch 9/300 -- Iteration 8253 - Batch 549/963 - Train loss: 0.35462872  - Train acc: 0.8656 - Val loss: 1.21623480\n",
      "(1.70 min) Epoch 9/300 -- Iteration 8262 - Batch 558/963 - Train loss: 0.35494253  - Train acc: 0.8654 - Val loss: 1.21623480\n",
      "(1.70 min) Epoch 9/300 -- Iteration 8271 - Batch 567/963 - Train loss: 0.35444104  - Train acc: 0.8655 - Val loss: 1.21623480\n",
      "(1.70 min) Epoch 9/300 -- Iteration 8280 - Batch 576/963 - Train loss: 0.35437812  - Train acc: 0.8655 - Val loss: 1.21623480\n",
      "(1.70 min) Epoch 9/300 -- Iteration 8289 - Batch 585/963 - Train loss: 0.35457855  - Train acc: 0.8654 - Val loss: 1.21623480\n",
      "(1.71 min) Epoch 9/300 -- Iteration 8298 - Batch 594/963 - Train loss: 0.35412285  - Train acc: 0.8656 - Val loss: 1.21623480\n",
      "(1.71 min) Epoch 9/300 -- Iteration 8307 - Batch 603/963 - Train loss: 0.35402761  - Train acc: 0.8656 - Val loss: 1.21623480\n",
      "(1.71 min) Epoch 9/300 -- Iteration 8316 - Batch 612/963 - Train loss: 0.35383291  - Train acc: 0.8656 - Val loss: 1.21623480\n",
      "(1.71 min) Epoch 9/300 -- Iteration 8325 - Batch 621/963 - Train loss: 0.35369033  - Train acc: 0.8657 - Val loss: 1.21623480\n",
      "(1.71 min) Epoch 9/300 -- Iteration 8334 - Batch 630/963 - Train loss: 0.35442126  - Train acc: 0.8655 - Val loss: 1.21623480\n",
      "(1.71 min) Epoch 9/300 -- Iteration 8343 - Batch 639/963 - Train loss: 0.35471277  - Train acc: 0.8654 - Val loss: 1.21623480\n",
      "(1.72 min) Epoch 9/300 -- Iteration 8352 - Batch 648/963 - Train loss: 0.35462534  - Train acc: 0.8654 - Val loss: 1.21623480\n",
      "(1.72 min) Epoch 9/300 -- Iteration 8361 - Batch 657/963 - Train loss: 0.35529262  - Train acc: 0.8651 - Val loss: 1.21623480\n",
      "(1.72 min) Epoch 9/300 -- Iteration 8370 - Batch 666/963 - Train loss: 0.35527052  - Train acc: 0.8651 - Val loss: 1.21623480\n",
      "(1.72 min) Epoch 9/300 -- Iteration 8379 - Batch 675/963 - Train loss: 0.35526974  - Train acc: 0.8650 - Val loss: 1.21623480\n",
      "(1.72 min) Epoch 9/300 -- Iteration 8388 - Batch 684/963 - Train loss: 0.35562738  - Train acc: 0.8647 - Val loss: 1.21623480\n",
      "(1.73 min) Epoch 9/300 -- Iteration 8397 - Batch 693/963 - Train loss: 0.35590839  - Train acc: 0.8647 - Val loss: 1.21623480\n",
      "(1.73 min) Epoch 9/300 -- Iteration 8406 - Batch 702/963 - Train loss: 0.35595381  - Train acc: 0.8647 - Val loss: 1.21623480\n",
      "(1.73 min) Epoch 9/300 -- Iteration 8415 - Batch 711/963 - Train loss: 0.35608417  - Train acc: 0.8646 - Val loss: 1.21623480\n",
      "(1.73 min) Epoch 9/300 -- Iteration 8424 - Batch 720/963 - Train loss: 0.35643538  - Train acc: 0.8643 - Val loss: 1.21623480\n",
      "(1.73 min) Epoch 9/300 -- Iteration 8433 - Batch 729/963 - Train loss: 0.35657917  - Train acc: 0.8642 - Val loss: 1.21623480\n",
      "(1.73 min) Epoch 9/300 -- Iteration 8442 - Batch 738/963 - Train loss: 0.35651043  - Train acc: 0.8642 - Val loss: 1.21623480\n",
      "(1.74 min) Epoch 9/300 -- Iteration 8451 - Batch 747/963 - Train loss: 0.35653288  - Train acc: 0.8643 - Val loss: 1.21623480\n",
      "(1.74 min) Epoch 9/300 -- Iteration 8460 - Batch 756/963 - Train loss: 0.35641528  - Train acc: 0.8643 - Val loss: 1.21623480\n",
      "(1.74 min) Epoch 9/300 -- Iteration 8469 - Batch 765/963 - Train loss: 0.35625754  - Train acc: 0.8645 - Val loss: 1.21623480\n",
      "(1.74 min) Epoch 9/300 -- Iteration 8478 - Batch 774/963 - Train loss: 0.35573773  - Train acc: 0.8648 - Val loss: 1.21623480\n",
      "(1.74 min) Epoch 9/300 -- Iteration 8487 - Batch 783/963 - Train loss: 0.35548221  - Train acc: 0.8650 - Val loss: 1.21623480\n",
      "(1.75 min) Epoch 9/300 -- Iteration 8496 - Batch 792/963 - Train loss: 0.35542554  - Train acc: 0.8650 - Val loss: 1.21623480\n",
      "(1.75 min) Epoch 9/300 -- Iteration 8505 - Batch 801/963 - Train loss: 0.35585933  - Train acc: 0.8649 - Val loss: 1.21623480\n",
      "(1.75 min) Epoch 9/300 -- Iteration 8514 - Batch 810/963 - Train loss: 0.35540546  - Train acc: 0.8651 - Val loss: 1.21623480\n",
      "(1.75 min) Epoch 9/300 -- Iteration 8523 - Batch 819/963 - Train loss: 0.35499806  - Train acc: 0.8652 - Val loss: 1.21623480\n",
      "(1.75 min) Epoch 9/300 -- Iteration 8532 - Batch 828/963 - Train loss: 0.35522391  - Train acc: 0.8651 - Val loss: 1.21623480\n",
      "(1.75 min) Epoch 9/300 -- Iteration 8541 - Batch 837/963 - Train loss: 0.35555521  - Train acc: 0.8649 - Val loss: 1.21623480\n",
      "(1.76 min) Epoch 9/300 -- Iteration 8550 - Batch 846/963 - Train loss: 0.35546274  - Train acc: 0.8649 - Val loss: 1.21623480\n",
      "(1.76 min) Epoch 9/300 -- Iteration 8559 - Batch 855/963 - Train loss: 0.35548775  - Train acc: 0.8650 - Val loss: 1.21623480\n",
      "(1.76 min) Epoch 9/300 -- Iteration 8568 - Batch 864/963 - Train loss: 0.35523927  - Train acc: 0.8651 - Val loss: 1.21623480\n",
      "(1.76 min) Epoch 9/300 -- Iteration 8577 - Batch 873/963 - Train loss: 0.35537845  - Train acc: 0.8650 - Val loss: 1.21623480\n",
      "(1.76 min) Epoch 9/300 -- Iteration 8586 - Batch 882/963 - Train loss: 0.35505050  - Train acc: 0.8652 - Val loss: 1.21623480\n",
      "(1.77 min) Epoch 9/300 -- Iteration 8595 - Batch 891/963 - Train loss: 0.35485262  - Train acc: 0.8653 - Val loss: 1.21623480\n",
      "(1.77 min) Epoch 9/300 -- Iteration 8604 - Batch 900/963 - Train loss: 0.35456638  - Train acc: 0.8654 - Val loss: 1.21623480\n",
      "(1.77 min) Epoch 9/300 -- Iteration 8613 - Batch 909/963 - Train loss: 0.35471895  - Train acc: 0.8654 - Val loss: 1.21623480\n",
      "(1.77 min) Epoch 9/300 -- Iteration 8622 - Batch 918/963 - Train loss: 0.35469055  - Train acc: 0.8654 - Val loss: 1.21623480\n",
      "(1.77 min) Epoch 9/300 -- Iteration 8631 - Batch 927/963 - Train loss: 0.35482642  - Train acc: 0.8653 - Val loss: 1.21623480\n",
      "(1.77 min) Epoch 9/300 -- Iteration 8640 - Batch 936/963 - Train loss: 0.35464878  - Train acc: 0.8654 - Val loss: 1.21623480\n",
      "(1.78 min) Epoch 9/300 -- Iteration 8649 - Batch 945/963 - Train loss: 0.35460227  - Train acc: 0.8653 - Val loss: 1.21623480\n",
      "(1.78 min) Epoch 9/300 -- Iteration 8658 - Batch 954/963 - Train loss: 0.35444199  - Train acc: 0.8654 - Val loss: 1.21623480\n",
      "(1.78 min) Epoch 9/300 -- Iteration 8667 - Batch 962/963 - Train loss: 0.35459444  - Train acc: 0.8655 - Val loss: 1.21431124 - Val acc: 0.6050\n",
      "(1.78 min) Epoch 10/300 -- Iteration 8676 - Batch 9/963 - Train loss: 0.36039978  - Train acc: 0.8719 - Val loss: 1.21431124\n",
      "(1.78 min) Epoch 10/300 -- Iteration 8685 - Batch 18/963 - Train loss: 0.34273098  - Train acc: 0.8758 - Val loss: 1.21431124\n",
      "(1.79 min) Epoch 10/300 -- Iteration 8694 - Batch 27/963 - Train loss: 0.34507570  - Train acc: 0.8733 - Val loss: 1.21431124\n",
      "(1.79 min) Epoch 10/300 -- Iteration 8703 - Batch 36/963 - Train loss: 0.34527282  - Train acc: 0.8739 - Val loss: 1.21431124\n",
      "(1.79 min) Epoch 10/300 -- Iteration 8712 - Batch 45/963 - Train loss: 0.34443643  - Train acc: 0.8726 - Val loss: 1.21431124\n",
      "(1.79 min) Epoch 10/300 -- Iteration 8721 - Batch 54/963 - Train loss: 0.35024465  - Train acc: 0.8692 - Val loss: 1.21431124\n",
      "(1.79 min) Epoch 10/300 -- Iteration 8730 - Batch 63/963 - Train loss: 0.35270092  - Train acc: 0.8674 - Val loss: 1.21431124\n",
      "(1.80 min) Epoch 10/300 -- Iteration 8739 - Batch 72/963 - Train loss: 0.35344939  - Train acc: 0.8662 - Val loss: 1.21431124\n",
      "(1.80 min) Epoch 10/300 -- Iteration 8748 - Batch 81/963 - Train loss: 0.35421935  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.80 min) Epoch 10/300 -- Iteration 8757 - Batch 90/963 - Train loss: 0.35323610  - Train acc: 0.8663 - Val loss: 1.21431124\n",
      "(1.80 min) Epoch 10/300 -- Iteration 8766 - Batch 99/963 - Train loss: 0.35362525  - Train acc: 0.8662 - Val loss: 1.21431124\n",
      "(1.80 min) Epoch 10/300 -- Iteration 8775 - Batch 108/963 - Train loss: 0.35601811  - Train acc: 0.8644 - Val loss: 1.21431124\n",
      "(1.80 min) Epoch 10/300 -- Iteration 8784 - Batch 117/963 - Train loss: 0.35545405  - Train acc: 0.8650 - Val loss: 1.21431124\n",
      "(1.81 min) Epoch 10/300 -- Iteration 8793 - Batch 126/963 - Train loss: 0.35587097  - Train acc: 0.8647 - Val loss: 1.21431124\n",
      "(1.81 min) Epoch 10/300 -- Iteration 8802 - Batch 135/963 - Train loss: 0.35599526  - Train acc: 0.8641 - Val loss: 1.21431124\n",
      "(1.81 min) Epoch 10/300 -- Iteration 8811 - Batch 144/963 - Train loss: 0.35703586  - Train acc: 0.8630 - Val loss: 1.21431124\n",
      "(1.81 min) Epoch 10/300 -- Iteration 8820 - Batch 153/963 - Train loss: 0.35488812  - Train acc: 0.8644 - Val loss: 1.21431124\n",
      "(1.81 min) Epoch 10/300 -- Iteration 8829 - Batch 162/963 - Train loss: 0.35669594  - Train acc: 0.8635 - Val loss: 1.21431124\n",
      "(1.82 min) Epoch 10/300 -- Iteration 8838 - Batch 171/963 - Train loss: 0.35454653  - Train acc: 0.8642 - Val loss: 1.21431124\n",
      "(1.82 min) Epoch 10/300 -- Iteration 8847 - Batch 180/963 - Train loss: 0.35223867  - Train acc: 0.8655 - Val loss: 1.21431124\n",
      "(1.82 min) Epoch 10/300 -- Iteration 8856 - Batch 189/963 - Train loss: 0.35238445  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.82 min) Epoch 10/300 -- Iteration 8865 - Batch 198/963 - Train loss: 0.35139011  - Train acc: 0.8663 - Val loss: 1.21431124\n",
      "(1.82 min) Epoch 10/300 -- Iteration 8874 - Batch 207/963 - Train loss: 0.35217372  - Train acc: 0.8662 - Val loss: 1.21431124\n",
      "(1.82 min) Epoch 10/300 -- Iteration 8883 - Batch 216/963 - Train loss: 0.35319950  - Train acc: 0.8660 - Val loss: 1.21431124\n",
      "(1.83 min) Epoch 10/300 -- Iteration 8892 - Batch 225/963 - Train loss: 0.35414190  - Train acc: 0.8654 - Val loss: 1.21431124\n",
      "(1.83 min) Epoch 10/300 -- Iteration 8901 - Batch 234/963 - Train loss: 0.35394879  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.83 min) Epoch 10/300 -- Iteration 8910 - Batch 243/963 - Train loss: 0.35446661  - Train acc: 0.8655 - Val loss: 1.21431124\n",
      "(1.83 min) Epoch 10/300 -- Iteration 8919 - Batch 252/963 - Train loss: 0.35360490  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.83 min) Epoch 10/300 -- Iteration 8928 - Batch 261/963 - Train loss: 0.35343173  - Train acc: 0.8659 - Val loss: 1.21431124\n",
      "(1.84 min) Epoch 10/300 -- Iteration 8937 - Batch 270/963 - Train loss: 0.35271196  - Train acc: 0.8664 - Val loss: 1.21431124\n",
      "(1.84 min) Epoch 10/300 -- Iteration 8946 - Batch 279/963 - Train loss: 0.35270649  - Train acc: 0.8664 - Val loss: 1.21431124\n",
      "(1.84 min) Epoch 10/300 -- Iteration 8955 - Batch 288/963 - Train loss: 0.35275724  - Train acc: 0.8663 - Val loss: 1.21431124\n",
      "(1.84 min) Epoch 10/300 -- Iteration 8964 - Batch 297/963 - Train loss: 0.35253594  - Train acc: 0.8663 - Val loss: 1.21431124\n",
      "(1.84 min) Epoch 10/300 -- Iteration 8973 - Batch 306/963 - Train loss: 0.35326391  - Train acc: 0.8663 - Val loss: 1.21431124\n",
      "(1.84 min) Epoch 10/300 -- Iteration 8982 - Batch 315/963 - Train loss: 0.35317624  - Train acc: 0.8664 - Val loss: 1.21431124\n",
      "(1.85 min) Epoch 10/300 -- Iteration 8991 - Batch 324/963 - Train loss: 0.35311943  - Train acc: 0.8669 - Val loss: 1.21431124\n",
      "(1.85 min) Epoch 10/300 -- Iteration 9000 - Batch 333/963 - Train loss: 0.35260963  - Train acc: 0.8671 - Val loss: 1.21431124\n",
      "(1.85 min) Epoch 10/300 -- Iteration 9009 - Batch 342/963 - Train loss: 0.35264377  - Train acc: 0.8671 - Val loss: 1.21431124\n",
      "(1.85 min) Epoch 10/300 -- Iteration 9018 - Batch 351/963 - Train loss: 0.35251679  - Train acc: 0.8672 - Val loss: 1.21431124\n",
      "(1.85 min) Epoch 10/300 -- Iteration 9027 - Batch 360/963 - Train loss: 0.35253984  - Train acc: 0.8674 - Val loss: 1.21431124\n",
      "(1.86 min) Epoch 10/300 -- Iteration 9036 - Batch 369/963 - Train loss: 0.35292555  - Train acc: 0.8674 - Val loss: 1.21431124\n",
      "(1.86 min) Epoch 10/300 -- Iteration 9045 - Batch 378/963 - Train loss: 0.35284338  - Train acc: 0.8672 - Val loss: 1.21431124\n",
      "(1.86 min) Epoch 10/300 -- Iteration 9054 - Batch 387/963 - Train loss: 0.35298388  - Train acc: 0.8672 - Val loss: 1.21431124\n",
      "(1.86 min) Epoch 10/300 -- Iteration 9063 - Batch 396/963 - Train loss: 0.35237543  - Train acc: 0.8674 - Val loss: 1.21431124\n",
      "(1.86 min) Epoch 10/300 -- Iteration 9072 - Batch 405/963 - Train loss: 0.35267637  - Train acc: 0.8672 - Val loss: 1.21431124\n",
      "(1.86 min) Epoch 10/300 -- Iteration 9081 - Batch 414/963 - Train loss: 0.35317061  - Train acc: 0.8671 - Val loss: 1.21431124\n",
      "(1.87 min) Epoch 10/300 -- Iteration 9090 - Batch 423/963 - Train loss: 0.35328292  - Train acc: 0.8670 - Val loss: 1.21431124\n",
      "(1.87 min) Epoch 10/300 -- Iteration 9099 - Batch 432/963 - Train loss: 0.35365559  - Train acc: 0.8668 - Val loss: 1.21431124\n",
      "(1.87 min) Epoch 10/300 -- Iteration 9108 - Batch 441/963 - Train loss: 0.35371808  - Train acc: 0.8667 - Val loss: 1.21431124\n",
      "(1.87 min) Epoch 10/300 -- Iteration 9117 - Batch 450/963 - Train loss: 0.35393594  - Train acc: 0.8664 - Val loss: 1.21431124\n",
      "(1.87 min) Epoch 10/300 -- Iteration 9126 - Batch 459/963 - Train loss: 0.35421537  - Train acc: 0.8662 - Val loss: 1.21431124\n",
      "(1.88 min) Epoch 10/300 -- Iteration 9135 - Batch 468/963 - Train loss: 0.35443769  - Train acc: 0.8661 - Val loss: 1.21431124\n",
      "(1.88 min) Epoch 10/300 -- Iteration 9144 - Batch 477/963 - Train loss: 0.35448331  - Train acc: 0.8662 - Val loss: 1.21431124\n",
      "(1.88 min) Epoch 10/300 -- Iteration 9153 - Batch 486/963 - Train loss: 0.35455731  - Train acc: 0.8663 - Val loss: 1.21431124\n",
      "(1.88 min) Epoch 10/300 -- Iteration 9162 - Batch 495/963 - Train loss: 0.35447625  - Train acc: 0.8662 - Val loss: 1.21431124\n",
      "(1.88 min) Epoch 10/300 -- Iteration 9171 - Batch 504/963 - Train loss: 0.35449084  - Train acc: 0.8662 - Val loss: 1.21431124\n",
      "(1.88 min) Epoch 10/300 -- Iteration 9180 - Batch 513/963 - Train loss: 0.35482697  - Train acc: 0.8660 - Val loss: 1.21431124\n",
      "(1.89 min) Epoch 10/300 -- Iteration 9189 - Batch 522/963 - Train loss: 0.35426054  - Train acc: 0.8663 - Val loss: 1.21431124\n",
      "(1.89 min) Epoch 10/300 -- Iteration 9198 - Batch 531/963 - Train loss: 0.35403490  - Train acc: 0.8665 - Val loss: 1.21431124\n",
      "(1.89 min) Epoch 10/300 -- Iteration 9207 - Batch 540/963 - Train loss: 0.35394495  - Train acc: 0.8665 - Val loss: 1.21431124\n",
      "(1.89 min) Epoch 10/300 -- Iteration 9216 - Batch 549/963 - Train loss: 0.35415096  - Train acc: 0.8662 - Val loss: 1.21431124\n",
      "(1.89 min) Epoch 10/300 -- Iteration 9225 - Batch 558/963 - Train loss: 0.35424085  - Train acc: 0.8661 - Val loss: 1.21431124\n",
      "(1.89 min) Epoch 10/300 -- Iteration 9234 - Batch 567/963 - Train loss: 0.35427074  - Train acc: 0.8659 - Val loss: 1.21431124\n",
      "(1.90 min) Epoch 10/300 -- Iteration 9243 - Batch 576/963 - Train loss: 0.35437544  - Train acc: 0.8660 - Val loss: 1.21431124\n",
      "(1.90 min) Epoch 10/300 -- Iteration 9252 - Batch 585/963 - Train loss: 0.35392652  - Train acc: 0.8661 - Val loss: 1.21431124\n",
      "(1.90 min) Epoch 10/300 -- Iteration 9261 - Batch 594/963 - Train loss: 0.35450676  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.90 min) Epoch 10/300 -- Iteration 9270 - Batch 603/963 - Train loss: 0.35460842  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.90 min) Epoch 10/300 -- Iteration 9279 - Batch 612/963 - Train loss: 0.35496547  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.91 min) Epoch 10/300 -- Iteration 9288 - Batch 621/963 - Train loss: 0.35481895  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.91 min) Epoch 10/300 -- Iteration 9297 - Batch 630/963 - Train loss: 0.35498713  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.91 min) Epoch 10/300 -- Iteration 9306 - Batch 639/963 - Train loss: 0.35488065  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.91 min) Epoch 10/300 -- Iteration 9315 - Batch 648/963 - Train loss: 0.35486255  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.91 min) Epoch 10/300 -- Iteration 9324 - Batch 657/963 - Train loss: 0.35466392  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.92 min) Epoch 10/300 -- Iteration 9333 - Batch 666/963 - Train loss: 0.35467624  - Train acc: 0.8659 - Val loss: 1.21431124\n",
      "(1.92 min) Epoch 10/300 -- Iteration 9342 - Batch 675/963 - Train loss: 0.35429471  - Train acc: 0.8660 - Val loss: 1.21431124\n",
      "(1.92 min) Epoch 10/300 -- Iteration 9351 - Batch 684/963 - Train loss: 0.35456961  - Train acc: 0.8659 - Val loss: 1.21431124\n",
      "(1.92 min) Epoch 10/300 -- Iteration 9360 - Batch 693/963 - Train loss: 0.35463189  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.92 min) Epoch 10/300 -- Iteration 9369 - Batch 702/963 - Train loss: 0.35479550  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.92 min) Epoch 10/300 -- Iteration 9378 - Batch 711/963 - Train loss: 0.35473432  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.93 min) Epoch 10/300 -- Iteration 9387 - Batch 720/963 - Train loss: 0.35458910  - Train acc: 0.8655 - Val loss: 1.21431124\n",
      "(1.93 min) Epoch 10/300 -- Iteration 9396 - Batch 729/963 - Train loss: 0.35477403  - Train acc: 0.8655 - Val loss: 1.21431124\n",
      "(1.93 min) Epoch 10/300 -- Iteration 9405 - Batch 738/963 - Train loss: 0.35470328  - Train acc: 0.8655 - Val loss: 1.21431124\n",
      "(1.93 min) Epoch 10/300 -- Iteration 9414 - Batch 747/963 - Train loss: 0.35438646  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.93 min) Epoch 10/300 -- Iteration 9423 - Batch 756/963 - Train loss: 0.35415350  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.94 min) Epoch 10/300 -- Iteration 9432 - Batch 765/963 - Train loss: 0.35395112  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.94 min) Epoch 10/300 -- Iteration 9441 - Batch 774/963 - Train loss: 0.35406090  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.94 min) Epoch 10/300 -- Iteration 9450 - Batch 783/963 - Train loss: 0.35410466  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.94 min) Epoch 10/300 -- Iteration 9459 - Batch 792/963 - Train loss: 0.35441046  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.94 min) Epoch 10/300 -- Iteration 9468 - Batch 801/963 - Train loss: 0.35460538  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.94 min) Epoch 10/300 -- Iteration 9477 - Batch 810/963 - Train loss: 0.35447199  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.95 min) Epoch 10/300 -- Iteration 9486 - Batch 819/963 - Train loss: 0.35464462  - Train acc: 0.8655 - Val loss: 1.21431124\n",
      "(1.95 min) Epoch 10/300 -- Iteration 9495 - Batch 828/963 - Train loss: 0.35470576  - Train acc: 0.8654 - Val loss: 1.21431124\n",
      "(1.95 min) Epoch 10/300 -- Iteration 9504 - Batch 837/963 - Train loss: 0.35443870  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.95 min) Epoch 10/300 -- Iteration 9513 - Batch 846/963 - Train loss: 0.35465716  - Train acc: 0.8655 - Val loss: 1.21431124\n",
      "(1.95 min) Epoch 10/300 -- Iteration 9522 - Batch 855/963 - Train loss: 0.35430899  - Train acc: 0.8655 - Val loss: 1.21431124\n",
      "(1.95 min) Epoch 10/300 -- Iteration 9531 - Batch 864/963 - Train loss: 0.35446409  - Train acc: 0.8655 - Val loss: 1.21431124\n",
      "(1.96 min) Epoch 10/300 -- Iteration 9540 - Batch 873/963 - Train loss: 0.35382261  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.96 min) Epoch 10/300 -- Iteration 9549 - Batch 882/963 - Train loss: 0.35388994  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.96 min) Epoch 10/300 -- Iteration 9558 - Batch 891/963 - Train loss: 0.35391673  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.96 min) Epoch 10/300 -- Iteration 9567 - Batch 900/963 - Train loss: 0.35382955  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.96 min) Epoch 10/300 -- Iteration 9576 - Batch 909/963 - Train loss: 0.35379281  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.97 min) Epoch 10/300 -- Iteration 9585 - Batch 918/963 - Train loss: 0.35368236  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.97 min) Epoch 10/300 -- Iteration 9594 - Batch 927/963 - Train loss: 0.35369002  - Train acc: 0.8658 - Val loss: 1.21431124\n",
      "(1.97 min) Epoch 10/300 -- Iteration 9603 - Batch 936/963 - Train loss: 0.35386520  - Train acc: 0.8656 - Val loss: 1.21431124\n",
      "(1.97 min) Epoch 10/300 -- Iteration 9612 - Batch 945/963 - Train loss: 0.35365857  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.97 min) Epoch 10/300 -- Iteration 9621 - Batch 954/963 - Train loss: 0.35360483  - Train acc: 0.8657 - Val loss: 1.21431124\n",
      "(1.97 min) Epoch 10/300 -- Iteration 9630 - Batch 962/963 - Train loss: 0.35361434  - Train acc: 0.8657 - Val loss: 1.20524800 - Val acc: 0.6067\n",
      "(1.98 min) Epoch 11/300 -- Iteration 9639 - Batch 9/963 - Train loss: 0.33103471  - Train acc: 0.8703 - Val loss: 1.20524800\n",
      "(1.98 min) Epoch 11/300 -- Iteration 9648 - Batch 18/963 - Train loss: 0.33290794  - Train acc: 0.8709 - Val loss: 1.20524800\n",
      "(1.98 min) Epoch 11/300 -- Iteration 9657 - Batch 27/963 - Train loss: 0.32963033  - Train acc: 0.8744 - Val loss: 1.20524800\n",
      "(1.98 min) Epoch 11/300 -- Iteration 9666 - Batch 36/963 - Train loss: 0.33395804  - Train acc: 0.8735 - Val loss: 1.20524800\n",
      "(1.98 min) Epoch 11/300 -- Iteration 9675 - Batch 45/963 - Train loss: 0.33499645  - Train acc: 0.8726 - Val loss: 1.20524800\n",
      "(1.99 min) Epoch 11/300 -- Iteration 9684 - Batch 54/963 - Train loss: 0.34550979  - Train acc: 0.8696 - Val loss: 1.20524800\n",
      "(1.99 min) Epoch 11/300 -- Iteration 9693 - Batch 63/963 - Train loss: 0.34484796  - Train acc: 0.8694 - Val loss: 1.20524800\n",
      "(1.99 min) Epoch 11/300 -- Iteration 9702 - Batch 72/963 - Train loss: 0.34527763  - Train acc: 0.8689 - Val loss: 1.20524800\n",
      "(1.99 min) Epoch 11/300 -- Iteration 9711 - Batch 81/963 - Train loss: 0.35132034  - Train acc: 0.8675 - Val loss: 1.20524800\n",
      "(1.99 min) Epoch 11/300 -- Iteration 9720 - Batch 90/963 - Train loss: 0.35176744  - Train acc: 0.8668 - Val loss: 1.20524800\n",
      "(2.00 min) Epoch 11/300 -- Iteration 9729 - Batch 99/963 - Train loss: 0.34984492  - Train acc: 0.8673 - Val loss: 1.20524800\n",
      "(2.00 min) Epoch 11/300 -- Iteration 9738 - Batch 108/963 - Train loss: 0.34847055  - Train acc: 0.8680 - Val loss: 1.20524800\n",
      "(2.00 min) Epoch 11/300 -- Iteration 9747 - Batch 117/963 - Train loss: 0.34705825  - Train acc: 0.8685 - Val loss: 1.20524800\n",
      "(2.00 min) Epoch 11/300 -- Iteration 9756 - Batch 126/963 - Train loss: 0.34755837  - Train acc: 0.8685 - Val loss: 1.20524800\n",
      "(2.00 min) Epoch 11/300 -- Iteration 9765 - Batch 135/963 - Train loss: 0.34749448  - Train acc: 0.8684 - Val loss: 1.20524800\n",
      "(2.00 min) Epoch 11/300 -- Iteration 9774 - Batch 144/963 - Train loss: 0.34949412  - Train acc: 0.8676 - Val loss: 1.20524800\n",
      "(2.01 min) Epoch 11/300 -- Iteration 9783 - Batch 153/963 - Train loss: 0.34947272  - Train acc: 0.8675 - Val loss: 1.20524800\n",
      "(2.01 min) Epoch 11/300 -- Iteration 9792 - Batch 162/963 - Train loss: 0.34928773  - Train acc: 0.8679 - Val loss: 1.20524800\n",
      "(2.01 min) Epoch 11/300 -- Iteration 9801 - Batch 171/963 - Train loss: 0.35064614  - Train acc: 0.8673 - Val loss: 1.20524800\n",
      "(2.01 min) Epoch 11/300 -- Iteration 9810 - Batch 180/963 - Train loss: 0.35001918  - Train acc: 0.8672 - Val loss: 1.20524800\n",
      "(2.01 min) Epoch 11/300 -- Iteration 9819 - Batch 189/963 - Train loss: 0.35039502  - Train acc: 0.8668 - Val loss: 1.20524800\n",
      "(2.02 min) Epoch 11/300 -- Iteration 9828 - Batch 198/963 - Train loss: 0.35005485  - Train acc: 0.8667 - Val loss: 1.20524800\n",
      "(2.02 min) Epoch 11/300 -- Iteration 9837 - Batch 207/963 - Train loss: 0.35042593  - Train acc: 0.8667 - Val loss: 1.20524800\n",
      "(2.02 min) Epoch 11/300 -- Iteration 9846 - Batch 216/963 - Train loss: 0.34978730  - Train acc: 0.8670 - Val loss: 1.20524800\n",
      "(2.02 min) Epoch 11/300 -- Iteration 9855 - Batch 225/963 - Train loss: 0.34910934  - Train acc: 0.8671 - Val loss: 1.20524800\n",
      "(2.02 min) Epoch 11/300 -- Iteration 9864 - Batch 234/963 - Train loss: 0.35034589  - Train acc: 0.8668 - Val loss: 1.20524800\n",
      "(2.02 min) Epoch 11/300 -- Iteration 9873 - Batch 243/963 - Train loss: 0.34983668  - Train acc: 0.8672 - Val loss: 1.20524800\n",
      "(2.03 min) Epoch 11/300 -- Iteration 9882 - Batch 252/963 - Train loss: 0.35050585  - Train acc: 0.8672 - Val loss: 1.20524800\n",
      "(2.03 min) Epoch 11/300 -- Iteration 9891 - Batch 261/963 - Train loss: 0.35035679  - Train acc: 0.8673 - Val loss: 1.20524800\n",
      "(2.03 min) Epoch 11/300 -- Iteration 9900 - Batch 270/963 - Train loss: 0.34868024  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.03 min) Epoch 11/300 -- Iteration 9909 - Batch 279/963 - Train loss: 0.34894206  - Train acc: 0.8681 - Val loss: 1.20524800\n",
      "(2.03 min) Epoch 11/300 -- Iteration 9918 - Batch 288/963 - Train loss: 0.34918250  - Train acc: 0.8679 - Val loss: 1.20524800\n",
      "(2.04 min) Epoch 11/300 -- Iteration 9927 - Batch 297/963 - Train loss: 0.34931971  - Train acc: 0.8681 - Val loss: 1.20524800\n",
      "(2.04 min) Epoch 11/300 -- Iteration 9936 - Batch 306/963 - Train loss: 0.34901871  - Train acc: 0.8684 - Val loss: 1.20524800\n",
      "(2.04 min) Epoch 11/300 -- Iteration 9945 - Batch 315/963 - Train loss: 0.34881407  - Train acc: 0.8686 - Val loss: 1.20524800\n",
      "(2.04 min) Epoch 11/300 -- Iteration 9954 - Batch 324/963 - Train loss: 0.34930841  - Train acc: 0.8686 - Val loss: 1.20524800\n",
      "(2.04 min) Epoch 11/300 -- Iteration 9963 - Batch 333/963 - Train loss: 0.34904699  - Train acc: 0.8685 - Val loss: 1.20524800\n",
      "(2.04 min) Epoch 11/300 -- Iteration 9972 - Batch 342/963 - Train loss: 0.34904754  - Train acc: 0.8686 - Val loss: 1.20524800\n",
      "(2.05 min) Epoch 11/300 -- Iteration 9981 - Batch 351/963 - Train loss: 0.34959787  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.05 min) Epoch 11/300 -- Iteration 9990 - Batch 360/963 - Train loss: 0.34916313  - Train acc: 0.8684 - Val loss: 1.20524800\n",
      "(2.05 min) Epoch 11/300 -- Iteration 9999 - Batch 369/963 - Train loss: 0.34904847  - Train acc: 0.8685 - Val loss: 1.20524800\n",
      "(2.05 min) Epoch 11/300 -- Iteration 10008 - Batch 378/963 - Train loss: 0.34870887  - Train acc: 0.8688 - Val loss: 1.20524800\n",
      "(2.05 min) Epoch 11/300 -- Iteration 10017 - Batch 387/963 - Train loss: 0.34873751  - Train acc: 0.8688 - Val loss: 1.20524800\n",
      "(2.06 min) Epoch 11/300 -- Iteration 10026 - Batch 396/963 - Train loss: 0.34840769  - Train acc: 0.8688 - Val loss: 1.20524800\n",
      "(2.06 min) Epoch 11/300 -- Iteration 10035 - Batch 405/963 - Train loss: 0.34902200  - Train acc: 0.8687 - Val loss: 1.20524800\n",
      "(2.06 min) Epoch 11/300 -- Iteration 10044 - Batch 414/963 - Train loss: 0.34922321  - Train acc: 0.8685 - Val loss: 1.20524800\n",
      "(2.06 min) Epoch 11/300 -- Iteration 10053 - Batch 423/963 - Train loss: 0.34871100  - Train acc: 0.8690 - Val loss: 1.20524800\n",
      "(2.06 min) Epoch 11/300 -- Iteration 10062 - Batch 432/963 - Train loss: 0.34859451  - Train acc: 0.8692 - Val loss: 1.20524800\n",
      "(2.06 min) Epoch 11/300 -- Iteration 10071 - Batch 441/963 - Train loss: 0.34822621  - Train acc: 0.8691 - Val loss: 1.20524800\n",
      "(2.07 min) Epoch 11/300 -- Iteration 10080 - Batch 450/963 - Train loss: 0.34848333  - Train acc: 0.8691 - Val loss: 1.20524800\n",
      "(2.07 min) Epoch 11/300 -- Iteration 10089 - Batch 459/963 - Train loss: 0.34800964  - Train acc: 0.8692 - Val loss: 1.20524800\n",
      "(2.07 min) Epoch 11/300 -- Iteration 10098 - Batch 468/963 - Train loss: 0.34774442  - Train acc: 0.8693 - Val loss: 1.20524800\n",
      "(2.07 min) Epoch 11/300 -- Iteration 10107 - Batch 477/963 - Train loss: 0.34758768  - Train acc: 0.8694 - Val loss: 1.20524800\n",
      "(2.07 min) Epoch 11/300 -- Iteration 10116 - Batch 486/963 - Train loss: 0.34721223  - Train acc: 0.8695 - Val loss: 1.20524800\n",
      "(2.08 min) Epoch 11/300 -- Iteration 10125 - Batch 495/963 - Train loss: 0.34755472  - Train acc: 0.8693 - Val loss: 1.20524800\n",
      "(2.08 min) Epoch 11/300 -- Iteration 10134 - Batch 504/963 - Train loss: 0.34708981  - Train acc: 0.8694 - Val loss: 1.20524800\n",
      "(2.08 min) Epoch 11/300 -- Iteration 10143 - Batch 513/963 - Train loss: 0.34732577  - Train acc: 0.8693 - Val loss: 1.20524800\n",
      "(2.08 min) Epoch 11/300 -- Iteration 10152 - Batch 522/963 - Train loss: 0.34749079  - Train acc: 0.8692 - Val loss: 1.20524800\n",
      "(2.08 min) Epoch 11/300 -- Iteration 10161 - Batch 531/963 - Train loss: 0.34716418  - Train acc: 0.8690 - Val loss: 1.20524800\n",
      "(2.09 min) Epoch 11/300 -- Iteration 10170 - Batch 540/963 - Train loss: 0.34661627  - Train acc: 0.8692 - Val loss: 1.20524800\n",
      "(2.09 min) Epoch 11/300 -- Iteration 10179 - Batch 549/963 - Train loss: 0.34663505  - Train acc: 0.8690 - Val loss: 1.20524800\n",
      "(2.09 min) Epoch 11/300 -- Iteration 10188 - Batch 558/963 - Train loss: 0.34655320  - Train acc: 0.8691 - Val loss: 1.20524800\n",
      "(2.09 min) Epoch 11/300 -- Iteration 10197 - Batch 567/963 - Train loss: 0.34662961  - Train acc: 0.8690 - Val loss: 1.20524800\n",
      "(2.09 min) Epoch 11/300 -- Iteration 10206 - Batch 576/963 - Train loss: 0.34709543  - Train acc: 0.8687 - Val loss: 1.20524800\n",
      "(2.09 min) Epoch 11/300 -- Iteration 10215 - Batch 585/963 - Train loss: 0.34727250  - Train acc: 0.8685 - Val loss: 1.20524800\n",
      "(2.10 min) Epoch 11/300 -- Iteration 10224 - Batch 594/963 - Train loss: 0.34747880  - Train acc: 0.8684 - Val loss: 1.20524800\n",
      "(2.10 min) Epoch 11/300 -- Iteration 10233 - Batch 603/963 - Train loss: 0.34756077  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.10 min) Epoch 11/300 -- Iteration 10242 - Batch 612/963 - Train loss: 0.34736224  - Train acc: 0.8685 - Val loss: 1.20524800\n",
      "(2.10 min) Epoch 11/300 -- Iteration 10251 - Batch 621/963 - Train loss: 0.34731050  - Train acc: 0.8685 - Val loss: 1.20524800\n",
      "(2.10 min) Epoch 11/300 -- Iteration 10260 - Batch 630/963 - Train loss: 0.34802889  - Train acc: 0.8681 - Val loss: 1.20524800\n",
      "(2.11 min) Epoch 11/300 -- Iteration 10269 - Batch 639/963 - Train loss: 0.34783379  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.11 min) Epoch 11/300 -- Iteration 10278 - Batch 648/963 - Train loss: 0.34792847  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.11 min) Epoch 11/300 -- Iteration 10287 - Batch 657/963 - Train loss: 0.34807190  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.11 min) Epoch 11/300 -- Iteration 10296 - Batch 666/963 - Train loss: 0.34812267  - Train acc: 0.8681 - Val loss: 1.20524800\n",
      "(2.11 min) Epoch 11/300 -- Iteration 10305 - Batch 675/963 - Train loss: 0.34809381  - Train acc: 0.8682 - Val loss: 1.20524800\n",
      "(2.11 min) Epoch 11/300 -- Iteration 10314 - Batch 684/963 - Train loss: 0.34833015  - Train acc: 0.8681 - Val loss: 1.20524800\n",
      "(2.12 min) Epoch 11/300 -- Iteration 10323 - Batch 693/963 - Train loss: 0.34828527  - Train acc: 0.8680 - Val loss: 1.20524800\n",
      "(2.12 min) Epoch 11/300 -- Iteration 10332 - Batch 702/963 - Train loss: 0.34845884  - Train acc: 0.8680 - Val loss: 1.20524800\n",
      "(2.12 min) Epoch 11/300 -- Iteration 10341 - Batch 711/963 - Train loss: 0.34799618  - Train acc: 0.8681 - Val loss: 1.20524800\n",
      "(2.12 min) Epoch 11/300 -- Iteration 10350 - Batch 720/963 - Train loss: 0.34803554  - Train acc: 0.8682 - Val loss: 1.20524800\n",
      "(2.12 min) Epoch 11/300 -- Iteration 10359 - Batch 729/963 - Train loss: 0.34805330  - Train acc: 0.8681 - Val loss: 1.20524800\n",
      "(2.13 min) Epoch 11/300 -- Iteration 10368 - Batch 738/963 - Train loss: 0.34786390  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.13 min) Epoch 11/300 -- Iteration 10377 - Batch 747/963 - Train loss: 0.34787695  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.13 min) Epoch 11/300 -- Iteration 10386 - Batch 756/963 - Train loss: 0.34811885  - Train acc: 0.8682 - Val loss: 1.20524800\n",
      "(2.13 min) Epoch 11/300 -- Iteration 10395 - Batch 765/963 - Train loss: 0.34819827  - Train acc: 0.8682 - Val loss: 1.20524800\n",
      "(2.13 min) Epoch 11/300 -- Iteration 10404 - Batch 774/963 - Train loss: 0.34811570  - Train acc: 0.8682 - Val loss: 1.20524800\n",
      "(2.13 min) Epoch 11/300 -- Iteration 10413 - Batch 783/963 - Train loss: 0.34796094  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.14 min) Epoch 11/300 -- Iteration 10422 - Batch 792/963 - Train loss: 0.34792808  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.14 min) Epoch 11/300 -- Iteration 10431 - Batch 801/963 - Train loss: 0.34803257  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.14 min) Epoch 11/300 -- Iteration 10440 - Batch 810/963 - Train loss: 0.34814369  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.14 min) Epoch 11/300 -- Iteration 10449 - Batch 819/963 - Train loss: 0.34805045  - Train acc: 0.8683 - Val loss: 1.20524800\n",
      "(2.14 min) Epoch 11/300 -- Iteration 10458 - Batch 828/963 - Train loss: 0.34813566  - Train acc: 0.8682 - Val loss: 1.20524800\n",
      "(2.15 min) Epoch 11/300 -- Iteration 10467 - Batch 837/963 - Train loss: 0.34854874  - Train acc: 0.8681 - Val loss: 1.20524800\n",
      "(2.15 min) Epoch 11/300 -- Iteration 10476 - Batch 846/963 - Train loss: 0.34866099  - Train acc: 0.8681 - Val loss: 1.20524800\n",
      "(2.15 min) Epoch 11/300 -- Iteration 10485 - Batch 855/963 - Train loss: 0.34857939  - Train acc: 0.8680 - Val loss: 1.20524800\n",
      "(2.15 min) Epoch 11/300 -- Iteration 10494 - Batch 864/963 - Train loss: 0.34841735  - Train acc: 0.8680 - Val loss: 1.20524800\n",
      "(2.15 min) Epoch 11/300 -- Iteration 10503 - Batch 873/963 - Train loss: 0.34853890  - Train acc: 0.8679 - Val loss: 1.20524800\n",
      "(2.15 min) Epoch 11/300 -- Iteration 10512 - Batch 882/963 - Train loss: 0.34843230  - Train acc: 0.8679 - Val loss: 1.20524800\n",
      "(2.16 min) Epoch 11/300 -- Iteration 10521 - Batch 891/963 - Train loss: 0.34836443  - Train acc: 0.8679 - Val loss: 1.20524800\n",
      "(2.16 min) Epoch 11/300 -- Iteration 10530 - Batch 900/963 - Train loss: 0.34829656  - Train acc: 0.8679 - Val loss: 1.20524800\n",
      "(2.16 min) Epoch 11/300 -- Iteration 10539 - Batch 909/963 - Train loss: 0.34815058  - Train acc: 0.8680 - Val loss: 1.20524800\n",
      "(2.16 min) Epoch 11/300 -- Iteration 10548 - Batch 918/963 - Train loss: 0.34821526  - Train acc: 0.8679 - Val loss: 1.20524800\n",
      "(2.16 min) Epoch 11/300 -- Iteration 10557 - Batch 927/963 - Train loss: 0.34818408  - Train acc: 0.8679 - Val loss: 1.20524800\n",
      "(2.17 min) Epoch 11/300 -- Iteration 10566 - Batch 936/963 - Train loss: 0.34804726  - Train acc: 0.8679 - Val loss: 1.20524800\n",
      "(2.17 min) Epoch 11/300 -- Iteration 10575 - Batch 945/963 - Train loss: 0.34782747  - Train acc: 0.8680 - Val loss: 1.20524800\n",
      "(2.17 min) Epoch 11/300 -- Iteration 10584 - Batch 954/963 - Train loss: 0.34772255  - Train acc: 0.8680 - Val loss: 1.20524800\n",
      "(2.17 min) Epoch 11/300 -- Iteration 10593 - Batch 962/963 - Train loss: 0.34734270  - Train acc: 0.8682 - Val loss: 1.20747805 - Val acc: 0.6083\n",
      "(2.17 min) Epoch 12/300 -- Iteration 10602 - Batch 9/963 - Train loss: 0.32743149  - Train acc: 0.8828 - Val loss: 1.20747805\n",
      "(2.18 min) Epoch 12/300 -- Iteration 10611 - Batch 18/963 - Train loss: 0.34136313  - Train acc: 0.8758 - Val loss: 1.20747805\n",
      "(2.18 min) Epoch 12/300 -- Iteration 10620 - Batch 27/963 - Train loss: 0.34392648  - Train acc: 0.8750 - Val loss: 1.20747805\n",
      "(2.18 min) Epoch 12/300 -- Iteration 10629 - Batch 36/963 - Train loss: 0.34559880  - Train acc: 0.8701 - Val loss: 1.20747805\n",
      "(2.18 min) Epoch 12/300 -- Iteration 10638 - Batch 45/963 - Train loss: 0.34465989  - Train acc: 0.8709 - Val loss: 1.20747805\n",
      "(2.18 min) Epoch 12/300 -- Iteration 10647 - Batch 54/963 - Train loss: 0.34424514  - Train acc: 0.8709 - Val loss: 1.20747805\n",
      "(2.18 min) Epoch 12/300 -- Iteration 10656 - Batch 63/963 - Train loss: 0.34279715  - Train acc: 0.8718 - Val loss: 1.20747805\n",
      "(2.19 min) Epoch 12/300 -- Iteration 10665 - Batch 72/963 - Train loss: 0.34485991  - Train acc: 0.8717 - Val loss: 1.20747805\n",
      "(2.19 min) Epoch 12/300 -- Iteration 10674 - Batch 81/963 - Train loss: 0.34745951  - Train acc: 0.8704 - Val loss: 1.20747805\n",
      "(2.19 min) Epoch 12/300 -- Iteration 10683 - Batch 90/963 - Train loss: 0.34720051  - Train acc: 0.8706 - Val loss: 1.20747805\n",
      "(2.19 min) Epoch 12/300 -- Iteration 10692 - Batch 99/963 - Train loss: 0.34664247  - Train acc: 0.8713 - Val loss: 1.20747805\n",
      "(2.19 min) Epoch 12/300 -- Iteration 10701 - Batch 108/963 - Train loss: 0.34646497  - Train acc: 0.8708 - Val loss: 1.20747805\n",
      "(2.20 min) Epoch 12/300 -- Iteration 10710 - Batch 117/963 - Train loss: 0.34629275  - Train acc: 0.8708 - Val loss: 1.20747805\n",
      "(2.20 min) Epoch 12/300 -- Iteration 10719 - Batch 126/963 - Train loss: 0.34733625  - Train acc: 0.8704 - Val loss: 1.20747805\n",
      "(2.20 min) Epoch 12/300 -- Iteration 10728 - Batch 135/963 - Train loss: 0.34644529  - Train acc: 0.8706 - Val loss: 1.20747805\n",
      "(2.20 min) Epoch 12/300 -- Iteration 10737 - Batch 144/963 - Train loss: 0.34517502  - Train acc: 0.8715 - Val loss: 1.20747805\n",
      "(2.20 min) Epoch 12/300 -- Iteration 10746 - Batch 153/963 - Train loss: 0.34556849  - Train acc: 0.8708 - Val loss: 1.20747805\n",
      "(2.20 min) Epoch 12/300 -- Iteration 10755 - Batch 162/963 - Train loss: 0.34633812  - Train acc: 0.8704 - Val loss: 1.20747805\n",
      "(2.21 min) Epoch 12/300 -- Iteration 10764 - Batch 171/963 - Train loss: 0.34487973  - Train acc: 0.8710 - Val loss: 1.20747805\n",
      "(2.21 min) Epoch 12/300 -- Iteration 10773 - Batch 180/963 - Train loss: 0.34464201  - Train acc: 0.8712 - Val loss: 1.20747805\n",
      "(2.21 min) Epoch 12/300 -- Iteration 10782 - Batch 189/963 - Train loss: 0.34600819  - Train acc: 0.8703 - Val loss: 1.20747805\n",
      "(2.21 min) Epoch 12/300 -- Iteration 10791 - Batch 198/963 - Train loss: 0.34592794  - Train acc: 0.8702 - Val loss: 1.20747805\n",
      "(2.21 min) Epoch 12/300 -- Iteration 10800 - Batch 207/963 - Train loss: 0.34591767  - Train acc: 0.8700 - Val loss: 1.20747805\n",
      "(2.22 min) Epoch 12/300 -- Iteration 10809 - Batch 216/963 - Train loss: 0.34705964  - Train acc: 0.8697 - Val loss: 1.20747805\n",
      "(2.22 min) Epoch 12/300 -- Iteration 10818 - Batch 225/963 - Train loss: 0.34579068  - Train acc: 0.8705 - Val loss: 1.20747805\n",
      "(2.22 min) Epoch 12/300 -- Iteration 10827 - Batch 234/963 - Train loss: 0.34567596  - Train acc: 0.8699 - Val loss: 1.20747805\n",
      "(2.22 min) Epoch 12/300 -- Iteration 10836 - Batch 243/963 - Train loss: 0.34541978  - Train acc: 0.8702 - Val loss: 1.20747805\n",
      "(2.22 min) Epoch 12/300 -- Iteration 10845 - Batch 252/963 - Train loss: 0.34532841  - Train acc: 0.8702 - Val loss: 1.20747805\n",
      "(2.22 min) Epoch 12/300 -- Iteration 10854 - Batch 261/963 - Train loss: 0.34451490  - Train acc: 0.8705 - Val loss: 1.20747805\n",
      "(2.23 min) Epoch 12/300 -- Iteration 10863 - Batch 270/963 - Train loss: 0.34473985  - Train acc: 0.8704 - Val loss: 1.20747805\n",
      "(2.23 min) Epoch 12/300 -- Iteration 10872 - Batch 279/963 - Train loss: 0.34479341  - Train acc: 0.8706 - Val loss: 1.20747805\n",
      "(2.23 min) Epoch 12/300 -- Iteration 10881 - Batch 288/963 - Train loss: 0.34423713  - Train acc: 0.8709 - Val loss: 1.20747805\n",
      "(2.23 min) Epoch 12/300 -- Iteration 10890 - Batch 297/963 - Train loss: 0.34519220  - Train acc: 0.8704 - Val loss: 1.20747805\n",
      "(2.23 min) Epoch 12/300 -- Iteration 10899 - Batch 306/963 - Train loss: 0.34446153  - Train acc: 0.8706 - Val loss: 1.20747805\n",
      "(2.24 min) Epoch 12/300 -- Iteration 10908 - Batch 315/963 - Train loss: 0.34461671  - Train acc: 0.8705 - Val loss: 1.20747805\n",
      "(2.24 min) Epoch 12/300 -- Iteration 10917 - Batch 324/963 - Train loss: 0.34425319  - Train acc: 0.8707 - Val loss: 1.20747805\n",
      "(2.24 min) Epoch 12/300 -- Iteration 10926 - Batch 333/963 - Train loss: 0.34413730  - Train acc: 0.8710 - Val loss: 1.20747805\n",
      "(2.24 min) Epoch 12/300 -- Iteration 10935 - Batch 342/963 - Train loss: 0.34462092  - Train acc: 0.8708 - Val loss: 1.20747805\n",
      "(2.24 min) Epoch 12/300 -- Iteration 10944 - Batch 351/963 - Train loss: 0.34422101  - Train acc: 0.8707 - Val loss: 1.20747805\n",
      "(2.24 min) Epoch 12/300 -- Iteration 10953 - Batch 360/963 - Train loss: 0.34382329  - Train acc: 0.8710 - Val loss: 1.20747805\n",
      "(2.25 min) Epoch 12/300 -- Iteration 10962 - Batch 369/963 - Train loss: 0.34367038  - Train acc: 0.8711 - Val loss: 1.20747805\n",
      "(2.25 min) Epoch 12/300 -- Iteration 10971 - Batch 378/963 - Train loss: 0.34442259  - Train acc: 0.8707 - Val loss: 1.20747805\n",
      "(2.25 min) Epoch 12/300 -- Iteration 10980 - Batch 387/963 - Train loss: 0.34507416  - Train acc: 0.8703 - Val loss: 1.20747805\n",
      "(2.25 min) Epoch 12/300 -- Iteration 10989 - Batch 396/963 - Train loss: 0.34477381  - Train acc: 0.8703 - Val loss: 1.20747805\n",
      "(2.25 min) Epoch 12/300 -- Iteration 10998 - Batch 405/963 - Train loss: 0.34460494  - Train acc: 0.8704 - Val loss: 1.20747805\n",
      "(2.26 min) Epoch 12/300 -- Iteration 11007 - Batch 414/963 - Train loss: 0.34465992  - Train acc: 0.8701 - Val loss: 1.20747805\n",
      "(2.26 min) Epoch 12/300 -- Iteration 11016 - Batch 423/963 - Train loss: 0.34524494  - Train acc: 0.8700 - Val loss: 1.20747805\n",
      "(2.26 min) Epoch 12/300 -- Iteration 11025 - Batch 432/963 - Train loss: 0.34497747  - Train acc: 0.8701 - Val loss: 1.20747805\n",
      "(2.26 min) Epoch 12/300 -- Iteration 11034 - Batch 441/963 - Train loss: 0.34538283  - Train acc: 0.8699 - Val loss: 1.20747805\n",
      "(2.26 min) Epoch 12/300 -- Iteration 11043 - Batch 450/963 - Train loss: 0.34516830  - Train acc: 0.8698 - Val loss: 1.20747805\n",
      "(2.26 min) Epoch 12/300 -- Iteration 11052 - Batch 459/963 - Train loss: 0.34542713  - Train acc: 0.8695 - Val loss: 1.20747805\n",
      "(2.27 min) Epoch 12/300 -- Iteration 11061 - Batch 468/963 - Train loss: 0.34503713  - Train acc: 0.8694 - Val loss: 1.20747805\n",
      "(2.27 min) Epoch 12/300 -- Iteration 11070 - Batch 477/963 - Train loss: 0.34485388  - Train acc: 0.8692 - Val loss: 1.20747805\n",
      "(2.27 min) Epoch 12/300 -- Iteration 11079 - Batch 486/963 - Train loss: 0.34526166  - Train acc: 0.8690 - Val loss: 1.20747805\n",
      "(2.27 min) Epoch 12/300 -- Iteration 11088 - Batch 495/963 - Train loss: 0.34502027  - Train acc: 0.8692 - Val loss: 1.20747805\n",
      "(2.27 min) Epoch 12/300 -- Iteration 11097 - Batch 504/963 - Train loss: 0.34538643  - Train acc: 0.8688 - Val loss: 1.20747805\n",
      "(2.28 min) Epoch 12/300 -- Iteration 11106 - Batch 513/963 - Train loss: 0.34582490  - Train acc: 0.8687 - Val loss: 1.20747805\n",
      "(2.28 min) Epoch 12/300 -- Iteration 11115 - Batch 522/963 - Train loss: 0.34574397  - Train acc: 0.8687 - Val loss: 1.20747805\n",
      "(2.28 min) Epoch 12/300 -- Iteration 11124 - Batch 531/963 - Train loss: 0.34550446  - Train acc: 0.8690 - Val loss: 1.20747805\n",
      "(2.28 min) Epoch 12/300 -- Iteration 11133 - Batch 540/963 - Train loss: 0.34579179  - Train acc: 0.8688 - Val loss: 1.20747805\n",
      "(2.28 min) Epoch 12/300 -- Iteration 11142 - Batch 549/963 - Train loss: 0.34620460  - Train acc: 0.8687 - Val loss: 1.20747805\n",
      "(2.28 min) Epoch 12/300 -- Iteration 11151 - Batch 558/963 - Train loss: 0.34634921  - Train acc: 0.8687 - Val loss: 1.20747805\n",
      "(2.29 min) Epoch 12/300 -- Iteration 11160 - Batch 567/963 - Train loss: 0.34678771  - Train acc: 0.8685 - Val loss: 1.20747805\n",
      "(2.29 min) Epoch 12/300 -- Iteration 11169 - Batch 576/963 - Train loss: 0.34713800  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.29 min) Epoch 12/300 -- Iteration 11178 - Batch 585/963 - Train loss: 0.34687782  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.29 min) Epoch 12/300 -- Iteration 11187 - Batch 594/963 - Train loss: 0.34655295  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.29 min) Epoch 12/300 -- Iteration 11196 - Batch 603/963 - Train loss: 0.34647397  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.30 min) Epoch 12/300 -- Iteration 11205 - Batch 612/963 - Train loss: 0.34652409  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.30 min) Epoch 12/300 -- Iteration 11214 - Batch 621/963 - Train loss: 0.34676449  - Train acc: 0.8681 - Val loss: 1.20747805\n",
      "(2.30 min) Epoch 12/300 -- Iteration 11223 - Batch 630/963 - Train loss: 0.34674203  - Train acc: 0.8681 - Val loss: 1.20747805\n",
      "(2.30 min) Epoch 12/300 -- Iteration 11232 - Batch 639/963 - Train loss: 0.34692847  - Train acc: 0.8680 - Val loss: 1.20747805\n",
      "(2.30 min) Epoch 12/300 -- Iteration 11241 - Batch 648/963 - Train loss: 0.34672721  - Train acc: 0.8680 - Val loss: 1.20747805\n",
      "(2.30 min) Epoch 12/300 -- Iteration 11250 - Batch 657/963 - Train loss: 0.34683271  - Train acc: 0.8680 - Val loss: 1.20747805\n",
      "(2.31 min) Epoch 12/300 -- Iteration 11259 - Batch 666/963 - Train loss: 0.34630178  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.31 min) Epoch 12/300 -- Iteration 11268 - Batch 675/963 - Train loss: 0.34608550  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.31 min) Epoch 12/300 -- Iteration 11277 - Batch 684/963 - Train loss: 0.34626493  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.31 min) Epoch 12/300 -- Iteration 11286 - Batch 693/963 - Train loss: 0.34625075  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.31 min) Epoch 12/300 -- Iteration 11295 - Batch 702/963 - Train loss: 0.34614039  - Train acc: 0.8681 - Val loss: 1.20747805\n",
      "(2.32 min) Epoch 12/300 -- Iteration 11304 - Batch 711/963 - Train loss: 0.34599925  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.32 min) Epoch 12/300 -- Iteration 11313 - Batch 720/963 - Train loss: 0.34542928  - Train acc: 0.8684 - Val loss: 1.20747805\n",
      "(2.32 min) Epoch 12/300 -- Iteration 11322 - Batch 729/963 - Train loss: 0.34556702  - Train acc: 0.8684 - Val loss: 1.20747805\n",
      "(2.32 min) Epoch 12/300 -- Iteration 11331 - Batch 738/963 - Train loss: 0.34585066  - Train acc: 0.8684 - Val loss: 1.20747805\n",
      "(2.32 min) Epoch 12/300 -- Iteration 11340 - Batch 747/963 - Train loss: 0.34543618  - Train acc: 0.8686 - Val loss: 1.20747805\n",
      "(2.32 min) Epoch 12/300 -- Iteration 11349 - Batch 756/963 - Train loss: 0.34562538  - Train acc: 0.8686 - Val loss: 1.20747805\n",
      "(2.33 min) Epoch 12/300 -- Iteration 11358 - Batch 765/963 - Train loss: 0.34527809  - Train acc: 0.8687 - Val loss: 1.20747805\n",
      "(2.33 min) Epoch 12/300 -- Iteration 11367 - Batch 774/963 - Train loss: 0.34557417  - Train acc: 0.8685 - Val loss: 1.20747805\n",
      "(2.33 min) Epoch 12/300 -- Iteration 11376 - Batch 783/963 - Train loss: 0.34579946  - Train acc: 0.8684 - Val loss: 1.20747805\n",
      "(2.33 min) Epoch 12/300 -- Iteration 11385 - Batch 792/963 - Train loss: 0.34560204  - Train acc: 0.8685 - Val loss: 1.20747805\n",
      "(2.33 min) Epoch 12/300 -- Iteration 11394 - Batch 801/963 - Train loss: 0.34563418  - Train acc: 0.8684 - Val loss: 1.20747805\n",
      "(2.34 min) Epoch 12/300 -- Iteration 11403 - Batch 810/963 - Train loss: 0.34552477  - Train acc: 0.8684 - Val loss: 1.20747805\n",
      "(2.34 min) Epoch 12/300 -- Iteration 11412 - Batch 819/963 - Train loss: 0.34548681  - Train acc: 0.8684 - Val loss: 1.20747805\n",
      "(2.34 min) Epoch 12/300 -- Iteration 11421 - Batch 828/963 - Train loss: 0.34547489  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.34 min) Epoch 12/300 -- Iteration 11430 - Batch 837/963 - Train loss: 0.34541335  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.34 min) Epoch 12/300 -- Iteration 11439 - Batch 846/963 - Train loss: 0.34561154  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.34 min) Epoch 12/300 -- Iteration 11448 - Batch 855/963 - Train loss: 0.34547541  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.35 min) Epoch 12/300 -- Iteration 11457 - Batch 864/963 - Train loss: 0.34543314  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.35 min) Epoch 12/300 -- Iteration 11466 - Batch 873/963 - Train loss: 0.34529390  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.35 min) Epoch 12/300 -- Iteration 11475 - Batch 882/963 - Train loss: 0.34504917  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.35 min) Epoch 12/300 -- Iteration 11484 - Batch 891/963 - Train loss: 0.34507629  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.35 min) Epoch 12/300 -- Iteration 11493 - Batch 900/963 - Train loss: 0.34499282  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.36 min) Epoch 12/300 -- Iteration 11502 - Batch 909/963 - Train loss: 0.34517853  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.36 min) Epoch 12/300 -- Iteration 11511 - Batch 918/963 - Train loss: 0.34500142  - Train acc: 0.8683 - Val loss: 1.20747805\n",
      "(2.36 min) Epoch 12/300 -- Iteration 11520 - Batch 927/963 - Train loss: 0.34506411  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.36 min) Epoch 12/300 -- Iteration 11529 - Batch 936/963 - Train loss: 0.34513926  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.36 min) Epoch 12/300 -- Iteration 11538 - Batch 945/963 - Train loss: 0.34536605  - Train acc: 0.8682 - Val loss: 1.20747805\n",
      "(2.36 min) Epoch 12/300 -- Iteration 11547 - Batch 954/963 - Train loss: 0.34539327  - Train acc: 0.8681 - Val loss: 1.20747805\n",
      "(2.37 min) Epoch 12/300 -- Iteration 11556 - Batch 962/963 - Train loss: 0.34515425  - Train acc: 0.8682 - Val loss: 1.18794262 - Val acc: 0.6067\n",
      "(2.37 min) Epoch 13/300 -- Iteration 11565 - Batch 9/963 - Train loss: 0.34742906  - Train acc: 0.8742 - Val loss: 1.18794262\n",
      "(2.37 min) Epoch 13/300 -- Iteration 11574 - Batch 18/963 - Train loss: 0.35593731  - Train acc: 0.8738 - Val loss: 1.18794262\n",
      "(2.37 min) Epoch 13/300 -- Iteration 11583 - Batch 27/963 - Train loss: 0.36057707  - Train acc: 0.8686 - Val loss: 1.18794262\n",
      "(2.37 min) Epoch 13/300 -- Iteration 11592 - Batch 36/963 - Train loss: 0.35789155  - Train acc: 0.8678 - Val loss: 1.18794262\n",
      "(2.38 min) Epoch 13/300 -- Iteration 11601 - Batch 45/963 - Train loss: 0.36019845  - Train acc: 0.8658 - Val loss: 1.18794262\n",
      "(2.38 min) Epoch 13/300 -- Iteration 11610 - Batch 54/963 - Train loss: 0.35263961  - Train acc: 0.8685 - Val loss: 1.18794262\n",
      "(2.38 min) Epoch 13/300 -- Iteration 11619 - Batch 63/963 - Train loss: 0.34953991  - Train acc: 0.8701 - Val loss: 1.18794262\n",
      "(2.38 min) Epoch 13/300 -- Iteration 11628 - Batch 72/963 - Train loss: 0.34713520  - Train acc: 0.8707 - Val loss: 1.18794262\n",
      "(2.38 min) Epoch 13/300 -- Iteration 11637 - Batch 81/963 - Train loss: 0.34661974  - Train acc: 0.8710 - Val loss: 1.18794262\n",
      "(2.38 min) Epoch 13/300 -- Iteration 11646 - Batch 90/963 - Train loss: 0.34594368  - Train acc: 0.8715 - Val loss: 1.18794262\n",
      "(2.39 min) Epoch 13/300 -- Iteration 11655 - Batch 99/963 - Train loss: 0.34901580  - Train acc: 0.8699 - Val loss: 1.18794262\n",
      "(2.39 min) Epoch 13/300 -- Iteration 11664 - Batch 108/963 - Train loss: 0.34707342  - Train acc: 0.8705 - Val loss: 1.18794262\n",
      "(2.39 min) Epoch 13/300 -- Iteration 11673 - Batch 117/963 - Train loss: 0.34599514  - Train acc: 0.8706 - Val loss: 1.18794262\n",
      "(2.39 min) Epoch 13/300 -- Iteration 11682 - Batch 126/963 - Train loss: 0.34769181  - Train acc: 0.8699 - Val loss: 1.18794262\n",
      "(2.39 min) Epoch 13/300 -- Iteration 11691 - Batch 135/963 - Train loss: 0.34942898  - Train acc: 0.8685 - Val loss: 1.18794262\n",
      "(2.40 min) Epoch 13/300 -- Iteration 11700 - Batch 144/963 - Train loss: 0.34681060  - Train acc: 0.8697 - Val loss: 1.18794262\n",
      "(2.40 min) Epoch 13/300 -- Iteration 11709 - Batch 153/963 - Train loss: 0.34827849  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.40 min) Epoch 13/300 -- Iteration 11718 - Batch 162/963 - Train loss: 0.34817727  - Train acc: 0.8689 - Val loss: 1.18794262\n",
      "(2.40 min) Epoch 13/300 -- Iteration 11727 - Batch 171/963 - Train loss: 0.34816865  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.40 min) Epoch 13/300 -- Iteration 11736 - Batch 180/963 - Train loss: 0.34850833  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.41 min) Epoch 13/300 -- Iteration 11745 - Batch 189/963 - Train loss: 0.34699607  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.41 min) Epoch 13/300 -- Iteration 11754 - Batch 198/963 - Train loss: 0.34675961  - Train acc: 0.8693 - Val loss: 1.18794262\n",
      "(2.41 min) Epoch 13/300 -- Iteration 11763 - Batch 207/963 - Train loss: 0.34666967  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.41 min) Epoch 13/300 -- Iteration 11772 - Batch 216/963 - Train loss: 0.34668282  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.41 min) Epoch 13/300 -- Iteration 11781 - Batch 225/963 - Train loss: 0.34644898  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.41 min) Epoch 13/300 -- Iteration 11790 - Batch 234/963 - Train loss: 0.34581015  - Train acc: 0.8694 - Val loss: 1.18794262\n",
      "(2.42 min) Epoch 13/300 -- Iteration 11799 - Batch 243/963 - Train loss: 0.34512960  - Train acc: 0.8694 - Val loss: 1.18794262\n",
      "(2.42 min) Epoch 13/300 -- Iteration 11808 - Batch 252/963 - Train loss: 0.34513503  - Train acc: 0.8693 - Val loss: 1.18794262\n",
      "(2.42 min) Epoch 13/300 -- Iteration 11817 - Batch 261/963 - Train loss: 0.34498704  - Train acc: 0.8695 - Val loss: 1.18794262\n",
      "(2.42 min) Epoch 13/300 -- Iteration 11826 - Batch 270/963 - Train loss: 0.34478313  - Train acc: 0.8694 - Val loss: 1.18794262\n",
      "(2.42 min) Epoch 13/300 -- Iteration 11835 - Batch 279/963 - Train loss: 0.34518817  - Train acc: 0.8693 - Val loss: 1.18794262\n",
      "(2.43 min) Epoch 13/300 -- Iteration 11844 - Batch 288/963 - Train loss: 0.34581791  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.43 min) Epoch 13/300 -- Iteration 11853 - Batch 297/963 - Train loss: 0.34582430  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.43 min) Epoch 13/300 -- Iteration 11862 - Batch 306/963 - Train loss: 0.34551207  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.43 min) Epoch 13/300 -- Iteration 11871 - Batch 315/963 - Train loss: 0.34548766  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.43 min) Epoch 13/300 -- Iteration 11880 - Batch 324/963 - Train loss: 0.34502579  - Train acc: 0.8693 - Val loss: 1.18794262\n",
      "(2.43 min) Epoch 13/300 -- Iteration 11889 - Batch 333/963 - Train loss: 0.34527334  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.44 min) Epoch 13/300 -- Iteration 11898 - Batch 342/963 - Train loss: 0.34442764  - Train acc: 0.8696 - Val loss: 1.18794262\n",
      "(2.44 min) Epoch 13/300 -- Iteration 11907 - Batch 351/963 - Train loss: 0.34406126  - Train acc: 0.8697 - Val loss: 1.18794262\n",
      "(2.44 min) Epoch 13/300 -- Iteration 11916 - Batch 360/963 - Train loss: 0.34392030  - Train acc: 0.8695 - Val loss: 1.18794262\n",
      "(2.44 min) Epoch 13/300 -- Iteration 11925 - Batch 369/963 - Train loss: 0.34401980  - Train acc: 0.8696 - Val loss: 1.18794262\n",
      "(2.44 min) Epoch 13/300 -- Iteration 11934 - Batch 378/963 - Train loss: 0.34347125  - Train acc: 0.8697 - Val loss: 1.18794262\n",
      "(2.45 min) Epoch 13/300 -- Iteration 11943 - Batch 387/963 - Train loss: 0.34356478  - Train acc: 0.8696 - Val loss: 1.18794262\n",
      "(2.45 min) Epoch 13/300 -- Iteration 11952 - Batch 396/963 - Train loss: 0.34402859  - Train acc: 0.8695 - Val loss: 1.18794262\n",
      "(2.45 min) Epoch 13/300 -- Iteration 11961 - Batch 405/963 - Train loss: 0.34389161  - Train acc: 0.8695 - Val loss: 1.18794262\n",
      "(2.45 min) Epoch 13/300 -- Iteration 11970 - Batch 414/963 - Train loss: 0.34436024  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.45 min) Epoch 13/300 -- Iteration 11979 - Batch 423/963 - Train loss: 0.34452883  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.45 min) Epoch 13/300 -- Iteration 11988 - Batch 432/963 - Train loss: 0.34522750  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.46 min) Epoch 13/300 -- Iteration 11997 - Batch 441/963 - Train loss: 0.34481784  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.46 min) Epoch 13/300 -- Iteration 12006 - Batch 450/963 - Train loss: 0.34478740  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.46 min) Epoch 13/300 -- Iteration 12015 - Batch 459/963 - Train loss: 0.34572756  - Train acc: 0.8685 - Val loss: 1.18794262\n",
      "(2.46 min) Epoch 13/300 -- Iteration 12024 - Batch 468/963 - Train loss: 0.34578429  - Train acc: 0.8687 - Val loss: 1.18794262\n",
      "(2.46 min) Epoch 13/300 -- Iteration 12033 - Batch 477/963 - Train loss: 0.34544858  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.47 min) Epoch 13/300 -- Iteration 12042 - Batch 486/963 - Train loss: 0.34561558  - Train acc: 0.8689 - Val loss: 1.18794262\n",
      "(2.47 min) Epoch 13/300 -- Iteration 12051 - Batch 495/963 - Train loss: 0.34585517  - Train acc: 0.8685 - Val loss: 1.18794262\n",
      "(2.47 min) Epoch 13/300 -- Iteration 12060 - Batch 504/963 - Train loss: 0.34536885  - Train acc: 0.8687 - Val loss: 1.18794262\n",
      "(2.47 min) Epoch 13/300 -- Iteration 12069 - Batch 513/963 - Train loss: 0.34553073  - Train acc: 0.8686 - Val loss: 1.18794262\n",
      "(2.47 min) Epoch 13/300 -- Iteration 12078 - Batch 522/963 - Train loss: 0.34522937  - Train acc: 0.8687 - Val loss: 1.18794262\n",
      "(2.47 min) Epoch 13/300 -- Iteration 12087 - Batch 531/963 - Train loss: 0.34513141  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.48 min) Epoch 13/300 -- Iteration 12096 - Batch 540/963 - Train loss: 0.34537315  - Train acc: 0.8686 - Val loss: 1.18794262\n",
      "(2.48 min) Epoch 13/300 -- Iteration 12105 - Batch 549/963 - Train loss: 0.34501809  - Train acc: 0.8689 - Val loss: 1.18794262\n",
      "(2.48 min) Epoch 13/300 -- Iteration 12114 - Batch 558/963 - Train loss: 0.34520254  - Train acc: 0.8687 - Val loss: 1.18794262\n",
      "(2.48 min) Epoch 13/300 -- Iteration 12123 - Batch 567/963 - Train loss: 0.34518679  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.48 min) Epoch 13/300 -- Iteration 12132 - Batch 576/963 - Train loss: 0.34521001  - Train acc: 0.8687 - Val loss: 1.18794262\n",
      "(2.49 min) Epoch 13/300 -- Iteration 12141 - Batch 585/963 - Train loss: 0.34513239  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.49 min) Epoch 13/300 -- Iteration 12150 - Batch 594/963 - Train loss: 0.34511827  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.49 min) Epoch 13/300 -- Iteration 12159 - Batch 603/963 - Train loss: 0.34506807  - Train acc: 0.8687 - Val loss: 1.18794262\n",
      "(2.49 min) Epoch 13/300 -- Iteration 12168 - Batch 612/963 - Train loss: 0.34455569  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.49 min) Epoch 13/300 -- Iteration 12177 - Batch 621/963 - Train loss: 0.34471757  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.49 min) Epoch 13/300 -- Iteration 12186 - Batch 630/963 - Train loss: 0.34441489  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.50 min) Epoch 13/300 -- Iteration 12195 - Batch 639/963 - Train loss: 0.34405793  - Train acc: 0.8693 - Val loss: 1.18794262\n",
      "(2.50 min) Epoch 13/300 -- Iteration 12204 - Batch 648/963 - Train loss: 0.34427626  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.50 min) Epoch 13/300 -- Iteration 12213 - Batch 657/963 - Train loss: 0.34519801  - Train acc: 0.8688 - Val loss: 1.18794262\n",
      "(2.50 min) Epoch 13/300 -- Iteration 12222 - Batch 666/963 - Train loss: 0.34505078  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.50 min) Epoch 13/300 -- Iteration 12231 - Batch 675/963 - Train loss: 0.34521640  - Train acc: 0.8689 - Val loss: 1.18794262\n",
      "(2.51 min) Epoch 13/300 -- Iteration 12240 - Batch 684/963 - Train loss: 0.34493296  - Train acc: 0.8689 - Val loss: 1.18794262\n",
      "(2.51 min) Epoch 13/300 -- Iteration 12249 - Batch 693/963 - Train loss: 0.34469594  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.51 min) Epoch 13/300 -- Iteration 12258 - Batch 702/963 - Train loss: 0.34507287  - Train acc: 0.8687 - Val loss: 1.18794262\n",
      "(2.51 min) Epoch 13/300 -- Iteration 12267 - Batch 711/963 - Train loss: 0.34497414  - Train acc: 0.8689 - Val loss: 1.18794262\n",
      "(2.51 min) Epoch 13/300 -- Iteration 12276 - Batch 720/963 - Train loss: 0.34458191  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.51 min) Epoch 13/300 -- Iteration 12285 - Batch 729/963 - Train loss: 0.34442717  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.52 min) Epoch 13/300 -- Iteration 12294 - Batch 738/963 - Train loss: 0.34477031  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.52 min) Epoch 13/300 -- Iteration 12303 - Batch 747/963 - Train loss: 0.34467121  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.52 min) Epoch 13/300 -- Iteration 12312 - Batch 756/963 - Train loss: 0.34459273  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.52 min) Epoch 13/300 -- Iteration 12321 - Batch 765/963 - Train loss: 0.34471828  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.52 min) Epoch 13/300 -- Iteration 12330 - Batch 774/963 - Train loss: 0.34469801  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.53 min) Epoch 13/300 -- Iteration 12339 - Batch 783/963 - Train loss: 0.34450925  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.53 min) Epoch 13/300 -- Iteration 12348 - Batch 792/963 - Train loss: 0.34408330  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.53 min) Epoch 13/300 -- Iteration 12357 - Batch 801/963 - Train loss: 0.34407043  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.53 min) Epoch 13/300 -- Iteration 12366 - Batch 810/963 - Train loss: 0.34371307  - Train acc: 0.8694 - Val loss: 1.18794262\n",
      "(2.53 min) Epoch 13/300 -- Iteration 12375 - Batch 819/963 - Train loss: 0.34401861  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.53 min) Epoch 13/300 -- Iteration 12384 - Batch 828/963 - Train loss: 0.34407707  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.54 min) Epoch 13/300 -- Iteration 12393 - Batch 837/963 - Train loss: 0.34404399  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.54 min) Epoch 13/300 -- Iteration 12402 - Batch 846/963 - Train loss: 0.34432263  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.54 min) Epoch 13/300 -- Iteration 12411 - Batch 855/963 - Train loss: 0.34452821  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.54 min) Epoch 13/300 -- Iteration 12420 - Batch 864/963 - Train loss: 0.34442582  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.54 min) Epoch 13/300 -- Iteration 12429 - Batch 873/963 - Train loss: 0.34471537  - Train acc: 0.8689 - Val loss: 1.18794262\n",
      "(2.55 min) Epoch 13/300 -- Iteration 12438 - Batch 882/963 - Train loss: 0.34457990  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.55 min) Epoch 13/300 -- Iteration 12447 - Batch 891/963 - Train loss: 0.34418331  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.55 min) Epoch 13/300 -- Iteration 12456 - Batch 900/963 - Train loss: 0.34445584  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.55 min) Epoch 13/300 -- Iteration 12465 - Batch 909/963 - Train loss: 0.34427168  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.55 min) Epoch 13/300 -- Iteration 12474 - Batch 918/963 - Train loss: 0.34440027  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.55 min) Epoch 13/300 -- Iteration 12483 - Batch 927/963 - Train loss: 0.34448418  - Train acc: 0.8692 - Val loss: 1.18794262\n",
      "(2.56 min) Epoch 13/300 -- Iteration 12492 - Batch 936/963 - Train loss: 0.34452140  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.56 min) Epoch 13/300 -- Iteration 12501 - Batch 945/963 - Train loss: 0.34454111  - Train acc: 0.8690 - Val loss: 1.18794262\n",
      "(2.56 min) Epoch 13/300 -- Iteration 12510 - Batch 954/963 - Train loss: 0.34432373  - Train acc: 0.8691 - Val loss: 1.18794262\n",
      "(2.56 min) Epoch 13/300 -- Iteration 12519 - Batch 962/963 - Train loss: 0.34447550  - Train acc: 0.8691 - Val loss: 1.16719866 - Val acc: 0.6067\n",
      "(2.56 min) Epoch 14/300 -- Iteration 12528 - Batch 9/963 - Train loss: 0.34118342  - Train acc: 0.8711 - Val loss: 1.16719866\n",
      "(2.57 min) Epoch 14/300 -- Iteration 12537 - Batch 18/963 - Train loss: 0.35882448  - Train acc: 0.8623 - Val loss: 1.16719866\n",
      "(2.57 min) Epoch 14/300 -- Iteration 12546 - Batch 27/963 - Train loss: 0.34662304  - Train acc: 0.8675 - Val loss: 1.16719866\n",
      "(2.57 min) Epoch 14/300 -- Iteration 12555 - Batch 36/963 - Train loss: 0.34422433  - Train acc: 0.8687 - Val loss: 1.16719866\n",
      "(2.57 min) Epoch 14/300 -- Iteration 12564 - Batch 45/963 - Train loss: 0.34455127  - Train acc: 0.8680 - Val loss: 1.16719866\n",
      "(2.57 min) Epoch 14/300 -- Iteration 12573 - Batch 54/963 - Train loss: 0.33908722  - Train acc: 0.8696 - Val loss: 1.16719866\n",
      "(2.58 min) Epoch 14/300 -- Iteration 12582 - Batch 63/963 - Train loss: 0.34005592  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.58 min) Epoch 14/300 -- Iteration 12591 - Batch 72/963 - Train loss: 0.33870950  - Train acc: 0.8709 - Val loss: 1.16719866\n",
      "(2.58 min) Epoch 14/300 -- Iteration 12600 - Batch 81/963 - Train loss: 0.33818352  - Train acc: 0.8728 - Val loss: 1.16719866\n",
      "(2.58 min) Epoch 14/300 -- Iteration 12609 - Batch 90/963 - Train loss: 0.33697168  - Train acc: 0.8729 - Val loss: 1.16719866\n",
      "(2.58 min) Epoch 14/300 -- Iteration 12618 - Batch 99/963 - Train loss: 0.33839006  - Train acc: 0.8725 - Val loss: 1.16719866\n",
      "(2.58 min) Epoch 14/300 -- Iteration 12627 - Batch 108/963 - Train loss: 0.34253843  - Train acc: 0.8711 - Val loss: 1.16719866\n",
      "(2.59 min) Epoch 14/300 -- Iteration 12636 - Batch 117/963 - Train loss: 0.34463868  - Train acc: 0.8696 - Val loss: 1.16719866\n",
      "(2.59 min) Epoch 14/300 -- Iteration 12645 - Batch 126/963 - Train loss: 0.34530113  - Train acc: 0.8691 - Val loss: 1.16719866\n",
      "(2.59 min) Epoch 14/300 -- Iteration 12654 - Batch 135/963 - Train loss: 0.34657569  - Train acc: 0.8690 - Val loss: 1.16719866\n",
      "(2.59 min) Epoch 14/300 -- Iteration 12663 - Batch 144/963 - Train loss: 0.34644964  - Train acc: 0.8682 - Val loss: 1.16719866\n",
      "(2.59 min) Epoch 14/300 -- Iteration 12672 - Batch 153/963 - Train loss: 0.34739755  - Train acc: 0.8677 - Val loss: 1.16719866\n",
      "(2.60 min) Epoch 14/300 -- Iteration 12681 - Batch 162/963 - Train loss: 0.34667755  - Train acc: 0.8680 - Val loss: 1.16719866\n",
      "(2.60 min) Epoch 14/300 -- Iteration 12690 - Batch 171/963 - Train loss: 0.34578405  - Train acc: 0.8686 - Val loss: 1.16719866\n",
      "(2.60 min) Epoch 14/300 -- Iteration 12699 - Batch 180/963 - Train loss: 0.34597566  - Train acc: 0.8683 - Val loss: 1.16719866\n",
      "(2.60 min) Epoch 14/300 -- Iteration 12708 - Batch 189/963 - Train loss: 0.34680200  - Train acc: 0.8679 - Val loss: 1.16719866\n",
      "(2.60 min) Epoch 14/300 -- Iteration 12717 - Batch 198/963 - Train loss: 0.34567748  - Train acc: 0.8677 - Val loss: 1.16719866\n",
      "(2.60 min) Epoch 14/300 -- Iteration 12726 - Batch 207/963 - Train loss: 0.34621501  - Train acc: 0.8679 - Val loss: 1.16719866\n",
      "(2.61 min) Epoch 14/300 -- Iteration 12735 - Batch 216/963 - Train loss: 0.34573118  - Train acc: 0.8682 - Val loss: 1.16719866\n",
      "(2.61 min) Epoch 14/300 -- Iteration 12744 - Batch 225/963 - Train loss: 0.34603356  - Train acc: 0.8679 - Val loss: 1.16719866\n",
      "(2.61 min) Epoch 14/300 -- Iteration 12753 - Batch 234/963 - Train loss: 0.34611738  - Train acc: 0.8680 - Val loss: 1.16719866\n",
      "(2.61 min) Epoch 14/300 -- Iteration 12762 - Batch 243/963 - Train loss: 0.34567039  - Train acc: 0.8679 - Val loss: 1.16719866\n",
      "(2.61 min) Epoch 14/300 -- Iteration 12771 - Batch 252/963 - Train loss: 0.34487690  - Train acc: 0.8684 - Val loss: 1.16719866\n",
      "(2.62 min) Epoch 14/300 -- Iteration 12780 - Batch 261/963 - Train loss: 0.34509993  - Train acc: 0.8685 - Val loss: 1.16719866\n",
      "(2.62 min) Epoch 14/300 -- Iteration 12789 - Batch 270/963 - Train loss: 0.34540639  - Train acc: 0.8681 - Val loss: 1.16719866\n",
      "(2.62 min) Epoch 14/300 -- Iteration 12798 - Batch 279/963 - Train loss: 0.34556717  - Train acc: 0.8680 - Val loss: 1.16719866\n",
      "(2.62 min) Epoch 14/300 -- Iteration 12807 - Batch 288/963 - Train loss: 0.34522360  - Train acc: 0.8681 - Val loss: 1.16719866\n",
      "(2.62 min) Epoch 14/300 -- Iteration 12816 - Batch 297/963 - Train loss: 0.34373684  - Train acc: 0.8687 - Val loss: 1.16719866\n",
      "(2.62 min) Epoch 14/300 -- Iteration 12825 - Batch 306/963 - Train loss: 0.34348131  - Train acc: 0.8688 - Val loss: 1.16719866\n",
      "(2.63 min) Epoch 14/300 -- Iteration 12834 - Batch 315/963 - Train loss: 0.34327360  - Train acc: 0.8690 - Val loss: 1.16719866\n",
      "(2.63 min) Epoch 14/300 -- Iteration 12843 - Batch 324/963 - Train loss: 0.34284575  - Train acc: 0.8692 - Val loss: 1.16719866\n",
      "(2.63 min) Epoch 14/300 -- Iteration 12852 - Batch 333/963 - Train loss: 0.34216080  - Train acc: 0.8695 - Val loss: 1.16719866\n",
      "(2.63 min) Epoch 14/300 -- Iteration 12861 - Batch 342/963 - Train loss: 0.34176800  - Train acc: 0.8699 - Val loss: 1.16719866\n",
      "(2.63 min) Epoch 14/300 -- Iteration 12870 - Batch 351/963 - Train loss: 0.34281925  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.64 min) Epoch 14/300 -- Iteration 12879 - Batch 360/963 - Train loss: 0.34189320  - Train acc: 0.8698 - Val loss: 1.16719866\n",
      "(2.64 min) Epoch 14/300 -- Iteration 12888 - Batch 369/963 - Train loss: 0.34200125  - Train acc: 0.8697 - Val loss: 1.16719866\n",
      "(2.64 min) Epoch 14/300 -- Iteration 12897 - Batch 378/963 - Train loss: 0.34115018  - Train acc: 0.8701 - Val loss: 1.16719866\n",
      "(2.64 min) Epoch 14/300 -- Iteration 12906 - Batch 387/963 - Train loss: 0.34091711  - Train acc: 0.8701 - Val loss: 1.16719866\n",
      "(2.64 min) Epoch 14/300 -- Iteration 12915 - Batch 396/963 - Train loss: 0.34159556  - Train acc: 0.8700 - Val loss: 1.16719866\n",
      "(2.64 min) Epoch 14/300 -- Iteration 12924 - Batch 405/963 - Train loss: 0.34155456  - Train acc: 0.8701 - Val loss: 1.16719866\n",
      "(2.65 min) Epoch 14/300 -- Iteration 12933 - Batch 414/963 - Train loss: 0.34229429  - Train acc: 0.8698 - Val loss: 1.16719866\n",
      "(2.65 min) Epoch 14/300 -- Iteration 12942 - Batch 423/963 - Train loss: 0.34223959  - Train acc: 0.8697 - Val loss: 1.16719866\n",
      "(2.65 min) Epoch 14/300 -- Iteration 12951 - Batch 432/963 - Train loss: 0.34178509  - Train acc: 0.8700 - Val loss: 1.16719866\n",
      "(2.65 min) Epoch 14/300 -- Iteration 12960 - Batch 441/963 - Train loss: 0.34155442  - Train acc: 0.8700 - Val loss: 1.16719866\n",
      "(2.65 min) Epoch 14/300 -- Iteration 12969 - Batch 450/963 - Train loss: 0.34121421  - Train acc: 0.8701 - Val loss: 1.16719866\n",
      "(2.66 min) Epoch 14/300 -- Iteration 12978 - Batch 459/963 - Train loss: 0.34079341  - Train acc: 0.8703 - Val loss: 1.16719866\n",
      "(2.66 min) Epoch 14/300 -- Iteration 12987 - Batch 468/963 - Train loss: 0.34115659  - Train acc: 0.8702 - Val loss: 1.16719866\n",
      "(2.66 min) Epoch 14/300 -- Iteration 12996 - Batch 477/963 - Train loss: 0.34126239  - Train acc: 0.8701 - Val loss: 1.16719866\n",
      "(2.66 min) Epoch 14/300 -- Iteration 13005 - Batch 486/963 - Train loss: 0.34166065  - Train acc: 0.8700 - Val loss: 1.16719866\n",
      "(2.66 min) Epoch 14/300 -- Iteration 13014 - Batch 495/963 - Train loss: 0.34161065  - Train acc: 0.8701 - Val loss: 1.16719866\n",
      "(2.66 min) Epoch 14/300 -- Iteration 13023 - Batch 504/963 - Train loss: 0.34135698  - Train acc: 0.8702 - Val loss: 1.16719866\n",
      "(2.67 min) Epoch 14/300 -- Iteration 13032 - Batch 513/963 - Train loss: 0.34207436  - Train acc: 0.8699 - Val loss: 1.16719866\n",
      "(2.67 min) Epoch 14/300 -- Iteration 13041 - Batch 522/963 - Train loss: 0.34221373  - Train acc: 0.8697 - Val loss: 1.16719866\n",
      "(2.67 min) Epoch 14/300 -- Iteration 13050 - Batch 531/963 - Train loss: 0.34229543  - Train acc: 0.8696 - Val loss: 1.16719866\n",
      "(2.67 min) Epoch 14/300 -- Iteration 13059 - Batch 540/963 - Train loss: 0.34246694  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.67 min) Epoch 14/300 -- Iteration 13068 - Batch 549/963 - Train loss: 0.34278306  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.68 min) Epoch 14/300 -- Iteration 13077 - Batch 558/963 - Train loss: 0.34291129  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.68 min) Epoch 14/300 -- Iteration 13086 - Batch 567/963 - Train loss: 0.34288979  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.68 min) Epoch 14/300 -- Iteration 13095 - Batch 576/963 - Train loss: 0.34293719  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.68 min) Epoch 14/300 -- Iteration 13104 - Batch 585/963 - Train loss: 0.34272223  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.68 min) Epoch 14/300 -- Iteration 13113 - Batch 594/963 - Train loss: 0.34246084  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.68 min) Epoch 14/300 -- Iteration 13122 - Batch 603/963 - Train loss: 0.34249360  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.69 min) Epoch 14/300 -- Iteration 13131 - Batch 612/963 - Train loss: 0.34201847  - Train acc: 0.8696 - Val loss: 1.16719866\n",
      "(2.69 min) Epoch 14/300 -- Iteration 13140 - Batch 621/963 - Train loss: 0.34206154  - Train acc: 0.8696 - Val loss: 1.16719866\n",
      "(2.69 min) Epoch 14/300 -- Iteration 13149 - Batch 630/963 - Train loss: 0.34221856  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.69 min) Epoch 14/300 -- Iteration 13158 - Batch 639/963 - Train loss: 0.34185732  - Train acc: 0.8696 - Val loss: 1.16719866\n",
      "(2.69 min) Epoch 14/300 -- Iteration 13167 - Batch 648/963 - Train loss: 0.34235778  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.70 min) Epoch 14/300 -- Iteration 13176 - Batch 657/963 - Train loss: 0.34215940  - Train acc: 0.8695 - Val loss: 1.16719866\n",
      "(2.70 min) Epoch 14/300 -- Iteration 13185 - Batch 666/963 - Train loss: 0.34172673  - Train acc: 0.8696 - Val loss: 1.16719866\n",
      "(2.70 min) Epoch 14/300 -- Iteration 13194 - Batch 675/963 - Train loss: 0.34172572  - Train acc: 0.8695 - Val loss: 1.16719866\n",
      "(2.70 min) Epoch 14/300 -- Iteration 13203 - Batch 684/963 - Train loss: 0.34194041  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.70 min) Epoch 14/300 -- Iteration 13212 - Batch 693/963 - Train loss: 0.34213735  - Train acc: 0.8692 - Val loss: 1.16719866\n",
      "(2.70 min) Epoch 14/300 -- Iteration 13221 - Batch 702/963 - Train loss: 0.34208998  - Train acc: 0.8691 - Val loss: 1.16719866\n",
      "(2.71 min) Epoch 14/300 -- Iteration 13230 - Batch 711/963 - Train loss: 0.34207450  - Train acc: 0.8690 - Val loss: 1.16719866\n",
      "(2.71 min) Epoch 14/300 -- Iteration 13239 - Batch 720/963 - Train loss: 0.34206016  - Train acc: 0.8691 - Val loss: 1.16719866\n",
      "(2.71 min) Epoch 14/300 -- Iteration 13248 - Batch 729/963 - Train loss: 0.34207922  - Train acc: 0.8691 - Val loss: 1.16719866\n",
      "(2.71 min) Epoch 14/300 -- Iteration 13257 - Batch 738/963 - Train loss: 0.34258775  - Train acc: 0.8688 - Val loss: 1.16719866\n",
      "(2.71 min) Epoch 14/300 -- Iteration 13266 - Batch 747/963 - Train loss: 0.34223800  - Train acc: 0.8689 - Val loss: 1.16719866\n",
      "(2.72 min) Epoch 14/300 -- Iteration 13275 - Batch 756/963 - Train loss: 0.34199438  - Train acc: 0.8691 - Val loss: 1.16719866\n",
      "(2.72 min) Epoch 14/300 -- Iteration 13284 - Batch 765/963 - Train loss: 0.34187376  - Train acc: 0.8692 - Val loss: 1.16719866\n",
      "(2.72 min) Epoch 14/300 -- Iteration 13293 - Batch 774/963 - Train loss: 0.34183434  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.72 min) Epoch 14/300 -- Iteration 13302 - Batch 783/963 - Train loss: 0.34157561  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.72 min) Epoch 14/300 -- Iteration 13311 - Batch 792/963 - Train loss: 0.34173080  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.72 min) Epoch 14/300 -- Iteration 13320 - Batch 801/963 - Train loss: 0.34161113  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.73 min) Epoch 14/300 -- Iteration 13329 - Batch 810/963 - Train loss: 0.34171972  - Train acc: 0.8695 - Val loss: 1.16719866\n",
      "(2.73 min) Epoch 14/300 -- Iteration 13338 - Batch 819/963 - Train loss: 0.34208030  - Train acc: 0.8692 - Val loss: 1.16719866\n",
      "(2.73 min) Epoch 14/300 -- Iteration 13347 - Batch 828/963 - Train loss: 0.34167315  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.73 min) Epoch 14/300 -- Iteration 13356 - Batch 837/963 - Train loss: 0.34191045  - Train acc: 0.8692 - Val loss: 1.16719866\n",
      "(2.73 min) Epoch 14/300 -- Iteration 13365 - Batch 846/963 - Train loss: 0.34172942  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.74 min) Epoch 14/300 -- Iteration 13374 - Batch 855/963 - Train loss: 0.34184878  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.74 min) Epoch 14/300 -- Iteration 13383 - Batch 864/963 - Train loss: 0.34216397  - Train acc: 0.8692 - Val loss: 1.16719866\n",
      "(2.74 min) Epoch 14/300 -- Iteration 13392 - Batch 873/963 - Train loss: 0.34202067  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.74 min) Epoch 14/300 -- Iteration 13401 - Batch 882/963 - Train loss: 0.34193530  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.74 min) Epoch 14/300 -- Iteration 13410 - Batch 891/963 - Train loss: 0.34160965  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.74 min) Epoch 14/300 -- Iteration 13419 - Batch 900/963 - Train loss: 0.34165538  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.75 min) Epoch 14/300 -- Iteration 13428 - Batch 909/963 - Train loss: 0.34183058  - Train acc: 0.8693 - Val loss: 1.16719866\n",
      "(2.75 min) Epoch 14/300 -- Iteration 13437 - Batch 918/963 - Train loss: 0.34179904  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.75 min) Epoch 14/300 -- Iteration 13446 - Batch 927/963 - Train loss: 0.34172972  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.75 min) Epoch 14/300 -- Iteration 13455 - Batch 936/963 - Train loss: 0.34173423  - Train acc: 0.8694 - Val loss: 1.16719866\n",
      "(2.75 min) Epoch 14/300 -- Iteration 13464 - Batch 945/963 - Train loss: 0.34178228  - Train acc: 0.8695 - Val loss: 1.16719866\n",
      "(2.76 min) Epoch 14/300 -- Iteration 13473 - Batch 954/963 - Train loss: 0.34164145  - Train acc: 0.8695 - Val loss: 1.16719866\n",
      "(2.76 min) Epoch 14/300 -- Iteration 13482 - Batch 962/963 - Train loss: 0.34176466  - Train acc: 0.8695 - Val loss: 1.18015075 - Val acc: 0.6100\n",
      "(2.76 min) Epoch 15/300 -- Iteration 13491 - Batch 9/963 - Train loss: 0.34658104  - Train acc: 0.8734 - Val loss: 1.18015075\n",
      "(2.76 min) Epoch 15/300 -- Iteration 13500 - Batch 18/963 - Train loss: 0.35535267  - Train acc: 0.8660 - Val loss: 1.18015075\n",
      "(2.76 min) Epoch 15/300 -- Iteration 13509 - Batch 27/963 - Train loss: 0.34718679  - Train acc: 0.8689 - Val loss: 1.18015075\n",
      "(2.76 min) Epoch 15/300 -- Iteration 13518 - Batch 36/963 - Train loss: 0.34270982  - Train acc: 0.8693 - Val loss: 1.18015075\n",
      "(2.77 min) Epoch 15/300 -- Iteration 13527 - Batch 45/963 - Train loss: 0.34516902  - Train acc: 0.8692 - Val loss: 1.18015075\n",
      "(2.77 min) Epoch 15/300 -- Iteration 13536 - Batch 54/963 - Train loss: 0.34935876  - Train acc: 0.8682 - Val loss: 1.18015075\n",
      "(2.77 min) Epoch 15/300 -- Iteration 13545 - Batch 63/963 - Train loss: 0.34691759  - Train acc: 0.8685 - Val loss: 1.18015075\n",
      "(2.77 min) Epoch 15/300 -- Iteration 13554 - Batch 72/963 - Train loss: 0.34592021  - Train acc: 0.8688 - Val loss: 1.18015075\n",
      "(2.77 min) Epoch 15/300 -- Iteration 13563 - Batch 81/963 - Train loss: 0.34758070  - Train acc: 0.8680 - Val loss: 1.18015075\n",
      "(2.78 min) Epoch 15/300 -- Iteration 13572 - Batch 90/963 - Train loss: 0.34570553  - Train acc: 0.8692 - Val loss: 1.18015075\n",
      "(2.78 min) Epoch 15/300 -- Iteration 13581 - Batch 99/963 - Train loss: 0.34573574  - Train acc: 0.8688 - Val loss: 1.18015075\n",
      "(2.78 min) Epoch 15/300 -- Iteration 13590 - Batch 108/963 - Train loss: 0.34565964  - Train acc: 0.8678 - Val loss: 1.18015075\n",
      "(2.78 min) Epoch 15/300 -- Iteration 13599 - Batch 117/963 - Train loss: 0.34423354  - Train acc: 0.8688 - Val loss: 1.18015075\n",
      "(2.78 min) Epoch 15/300 -- Iteration 13608 - Batch 126/963 - Train loss: 0.34397350  - Train acc: 0.8686 - Val loss: 1.18015075\n",
      "(2.79 min) Epoch 15/300 -- Iteration 13617 - Batch 135/963 - Train loss: 0.34322033  - Train acc: 0.8689 - Val loss: 1.18015075\n",
      "(2.79 min) Epoch 15/300 -- Iteration 13626 - Batch 144/963 - Train loss: 0.34146797  - Train acc: 0.8696 - Val loss: 1.18015075\n",
      "(2.79 min) Epoch 15/300 -- Iteration 13635 - Batch 153/963 - Train loss: 0.34109630  - Train acc: 0.8701 - Val loss: 1.18015075\n",
      "(2.79 min) Epoch 15/300 -- Iteration 13644 - Batch 162/963 - Train loss: 0.34179773  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.79 min) Epoch 15/300 -- Iteration 13653 - Batch 171/963 - Train loss: 0.34132821  - Train acc: 0.8702 - Val loss: 1.18015075\n",
      "(2.79 min) Epoch 15/300 -- Iteration 13662 - Batch 180/963 - Train loss: 0.34167391  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.80 min) Epoch 15/300 -- Iteration 13671 - Batch 189/963 - Train loss: 0.34068005  - Train acc: 0.8704 - Val loss: 1.18015075\n",
      "(2.80 min) Epoch 15/300 -- Iteration 13680 - Batch 198/963 - Train loss: 0.34070255  - Train acc: 0.8701 - Val loss: 1.18015075\n",
      "(2.80 min) Epoch 15/300 -- Iteration 13689 - Batch 207/963 - Train loss: 0.34108598  - Train acc: 0.8705 - Val loss: 1.18015075\n",
      "(2.80 min) Epoch 15/300 -- Iteration 13698 - Batch 216/963 - Train loss: 0.34090597  - Train acc: 0.8706 - Val loss: 1.18015075\n",
      "(2.80 min) Epoch 15/300 -- Iteration 13707 - Batch 225/963 - Train loss: 0.34139536  - Train acc: 0.8704 - Val loss: 1.18015075\n",
      "(2.80 min) Epoch 15/300 -- Iteration 13716 - Batch 234/963 - Train loss: 0.34073890  - Train acc: 0.8706 - Val loss: 1.18015075\n",
      "(2.81 min) Epoch 15/300 -- Iteration 13725 - Batch 243/963 - Train loss: 0.33987747  - Train acc: 0.8711 - Val loss: 1.18015075\n",
      "(2.81 min) Epoch 15/300 -- Iteration 13734 - Batch 252/963 - Train loss: 0.33902700  - Train acc: 0.8714 - Val loss: 1.18015075\n",
      "(2.81 min) Epoch 15/300 -- Iteration 13743 - Batch 261/963 - Train loss: 0.34015043  - Train acc: 0.8711 - Val loss: 1.18015075\n",
      "(2.81 min) Epoch 15/300 -- Iteration 13752 - Batch 270/963 - Train loss: 0.33995980  - Train acc: 0.8710 - Val loss: 1.18015075\n",
      "(2.81 min) Epoch 15/300 -- Iteration 13761 - Batch 279/963 - Train loss: 0.34068850  - Train acc: 0.8705 - Val loss: 1.18015075\n",
      "(2.82 min) Epoch 15/300 -- Iteration 13770 - Batch 288/963 - Train loss: 0.34148358  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.82 min) Epoch 15/300 -- Iteration 13779 - Batch 297/963 - Train loss: 0.34127544  - Train acc: 0.8697 - Val loss: 1.18015075\n",
      "(2.82 min) Epoch 15/300 -- Iteration 13788 - Batch 306/963 - Train loss: 0.34080379  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.82 min) Epoch 15/300 -- Iteration 13797 - Batch 315/963 - Train loss: 0.34139193  - Train acc: 0.8696 - Val loss: 1.18015075\n",
      "(2.82 min) Epoch 15/300 -- Iteration 13806 - Batch 324/963 - Train loss: 0.34206444  - Train acc: 0.8694 - Val loss: 1.18015075\n",
      "(2.82 min) Epoch 15/300 -- Iteration 13815 - Batch 333/963 - Train loss: 0.34154787  - Train acc: 0.8695 - Val loss: 1.18015075\n",
      "(2.83 min) Epoch 15/300 -- Iteration 13824 - Batch 342/963 - Train loss: 0.34229784  - Train acc: 0.8691 - Val loss: 1.18015075\n",
      "(2.83 min) Epoch 15/300 -- Iteration 13833 - Batch 351/963 - Train loss: 0.34177686  - Train acc: 0.8694 - Val loss: 1.18015075\n",
      "(2.83 min) Epoch 15/300 -- Iteration 13842 - Batch 360/963 - Train loss: 0.34121865  - Train acc: 0.8697 - Val loss: 1.18015075\n",
      "(2.83 min) Epoch 15/300 -- Iteration 13851 - Batch 369/963 - Train loss: 0.34026643  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.83 min) Epoch 15/300 -- Iteration 13860 - Batch 378/963 - Train loss: 0.34023496  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.84 min) Epoch 15/300 -- Iteration 13869 - Batch 387/963 - Train loss: 0.34069222  - Train acc: 0.8696 - Val loss: 1.18015075\n",
      "(2.84 min) Epoch 15/300 -- Iteration 13878 - Batch 396/963 - Train loss: 0.34096904  - Train acc: 0.8694 - Val loss: 1.18015075\n",
      "(2.84 min) Epoch 15/300 -- Iteration 13887 - Batch 405/963 - Train loss: 0.34045577  - Train acc: 0.8696 - Val loss: 1.18015075\n",
      "(2.84 min) Epoch 15/300 -- Iteration 13896 - Batch 414/963 - Train loss: 0.34023724  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.84 min) Epoch 15/300 -- Iteration 13905 - Batch 423/963 - Train loss: 0.34084151  - Train acc: 0.8694 - Val loss: 1.18015075\n",
      "(2.84 min) Epoch 15/300 -- Iteration 13914 - Batch 432/963 - Train loss: 0.34011837  - Train acc: 0.8697 - Val loss: 1.18015075\n",
      "(2.85 min) Epoch 15/300 -- Iteration 13923 - Batch 441/963 - Train loss: 0.34032750  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.85 min) Epoch 15/300 -- Iteration 13932 - Batch 450/963 - Train loss: 0.34030122  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.85 min) Epoch 15/300 -- Iteration 13941 - Batch 459/963 - Train loss: 0.34081617  - Train acc: 0.8696 - Val loss: 1.18015075\n",
      "(2.85 min) Epoch 15/300 -- Iteration 13950 - Batch 468/963 - Train loss: 0.34070049  - Train acc: 0.8696 - Val loss: 1.18015075\n",
      "(2.85 min) Epoch 15/300 -- Iteration 13959 - Batch 477/963 - Train loss: 0.34088905  - Train acc: 0.8694 - Val loss: 1.18015075\n",
      "(2.86 min) Epoch 15/300 -- Iteration 13968 - Batch 486/963 - Train loss: 0.34028036  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.86 min) Epoch 15/300 -- Iteration 13977 - Batch 495/963 - Train loss: 0.34031935  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.86 min) Epoch 15/300 -- Iteration 13986 - Batch 504/963 - Train loss: 0.33973164  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.86 min) Epoch 15/300 -- Iteration 13995 - Batch 513/963 - Train loss: 0.33980200  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.86 min) Epoch 15/300 -- Iteration 14004 - Batch 522/963 - Train loss: 0.33988635  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.86 min) Epoch 15/300 -- Iteration 14013 - Batch 531/963 - Train loss: 0.34000395  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.87 min) Epoch 15/300 -- Iteration 14022 - Batch 540/963 - Train loss: 0.34082403  - Train acc: 0.8696 - Val loss: 1.18015075\n",
      "(2.87 min) Epoch 15/300 -- Iteration 14031 - Batch 549/963 - Train loss: 0.34083326  - Train acc: 0.8695 - Val loss: 1.18015075\n",
      "(2.87 min) Epoch 15/300 -- Iteration 14040 - Batch 558/963 - Train loss: 0.34084798  - Train acc: 0.8696 - Val loss: 1.18015075\n",
      "(2.87 min) Epoch 15/300 -- Iteration 14049 - Batch 567/963 - Train loss: 0.34103832  - Train acc: 0.8695 - Val loss: 1.18015075\n",
      "(2.87 min) Epoch 15/300 -- Iteration 14058 - Batch 576/963 - Train loss: 0.34088337  - Train acc: 0.8696 - Val loss: 1.18015075\n",
      "(2.88 min) Epoch 15/300 -- Iteration 14067 - Batch 585/963 - Train loss: 0.34061983  - Train acc: 0.8697 - Val loss: 1.18015075\n",
      "(2.88 min) Epoch 15/300 -- Iteration 14076 - Batch 594/963 - Train loss: 0.34058481  - Train acc: 0.8697 - Val loss: 1.18015075\n",
      "(2.88 min) Epoch 15/300 -- Iteration 14085 - Batch 603/963 - Train loss: 0.34069699  - Train acc: 0.8697 - Val loss: 1.18015075\n",
      "(2.88 min) Epoch 15/300 -- Iteration 14094 - Batch 612/963 - Train loss: 0.34121093  - Train acc: 0.8695 - Val loss: 1.18015075\n",
      "(2.88 min) Epoch 15/300 -- Iteration 14103 - Batch 621/963 - Train loss: 0.34108977  - Train acc: 0.8695 - Val loss: 1.18015075\n",
      "(2.88 min) Epoch 15/300 -- Iteration 14112 - Batch 630/963 - Train loss: 0.34107386  - Train acc: 0.8695 - Val loss: 1.18015075\n",
      "(2.89 min) Epoch 15/300 -- Iteration 14121 - Batch 639/963 - Train loss: 0.34111241  - Train acc: 0.8695 - Val loss: 1.18015075\n",
      "(2.89 min) Epoch 15/300 -- Iteration 14130 - Batch 648/963 - Train loss: 0.34120894  - Train acc: 0.8694 - Val loss: 1.18015075\n",
      "(2.89 min) Epoch 15/300 -- Iteration 14139 - Batch 657/963 - Train loss: 0.34142361  - Train acc: 0.8694 - Val loss: 1.18015075\n",
      "(2.89 min) Epoch 15/300 -- Iteration 14148 - Batch 666/963 - Train loss: 0.34138710  - Train acc: 0.8695 - Val loss: 1.18015075\n",
      "(2.89 min) Epoch 15/300 -- Iteration 14157 - Batch 675/963 - Train loss: 0.34068481  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.90 min) Epoch 15/300 -- Iteration 14166 - Batch 684/963 - Train loss: 0.34070562  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.90 min) Epoch 15/300 -- Iteration 14175 - Batch 693/963 - Train loss: 0.34065918  - Train acc: 0.8701 - Val loss: 1.18015075\n",
      "(2.90 min) Epoch 15/300 -- Iteration 14184 - Batch 702/963 - Train loss: 0.34083250  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.90 min) Epoch 15/300 -- Iteration 14193 - Batch 711/963 - Train loss: 0.34109491  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.90 min) Epoch 15/300 -- Iteration 14202 - Batch 720/963 - Train loss: 0.34098151  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.91 min) Epoch 15/300 -- Iteration 14211 - Batch 729/963 - Train loss: 0.34095362  - Train acc: 0.8701 - Val loss: 1.18015075\n",
      "(2.91 min) Epoch 15/300 -- Iteration 14220 - Batch 738/963 - Train loss: 0.34109223  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.91 min) Epoch 15/300 -- Iteration 14229 - Batch 747/963 - Train loss: 0.34137746  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.91 min) Epoch 15/300 -- Iteration 14238 - Batch 756/963 - Train loss: 0.34160143  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.91 min) Epoch 15/300 -- Iteration 14247 - Batch 765/963 - Train loss: 0.34128649  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.91 min) Epoch 15/300 -- Iteration 14256 - Batch 774/963 - Train loss: 0.34168610  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.92 min) Epoch 15/300 -- Iteration 14265 - Batch 783/963 - Train loss: 0.34174771  - Train acc: 0.8697 - Val loss: 1.18015075\n",
      "(2.92 min) Epoch 15/300 -- Iteration 14274 - Batch 792/963 - Train loss: 0.34170458  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.92 min) Epoch 15/300 -- Iteration 14283 - Batch 801/963 - Train loss: 0.34165376  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.92 min) Epoch 15/300 -- Iteration 14292 - Batch 810/963 - Train loss: 0.34165883  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.92 min) Epoch 15/300 -- Iteration 14301 - Batch 819/963 - Train loss: 0.34186754  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.93 min) Epoch 15/300 -- Iteration 14310 - Batch 828/963 - Train loss: 0.34191565  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.93 min) Epoch 15/300 -- Iteration 14319 - Batch 837/963 - Train loss: 0.34179294  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.93 min) Epoch 15/300 -- Iteration 14328 - Batch 846/963 - Train loss: 0.34168629  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.93 min) Epoch 15/300 -- Iteration 14337 - Batch 855/963 - Train loss: 0.34177983  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.93 min) Epoch 15/300 -- Iteration 14346 - Batch 864/963 - Train loss: 0.34177261  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.93 min) Epoch 15/300 -- Iteration 14355 - Batch 873/963 - Train loss: 0.34136019  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.94 min) Epoch 15/300 -- Iteration 14364 - Batch 882/963 - Train loss: 0.34115518  - Train acc: 0.8701 - Val loss: 1.18015075\n",
      "(2.94 min) Epoch 15/300 -- Iteration 14373 - Batch 891/963 - Train loss: 0.34106535  - Train acc: 0.8702 - Val loss: 1.18015075\n",
      "(2.94 min) Epoch 15/300 -- Iteration 14382 - Batch 900/963 - Train loss: 0.34141299  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.94 min) Epoch 15/300 -- Iteration 14391 - Batch 909/963 - Train loss: 0.34120033  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.94 min) Epoch 15/300 -- Iteration 14400 - Batch 918/963 - Train loss: 0.34146941  - Train acc: 0.8698 - Val loss: 1.18015075\n",
      "(2.95 min) Epoch 15/300 -- Iteration 14409 - Batch 927/963 - Train loss: 0.34114124  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.95 min) Epoch 15/300 -- Iteration 14418 - Batch 936/963 - Train loss: 0.34093453  - Train acc: 0.8700 - Val loss: 1.18015075\n",
      "(2.95 min) Epoch 15/300 -- Iteration 14427 - Batch 945/963 - Train loss: 0.34126502  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.95 min) Epoch 15/300 -- Iteration 14436 - Batch 954/963 - Train loss: 0.34121227  - Train acc: 0.8699 - Val loss: 1.18015075\n",
      "(2.95 min) Epoch 15/300 -- Iteration 14445 - Batch 962/963 - Train loss: 0.34111704  - Train acc: 0.8699 - Val loss: 1.16580021 - Val acc: 0.6083\n",
      "(2.95 min) Epoch 16/300 -- Iteration 14454 - Batch 9/963 - Train loss: 0.28422725  - Train acc: 0.8844 - Val loss: 1.16580021\n",
      "(2.96 min) Epoch 16/300 -- Iteration 14463 - Batch 18/963 - Train loss: 0.29835857  - Train acc: 0.8824 - Val loss: 1.16580021\n",
      "(2.96 min) Epoch 16/300 -- Iteration 14472 - Batch 27/963 - Train loss: 0.30651011  - Train acc: 0.8806 - Val loss: 1.16580021\n",
      "(2.96 min) Epoch 16/300 -- Iteration 14481 - Batch 36/963 - Train loss: 0.31401725  - Train acc: 0.8799 - Val loss: 1.16580021\n",
      "(2.96 min) Epoch 16/300 -- Iteration 14490 - Batch 45/963 - Train loss: 0.31872575  - Train acc: 0.8769 - Val loss: 1.16580021\n",
      "(2.96 min) Epoch 16/300 -- Iteration 14499 - Batch 54/963 - Train loss: 0.31890707  - Train acc: 0.8781 - Val loss: 1.16580021\n",
      "(2.97 min) Epoch 16/300 -- Iteration 14508 - Batch 63/963 - Train loss: 0.32218778  - Train acc: 0.8772 - Val loss: 1.16580021\n",
      "(2.97 min) Epoch 16/300 -- Iteration 14517 - Batch 72/963 - Train loss: 0.32863795  - Train acc: 0.8746 - Val loss: 1.16580021\n",
      "(2.97 min) Epoch 16/300 -- Iteration 14526 - Batch 81/963 - Train loss: 0.33027220  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(2.97 min) Epoch 16/300 -- Iteration 14535 - Batch 90/963 - Train loss: 0.32984000  - Train acc: 0.8740 - Val loss: 1.16580021\n",
      "(2.97 min) Epoch 16/300 -- Iteration 14544 - Batch 99/963 - Train loss: 0.33152226  - Train acc: 0.8722 - Val loss: 1.16580021\n",
      "(2.97 min) Epoch 16/300 -- Iteration 14553 - Batch 108/963 - Train loss: 0.33382593  - Train acc: 0.8718 - Val loss: 1.16580021\n",
      "(2.98 min) Epoch 16/300 -- Iteration 14562 - Batch 117/963 - Train loss: 0.33455246  - Train acc: 0.8724 - Val loss: 1.16580021\n",
      "(2.98 min) Epoch 16/300 -- Iteration 14571 - Batch 126/963 - Train loss: 0.33356468  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(2.98 min) Epoch 16/300 -- Iteration 14580 - Batch 135/963 - Train loss: 0.33171573  - Train acc: 0.8741 - Val loss: 1.16580021\n",
      "(2.98 min) Epoch 16/300 -- Iteration 14589 - Batch 144/963 - Train loss: 0.33268138  - Train acc: 0.8742 - Val loss: 1.16580021\n",
      "(2.98 min) Epoch 16/300 -- Iteration 14598 - Batch 153/963 - Train loss: 0.33370184  - Train acc: 0.8741 - Val loss: 1.16580021\n",
      "(2.99 min) Epoch 16/300 -- Iteration 14607 - Batch 162/963 - Train loss: 0.33475169  - Train acc: 0.8736 - Val loss: 1.16580021\n",
      "(2.99 min) Epoch 16/300 -- Iteration 14616 - Batch 171/963 - Train loss: 0.33552998  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(2.99 min) Epoch 16/300 -- Iteration 14625 - Batch 180/963 - Train loss: 0.33582010  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(2.99 min) Epoch 16/300 -- Iteration 14634 - Batch 189/963 - Train loss: 0.33650611  - Train acc: 0.8725 - Val loss: 1.16580021\n",
      "(2.99 min) Epoch 16/300 -- Iteration 14643 - Batch 198/963 - Train loss: 0.33607657  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.00 min) Epoch 16/300 -- Iteration 14652 - Batch 207/963 - Train loss: 0.33704646  - Train acc: 0.8722 - Val loss: 1.16580021\n",
      "(3.00 min) Epoch 16/300 -- Iteration 14661 - Batch 216/963 - Train loss: 0.33776366  - Train acc: 0.8719 - Val loss: 1.16580021\n",
      "(3.00 min) Epoch 16/300 -- Iteration 14670 - Batch 225/963 - Train loss: 0.33716165  - Train acc: 0.8720 - Val loss: 1.16580021\n",
      "(3.00 min) Epoch 16/300 -- Iteration 14679 - Batch 234/963 - Train loss: 0.33700265  - Train acc: 0.8723 - Val loss: 1.16580021\n",
      "(3.00 min) Epoch 16/300 -- Iteration 14688 - Batch 243/963 - Train loss: 0.33619387  - Train acc: 0.8726 - Val loss: 1.16580021\n",
      "(3.00 min) Epoch 16/300 -- Iteration 14697 - Batch 252/963 - Train loss: 0.33668475  - Train acc: 0.8723 - Val loss: 1.16580021\n",
      "(3.01 min) Epoch 16/300 -- Iteration 14706 - Batch 261/963 - Train loss: 0.33757291  - Train acc: 0.8717 - Val loss: 1.16580021\n",
      "(3.01 min) Epoch 16/300 -- Iteration 14715 - Batch 270/963 - Train loss: 0.33791195  - Train acc: 0.8714 - Val loss: 1.16580021\n",
      "(3.01 min) Epoch 16/300 -- Iteration 14724 - Batch 279/963 - Train loss: 0.33746561  - Train acc: 0.8716 - Val loss: 1.16580021\n",
      "(3.01 min) Epoch 16/300 -- Iteration 14733 - Batch 288/963 - Train loss: 0.33822294  - Train acc: 0.8712 - Val loss: 1.16580021\n",
      "(3.01 min) Epoch 16/300 -- Iteration 14742 - Batch 297/963 - Train loss: 0.33806048  - Train acc: 0.8712 - Val loss: 1.16580021\n",
      "(3.01 min) Epoch 16/300 -- Iteration 14751 - Batch 306/963 - Train loss: 0.33922011  - Train acc: 0.8709 - Val loss: 1.16580021\n",
      "(3.02 min) Epoch 16/300 -- Iteration 14760 - Batch 315/963 - Train loss: 0.33824943  - Train acc: 0.8716 - Val loss: 1.16580021\n",
      "(3.02 min) Epoch 16/300 -- Iteration 14769 - Batch 324/963 - Train loss: 0.33791032  - Train acc: 0.8718 - Val loss: 1.16580021\n",
      "(3.02 min) Epoch 16/300 -- Iteration 14778 - Batch 333/963 - Train loss: 0.33770943  - Train acc: 0.8720 - Val loss: 1.16580021\n",
      "(3.02 min) Epoch 16/300 -- Iteration 14787 - Batch 342/963 - Train loss: 0.33837960  - Train acc: 0.8719 - Val loss: 1.16580021\n",
      "(3.02 min) Epoch 16/300 -- Iteration 14796 - Batch 351/963 - Train loss: 0.33777786  - Train acc: 0.8722 - Val loss: 1.16580021\n",
      "(3.03 min) Epoch 16/300 -- Iteration 14805 - Batch 360/963 - Train loss: 0.33740026  - Train acc: 0.8724 - Val loss: 1.16580021\n",
      "(3.03 min) Epoch 16/300 -- Iteration 14814 - Batch 369/963 - Train loss: 0.33632376  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.03 min) Epoch 16/300 -- Iteration 14823 - Batch 378/963 - Train loss: 0.33646134  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.03 min) Epoch 16/300 -- Iteration 14832 - Batch 387/963 - Train loss: 0.33681484  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.03 min) Epoch 16/300 -- Iteration 14841 - Batch 396/963 - Train loss: 0.33658336  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.03 min) Epoch 16/300 -- Iteration 14850 - Batch 405/963 - Train loss: 0.33638956  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.04 min) Epoch 16/300 -- Iteration 14859 - Batch 414/963 - Train loss: 0.33660550  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.04 min) Epoch 16/300 -- Iteration 14868 - Batch 423/963 - Train loss: 0.33610186  - Train acc: 0.8731 - Val loss: 1.16580021\n",
      "(3.04 min) Epoch 16/300 -- Iteration 14877 - Batch 432/963 - Train loss: 0.33700973  - Train acc: 0.8726 - Val loss: 1.16580021\n",
      "(3.04 min) Epoch 16/300 -- Iteration 14886 - Batch 441/963 - Train loss: 0.33724090  - Train acc: 0.8724 - Val loss: 1.16580021\n",
      "(3.04 min) Epoch 16/300 -- Iteration 14895 - Batch 450/963 - Train loss: 0.33670711  - Train acc: 0.8726 - Val loss: 1.16580021\n",
      "(3.05 min) Epoch 16/300 -- Iteration 14904 - Batch 459/963 - Train loss: 0.33651586  - Train acc: 0.8729 - Val loss: 1.16580021\n",
      "(3.05 min) Epoch 16/300 -- Iteration 14913 - Batch 468/963 - Train loss: 0.33584451  - Train acc: 0.8731 - Val loss: 1.16580021\n",
      "(3.05 min) Epoch 16/300 -- Iteration 14922 - Batch 477/963 - Train loss: 0.33622822  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.05 min) Epoch 16/300 -- Iteration 14931 - Batch 486/963 - Train loss: 0.33686596  - Train acc: 0.8726 - Val loss: 1.16580021\n",
      "(3.05 min) Epoch 16/300 -- Iteration 14940 - Batch 495/963 - Train loss: 0.33698722  - Train acc: 0.8727 - Val loss: 1.16580021\n",
      "(3.05 min) Epoch 16/300 -- Iteration 14949 - Batch 504/963 - Train loss: 0.33687600  - Train acc: 0.8729 - Val loss: 1.16580021\n",
      "(3.06 min) Epoch 16/300 -- Iteration 14958 - Batch 513/963 - Train loss: 0.33708578  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.06 min) Epoch 16/300 -- Iteration 14967 - Batch 522/963 - Train loss: 0.33728340  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.06 min) Epoch 16/300 -- Iteration 14976 - Batch 531/963 - Train loss: 0.33750883  - Train acc: 0.8727 - Val loss: 1.16580021\n",
      "(3.06 min) Epoch 16/300 -- Iteration 14985 - Batch 540/963 - Train loss: 0.33758029  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.06 min) Epoch 16/300 -- Iteration 14994 - Batch 549/963 - Train loss: 0.33812538  - Train acc: 0.8725 - Val loss: 1.16580021\n",
      "(3.07 min) Epoch 16/300 -- Iteration 15003 - Batch 558/963 - Train loss: 0.33785654  - Train acc: 0.8725 - Val loss: 1.16580021\n",
      "(3.07 min) Epoch 16/300 -- Iteration 15012 - Batch 567/963 - Train loss: 0.33816785  - Train acc: 0.8723 - Val loss: 1.16580021\n",
      "(3.07 min) Epoch 16/300 -- Iteration 15021 - Batch 576/963 - Train loss: 0.33793369  - Train acc: 0.8724 - Val loss: 1.16580021\n",
      "(3.07 min) Epoch 16/300 -- Iteration 15030 - Batch 585/963 - Train loss: 0.33813340  - Train acc: 0.8722 - Val loss: 1.16580021\n",
      "(3.07 min) Epoch 16/300 -- Iteration 15039 - Batch 594/963 - Train loss: 0.33811995  - Train acc: 0.8723 - Val loss: 1.16580021\n",
      "(3.08 min) Epoch 16/300 -- Iteration 15048 - Batch 603/963 - Train loss: 0.33733658  - Train acc: 0.8727 - Val loss: 1.16580021\n",
      "(3.08 min) Epoch 16/300 -- Iteration 15057 - Batch 612/963 - Train loss: 0.33682361  - Train acc: 0.8729 - Val loss: 1.16580021\n",
      "(3.08 min) Epoch 16/300 -- Iteration 15066 - Batch 621/963 - Train loss: 0.33660711  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.08 min) Epoch 16/300 -- Iteration 15075 - Batch 630/963 - Train loss: 0.33676877  - Train acc: 0.8727 - Val loss: 1.16580021\n",
      "(3.08 min) Epoch 16/300 -- Iteration 15084 - Batch 639/963 - Train loss: 0.33660464  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.08 min) Epoch 16/300 -- Iteration 15093 - Batch 648/963 - Train loss: 0.33670855  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.09 min) Epoch 16/300 -- Iteration 15102 - Batch 657/963 - Train loss: 0.33648058  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.09 min) Epoch 16/300 -- Iteration 15111 - Batch 666/963 - Train loss: 0.33625518  - Train acc: 0.8729 - Val loss: 1.16580021\n",
      "(3.09 min) Epoch 16/300 -- Iteration 15120 - Batch 675/963 - Train loss: 0.33641012  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.09 min) Epoch 16/300 -- Iteration 15129 - Batch 684/963 - Train loss: 0.33664123  - Train acc: 0.8727 - Val loss: 1.16580021\n",
      "(3.09 min) Epoch 16/300 -- Iteration 15138 - Batch 693/963 - Train loss: 0.33647242  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.10 min) Epoch 16/300 -- Iteration 15147 - Batch 702/963 - Train loss: 0.33617251  - Train acc: 0.8727 - Val loss: 1.16580021\n",
      "(3.10 min) Epoch 16/300 -- Iteration 15156 - Batch 711/963 - Train loss: 0.33630657  - Train acc: 0.8726 - Val loss: 1.16580021\n",
      "(3.10 min) Epoch 16/300 -- Iteration 15165 - Batch 720/963 - Train loss: 0.33602085  - Train acc: 0.8728 - Val loss: 1.16580021\n",
      "(3.10 min) Epoch 16/300 -- Iteration 15174 - Batch 729/963 - Train loss: 0.33582285  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.10 min) Epoch 16/300 -- Iteration 15183 - Batch 738/963 - Train loss: 0.33577714  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.10 min) Epoch 16/300 -- Iteration 15192 - Batch 747/963 - Train loss: 0.33548444  - Train acc: 0.8731 - Val loss: 1.16580021\n",
      "(3.11 min) Epoch 16/300 -- Iteration 15201 - Batch 756/963 - Train loss: 0.33525862  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.11 min) Epoch 16/300 -- Iteration 15210 - Batch 765/963 - Train loss: 0.33499526  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.11 min) Epoch 16/300 -- Iteration 15219 - Batch 774/963 - Train loss: 0.33506890  - Train acc: 0.8733 - Val loss: 1.16580021\n",
      "(3.11 min) Epoch 16/300 -- Iteration 15228 - Batch 783/963 - Train loss: 0.33517566  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.11 min) Epoch 16/300 -- Iteration 15237 - Batch 792/963 - Train loss: 0.33527932  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.12 min) Epoch 16/300 -- Iteration 15246 - Batch 801/963 - Train loss: 0.33537875  - Train acc: 0.8733 - Val loss: 1.16580021\n",
      "(3.12 min) Epoch 16/300 -- Iteration 15255 - Batch 810/963 - Train loss: 0.33550167  - Train acc: 0.8733 - Val loss: 1.16580021\n",
      "(3.12 min) Epoch 16/300 -- Iteration 15264 - Batch 819/963 - Train loss: 0.33553893  - Train acc: 0.8733 - Val loss: 1.16580021\n",
      "(3.12 min) Epoch 16/300 -- Iteration 15273 - Batch 828/963 - Train loss: 0.33553240  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.12 min) Epoch 16/300 -- Iteration 15282 - Batch 837/963 - Train loss: 0.33535390  - Train acc: 0.8733 - Val loss: 1.16580021\n",
      "(3.12 min) Epoch 16/300 -- Iteration 15291 - Batch 846/963 - Train loss: 0.33559358  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.13 min) Epoch 16/300 -- Iteration 15300 - Batch 855/963 - Train loss: 0.33578543  - Train acc: 0.8731 - Val loss: 1.16580021\n",
      "(3.13 min) Epoch 16/300 -- Iteration 15309 - Batch 864/963 - Train loss: 0.33551825  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.13 min) Epoch 16/300 -- Iteration 15318 - Batch 873/963 - Train loss: 0.33565035  - Train acc: 0.8731 - Val loss: 1.16580021\n",
      "(3.13 min) Epoch 16/300 -- Iteration 15327 - Batch 882/963 - Train loss: 0.33583374  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.13 min) Epoch 16/300 -- Iteration 15336 - Batch 891/963 - Train loss: 0.33618751  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.14 min) Epoch 16/300 -- Iteration 15345 - Batch 900/963 - Train loss: 0.33627298  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.14 min) Epoch 16/300 -- Iteration 15354 - Batch 909/963 - Train loss: 0.33637349  - Train acc: 0.8730 - Val loss: 1.16580021\n",
      "(3.14 min) Epoch 16/300 -- Iteration 15363 - Batch 918/963 - Train loss: 0.33616754  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.14 min) Epoch 16/300 -- Iteration 15372 - Batch 927/963 - Train loss: 0.33631288  - Train acc: 0.8731 - Val loss: 1.16580021\n",
      "(3.14 min) Epoch 16/300 -- Iteration 15381 - Batch 936/963 - Train loss: 0.33616471  - Train acc: 0.8732 - Val loss: 1.16580021\n",
      "(3.14 min) Epoch 16/300 -- Iteration 15390 - Batch 945/963 - Train loss: 0.33603846  - Train acc: 0.8733 - Val loss: 1.16580021\n",
      "(3.15 min) Epoch 16/300 -- Iteration 15399 - Batch 954/963 - Train loss: 0.33607984  - Train acc: 0.8733 - Val loss: 1.16580021\n",
      "(3.15 min) Epoch 16/300 -- Iteration 15408 - Batch 962/963 - Train loss: 0.33623808  - Train acc: 0.8732 - Val loss: 1.16599226 - Val acc: 0.6083\n",
      "(3.15 min) Epoch 17/300 -- Iteration 15417 - Batch 9/963 - Train loss: 0.34061440  - Train acc: 0.8711 - Val loss: 1.16599226\n",
      "(3.15 min) Epoch 17/300 -- Iteration 15426 - Batch 18/963 - Train loss: 0.35865392  - Train acc: 0.8647 - Val loss: 1.16599226\n",
      "(3.15 min) Epoch 17/300 -- Iteration 15435 - Batch 27/963 - Train loss: 0.35130241  - Train acc: 0.8658 - Val loss: 1.16599226\n",
      "(3.16 min) Epoch 17/300 -- Iteration 15444 - Batch 36/963 - Train loss: 0.34275119  - Train acc: 0.8689 - Val loss: 1.16599226\n",
      "(3.16 min) Epoch 17/300 -- Iteration 15453 - Batch 45/963 - Train loss: 0.34661693  - Train acc: 0.8677 - Val loss: 1.16599226\n",
      "(3.16 min) Epoch 17/300 -- Iteration 15462 - Batch 54/963 - Train loss: 0.34346039  - Train acc: 0.8683 - Val loss: 1.16599226\n",
      "(3.16 min) Epoch 17/300 -- Iteration 15471 - Batch 63/963 - Train loss: 0.34160914  - Train acc: 0.8696 - Val loss: 1.16599226\n",
      "(3.16 min) Epoch 17/300 -- Iteration 15480 - Batch 72/963 - Train loss: 0.33967852  - Train acc: 0.8693 - Val loss: 1.16599226\n",
      "(3.17 min) Epoch 17/300 -- Iteration 15489 - Batch 81/963 - Train loss: 0.33884264  - Train acc: 0.8703 - Val loss: 1.16599226\n",
      "(3.17 min) Epoch 17/300 -- Iteration 15498 - Batch 90/963 - Train loss: 0.34016228  - Train acc: 0.8693 - Val loss: 1.16599226\n",
      "(3.17 min) Epoch 17/300 -- Iteration 15507 - Batch 99/963 - Train loss: 0.33867530  - Train acc: 0.8691 - Val loss: 1.16599226\n",
      "(3.17 min) Epoch 17/300 -- Iteration 15516 - Batch 108/963 - Train loss: 0.33811226  - Train acc: 0.8688 - Val loss: 1.16599226\n",
      "(3.17 min) Epoch 17/300 -- Iteration 15525 - Batch 117/963 - Train loss: 0.33718081  - Train acc: 0.8692 - Val loss: 1.16599226\n",
      "(3.17 min) Epoch 17/300 -- Iteration 15534 - Batch 126/963 - Train loss: 0.33636263  - Train acc: 0.8701 - Val loss: 1.16599226\n",
      "(3.18 min) Epoch 17/300 -- Iteration 15543 - Batch 135/963 - Train loss: 0.33336152  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.18 min) Epoch 17/300 -- Iteration 15552 - Batch 144/963 - Train loss: 0.33284028  - Train acc: 0.8725 - Val loss: 1.16599226\n",
      "(3.18 min) Epoch 17/300 -- Iteration 15561 - Batch 153/963 - Train loss: 0.33296715  - Train acc: 0.8725 - Val loss: 1.16599226\n",
      "(3.18 min) Epoch 17/300 -- Iteration 15570 - Batch 162/963 - Train loss: 0.33255152  - Train acc: 0.8725 - Val loss: 1.16599226\n",
      "(3.18 min) Epoch 17/300 -- Iteration 15579 - Batch 171/963 - Train loss: 0.33354852  - Train acc: 0.8718 - Val loss: 1.16599226\n",
      "(3.19 min) Epoch 17/300 -- Iteration 15588 - Batch 180/963 - Train loss: 0.33345978  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.19 min) Epoch 17/300 -- Iteration 15597 - Batch 189/963 - Train loss: 0.33284217  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.19 min) Epoch 17/300 -- Iteration 15606 - Batch 198/963 - Train loss: 0.33482747  - Train acc: 0.8713 - Val loss: 1.16599226\n",
      "(3.19 min) Epoch 17/300 -- Iteration 15615 - Batch 207/963 - Train loss: 0.33567147  - Train acc: 0.8710 - Val loss: 1.16599226\n",
      "(3.19 min) Epoch 17/300 -- Iteration 15624 - Batch 216/963 - Train loss: 0.33466703  - Train acc: 0.8715 - Val loss: 1.16599226\n",
      "(3.19 min) Epoch 17/300 -- Iteration 15633 - Batch 225/963 - Train loss: 0.33384088  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.20 min) Epoch 17/300 -- Iteration 15642 - Batch 234/963 - Train loss: 0.33431135  - Train acc: 0.8721 - Val loss: 1.16599226\n",
      "(3.20 min) Epoch 17/300 -- Iteration 15651 - Batch 243/963 - Train loss: 0.33458363  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.20 min) Epoch 17/300 -- Iteration 15660 - Batch 252/963 - Train loss: 0.33570825  - Train acc: 0.8717 - Val loss: 1.16599226\n",
      "(3.20 min) Epoch 17/300 -- Iteration 15669 - Batch 261/963 - Train loss: 0.33576423  - Train acc: 0.8718 - Val loss: 1.16599226\n",
      "(3.20 min) Epoch 17/300 -- Iteration 15678 - Batch 270/963 - Train loss: 0.33608368  - Train acc: 0.8717 - Val loss: 1.16599226\n",
      "(3.21 min) Epoch 17/300 -- Iteration 15687 - Batch 279/963 - Train loss: 0.33676545  - Train acc: 0.8715 - Val loss: 1.16599226\n",
      "(3.21 min) Epoch 17/300 -- Iteration 15696 - Batch 288/963 - Train loss: 0.33664593  - Train acc: 0.8719 - Val loss: 1.16599226\n",
      "(3.21 min) Epoch 17/300 -- Iteration 15705 - Batch 297/963 - Train loss: 0.33607491  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.21 min) Epoch 17/300 -- Iteration 15714 - Batch 306/963 - Train loss: 0.33671913  - Train acc: 0.8719 - Val loss: 1.16599226\n",
      "(3.21 min) Epoch 17/300 -- Iteration 15723 - Batch 315/963 - Train loss: 0.33689613  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.21 min) Epoch 17/300 -- Iteration 15732 - Batch 324/963 - Train loss: 0.33750145  - Train acc: 0.8719 - Val loss: 1.16599226\n",
      "(3.22 min) Epoch 17/300 -- Iteration 15741 - Batch 333/963 - Train loss: 0.33786398  - Train acc: 0.8715 - Val loss: 1.16599226\n",
      "(3.22 min) Epoch 17/300 -- Iteration 15750 - Batch 342/963 - Train loss: 0.33806896  - Train acc: 0.8714 - Val loss: 1.16599226\n",
      "(3.22 min) Epoch 17/300 -- Iteration 15759 - Batch 351/963 - Train loss: 0.33774852  - Train acc: 0.8716 - Val loss: 1.16599226\n",
      "(3.22 min) Epoch 17/300 -- Iteration 15768 - Batch 360/963 - Train loss: 0.33787965  - Train acc: 0.8717 - Val loss: 1.16599226\n",
      "(3.22 min) Epoch 17/300 -- Iteration 15777 - Batch 369/963 - Train loss: 0.33797482  - Train acc: 0.8717 - Val loss: 1.16599226\n",
      "(3.23 min) Epoch 17/300 -- Iteration 15786 - Batch 378/963 - Train loss: 0.33755527  - Train acc: 0.8718 - Val loss: 1.16599226\n",
      "(3.23 min) Epoch 17/300 -- Iteration 15795 - Batch 387/963 - Train loss: 0.33761443  - Train acc: 0.8717 - Val loss: 1.16599226\n",
      "(3.23 min) Epoch 17/300 -- Iteration 15804 - Batch 396/963 - Train loss: 0.33743118  - Train acc: 0.8717 - Val loss: 1.16599226\n",
      "(3.23 min) Epoch 17/300 -- Iteration 15813 - Batch 405/963 - Train loss: 0.33798444  - Train acc: 0.8714 - Val loss: 1.16599226\n",
      "(3.23 min) Epoch 17/300 -- Iteration 15822 - Batch 414/963 - Train loss: 0.33835049  - Train acc: 0.8710 - Val loss: 1.16599226\n",
      "(3.23 min) Epoch 17/300 -- Iteration 15831 - Batch 423/963 - Train loss: 0.33793159  - Train acc: 0.8713 - Val loss: 1.16599226\n",
      "(3.24 min) Epoch 17/300 -- Iteration 15840 - Batch 432/963 - Train loss: 0.33789701  - Train acc: 0.8713 - Val loss: 1.16599226\n",
      "(3.24 min) Epoch 17/300 -- Iteration 15849 - Batch 441/963 - Train loss: 0.33854676  - Train acc: 0.8711 - Val loss: 1.16599226\n",
      "(3.24 min) Epoch 17/300 -- Iteration 15858 - Batch 450/963 - Train loss: 0.33839301  - Train acc: 0.8712 - Val loss: 1.16599226\n",
      "(3.24 min) Epoch 17/300 -- Iteration 15867 - Batch 459/963 - Train loss: 0.33785057  - Train acc: 0.8715 - Val loss: 1.16599226\n",
      "(3.24 min) Epoch 17/300 -- Iteration 15876 - Batch 468/963 - Train loss: 0.33773739  - Train acc: 0.8715 - Val loss: 1.16599226\n",
      "(3.25 min) Epoch 17/300 -- Iteration 15885 - Batch 477/963 - Train loss: 0.33732554  - Train acc: 0.8714 - Val loss: 1.16599226\n",
      "(3.25 min) Epoch 17/300 -- Iteration 15894 - Batch 486/963 - Train loss: 0.33760963  - Train acc: 0.8712 - Val loss: 1.16599226\n",
      "(3.25 min) Epoch 17/300 -- Iteration 15903 - Batch 495/963 - Train loss: 0.33743565  - Train acc: 0.8712 - Val loss: 1.16599226\n",
      "(3.25 min) Epoch 17/300 -- Iteration 15912 - Batch 504/963 - Train loss: 0.33754711  - Train acc: 0.8711 - Val loss: 1.16599226\n",
      "(3.25 min) Epoch 17/300 -- Iteration 15921 - Batch 513/963 - Train loss: 0.33750278  - Train acc: 0.8713 - Val loss: 1.16599226\n",
      "(3.25 min) Epoch 17/300 -- Iteration 15930 - Batch 522/963 - Train loss: 0.33733267  - Train acc: 0.8714 - Val loss: 1.16599226\n",
      "(3.26 min) Epoch 17/300 -- Iteration 15939 - Batch 531/963 - Train loss: 0.33774387  - Train acc: 0.8712 - Val loss: 1.16599226\n",
      "(3.26 min) Epoch 17/300 -- Iteration 15948 - Batch 540/963 - Train loss: 0.33786776  - Train acc: 0.8711 - Val loss: 1.16599226\n",
      "(3.26 min) Epoch 17/300 -- Iteration 15957 - Batch 549/963 - Train loss: 0.33803563  - Train acc: 0.8710 - Val loss: 1.16599226\n",
      "(3.26 min) Epoch 17/300 -- Iteration 15966 - Batch 558/963 - Train loss: 0.33758042  - Train acc: 0.8713 - Val loss: 1.16599226\n",
      "(3.26 min) Epoch 17/300 -- Iteration 15975 - Batch 567/963 - Train loss: 0.33761793  - Train acc: 0.8712 - Val loss: 1.16599226\n",
      "(3.27 min) Epoch 17/300 -- Iteration 15984 - Batch 576/963 - Train loss: 0.33724948  - Train acc: 0.8714 - Val loss: 1.16599226\n",
      "(3.27 min) Epoch 17/300 -- Iteration 15993 - Batch 585/963 - Train loss: 0.33699540  - Train acc: 0.8716 - Val loss: 1.16599226\n",
      "(3.27 min) Epoch 17/300 -- Iteration 16002 - Batch 594/963 - Train loss: 0.33702589  - Train acc: 0.8717 - Val loss: 1.16599226\n",
      "(3.27 min) Epoch 17/300 -- Iteration 16011 - Batch 603/963 - Train loss: 0.33712204  - Train acc: 0.8717 - Val loss: 1.16599226\n",
      "(3.27 min) Epoch 17/300 -- Iteration 16020 - Batch 612/963 - Train loss: 0.33685603  - Train acc: 0.8718 - Val loss: 1.16599226\n",
      "(3.27 min) Epoch 17/300 -- Iteration 16029 - Batch 621/963 - Train loss: 0.33706780  - Train acc: 0.8716 - Val loss: 1.16599226\n",
      "(3.28 min) Epoch 17/300 -- Iteration 16038 - Batch 630/963 - Train loss: 0.33696000  - Train acc: 0.8715 - Val loss: 1.16599226\n",
      "(3.28 min) Epoch 17/300 -- Iteration 16047 - Batch 639/963 - Train loss: 0.33667813  - Train acc: 0.8715 - Val loss: 1.16599226\n",
      "(3.28 min) Epoch 17/300 -- Iteration 16056 - Batch 648/963 - Train loss: 0.33671272  - Train acc: 0.8716 - Val loss: 1.16599226\n",
      "(3.28 min) Epoch 17/300 -- Iteration 16065 - Batch 657/963 - Train loss: 0.33670353  - Train acc: 0.8716 - Val loss: 1.16599226\n",
      "(3.28 min) Epoch 17/300 -- Iteration 16074 - Batch 666/963 - Train loss: 0.33636436  - Train acc: 0.8719 - Val loss: 1.16599226\n",
      "(3.29 min) Epoch 17/300 -- Iteration 16083 - Batch 675/963 - Train loss: 0.33565065  - Train acc: 0.8721 - Val loss: 1.16599226\n",
      "(3.29 min) Epoch 17/300 -- Iteration 16092 - Batch 684/963 - Train loss: 0.33568699  - Train acc: 0.8721 - Val loss: 1.16599226\n",
      "(3.29 min) Epoch 17/300 -- Iteration 16101 - Batch 693/963 - Train loss: 0.33587665  - Train acc: 0.8718 - Val loss: 1.16599226\n",
      "(3.29 min) Epoch 17/300 -- Iteration 16110 - Batch 702/963 - Train loss: 0.33601353  - Train acc: 0.8717 - Val loss: 1.16599226\n",
      "(3.29 min) Epoch 17/300 -- Iteration 16119 - Batch 711/963 - Train loss: 0.33577304  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.29 min) Epoch 17/300 -- Iteration 16128 - Batch 720/963 - Train loss: 0.33575547  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.30 min) Epoch 17/300 -- Iteration 16137 - Batch 729/963 - Train loss: 0.33565578  - Train acc: 0.8720 - Val loss: 1.16599226\n",
      "(3.30 min) Epoch 17/300 -- Iteration 16146 - Batch 738/963 - Train loss: 0.33552161  - Train acc: 0.8721 - Val loss: 1.16599226\n",
      "(3.30 min) Epoch 17/300 -- Iteration 16155 - Batch 747/963 - Train loss: 0.33538116  - Train acc: 0.8721 - Val loss: 1.16599226\n",
      "(3.30 min) Epoch 17/300 -- Iteration 16164 - Batch 756/963 - Train loss: 0.33548448  - Train acc: 0.8721 - Val loss: 1.16599226\n",
      "(3.30 min) Epoch 17/300 -- Iteration 16173 - Batch 765/963 - Train loss: 0.33526442  - Train acc: 0.8721 - Val loss: 1.16599226\n",
      "(3.31 min) Epoch 17/300 -- Iteration 16182 - Batch 774/963 - Train loss: 0.33525933  - Train acc: 0.8722 - Val loss: 1.16599226\n",
      "(3.31 min) Epoch 17/300 -- Iteration 16191 - Batch 783/963 - Train loss: 0.33534488  - Train acc: 0.8722 - Val loss: 1.16599226\n",
      "(3.31 min) Epoch 17/300 -- Iteration 16200 - Batch 792/963 - Train loss: 0.33534646  - Train acc: 0.8722 - Val loss: 1.16599226\n",
      "(3.31 min) Epoch 17/300 -- Iteration 16209 - Batch 801/963 - Train loss: 0.33509020  - Train acc: 0.8723 - Val loss: 1.16599226\n",
      "(3.31 min) Epoch 17/300 -- Iteration 16218 - Batch 810/963 - Train loss: 0.33495258  - Train acc: 0.8723 - Val loss: 1.16599226\n",
      "(3.31 min) Epoch 17/300 -- Iteration 16227 - Batch 819/963 - Train loss: 0.33480370  - Train acc: 0.8723 - Val loss: 1.16599226\n",
      "(3.32 min) Epoch 17/300 -- Iteration 16236 - Batch 828/963 - Train loss: 0.33476808  - Train acc: 0.8723 - Val loss: 1.16599226\n",
      "(3.32 min) Epoch 17/300 -- Iteration 16245 - Batch 837/963 - Train loss: 0.33470842  - Train acc: 0.8724 - Val loss: 1.16599226\n",
      "(3.32 min) Epoch 17/300 -- Iteration 16254 - Batch 846/963 - Train loss: 0.33484434  - Train acc: 0.8724 - Val loss: 1.16599226\n",
      "(3.32 min) Epoch 17/300 -- Iteration 16263 - Batch 855/963 - Train loss: 0.33439684  - Train acc: 0.8726 - Val loss: 1.16599226\n",
      "(3.32 min) Epoch 17/300 -- Iteration 16272 - Batch 864/963 - Train loss: 0.33456206  - Train acc: 0.8727 - Val loss: 1.16599226\n",
      "(3.33 min) Epoch 17/300 -- Iteration 16281 - Batch 873/963 - Train loss: 0.33428847  - Train acc: 0.8727 - Val loss: 1.16599226\n",
      "(3.33 min) Epoch 17/300 -- Iteration 16290 - Batch 882/963 - Train loss: 0.33443617  - Train acc: 0.8726 - Val loss: 1.16599226\n",
      "(3.33 min) Epoch 17/300 -- Iteration 16299 - Batch 891/963 - Train loss: 0.33455969  - Train acc: 0.8726 - Val loss: 1.16599226\n",
      "(3.33 min) Epoch 17/300 -- Iteration 16308 - Batch 900/963 - Train loss: 0.33461788  - Train acc: 0.8726 - Val loss: 1.16599226\n",
      "(3.33 min) Epoch 17/300 -- Iteration 16317 - Batch 909/963 - Train loss: 0.33431170  - Train acc: 0.8727 - Val loss: 1.16599226\n",
      "(3.33 min) Epoch 17/300 -- Iteration 16326 - Batch 918/963 - Train loss: 0.33454670  - Train acc: 0.8727 - Val loss: 1.16599226\n",
      "(3.34 min) Epoch 17/300 -- Iteration 16335 - Batch 927/963 - Train loss: 0.33442615  - Train acc: 0.8727 - Val loss: 1.16599226\n",
      "(3.34 min) Epoch 17/300 -- Iteration 16344 - Batch 936/963 - Train loss: 0.33455737  - Train acc: 0.8727 - Val loss: 1.16599226\n",
      "(3.34 min) Epoch 17/300 -- Iteration 16353 - Batch 945/963 - Train loss: 0.33450494  - Train acc: 0.8727 - Val loss: 1.16599226\n",
      "(3.34 min) Epoch 17/300 -- Iteration 16362 - Batch 954/963 - Train loss: 0.33480249  - Train acc: 0.8726 - Val loss: 1.16599226\n",
      "(3.34 min) Epoch 17/300 -- Iteration 16371 - Batch 962/963 - Train loss: 0.33511519  - Train acc: 0.8725 - Val loss: 1.15977216 - Val acc: 0.6083\n",
      "(3.35 min) Epoch 18/300 -- Iteration 16380 - Batch 9/963 - Train loss: 0.34170347  - Train acc: 0.8797 - Val loss: 1.15977216\n",
      "(3.35 min) Epoch 18/300 -- Iteration 16389 - Batch 18/963 - Train loss: 0.34345687  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.35 min) Epoch 18/300 -- Iteration 16398 - Batch 27/963 - Train loss: 0.34268513  - Train acc: 0.8730 - Val loss: 1.15977216\n",
      "(3.35 min) Epoch 18/300 -- Iteration 16407 - Batch 36/963 - Train loss: 0.33958910  - Train acc: 0.8731 - Val loss: 1.15977216\n",
      "(3.35 min) Epoch 18/300 -- Iteration 16416 - Batch 45/963 - Train loss: 0.33182780  - Train acc: 0.8782 - Val loss: 1.15977216\n",
      "(3.36 min) Epoch 18/300 -- Iteration 16425 - Batch 54/963 - Train loss: 0.33757884  - Train acc: 0.8756 - Val loss: 1.15977216\n",
      "(3.36 min) Epoch 18/300 -- Iteration 16434 - Batch 63/963 - Train loss: 0.33498716  - Train acc: 0.8772 - Val loss: 1.15977216\n",
      "(3.36 min) Epoch 18/300 -- Iteration 16443 - Batch 72/963 - Train loss: 0.33165803  - Train acc: 0.8780 - Val loss: 1.15977216\n",
      "(3.36 min) Epoch 18/300 -- Iteration 16452 - Batch 81/963 - Train loss: 0.33232157  - Train acc: 0.8780 - Val loss: 1.15977216\n",
      "(3.36 min) Epoch 18/300 -- Iteration 16461 - Batch 90/963 - Train loss: 0.33181203  - Train acc: 0.8777 - Val loss: 1.15977216\n",
      "(3.36 min) Epoch 18/300 -- Iteration 16470 - Batch 99/963 - Train loss: 0.33324681  - Train acc: 0.8766 - Val loss: 1.15977216\n",
      "(3.37 min) Epoch 18/300 -- Iteration 16479 - Batch 108/963 - Train loss: 0.33428176  - Train acc: 0.8758 - Val loss: 1.15977216\n",
      "(3.37 min) Epoch 18/300 -- Iteration 16488 - Batch 117/963 - Train loss: 0.33427588  - Train acc: 0.8753 - Val loss: 1.15977216\n",
      "(3.37 min) Epoch 18/300 -- Iteration 16497 - Batch 126/963 - Train loss: 0.33344994  - Train acc: 0.8761 - Val loss: 1.15977216\n",
      "(3.37 min) Epoch 18/300 -- Iteration 16506 - Batch 135/963 - Train loss: 0.33059215  - Train acc: 0.8772 - Val loss: 1.15977216\n",
      "(3.37 min) Epoch 18/300 -- Iteration 16515 - Batch 144/963 - Train loss: 0.33084066  - Train acc: 0.8773 - Val loss: 1.15977216\n",
      "(3.38 min) Epoch 18/300 -- Iteration 16524 - Batch 153/963 - Train loss: 0.33131028  - Train acc: 0.8771 - Val loss: 1.15977216\n",
      "(3.38 min) Epoch 18/300 -- Iteration 16533 - Batch 162/963 - Train loss: 0.33081415  - Train acc: 0.8775 - Val loss: 1.15977216\n",
      "(3.38 min) Epoch 18/300 -- Iteration 16542 - Batch 171/963 - Train loss: 0.32994942  - Train acc: 0.8775 - Val loss: 1.15977216\n",
      "(3.38 min) Epoch 18/300 -- Iteration 16551 - Batch 180/963 - Train loss: 0.33083196  - Train acc: 0.8769 - Val loss: 1.15977216\n",
      "(3.38 min) Epoch 18/300 -- Iteration 16560 - Batch 189/963 - Train loss: 0.33185390  - Train acc: 0.8758 - Val loss: 1.15977216\n",
      "(3.38 min) Epoch 18/300 -- Iteration 16569 - Batch 198/963 - Train loss: 0.33102836  - Train acc: 0.8761 - Val loss: 1.15977216\n",
      "(3.39 min) Epoch 18/300 -- Iteration 16578 - Batch 207/963 - Train loss: 0.33155710  - Train acc: 0.8760 - Val loss: 1.15977216\n",
      "(3.39 min) Epoch 18/300 -- Iteration 16587 - Batch 216/963 - Train loss: 0.33091963  - Train acc: 0.8761 - Val loss: 1.15977216\n",
      "(3.39 min) Epoch 18/300 -- Iteration 16596 - Batch 225/963 - Train loss: 0.33081082  - Train acc: 0.8760 - Val loss: 1.15977216\n",
      "(3.39 min) Epoch 18/300 -- Iteration 16605 - Batch 234/963 - Train loss: 0.33229335  - Train acc: 0.8755 - Val loss: 1.15977216\n",
      "(3.39 min) Epoch 18/300 -- Iteration 16614 - Batch 243/963 - Train loss: 0.33205410  - Train acc: 0.8757 - Val loss: 1.15977216\n",
      "(3.40 min) Epoch 18/300 -- Iteration 16623 - Batch 252/963 - Train loss: 0.33135440  - Train acc: 0.8756 - Val loss: 1.15977216\n",
      "(3.40 min) Epoch 18/300 -- Iteration 16632 - Batch 261/963 - Train loss: 0.33128214  - Train acc: 0.8759 - Val loss: 1.15977216\n",
      "(3.40 min) Epoch 18/300 -- Iteration 16641 - Batch 270/963 - Train loss: 0.33082170  - Train acc: 0.8761 - Val loss: 1.15977216\n",
      "(3.40 min) Epoch 18/300 -- Iteration 16650 - Batch 279/963 - Train loss: 0.33047831  - Train acc: 0.8759 - Val loss: 1.15977216\n",
      "(3.40 min) Epoch 18/300 -- Iteration 16659 - Batch 288/963 - Train loss: 0.33046489  - Train acc: 0.8758 - Val loss: 1.15977216\n",
      "(3.40 min) Epoch 18/300 -- Iteration 16668 - Batch 297/963 - Train loss: 0.33150971  - Train acc: 0.8751 - Val loss: 1.15977216\n",
      "(3.41 min) Epoch 18/300 -- Iteration 16677 - Batch 306/963 - Train loss: 0.33125934  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.41 min) Epoch 18/300 -- Iteration 16686 - Batch 315/963 - Train loss: 0.33198952  - Train acc: 0.8747 - Val loss: 1.15977216\n",
      "(3.41 min) Epoch 18/300 -- Iteration 16695 - Batch 324/963 - Train loss: 0.33221874  - Train acc: 0.8744 - Val loss: 1.15977216\n",
      "(3.41 min) Epoch 18/300 -- Iteration 16704 - Batch 333/963 - Train loss: 0.33273203  - Train acc: 0.8745 - Val loss: 1.15977216\n",
      "(3.41 min) Epoch 18/300 -- Iteration 16713 - Batch 342/963 - Train loss: 0.33262880  - Train acc: 0.8745 - Val loss: 1.15977216\n",
      "(3.42 min) Epoch 18/300 -- Iteration 16722 - Batch 351/963 - Train loss: 0.33327778  - Train acc: 0.8740 - Val loss: 1.15977216\n",
      "(3.42 min) Epoch 18/300 -- Iteration 16731 - Batch 360/963 - Train loss: 0.33325261  - Train acc: 0.8740 - Val loss: 1.15977216\n",
      "(3.42 min) Epoch 18/300 -- Iteration 16740 - Batch 369/963 - Train loss: 0.33289893  - Train acc: 0.8740 - Val loss: 1.15977216\n",
      "(3.42 min) Epoch 18/300 -- Iteration 16749 - Batch 378/963 - Train loss: 0.33231988  - Train acc: 0.8744 - Val loss: 1.15977216\n",
      "(3.42 min) Epoch 18/300 -- Iteration 16758 - Batch 387/963 - Train loss: 0.33218400  - Train acc: 0.8743 - Val loss: 1.15977216\n",
      "(3.42 min) Epoch 18/300 -- Iteration 16767 - Batch 396/963 - Train loss: 0.33164719  - Train acc: 0.8744 - Val loss: 1.15977216\n",
      "(3.43 min) Epoch 18/300 -- Iteration 16776 - Batch 405/963 - Train loss: 0.33104580  - Train acc: 0.8746 - Val loss: 1.15977216\n",
      "(3.43 min) Epoch 18/300 -- Iteration 16785 - Batch 414/963 - Train loss: 0.33118129  - Train acc: 0.8744 - Val loss: 1.15977216\n",
      "(3.43 min) Epoch 18/300 -- Iteration 16794 - Batch 423/963 - Train loss: 0.33105682  - Train acc: 0.8745 - Val loss: 1.15977216\n",
      "(3.43 min) Epoch 18/300 -- Iteration 16803 - Batch 432/963 - Train loss: 0.33077586  - Train acc: 0.8746 - Val loss: 1.15977216\n",
      "(3.43 min) Epoch 18/300 -- Iteration 16812 - Batch 441/963 - Train loss: 0.33140730  - Train acc: 0.8743 - Val loss: 1.15977216\n",
      "(3.44 min) Epoch 18/300 -- Iteration 16821 - Batch 450/963 - Train loss: 0.33144134  - Train acc: 0.8742 - Val loss: 1.15977216\n",
      "(3.44 min) Epoch 18/300 -- Iteration 16830 - Batch 459/963 - Train loss: 0.33180463  - Train acc: 0.8742 - Val loss: 1.15977216\n",
      "(3.44 min) Epoch 18/300 -- Iteration 16839 - Batch 468/963 - Train loss: 0.33148285  - Train acc: 0.8743 - Val loss: 1.15977216\n",
      "(3.44 min) Epoch 18/300 -- Iteration 16848 - Batch 477/963 - Train loss: 0.33164531  - Train acc: 0.8744 - Val loss: 1.15977216\n",
      "(3.44 min) Epoch 18/300 -- Iteration 16857 - Batch 486/963 - Train loss: 0.33167898  - Train acc: 0.8744 - Val loss: 1.15977216\n",
      "(3.44 min) Epoch 18/300 -- Iteration 16866 - Batch 495/963 - Train loss: 0.33205718  - Train acc: 0.8741 - Val loss: 1.15977216\n",
      "(3.45 min) Epoch 18/300 -- Iteration 16875 - Batch 504/963 - Train loss: 0.33173151  - Train acc: 0.8743 - Val loss: 1.15977216\n",
      "(3.45 min) Epoch 18/300 -- Iteration 16884 - Batch 513/963 - Train loss: 0.33201791  - Train acc: 0.8740 - Val loss: 1.15977216\n",
      "(3.45 min) Epoch 18/300 -- Iteration 16893 - Batch 522/963 - Train loss: 0.33166932  - Train acc: 0.8741 - Val loss: 1.15977216\n",
      "(3.45 min) Epoch 18/300 -- Iteration 16902 - Batch 531/963 - Train loss: 0.33190700  - Train acc: 0.8741 - Val loss: 1.15977216\n",
      "(3.45 min) Epoch 18/300 -- Iteration 16911 - Batch 540/963 - Train loss: 0.33165324  - Train acc: 0.8742 - Val loss: 1.15977216\n",
      "(3.46 min) Epoch 18/300 -- Iteration 16920 - Batch 549/963 - Train loss: 0.33203058  - Train acc: 0.8741 - Val loss: 1.15977216\n",
      "(3.46 min) Epoch 18/300 -- Iteration 16929 - Batch 558/963 - Train loss: 0.33187965  - Train acc: 0.8741 - Val loss: 1.15977216\n",
      "(3.46 min) Epoch 18/300 -- Iteration 16938 - Batch 567/963 - Train loss: 0.33182419  - Train acc: 0.8741 - Val loss: 1.15977216\n",
      "(3.46 min) Epoch 18/300 -- Iteration 16947 - Batch 576/963 - Train loss: 0.33157187  - Train acc: 0.8742 - Val loss: 1.15977216\n",
      "(3.46 min) Epoch 18/300 -- Iteration 16956 - Batch 585/963 - Train loss: 0.33127369  - Train acc: 0.8743 - Val loss: 1.15977216\n",
      "(3.46 min) Epoch 18/300 -- Iteration 16965 - Batch 594/963 - Train loss: 0.33122825  - Train acc: 0.8741 - Val loss: 1.15977216\n",
      "(3.47 min) Epoch 18/300 -- Iteration 16974 - Batch 603/963 - Train loss: 0.33118751  - Train acc: 0.8742 - Val loss: 1.15977216\n",
      "(3.47 min) Epoch 18/300 -- Iteration 16983 - Batch 612/963 - Train loss: 0.33093411  - Train acc: 0.8742 - Val loss: 1.15977216\n",
      "(3.47 min) Epoch 18/300 -- Iteration 16992 - Batch 621/963 - Train loss: 0.33059447  - Train acc: 0.8743 - Val loss: 1.15977216\n",
      "(3.47 min) Epoch 18/300 -- Iteration 17001 - Batch 630/963 - Train loss: 0.33031835  - Train acc: 0.8745 - Val loss: 1.15977216\n",
      "(3.47 min) Epoch 18/300 -- Iteration 17010 - Batch 639/963 - Train loss: 0.33025818  - Train acc: 0.8746 - Val loss: 1.15977216\n",
      "(3.48 min) Epoch 18/300 -- Iteration 17019 - Batch 648/963 - Train loss: 0.33055890  - Train acc: 0.8744 - Val loss: 1.15977216\n",
      "(3.48 min) Epoch 18/300 -- Iteration 17028 - Batch 657/963 - Train loss: 0.33073162  - Train acc: 0.8744 - Val loss: 1.15977216\n",
      "(3.48 min) Epoch 18/300 -- Iteration 17037 - Batch 666/963 - Train loss: 0.33025999  - Train acc: 0.8746 - Val loss: 1.15977216\n",
      "(3.48 min) Epoch 18/300 -- Iteration 17046 - Batch 675/963 - Train loss: 0.33028039  - Train acc: 0.8746 - Val loss: 1.15977216\n",
      "(3.48 min) Epoch 18/300 -- Iteration 17055 - Batch 684/963 - Train loss: 0.33020739  - Train acc: 0.8747 - Val loss: 1.15977216\n",
      "(3.48 min) Epoch 18/300 -- Iteration 17064 - Batch 693/963 - Train loss: 0.33060836  - Train acc: 0.8745 - Val loss: 1.15977216\n",
      "(3.49 min) Epoch 18/300 -- Iteration 17073 - Batch 702/963 - Train loss: 0.33066713  - Train acc: 0.8745 - Val loss: 1.15977216\n",
      "(3.49 min) Epoch 18/300 -- Iteration 17082 - Batch 711/963 - Train loss: 0.33040747  - Train acc: 0.8747 - Val loss: 1.15977216\n",
      "(3.49 min) Epoch 18/300 -- Iteration 17091 - Batch 720/963 - Train loss: 0.33020278  - Train acc: 0.8747 - Val loss: 1.15977216\n",
      "(3.49 min) Epoch 18/300 -- Iteration 17100 - Batch 729/963 - Train loss: 0.32970092  - Train acc: 0.8749 - Val loss: 1.15977216\n",
      "(3.49 min) Epoch 18/300 -- Iteration 17109 - Batch 738/963 - Train loss: 0.32976556  - Train acc: 0.8749 - Val loss: 1.15977216\n",
      "(3.50 min) Epoch 18/300 -- Iteration 17118 - Batch 747/963 - Train loss: 0.32947261  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.50 min) Epoch 18/300 -- Iteration 17127 - Batch 756/963 - Train loss: 0.32954945  - Train acc: 0.8749 - Val loss: 1.15977216\n",
      "(3.50 min) Epoch 18/300 -- Iteration 17136 - Batch 765/963 - Train loss: 0.32926046  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.50 min) Epoch 18/300 -- Iteration 17145 - Batch 774/963 - Train loss: 0.32946498  - Train acc: 0.8748 - Val loss: 1.15977216\n",
      "(3.50 min) Epoch 18/300 -- Iteration 17154 - Batch 783/963 - Train loss: 0.32938653  - Train acc: 0.8749 - Val loss: 1.15977216\n",
      "(3.50 min) Epoch 18/300 -- Iteration 17163 - Batch 792/963 - Train loss: 0.32952427  - Train acc: 0.8749 - Val loss: 1.15977216\n",
      "(3.51 min) Epoch 18/300 -- Iteration 17172 - Batch 801/963 - Train loss: 0.32959870  - Train acc: 0.8749 - Val loss: 1.15977216\n",
      "(3.51 min) Epoch 18/300 -- Iteration 17181 - Batch 810/963 - Train loss: 0.32935613  - Train acc: 0.8751 - Val loss: 1.15977216\n",
      "(3.51 min) Epoch 18/300 -- Iteration 17190 - Batch 819/963 - Train loss: 0.32960450  - Train acc: 0.8749 - Val loss: 1.15977216\n",
      "(3.51 min) Epoch 18/300 -- Iteration 17199 - Batch 828/963 - Train loss: 0.32930242  - Train acc: 0.8751 - Val loss: 1.15977216\n",
      "(3.51 min) Epoch 18/300 -- Iteration 17208 - Batch 837/963 - Train loss: 0.32931908  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.52 min) Epoch 18/300 -- Iteration 17217 - Batch 846/963 - Train loss: 0.32963980  - Train acc: 0.8748 - Val loss: 1.15977216\n",
      "(3.52 min) Epoch 18/300 -- Iteration 17226 - Batch 855/963 - Train loss: 0.32945028  - Train acc: 0.8749 - Val loss: 1.15977216\n",
      "(3.52 min) Epoch 18/300 -- Iteration 17235 - Batch 864/963 - Train loss: 0.32915386  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.52 min) Epoch 18/300 -- Iteration 17244 - Batch 873/963 - Train loss: 0.32915826  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.52 min) Epoch 18/300 -- Iteration 17253 - Batch 882/963 - Train loss: 0.32903263  - Train acc: 0.8751 - Val loss: 1.15977216\n",
      "(3.53 min) Epoch 18/300 -- Iteration 17262 - Batch 891/963 - Train loss: 0.32911521  - Train acc: 0.8751 - Val loss: 1.15977216\n",
      "(3.53 min) Epoch 18/300 -- Iteration 17271 - Batch 900/963 - Train loss: 0.32892636  - Train acc: 0.8751 - Val loss: 1.15977216\n",
      "(3.53 min) Epoch 18/300 -- Iteration 17280 - Batch 909/963 - Train loss: 0.32902510  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.53 min) Epoch 18/300 -- Iteration 17289 - Batch 918/963 - Train loss: 0.32921124  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.53 min) Epoch 18/300 -- Iteration 17298 - Batch 927/963 - Train loss: 0.32925405  - Train acc: 0.8749 - Val loss: 1.15977216\n",
      "(3.53 min) Epoch 18/300 -- Iteration 17307 - Batch 936/963 - Train loss: 0.32898448  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.54 min) Epoch 18/300 -- Iteration 17316 - Batch 945/963 - Train loss: 0.32899499  - Train acc: 0.8750 - Val loss: 1.15977216\n",
      "(3.54 min) Epoch 18/300 -- Iteration 17325 - Batch 954/963 - Train loss: 0.32898453  - Train acc: 0.8751 - Val loss: 1.15977216\n",
      "(3.54 min) Epoch 18/300 -- Iteration 17334 - Batch 962/963 - Train loss: 0.32883014  - Train acc: 0.8752 - Val loss: 1.14314198 - Val acc: 0.6100\n",
      "(3.54 min) Epoch 19/300 -- Iteration 17343 - Batch 9/963 - Train loss: 0.30863384  - Train acc: 0.8789 - Val loss: 1.14314198\n",
      "(3.54 min) Epoch 19/300 -- Iteration 17352 - Batch 18/963 - Train loss: 0.32882183  - Train acc: 0.8738 - Val loss: 1.14314198\n",
      "(3.55 min) Epoch 19/300 -- Iteration 17361 - Batch 27/963 - Train loss: 0.32817829  - Train acc: 0.8744 - Val loss: 1.14314198\n",
      "(3.55 min) Epoch 19/300 -- Iteration 17370 - Batch 36/963 - Train loss: 0.32709244  - Train acc: 0.8729 - Val loss: 1.14314198\n",
      "(3.55 min) Epoch 19/300 -- Iteration 17379 - Batch 45/963 - Train loss: 0.32236486  - Train acc: 0.8748 - Val loss: 1.14314198\n",
      "(3.55 min) Epoch 19/300 -- Iteration 17388 - Batch 54/963 - Train loss: 0.32433742  - Train acc: 0.8747 - Val loss: 1.14314198\n",
      "(3.55 min) Epoch 19/300 -- Iteration 17397 - Batch 63/963 - Train loss: 0.32142521  - Train acc: 0.8766 - Val loss: 1.14314198\n",
      "(3.55 min) Epoch 19/300 -- Iteration 17406 - Batch 72/963 - Train loss: 0.31983580  - Train acc: 0.8764 - Val loss: 1.14314198\n",
      "(3.56 min) Epoch 19/300 -- Iteration 17415 - Batch 81/963 - Train loss: 0.32323746  - Train acc: 0.8757 - Val loss: 1.14314198\n",
      "(3.56 min) Epoch 19/300 -- Iteration 17424 - Batch 90/963 - Train loss: 0.32220129  - Train acc: 0.8760 - Val loss: 1.14314198\n",
      "(3.56 min) Epoch 19/300 -- Iteration 17433 - Batch 99/963 - Train loss: 0.32355954  - Train acc: 0.8759 - Val loss: 1.14314198\n",
      "(3.56 min) Epoch 19/300 -- Iteration 17442 - Batch 108/963 - Train loss: 0.32485726  - Train acc: 0.8754 - Val loss: 1.14314198\n",
      "(3.56 min) Epoch 19/300 -- Iteration 17451 - Batch 117/963 - Train loss: 0.32536864  - Train acc: 0.8743 - Val loss: 1.14314198\n",
      "(3.57 min) Epoch 19/300 -- Iteration 17460 - Batch 126/963 - Train loss: 0.32707934  - Train acc: 0.8745 - Val loss: 1.14314198\n",
      "(3.57 min) Epoch 19/300 -- Iteration 17469 - Batch 135/963 - Train loss: 0.32855842  - Train acc: 0.8739 - Val loss: 1.14314198\n",
      "(3.57 min) Epoch 19/300 -- Iteration 17478 - Batch 144/963 - Train loss: 0.32808514  - Train acc: 0.8746 - Val loss: 1.14314198\n",
      "(3.57 min) Epoch 19/300 -- Iteration 17487 - Batch 153/963 - Train loss: 0.32846277  - Train acc: 0.8748 - Val loss: 1.14314198\n",
      "(3.57 min) Epoch 19/300 -- Iteration 17496 - Batch 162/963 - Train loss: 0.32814862  - Train acc: 0.8749 - Val loss: 1.14314198\n",
      "(3.58 min) Epoch 19/300 -- Iteration 17505 - Batch 171/963 - Train loss: 0.32773612  - Train acc: 0.8750 - Val loss: 1.14314198\n",
      "(3.58 min) Epoch 19/300 -- Iteration 17514 - Batch 180/963 - Train loss: 0.32732213  - Train acc: 0.8753 - Val loss: 1.14314198\n",
      "(3.58 min) Epoch 19/300 -- Iteration 17523 - Batch 189/963 - Train loss: 0.32693594  - Train acc: 0.8755 - Val loss: 1.14314198\n",
      "(3.58 min) Epoch 19/300 -- Iteration 17532 - Batch 198/963 - Train loss: 0.32825043  - Train acc: 0.8752 - Val loss: 1.14314198\n",
      "(3.58 min) Epoch 19/300 -- Iteration 17541 - Batch 207/963 - Train loss: 0.32813755  - Train acc: 0.8755 - Val loss: 1.14314198\n",
      "(3.58 min) Epoch 19/300 -- Iteration 17550 - Batch 216/963 - Train loss: 0.32736477  - Train acc: 0.8760 - Val loss: 1.14314198\n",
      "(3.59 min) Epoch 19/300 -- Iteration 17559 - Batch 225/963 - Train loss: 0.32717006  - Train acc: 0.8762 - Val loss: 1.14314198\n",
      "(3.59 min) Epoch 19/300 -- Iteration 17568 - Batch 234/963 - Train loss: 0.32756189  - Train acc: 0.8761 - Val loss: 1.14314198\n",
      "(3.59 min) Epoch 19/300 -- Iteration 17577 - Batch 243/963 - Train loss: 0.32611858  - Train acc: 0.8770 - Val loss: 1.14314198\n",
      "(3.59 min) Epoch 19/300 -- Iteration 17586 - Batch 252/963 - Train loss: 0.32608076  - Train acc: 0.8771 - Val loss: 1.14314198\n",
      "(3.59 min) Epoch 19/300 -- Iteration 17595 - Batch 261/963 - Train loss: 0.32683913  - Train acc: 0.8767 - Val loss: 1.14314198\n",
      "(3.60 min) Epoch 19/300 -- Iteration 17604 - Batch 270/963 - Train loss: 0.32656636  - Train acc: 0.8768 - Val loss: 1.14314198\n",
      "(3.60 min) Epoch 19/300 -- Iteration 17613 - Batch 279/963 - Train loss: 0.32592025  - Train acc: 0.8770 - Val loss: 1.14314198\n",
      "(3.60 min) Epoch 19/300 -- Iteration 17622 - Batch 288/963 - Train loss: 0.32701898  - Train acc: 0.8764 - Val loss: 1.14314198\n",
      "(3.60 min) Epoch 19/300 -- Iteration 17631 - Batch 297/963 - Train loss: 0.32696403  - Train acc: 0.8763 - Val loss: 1.14314198\n",
      "(3.60 min) Epoch 19/300 -- Iteration 17640 - Batch 306/963 - Train loss: 0.32635878  - Train acc: 0.8765 - Val loss: 1.14314198\n",
      "(3.60 min) Epoch 19/300 -- Iteration 17649 - Batch 315/963 - Train loss: 0.32589866  - Train acc: 0.8767 - Val loss: 1.14314198\n",
      "(3.61 min) Epoch 19/300 -- Iteration 17658 - Batch 324/963 - Train loss: 0.32597834  - Train acc: 0.8767 - Val loss: 1.14314198\n",
      "(3.61 min) Epoch 19/300 -- Iteration 17667 - Batch 333/963 - Train loss: 0.32632133  - Train acc: 0.8767 - Val loss: 1.14314198\n",
      "(3.61 min) Epoch 19/300 -- Iteration 17676 - Batch 342/963 - Train loss: 0.32579329  - Train acc: 0.8768 - Val loss: 1.14314198\n",
      "(3.61 min) Epoch 19/300 -- Iteration 17685 - Batch 351/963 - Train loss: 0.32601262  - Train acc: 0.8766 - Val loss: 1.14314198\n",
      "(3.61 min) Epoch 19/300 -- Iteration 17694 - Batch 360/963 - Train loss: 0.32579473  - Train acc: 0.8767 - Val loss: 1.14314198\n",
      "(3.62 min) Epoch 19/300 -- Iteration 17703 - Batch 369/963 - Train loss: 0.32583214  - Train acc: 0.8770 - Val loss: 1.14314198\n",
      "(3.62 min) Epoch 19/300 -- Iteration 17712 - Batch 378/963 - Train loss: 0.32624136  - Train acc: 0.8770 - Val loss: 1.14314198\n",
      "(3.62 min) Epoch 19/300 -- Iteration 17721 - Batch 387/963 - Train loss: 0.32629504  - Train acc: 0.8770 - Val loss: 1.14314198\n",
      "(3.62 min) Epoch 19/300 -- Iteration 17730 - Batch 396/963 - Train loss: 0.32622700  - Train acc: 0.8771 - Val loss: 1.14314198\n",
      "(3.62 min) Epoch 19/300 -- Iteration 17739 - Batch 405/963 - Train loss: 0.32617425  - Train acc: 0.8772 - Val loss: 1.14314198\n",
      "(3.62 min) Epoch 19/300 -- Iteration 17748 - Batch 414/963 - Train loss: 0.32662994  - Train acc: 0.8771 - Val loss: 1.14314198\n",
      "(3.63 min) Epoch 19/300 -- Iteration 17757 - Batch 423/963 - Train loss: 0.32700173  - Train acc: 0.8768 - Val loss: 1.14314198\n",
      "(3.63 min) Epoch 19/300 -- Iteration 17766 - Batch 432/963 - Train loss: 0.32712325  - Train acc: 0.8765 - Val loss: 1.14314198\n",
      "(3.63 min) Epoch 19/300 -- Iteration 17775 - Batch 441/963 - Train loss: 0.32695245  - Train acc: 0.8767 - Val loss: 1.14314198\n",
      "(3.63 min) Epoch 19/300 -- Iteration 17784 - Batch 450/963 - Train loss: 0.32646533  - Train acc: 0.8771 - Val loss: 1.14314198\n",
      "(3.63 min) Epoch 19/300 -- Iteration 17793 - Batch 459/963 - Train loss: 0.32667805  - Train acc: 0.8769 - Val loss: 1.14314198\n",
      "(3.63 min) Epoch 19/300 -- Iteration 17802 - Batch 468/963 - Train loss: 0.32577847  - Train acc: 0.8773 - Val loss: 1.14314198\n",
      "(3.64 min) Epoch 19/300 -- Iteration 17811 - Batch 477/963 - Train loss: 0.32568632  - Train acc: 0.8774 - Val loss: 1.14314198\n",
      "(3.64 min) Epoch 19/300 -- Iteration 17820 - Batch 486/963 - Train loss: 0.32589767  - Train acc: 0.8772 - Val loss: 1.14314198\n",
      "(3.64 min) Epoch 19/300 -- Iteration 17829 - Batch 495/963 - Train loss: 0.32623405  - Train acc: 0.8773 - Val loss: 1.14314198\n",
      "(3.64 min) Epoch 19/300 -- Iteration 17838 - Batch 504/963 - Train loss: 0.32656251  - Train acc: 0.8773 - Val loss: 1.14314198\n",
      "(3.64 min) Epoch 19/300 -- Iteration 17847 - Batch 513/963 - Train loss: 0.32649851  - Train acc: 0.8773 - Val loss: 1.14314198\n",
      "(3.65 min) Epoch 19/300 -- Iteration 17856 - Batch 522/963 - Train loss: 0.32677656  - Train acc: 0.8771 - Val loss: 1.14314198\n",
      "(3.65 min) Epoch 19/300 -- Iteration 17865 - Batch 531/963 - Train loss: 0.32696474  - Train acc: 0.8769 - Val loss: 1.14314198\n",
      "(3.65 min) Epoch 19/300 -- Iteration 17874 - Batch 540/963 - Train loss: 0.32682403  - Train acc: 0.8770 - Val loss: 1.14314198\n",
      "(3.65 min) Epoch 19/300 -- Iteration 17883 - Batch 549/963 - Train loss: 0.32640925  - Train acc: 0.8771 - Val loss: 1.14314198\n",
      "(3.65 min) Epoch 19/300 -- Iteration 17892 - Batch 558/963 - Train loss: 0.32678071  - Train acc: 0.8769 - Val loss: 1.14314198\n",
      "(3.66 min) Epoch 19/300 -- Iteration 17901 - Batch 567/963 - Train loss: 0.32702329  - Train acc: 0.8768 - Val loss: 1.14314198\n",
      "(3.66 min) Epoch 19/300 -- Iteration 17910 - Batch 576/963 - Train loss: 0.32735524  - Train acc: 0.8766 - Val loss: 1.14314198\n",
      "(3.66 min) Epoch 19/300 -- Iteration 17919 - Batch 585/963 - Train loss: 0.32741191  - Train acc: 0.8765 - Val loss: 1.14314198\n",
      "(3.66 min) Epoch 19/300 -- Iteration 17928 - Batch 594/963 - Train loss: 0.32723574  - Train acc: 0.8765 - Val loss: 1.14314198\n",
      "(3.66 min) Epoch 19/300 -- Iteration 17937 - Batch 603/963 - Train loss: 0.32801541  - Train acc: 0.8761 - Val loss: 1.14314198\n",
      "(3.67 min) Epoch 19/300 -- Iteration 17946 - Batch 612/963 - Train loss: 0.32777153  - Train acc: 0.8762 - Val loss: 1.14314198\n",
      "(3.67 min) Epoch 19/300 -- Iteration 17955 - Batch 621/963 - Train loss: 0.32832815  - Train acc: 0.8760 - Val loss: 1.14314198\n",
      "(3.67 min) Epoch 19/300 -- Iteration 17964 - Batch 630/963 - Train loss: 0.32823002  - Train acc: 0.8760 - Val loss: 1.14314198\n",
      "(3.67 min) Epoch 19/300 -- Iteration 17973 - Batch 639/963 - Train loss: 0.32787866  - Train acc: 0.8762 - Val loss: 1.14314198\n",
      "(3.67 min) Epoch 19/300 -- Iteration 17982 - Batch 648/963 - Train loss: 0.32725364  - Train acc: 0.8764 - Val loss: 1.14314198\n",
      "(3.68 min) Epoch 19/300 -- Iteration 17991 - Batch 657/963 - Train loss: 0.32689785  - Train acc: 0.8766 - Val loss: 1.14314198\n",
      "(3.68 min) Epoch 19/300 -- Iteration 18000 - Batch 666/963 - Train loss: 0.32672397  - Train acc: 0.8766 - Val loss: 1.14314198\n",
      "(3.68 min) Epoch 19/300 -- Iteration 18009 - Batch 675/963 - Train loss: 0.32689007  - Train acc: 0.8766 - Val loss: 1.14314198\n",
      "(3.68 min) Epoch 19/300 -- Iteration 18018 - Batch 684/963 - Train loss: 0.32670211  - Train acc: 0.8766 - Val loss: 1.14314198\n",
      "(3.68 min) Epoch 19/300 -- Iteration 18027 - Batch 693/963 - Train loss: 0.32650218  - Train acc: 0.8767 - Val loss: 1.14314198\n",
      "(3.68 min) Epoch 19/300 -- Iteration 18036 - Batch 702/963 - Train loss: 0.32693960  - Train acc: 0.8764 - Val loss: 1.14314198\n",
      "(3.69 min) Epoch 19/300 -- Iteration 18045 - Batch 711/963 - Train loss: 0.32711955  - Train acc: 0.8763 - Val loss: 1.14314198\n",
      "(3.69 min) Epoch 19/300 -- Iteration 18054 - Batch 720/963 - Train loss: 0.32682504  - Train acc: 0.8764 - Val loss: 1.14314198\n",
      "(3.69 min) Epoch 19/300 -- Iteration 18063 - Batch 729/963 - Train loss: 0.32688006  - Train acc: 0.8765 - Val loss: 1.14314198\n",
      "(3.69 min) Epoch 19/300 -- Iteration 18072 - Batch 738/963 - Train loss: 0.32706266  - Train acc: 0.8764 - Val loss: 1.14314198\n",
      "(3.69 min) Epoch 19/300 -- Iteration 18081 - Batch 747/963 - Train loss: 0.32718723  - Train acc: 0.8763 - Val loss: 1.14314198\n",
      "(3.70 min) Epoch 19/300 -- Iteration 18090 - Batch 756/963 - Train loss: 0.32722705  - Train acc: 0.8763 - Val loss: 1.14314198\n",
      "(3.70 min) Epoch 19/300 -- Iteration 18099 - Batch 765/963 - Train loss: 0.32744939  - Train acc: 0.8761 - Val loss: 1.14314198\n",
      "(3.70 min) Epoch 19/300 -- Iteration 18108 - Batch 774/963 - Train loss: 0.32744168  - Train acc: 0.8761 - Val loss: 1.14314198\n",
      "(3.70 min) Epoch 19/300 -- Iteration 18117 - Batch 783/963 - Train loss: 0.32749146  - Train acc: 0.8761 - Val loss: 1.14314198\n",
      "(3.70 min) Epoch 19/300 -- Iteration 18126 - Batch 792/963 - Train loss: 0.32743822  - Train acc: 0.8761 - Val loss: 1.14314198\n",
      "(3.70 min) Epoch 19/300 -- Iteration 18135 - Batch 801/963 - Train loss: 0.32761982  - Train acc: 0.8761 - Val loss: 1.14314198\n",
      "(3.71 min) Epoch 19/300 -- Iteration 18144 - Batch 810/963 - Train loss: 0.32787268  - Train acc: 0.8759 - Val loss: 1.14314198\n",
      "(3.71 min) Epoch 19/300 -- Iteration 18153 - Batch 819/963 - Train loss: 0.32838490  - Train acc: 0.8756 - Val loss: 1.14314198\n",
      "(3.71 min) Epoch 19/300 -- Iteration 18162 - Batch 828/963 - Train loss: 0.32823638  - Train acc: 0.8757 - Val loss: 1.14314198\n",
      "(3.71 min) Epoch 19/300 -- Iteration 18171 - Batch 837/963 - Train loss: 0.32841254  - Train acc: 0.8756 - Val loss: 1.14314198\n",
      "(3.71 min) Epoch 19/300 -- Iteration 18180 - Batch 846/963 - Train loss: 0.32824958  - Train acc: 0.8757 - Val loss: 1.14314198\n",
      "(3.72 min) Epoch 19/300 -- Iteration 18189 - Batch 855/963 - Train loss: 0.32815424  - Train acc: 0.8757 - Val loss: 1.14314198\n",
      "(3.72 min) Epoch 19/300 -- Iteration 18198 - Batch 864/963 - Train loss: 0.32800846  - Train acc: 0.8759 - Val loss: 1.14314198\n",
      "(3.72 min) Epoch 19/300 -- Iteration 18207 - Batch 873/963 - Train loss: 0.32855197  - Train acc: 0.8756 - Val loss: 1.14314198\n",
      "(3.72 min) Epoch 19/300 -- Iteration 18216 - Batch 882/963 - Train loss: 0.32852831  - Train acc: 0.8756 - Val loss: 1.14314198\n",
      "(3.72 min) Epoch 19/300 -- Iteration 18225 - Batch 891/963 - Train loss: 0.32831056  - Train acc: 0.8757 - Val loss: 1.14314198\n",
      "(3.72 min) Epoch 19/300 -- Iteration 18234 - Batch 900/963 - Train loss: 0.32824613  - Train acc: 0.8757 - Val loss: 1.14314198\n",
      "(3.73 min) Epoch 19/300 -- Iteration 18243 - Batch 909/963 - Train loss: 0.32831694  - Train acc: 0.8758 - Val loss: 1.14314198\n",
      "(3.73 min) Epoch 19/300 -- Iteration 18252 - Batch 918/963 - Train loss: 0.32834698  - Train acc: 0.8757 - Val loss: 1.14314198\n",
      "(3.73 min) Epoch 19/300 -- Iteration 18261 - Batch 927/963 - Train loss: 0.32832064  - Train acc: 0.8758 - Val loss: 1.14314198\n",
      "(3.73 min) Epoch 19/300 -- Iteration 18270 - Batch 936/963 - Train loss: 0.32839930  - Train acc: 0.8758 - Val loss: 1.14314198\n",
      "(3.73 min) Epoch 19/300 -- Iteration 18279 - Batch 945/963 - Train loss: 0.32839254  - Train acc: 0.8757 - Val loss: 1.14314198\n",
      "(3.74 min) Epoch 19/300 -- Iteration 18288 - Batch 954/963 - Train loss: 0.32858046  - Train acc: 0.8757 - Val loss: 1.14314198\n",
      "(3.74 min) Epoch 19/300 -- Iteration 18297 - Batch 962/963 - Train loss: 0.32880557  - Train acc: 0.8756 - Val loss: 1.12439883 - Val acc: 0.6083\n",
      "(3.74 min) Epoch 20/300 -- Iteration 18306 - Batch 9/963 - Train loss: 0.31078208  - Train acc: 0.8812 - Val loss: 1.12439883\n",
      "(3.74 min) Epoch 20/300 -- Iteration 18315 - Batch 18/963 - Train loss: 0.32601247  - Train acc: 0.8787 - Val loss: 1.12439883\n",
      "(3.74 min) Epoch 20/300 -- Iteration 18324 - Batch 27/963 - Train loss: 0.32215484  - Train acc: 0.8772 - Val loss: 1.12439883\n",
      "(3.75 min) Epoch 20/300 -- Iteration 18333 - Batch 36/963 - Train loss: 0.32584828  - Train acc: 0.8765 - Val loss: 1.12439883\n",
      "(3.75 min) Epoch 20/300 -- Iteration 18342 - Batch 45/963 - Train loss: 0.32439528  - Train acc: 0.8782 - Val loss: 1.12439883\n",
      "(3.75 min) Epoch 20/300 -- Iteration 18351 - Batch 54/963 - Train loss: 0.32924748  - Train acc: 0.8744 - Val loss: 1.12439883\n",
      "(3.75 min) Epoch 20/300 -- Iteration 18360 - Batch 63/963 - Train loss: 0.32866533  - Train acc: 0.8757 - Val loss: 1.12439883\n",
      "(3.75 min) Epoch 20/300 -- Iteration 18369 - Batch 72/963 - Train loss: 0.32969366  - Train acc: 0.8747 - Val loss: 1.12439883\n",
      "(3.75 min) Epoch 20/300 -- Iteration 18378 - Batch 81/963 - Train loss: 0.32889257  - Train acc: 0.8746 - Val loss: 1.12439883\n",
      "(3.76 min) Epoch 20/300 -- Iteration 18387 - Batch 90/963 - Train loss: 0.33050016  - Train acc: 0.8745 - Val loss: 1.12439883\n",
      "(3.76 min) Epoch 20/300 -- Iteration 18396 - Batch 99/963 - Train loss: 0.33158887  - Train acc: 0.8738 - Val loss: 1.12439883\n",
      "(3.76 min) Epoch 20/300 -- Iteration 18405 - Batch 108/963 - Train loss: 0.33267832  - Train acc: 0.8732 - Val loss: 1.12439883\n",
      "(3.76 min) Epoch 20/300 -- Iteration 18414 - Batch 117/963 - Train loss: 0.33095541  - Train acc: 0.8743 - Val loss: 1.12439883\n",
      "(3.76 min) Epoch 20/300 -- Iteration 18423 - Batch 126/963 - Train loss: 0.33057773  - Train acc: 0.8744 - Val loss: 1.12439883\n",
      "(3.77 min) Epoch 20/300 -- Iteration 18432 - Batch 135/963 - Train loss: 0.33003553  - Train acc: 0.8745 - Val loss: 1.12439883\n",
      "(3.77 min) Epoch 20/300 -- Iteration 18441 - Batch 144/963 - Train loss: 0.33199173  - Train acc: 0.8731 - Val loss: 1.12439883\n",
      "(3.77 min) Epoch 20/300 -- Iteration 18450 - Batch 153/963 - Train loss: 0.33054472  - Train acc: 0.8734 - Val loss: 1.12439883\n",
      "(3.77 min) Epoch 20/300 -- Iteration 18459 - Batch 162/963 - Train loss: 0.32877351  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.77 min) Epoch 20/300 -- Iteration 18468 - Batch 171/963 - Train loss: 0.32864076  - Train acc: 0.8750 - Val loss: 1.12439883\n",
      "(3.77 min) Epoch 20/300 -- Iteration 18477 - Batch 180/963 - Train loss: 0.32779795  - Train acc: 0.8758 - Val loss: 1.12439883\n",
      "(3.78 min) Epoch 20/300 -- Iteration 18486 - Batch 189/963 - Train loss: 0.32756811  - Train acc: 0.8759 - Val loss: 1.12439883\n",
      "(3.78 min) Epoch 20/300 -- Iteration 18495 - Batch 198/963 - Train loss: 0.32764149  - Train acc: 0.8759 - Val loss: 1.12439883\n",
      "(3.78 min) Epoch 20/300 -- Iteration 18504 - Batch 207/963 - Train loss: 0.32721531  - Train acc: 0.8757 - Val loss: 1.12439883\n",
      "(3.78 min) Epoch 20/300 -- Iteration 18513 - Batch 216/963 - Train loss: 0.32723976  - Train acc: 0.8754 - Val loss: 1.12439883\n",
      "(3.78 min) Epoch 20/300 -- Iteration 18522 - Batch 225/963 - Train loss: 0.32684232  - Train acc: 0.8756 - Val loss: 1.12439883\n",
      "(3.79 min) Epoch 20/300 -- Iteration 18531 - Batch 234/963 - Train loss: 0.32644764  - Train acc: 0.8757 - Val loss: 1.12439883\n",
      "(3.79 min) Epoch 20/300 -- Iteration 18540 - Batch 243/963 - Train loss: 0.32599978  - Train acc: 0.8759 - Val loss: 1.12439883\n",
      "(3.79 min) Epoch 20/300 -- Iteration 18549 - Batch 252/963 - Train loss: 0.32665097  - Train acc: 0.8757 - Val loss: 1.12439883\n",
      "(3.79 min) Epoch 20/300 -- Iteration 18558 - Batch 261/963 - Train loss: 0.32667948  - Train acc: 0.8757 - Val loss: 1.12439883\n",
      "(3.79 min) Epoch 20/300 -- Iteration 18567 - Batch 270/963 - Train loss: 0.32631228  - Train acc: 0.8758 - Val loss: 1.12439883\n",
      "(3.79 min) Epoch 20/300 -- Iteration 18576 - Batch 279/963 - Train loss: 0.32548887  - Train acc: 0.8760 - Val loss: 1.12439883\n",
      "(3.80 min) Epoch 20/300 -- Iteration 18585 - Batch 288/963 - Train loss: 0.32568567  - Train acc: 0.8759 - Val loss: 1.12439883\n",
      "(3.80 min) Epoch 20/300 -- Iteration 18594 - Batch 297/963 - Train loss: 0.32688096  - Train acc: 0.8755 - Val loss: 1.12439883\n",
      "(3.80 min) Epoch 20/300 -- Iteration 18603 - Batch 306/963 - Train loss: 0.32718284  - Train acc: 0.8754 - Val loss: 1.12439883\n",
      "(3.80 min) Epoch 20/300 -- Iteration 18612 - Batch 315/963 - Train loss: 0.32680678  - Train acc: 0.8752 - Val loss: 1.12439883\n",
      "(3.80 min) Epoch 20/300 -- Iteration 18621 - Batch 324/963 - Train loss: 0.32785028  - Train acc: 0.8747 - Val loss: 1.12439883\n",
      "(3.81 min) Epoch 20/300 -- Iteration 18630 - Batch 333/963 - Train loss: 0.32836065  - Train acc: 0.8747 - Val loss: 1.12439883\n",
      "(3.81 min) Epoch 20/300 -- Iteration 18639 - Batch 342/963 - Train loss: 0.32921072  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.81 min) Epoch 20/300 -- Iteration 18648 - Batch 351/963 - Train loss: 0.32945672  - Train acc: 0.8741 - Val loss: 1.12439883\n",
      "(3.81 min) Epoch 20/300 -- Iteration 18657 - Batch 360/963 - Train loss: 0.32865831  - Train acc: 0.8744 - Val loss: 1.12439883\n",
      "(3.81 min) Epoch 20/300 -- Iteration 18666 - Batch 369/963 - Train loss: 0.32762458  - Train acc: 0.8748 - Val loss: 1.12439883\n",
      "(3.81 min) Epoch 20/300 -- Iteration 18675 - Batch 378/963 - Train loss: 0.32788546  - Train acc: 0.8746 - Val loss: 1.12439883\n",
      "(3.82 min) Epoch 20/300 -- Iteration 18684 - Batch 387/963 - Train loss: 0.32853178  - Train acc: 0.8744 - Val loss: 1.12439883\n",
      "(3.82 min) Epoch 20/300 -- Iteration 18693 - Batch 396/963 - Train loss: 0.32887638  - Train acc: 0.8740 - Val loss: 1.12439883\n",
      "(3.82 min) Epoch 20/300 -- Iteration 18702 - Batch 405/963 - Train loss: 0.32889100  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.82 min) Epoch 20/300 -- Iteration 18711 - Batch 414/963 - Train loss: 0.32885695  - Train acc: 0.8736 - Val loss: 1.12439883\n",
      "(3.82 min) Epoch 20/300 -- Iteration 18720 - Batch 423/963 - Train loss: 0.32866187  - Train acc: 0.8735 - Val loss: 1.12439883\n",
      "(3.83 min) Epoch 20/300 -- Iteration 18729 - Batch 432/963 - Train loss: 0.32836577  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.83 min) Epoch 20/300 -- Iteration 18738 - Batch 441/963 - Train loss: 0.32856928  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.83 min) Epoch 20/300 -- Iteration 18747 - Batch 450/963 - Train loss: 0.32865171  - Train acc: 0.8736 - Val loss: 1.12439883\n",
      "(3.83 min) Epoch 20/300 -- Iteration 18756 - Batch 459/963 - Train loss: 0.32891820  - Train acc: 0.8735 - Val loss: 1.12439883\n",
      "(3.83 min) Epoch 20/300 -- Iteration 18765 - Batch 468/963 - Train loss: 0.32937674  - Train acc: 0.8734 - Val loss: 1.12439883\n",
      "(3.83 min) Epoch 20/300 -- Iteration 18774 - Batch 477/963 - Train loss: 0.32927854  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.84 min) Epoch 20/300 -- Iteration 18783 - Batch 486/963 - Train loss: 0.32935556  - Train acc: 0.8735 - Val loss: 1.12439883\n",
      "(3.84 min) Epoch 20/300 -- Iteration 18792 - Batch 495/963 - Train loss: 0.32913317  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.84 min) Epoch 20/300 -- Iteration 18801 - Batch 504/963 - Train loss: 0.32873335  - Train acc: 0.8739 - Val loss: 1.12439883\n",
      "(3.84 min) Epoch 20/300 -- Iteration 18810 - Batch 513/963 - Train loss: 0.32848009  - Train acc: 0.8740 - Val loss: 1.12439883\n",
      "(3.84 min) Epoch 20/300 -- Iteration 18819 - Batch 522/963 - Train loss: 0.32850767  - Train acc: 0.8740 - Val loss: 1.12439883\n",
      "(3.85 min) Epoch 20/300 -- Iteration 18828 - Batch 531/963 - Train loss: 0.32848896  - Train acc: 0.8740 - Val loss: 1.12439883\n",
      "(3.85 min) Epoch 20/300 -- Iteration 18837 - Batch 540/963 - Train loss: 0.32851061  - Train acc: 0.8740 - Val loss: 1.12439883\n",
      "(3.85 min) Epoch 20/300 -- Iteration 18846 - Batch 549/963 - Train loss: 0.32825369  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.85 min) Epoch 20/300 -- Iteration 18855 - Batch 558/963 - Train loss: 0.32845991  - Train acc: 0.8741 - Val loss: 1.12439883\n",
      "(3.85 min) Epoch 20/300 -- Iteration 18864 - Batch 567/963 - Train loss: 0.32821986  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.85 min) Epoch 20/300 -- Iteration 18873 - Batch 576/963 - Train loss: 0.32849057  - Train acc: 0.8741 - Val loss: 1.12439883\n",
      "(3.86 min) Epoch 20/300 -- Iteration 18882 - Batch 585/963 - Train loss: 0.32835757  - Train acc: 0.8741 - Val loss: 1.12439883\n",
      "(3.86 min) Epoch 20/300 -- Iteration 18891 - Batch 594/963 - Train loss: 0.32862968  - Train acc: 0.8740 - Val loss: 1.12439883\n",
      "(3.86 min) Epoch 20/300 -- Iteration 18900 - Batch 603/963 - Train loss: 0.32885895  - Train acc: 0.8738 - Val loss: 1.12439883\n",
      "(3.86 min) Epoch 20/300 -- Iteration 18909 - Batch 612/963 - Train loss: 0.32904437  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.86 min) Epoch 20/300 -- Iteration 18918 - Batch 621/963 - Train loss: 0.32933718  - Train acc: 0.8736 - Val loss: 1.12439883\n",
      "(3.87 min) Epoch 20/300 -- Iteration 18927 - Batch 630/963 - Train loss: 0.32920251  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.87 min) Epoch 20/300 -- Iteration 18936 - Batch 639/963 - Train loss: 0.32918366  - Train acc: 0.8736 - Val loss: 1.12439883\n",
      "(3.87 min) Epoch 20/300 -- Iteration 18945 - Batch 648/963 - Train loss: 0.32967776  - Train acc: 0.8733 - Val loss: 1.12439883\n",
      "(3.87 min) Epoch 20/300 -- Iteration 18954 - Batch 657/963 - Train loss: 0.32942986  - Train acc: 0.8734 - Val loss: 1.12439883\n",
      "(3.87 min) Epoch 20/300 -- Iteration 18963 - Batch 666/963 - Train loss: 0.33017644  - Train acc: 0.8731 - Val loss: 1.12439883\n",
      "(3.87 min) Epoch 20/300 -- Iteration 18972 - Batch 675/963 - Train loss: 0.32973179  - Train acc: 0.8733 - Val loss: 1.12439883\n",
      "(3.88 min) Epoch 20/300 -- Iteration 18981 - Batch 684/963 - Train loss: 0.32981899  - Train acc: 0.8733 - Val loss: 1.12439883\n",
      "(3.88 min) Epoch 20/300 -- Iteration 18990 - Batch 693/963 - Train loss: 0.32980680  - Train acc: 0.8734 - Val loss: 1.12439883\n",
      "(3.88 min) Epoch 20/300 -- Iteration 18999 - Batch 702/963 - Train loss: 0.32994871  - Train acc: 0.8733 - Val loss: 1.12439883\n",
      "(3.88 min) Epoch 20/300 -- Iteration 19008 - Batch 711/963 - Train loss: 0.32975158  - Train acc: 0.8735 - Val loss: 1.12439883\n",
      "(3.88 min) Epoch 20/300 -- Iteration 19017 - Batch 720/963 - Train loss: 0.32962586  - Train acc: 0.8735 - Val loss: 1.12439883\n",
      "(3.89 min) Epoch 20/300 -- Iteration 19026 - Batch 729/963 - Train loss: 0.32912623  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.89 min) Epoch 20/300 -- Iteration 19035 - Batch 738/963 - Train loss: 0.32901487  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.89 min) Epoch 20/300 -- Iteration 19044 - Batch 747/963 - Train loss: 0.32910561  - Train acc: 0.8736 - Val loss: 1.12439883\n",
      "(3.89 min) Epoch 20/300 -- Iteration 19053 - Batch 756/963 - Train loss: 0.32896522  - Train acc: 0.8737 - Val loss: 1.12439883\n",
      "(3.89 min) Epoch 20/300 -- Iteration 19062 - Batch 765/963 - Train loss: 0.32936510  - Train acc: 0.8734 - Val loss: 1.12439883\n",
      "(3.89 min) Epoch 20/300 -- Iteration 19071 - Batch 774/963 - Train loss: 0.32953070  - Train acc: 0.8734 - Val loss: 1.12439883\n",
      "(3.90 min) Epoch 20/300 -- Iteration 19080 - Batch 783/963 - Train loss: 0.32931738  - Train acc: 0.8736 - Val loss: 1.12439883\n",
      "(3.90 min) Epoch 20/300 -- Iteration 19089 - Batch 792/963 - Train loss: 0.32892438  - Train acc: 0.8738 - Val loss: 1.12439883\n",
      "(3.90 min) Epoch 20/300 -- Iteration 19098 - Batch 801/963 - Train loss: 0.32858547  - Train acc: 0.8739 - Val loss: 1.12439883\n",
      "(3.90 min) Epoch 20/300 -- Iteration 19107 - Batch 810/963 - Train loss: 0.32849637  - Train acc: 0.8739 - Val loss: 1.12439883\n",
      "(3.90 min) Epoch 20/300 -- Iteration 19116 - Batch 819/963 - Train loss: 0.32850306  - Train acc: 0.8739 - Val loss: 1.12439883\n",
      "(3.91 min) Epoch 20/300 -- Iteration 19125 - Batch 828/963 - Train loss: 0.32817143  - Train acc: 0.8741 - Val loss: 1.12439883\n",
      "(3.91 min) Epoch 20/300 -- Iteration 19134 - Batch 837/963 - Train loss: 0.32821433  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.91 min) Epoch 20/300 -- Iteration 19143 - Batch 846/963 - Train loss: 0.32823735  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.91 min) Epoch 20/300 -- Iteration 19152 - Batch 855/963 - Train loss: 0.32849120  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.91 min) Epoch 20/300 -- Iteration 19161 - Batch 864/963 - Train loss: 0.32850344  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.91 min) Epoch 20/300 -- Iteration 19170 - Batch 873/963 - Train loss: 0.32809113  - Train acc: 0.8743 - Val loss: 1.12439883\n",
      "(3.92 min) Epoch 20/300 -- Iteration 19179 - Batch 882/963 - Train loss: 0.32825605  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.92 min) Epoch 20/300 -- Iteration 19188 - Batch 891/963 - Train loss: 0.32834896  - Train acc: 0.8742 - Val loss: 1.12439883\n",
      "(3.92 min) Epoch 20/300 -- Iteration 19197 - Batch 900/963 - Train loss: 0.32829756  - Train acc: 0.8743 - Val loss: 1.12439883\n",
      "(3.92 min) Epoch 20/300 -- Iteration 19206 - Batch 909/963 - Train loss: 0.32804091  - Train acc: 0.8744 - Val loss: 1.12439883\n",
      "(3.92 min) Epoch 20/300 -- Iteration 19215 - Batch 918/963 - Train loss: 0.32829902  - Train acc: 0.8744 - Val loss: 1.12439883\n",
      "(3.93 min) Epoch 20/300 -- Iteration 19224 - Batch 927/963 - Train loss: 0.32854149  - Train acc: 0.8743 - Val loss: 1.12439883\n",
      "(3.93 min) Epoch 20/300 -- Iteration 19233 - Batch 936/963 - Train loss: 0.32844331  - Train acc: 0.8743 - Val loss: 1.12439883\n",
      "(3.93 min) Epoch 20/300 -- Iteration 19242 - Batch 945/963 - Train loss: 0.32841277  - Train acc: 0.8744 - Val loss: 1.12439883\n",
      "(3.93 min) Epoch 20/300 -- Iteration 19251 - Batch 954/963 - Train loss: 0.32856582  - Train acc: 0.8743 - Val loss: 1.12439883\n",
      "(3.93 min) Epoch 20/300 -- Iteration 19260 - Batch 962/963 - Train loss: 0.32865019  - Train acc: 0.8743 - Val loss: 1.09649372 - Val acc: 0.6100\n",
      "(3.94 min) Epoch 21/300 -- Iteration 19269 - Batch 9/963 - Train loss: 0.34595152  - Train acc: 0.8734 - Val loss: 1.09649372\n",
      "(3.94 min) Epoch 21/300 -- Iteration 19278 - Batch 18/963 - Train loss: 0.34437026  - Train acc: 0.8742 - Val loss: 1.09649372\n",
      "(3.94 min) Epoch 21/300 -- Iteration 19287 - Batch 27/963 - Train loss: 0.34514091  - Train acc: 0.8744 - Val loss: 1.09649372\n",
      "(3.94 min) Epoch 21/300 -- Iteration 19296 - Batch 36/963 - Train loss: 0.34007693  - Train acc: 0.8742 - Val loss: 1.09649372\n",
      "(3.94 min) Epoch 21/300 -- Iteration 19305 - Batch 45/963 - Train loss: 0.33879173  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(3.94 min) Epoch 21/300 -- Iteration 19314 - Batch 54/963 - Train loss: 0.33831833  - Train acc: 0.8734 - Val loss: 1.09649372\n",
      "(3.95 min) Epoch 21/300 -- Iteration 19323 - Batch 63/963 - Train loss: 0.33555646  - Train acc: 0.8732 - Val loss: 1.09649372\n",
      "(3.95 min) Epoch 21/300 -- Iteration 19332 - Batch 72/963 - Train loss: 0.33713443  - Train acc: 0.8735 - Val loss: 1.09649372\n",
      "(3.95 min) Epoch 21/300 -- Iteration 19341 - Batch 81/963 - Train loss: 0.33710678  - Train acc: 0.8728 - Val loss: 1.09649372\n",
      "(3.95 min) Epoch 21/300 -- Iteration 19350 - Batch 90/963 - Train loss: 0.33541970  - Train acc: 0.8735 - Val loss: 1.09649372\n",
      "(3.95 min) Epoch 21/300 -- Iteration 19359 - Batch 99/963 - Train loss: 0.33298383  - Train acc: 0.8740 - Val loss: 1.09649372\n",
      "(3.96 min) Epoch 21/300 -- Iteration 19368 - Batch 108/963 - Train loss: 0.33440752  - Train acc: 0.8739 - Val loss: 1.09649372\n",
      "(3.96 min) Epoch 21/300 -- Iteration 19377 - Batch 117/963 - Train loss: 0.33435785  - Train acc: 0.8734 - Val loss: 1.09649372\n",
      "(3.96 min) Epoch 21/300 -- Iteration 19386 - Batch 126/963 - Train loss: 0.33244778  - Train acc: 0.8739 - Val loss: 1.09649372\n",
      "(3.96 min) Epoch 21/300 -- Iteration 19395 - Batch 135/963 - Train loss: 0.33085542  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(3.96 min) Epoch 21/300 -- Iteration 19404 - Batch 144/963 - Train loss: 0.33097996  - Train acc: 0.8741 - Val loss: 1.09649372\n",
      "(3.97 min) Epoch 21/300 -- Iteration 19413 - Batch 153/963 - Train loss: 0.32965194  - Train acc: 0.8746 - Val loss: 1.09649372\n",
      "(3.97 min) Epoch 21/300 -- Iteration 19422 - Batch 162/963 - Train loss: 0.33058582  - Train acc: 0.8744 - Val loss: 1.09649372\n",
      "(3.97 min) Epoch 21/300 -- Iteration 19431 - Batch 171/963 - Train loss: 0.32968959  - Train acc: 0.8746 - Val loss: 1.09649372\n",
      "(3.97 min) Epoch 21/300 -- Iteration 19440 - Batch 180/963 - Train loss: 0.33031228  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(3.97 min) Epoch 21/300 -- Iteration 19449 - Batch 189/963 - Train loss: 0.33064033  - Train acc: 0.8741 - Val loss: 1.09649372\n",
      "(3.97 min) Epoch 21/300 -- Iteration 19458 - Batch 198/963 - Train loss: 0.33111293  - Train acc: 0.8742 - Val loss: 1.09649372\n",
      "(3.98 min) Epoch 21/300 -- Iteration 19467 - Batch 207/963 - Train loss: 0.33030980  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(3.98 min) Epoch 21/300 -- Iteration 19476 - Batch 216/963 - Train loss: 0.33037288  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(3.98 min) Epoch 21/300 -- Iteration 19485 - Batch 225/963 - Train loss: 0.33141747  - Train acc: 0.8739 - Val loss: 1.09649372\n",
      "(3.98 min) Epoch 21/300 -- Iteration 19494 - Batch 234/963 - Train loss: 0.33166792  - Train acc: 0.8737 - Val loss: 1.09649372\n",
      "(3.98 min) Epoch 21/300 -- Iteration 19503 - Batch 243/963 - Train loss: 0.33100884  - Train acc: 0.8742 - Val loss: 1.09649372\n",
      "(3.98 min) Epoch 21/300 -- Iteration 19512 - Batch 252/963 - Train loss: 0.33028907  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(3.99 min) Epoch 21/300 -- Iteration 19521 - Batch 261/963 - Train loss: 0.33047658  - Train acc: 0.8743 - Val loss: 1.09649372\n",
      "(3.99 min) Epoch 21/300 -- Iteration 19530 - Batch 270/963 - Train loss: 0.33040898  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(3.99 min) Epoch 21/300 -- Iteration 19539 - Batch 279/963 - Train loss: 0.33117427  - Train acc: 0.8739 - Val loss: 1.09649372\n",
      "(3.99 min) Epoch 21/300 -- Iteration 19548 - Batch 288/963 - Train loss: 0.33108630  - Train acc: 0.8741 - Val loss: 1.09649372\n",
      "(3.99 min) Epoch 21/300 -- Iteration 19557 - Batch 297/963 - Train loss: 0.33155010  - Train acc: 0.8743 - Val loss: 1.09649372\n",
      "(4.00 min) Epoch 21/300 -- Iteration 19566 - Batch 306/963 - Train loss: 0.33151577  - Train acc: 0.8743 - Val loss: 1.09649372\n",
      "(4.00 min) Epoch 21/300 -- Iteration 19575 - Batch 315/963 - Train loss: 0.33153801  - Train acc: 0.8739 - Val loss: 1.09649372\n",
      "(4.00 min) Epoch 21/300 -- Iteration 19584 - Batch 324/963 - Train loss: 0.33127591  - Train acc: 0.8741 - Val loss: 1.09649372\n",
      "(4.00 min) Epoch 21/300 -- Iteration 19593 - Batch 333/963 - Train loss: 0.33041178  - Train acc: 0.8749 - Val loss: 1.09649372\n",
      "(4.00 min) Epoch 21/300 -- Iteration 19602 - Batch 342/963 - Train loss: 0.33002719  - Train acc: 0.8749 - Val loss: 1.09649372\n",
      "(4.01 min) Epoch 21/300 -- Iteration 19611 - Batch 351/963 - Train loss: 0.33029011  - Train acc: 0.8748 - Val loss: 1.09649372\n",
      "(4.01 min) Epoch 21/300 -- Iteration 19620 - Batch 360/963 - Train loss: 0.32983426  - Train acc: 0.8750 - Val loss: 1.09649372\n",
      "(4.01 min) Epoch 21/300 -- Iteration 19629 - Batch 369/963 - Train loss: 0.33090876  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(4.01 min) Epoch 21/300 -- Iteration 19638 - Batch 378/963 - Train loss: 0.33065405  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(4.01 min) Epoch 21/300 -- Iteration 19647 - Batch 387/963 - Train loss: 0.33062182  - Train acc: 0.8745 - Val loss: 1.09649372\n",
      "(4.01 min) Epoch 21/300 -- Iteration 19656 - Batch 396/963 - Train loss: 0.33135251  - Train acc: 0.8741 - Val loss: 1.09649372\n",
      "(4.02 min) Epoch 21/300 -- Iteration 19665 - Batch 405/963 - Train loss: 0.33120644  - Train acc: 0.8743 - Val loss: 1.09649372\n",
      "(4.02 min) Epoch 21/300 -- Iteration 19674 - Batch 414/963 - Train loss: 0.33099107  - Train acc: 0.8743 - Val loss: 1.09649372\n",
      "(4.02 min) Epoch 21/300 -- Iteration 19683 - Batch 423/963 - Train loss: 0.33037055  - Train acc: 0.8747 - Val loss: 1.09649372\n",
      "(4.02 min) Epoch 21/300 -- Iteration 19692 - Batch 432/963 - Train loss: 0.33038706  - Train acc: 0.8746 - Val loss: 1.09649372\n",
      "(4.02 min) Epoch 21/300 -- Iteration 19701 - Batch 441/963 - Train loss: 0.33013322  - Train acc: 0.8749 - Val loss: 1.09649372\n",
      "(4.02 min) Epoch 21/300 -- Iteration 19710 - Batch 450/963 - Train loss: 0.33017191  - Train acc: 0.8747 - Val loss: 1.09649372\n",
      "(4.03 min) Epoch 21/300 -- Iteration 19719 - Batch 459/963 - Train loss: 0.32978737  - Train acc: 0.8747 - Val loss: 1.09649372\n",
      "(4.03 min) Epoch 21/300 -- Iteration 19728 - Batch 468/963 - Train loss: 0.32975967  - Train acc: 0.8748 - Val loss: 1.09649372\n",
      "(4.03 min) Epoch 21/300 -- Iteration 19737 - Batch 477/963 - Train loss: 0.32937598  - Train acc: 0.8750 - Val loss: 1.09649372\n",
      "(4.03 min) Epoch 21/300 -- Iteration 19746 - Batch 486/963 - Train loss: 0.32953577  - Train acc: 0.8748 - Val loss: 1.09649372\n",
      "(4.03 min) Epoch 21/300 -- Iteration 19755 - Batch 495/963 - Train loss: 0.32922036  - Train acc: 0.8748 - Val loss: 1.09649372\n",
      "(4.04 min) Epoch 21/300 -- Iteration 19764 - Batch 504/963 - Train loss: 0.32875209  - Train acc: 0.8750 - Val loss: 1.09649372\n",
      "(4.04 min) Epoch 21/300 -- Iteration 19773 - Batch 513/963 - Train loss: 0.32889845  - Train acc: 0.8749 - Val loss: 1.09649372\n",
      "(4.04 min) Epoch 21/300 -- Iteration 19782 - Batch 522/963 - Train loss: 0.32858126  - Train acc: 0.8750 - Val loss: 1.09649372\n",
      "(4.04 min) Epoch 21/300 -- Iteration 19791 - Batch 531/963 - Train loss: 0.32800823  - Train acc: 0.8753 - Val loss: 1.09649372\n",
      "(4.04 min) Epoch 21/300 -- Iteration 19800 - Batch 540/963 - Train loss: 0.32770191  - Train acc: 0.8754 - Val loss: 1.09649372\n",
      "(4.04 min) Epoch 21/300 -- Iteration 19809 - Batch 549/963 - Train loss: 0.32747291  - Train acc: 0.8753 - Val loss: 1.09649372\n",
      "(4.05 min) Epoch 21/300 -- Iteration 19818 - Batch 558/963 - Train loss: 0.32758682  - Train acc: 0.8753 - Val loss: 1.09649372\n",
      "(4.05 min) Epoch 21/300 -- Iteration 19827 - Batch 567/963 - Train loss: 0.32771051  - Train acc: 0.8754 - Val loss: 1.09649372\n",
      "(4.05 min) Epoch 21/300 -- Iteration 19836 - Batch 576/963 - Train loss: 0.32786829  - Train acc: 0.8754 - Val loss: 1.09649372\n",
      "(4.05 min) Epoch 21/300 -- Iteration 19845 - Batch 585/963 - Train loss: 0.32793436  - Train acc: 0.8755 - Val loss: 1.09649372\n",
      "(4.05 min) Epoch 21/300 -- Iteration 19854 - Batch 594/963 - Train loss: 0.32792294  - Train acc: 0.8756 - Val loss: 1.09649372\n",
      "(4.06 min) Epoch 21/300 -- Iteration 19863 - Batch 603/963 - Train loss: 0.32794168  - Train acc: 0.8756 - Val loss: 1.09649372\n",
      "(4.06 min) Epoch 21/300 -- Iteration 19872 - Batch 612/963 - Train loss: 0.32773984  - Train acc: 0.8757 - Val loss: 1.09649372\n",
      "(4.06 min) Epoch 21/300 -- Iteration 19881 - Batch 621/963 - Train loss: 0.32735510  - Train acc: 0.8759 - Val loss: 1.09649372\n",
      "(4.06 min) Epoch 21/300 -- Iteration 19890 - Batch 630/963 - Train loss: 0.32765482  - Train acc: 0.8758 - Val loss: 1.09649372\n",
      "(4.06 min) Epoch 21/300 -- Iteration 19899 - Batch 639/963 - Train loss: 0.32762873  - Train acc: 0.8758 - Val loss: 1.09649372\n",
      "(4.07 min) Epoch 21/300 -- Iteration 19908 - Batch 648/963 - Train loss: 0.32763591  - Train acc: 0.8759 - Val loss: 1.09649372\n",
      "(4.07 min) Epoch 21/300 -- Iteration 19917 - Batch 657/963 - Train loss: 0.32775245  - Train acc: 0.8758 - Val loss: 1.09649372\n",
      "(4.07 min) Epoch 21/300 -- Iteration 19926 - Batch 666/963 - Train loss: 0.32790822  - Train acc: 0.8759 - Val loss: 1.09649372\n",
      "(4.07 min) Epoch 21/300 -- Iteration 19935 - Batch 675/963 - Train loss: 0.32789578  - Train acc: 0.8759 - Val loss: 1.09649372\n",
      "(4.07 min) Epoch 21/300 -- Iteration 19944 - Batch 684/963 - Train loss: 0.32833443  - Train acc: 0.8757 - Val loss: 1.09649372\n",
      "(4.07 min) Epoch 21/300 -- Iteration 19953 - Batch 693/963 - Train loss: 0.32842800  - Train acc: 0.8757 - Val loss: 1.09649372\n",
      "(4.08 min) Epoch 21/300 -- Iteration 19962 - Batch 702/963 - Train loss: 0.32866471  - Train acc: 0.8755 - Val loss: 1.09649372\n",
      "(4.08 min) Epoch 21/300 -- Iteration 19971 - Batch 711/963 - Train loss: 0.32838681  - Train acc: 0.8757 - Val loss: 1.09649372\n",
      "(4.08 min) Epoch 21/300 -- Iteration 19980 - Batch 720/963 - Train loss: 0.32859919  - Train acc: 0.8756 - Val loss: 1.09649372\n",
      "(4.08 min) Epoch 21/300 -- Iteration 19989 - Batch 729/963 - Train loss: 0.32900822  - Train acc: 0.8754 - Val loss: 1.09649372\n",
      "(4.08 min) Epoch 21/300 -- Iteration 19998 - Batch 738/963 - Train loss: 0.32876359  - Train acc: 0.8756 - Val loss: 1.09649372\n",
      "(4.09 min) Epoch 21/300 -- Iteration 20007 - Batch 747/963 - Train loss: 0.32871809  - Train acc: 0.8755 - Val loss: 1.09649372\n",
      "(4.09 min) Epoch 21/300 -- Iteration 20016 - Batch 756/963 - Train loss: 0.32811274  - Train acc: 0.8757 - Val loss: 1.09649372\n",
      "(4.09 min) Epoch 21/300 -- Iteration 20025 - Batch 765/963 - Train loss: 0.32810009  - Train acc: 0.8756 - Val loss: 1.09649372\n",
      "(4.09 min) Epoch 21/300 -- Iteration 20034 - Batch 774/963 - Train loss: 0.32816973  - Train acc: 0.8756 - Val loss: 1.09649372\n",
      "(4.09 min) Epoch 21/300 -- Iteration 20043 - Batch 783/963 - Train loss: 0.32808003  - Train acc: 0.8756 - Val loss: 1.09649372\n",
      "(4.09 min) Epoch 21/300 -- Iteration 20052 - Batch 792/963 - Train loss: 0.32805795  - Train acc: 0.8756 - Val loss: 1.09649372\n",
      "(4.10 min) Epoch 21/300 -- Iteration 20061 - Batch 801/963 - Train loss: 0.32830813  - Train acc: 0.8755 - Val loss: 1.09649372\n",
      "(4.10 min) Epoch 21/300 -- Iteration 20070 - Batch 810/963 - Train loss: 0.32818728  - Train acc: 0.8756 - Val loss: 1.09649372\n",
      "(4.10 min) Epoch 21/300 -- Iteration 20079 - Batch 819/963 - Train loss: 0.32798317  - Train acc: 0.8757 - Val loss: 1.09649372\n",
      "(4.10 min) Epoch 21/300 -- Iteration 20088 - Batch 828/963 - Train loss: 0.32816170  - Train acc: 0.8757 - Val loss: 1.09649372\n",
      "(4.10 min) Epoch 21/300 -- Iteration 20097 - Batch 837/963 - Train loss: 0.32814378  - Train acc: 0.8758 - Val loss: 1.09649372\n",
      "(4.11 min) Epoch 21/300 -- Iteration 20106 - Batch 846/963 - Train loss: 0.32797529  - Train acc: 0.8759 - Val loss: 1.09649372\n",
      "(4.11 min) Epoch 21/300 -- Iteration 20115 - Batch 855/963 - Train loss: 0.32837302  - Train acc: 0.8757 - Val loss: 1.09649372\n",
      "(4.11 min) Epoch 21/300 -- Iteration 20124 - Batch 864/963 - Train loss: 0.32864118  - Train acc: 0.8757 - Val loss: 1.09649372\n",
      "(4.11 min) Epoch 21/300 -- Iteration 20133 - Batch 873/963 - Train loss: 0.32854368  - Train acc: 0.8758 - Val loss: 1.09649372\n",
      "(4.11 min) Epoch 21/300 -- Iteration 20142 - Batch 882/963 - Train loss: 0.32824435  - Train acc: 0.8759 - Val loss: 1.09649372\n",
      "(4.11 min) Epoch 21/300 -- Iteration 20151 - Batch 891/963 - Train loss: 0.32837395  - Train acc: 0.8759 - Val loss: 1.09649372\n",
      "(4.12 min) Epoch 21/300 -- Iteration 20160 - Batch 900/963 - Train loss: 0.32803681  - Train acc: 0.8760 - Val loss: 1.09649372\n",
      "(4.12 min) Epoch 21/300 -- Iteration 20169 - Batch 909/963 - Train loss: 0.32798341  - Train acc: 0.8760 - Val loss: 1.09649372\n",
      "(4.12 min) Epoch 21/300 -- Iteration 20178 - Batch 918/963 - Train loss: 0.32782262  - Train acc: 0.8762 - Val loss: 1.09649372\n",
      "(4.12 min) Epoch 21/300 -- Iteration 20187 - Batch 927/963 - Train loss: 0.32800345  - Train acc: 0.8761 - Val loss: 1.09649372\n",
      "(4.12 min) Epoch 21/300 -- Iteration 20196 - Batch 936/963 - Train loss: 0.32799240  - Train acc: 0.8760 - Val loss: 1.09649372\n",
      "(4.13 min) Epoch 21/300 -- Iteration 20205 - Batch 945/963 - Train loss: 0.32803939  - Train acc: 0.8760 - Val loss: 1.09649372\n",
      "(4.13 min) Epoch 21/300 -- Iteration 20214 - Batch 954/963 - Train loss: 0.32798437  - Train acc: 0.8761 - Val loss: 1.09649372\n",
      "(4.13 min) Epoch 21/300 -- Iteration 20223 - Batch 962/963 - Train loss: 0.32792512  - Train acc: 0.8762 - Val loss: 1.09704602 - Val acc: 0.6067\n",
      "(4.13 min) Epoch 22/300 -- Iteration 20232 - Batch 9/963 - Train loss: 0.33770271  - Train acc: 0.8727 - Val loss: 1.09704602\n",
      "(4.13 min) Epoch 22/300 -- Iteration 20241 - Batch 18/963 - Train loss: 0.33685240  - Train acc: 0.8709 - Val loss: 1.09704602\n",
      "(4.14 min) Epoch 22/300 -- Iteration 20250 - Batch 27/963 - Train loss: 0.33509011  - Train acc: 0.8686 - Val loss: 1.09704602\n",
      "(4.14 min) Epoch 22/300 -- Iteration 20259 - Batch 36/963 - Train loss: 0.33017159  - Train acc: 0.8699 - Val loss: 1.09704602\n",
      "(4.14 min) Epoch 22/300 -- Iteration 20268 - Batch 45/963 - Train loss: 0.32401450  - Train acc: 0.8735 - Val loss: 1.09704602\n",
      "(4.14 min) Epoch 22/300 -- Iteration 20277 - Batch 54/963 - Train loss: 0.33169128  - Train acc: 0.8699 - Val loss: 1.09704602\n",
      "(4.14 min) Epoch 22/300 -- Iteration 20286 - Batch 63/963 - Train loss: 0.33213732  - Train acc: 0.8702 - Val loss: 1.09704602\n",
      "(4.14 min) Epoch 22/300 -- Iteration 20295 - Batch 72/963 - Train loss: 0.33376009  - Train acc: 0.8708 - Val loss: 1.09704602\n",
      "(4.15 min) Epoch 22/300 -- Iteration 20304 - Batch 81/963 - Train loss: 0.33330178  - Train acc: 0.8712 - Val loss: 1.09704602\n",
      "(4.15 min) Epoch 22/300 -- Iteration 20313 - Batch 90/963 - Train loss: 0.33518068  - Train acc: 0.8704 - Val loss: 1.09704602\n",
      "(4.15 min) Epoch 22/300 -- Iteration 20322 - Batch 99/963 - Train loss: 0.33616210  - Train acc: 0.8703 - Val loss: 1.09704602\n",
      "(4.15 min) Epoch 22/300 -- Iteration 20331 - Batch 108/963 - Train loss: 0.33616973  - Train acc: 0.8712 - Val loss: 1.09704602\n",
      "(4.15 min) Epoch 22/300 -- Iteration 20340 - Batch 117/963 - Train loss: 0.33800126  - Train acc: 0.8708 - Val loss: 1.09704602\n",
      "(4.16 min) Epoch 22/300 -- Iteration 20349 - Batch 126/963 - Train loss: 0.33695473  - Train acc: 0.8712 - Val loss: 1.09704602\n",
      "(4.16 min) Epoch 22/300 -- Iteration 20358 - Batch 135/963 - Train loss: 0.33675842  - Train acc: 0.8714 - Val loss: 1.09704602\n",
      "(4.16 min) Epoch 22/300 -- Iteration 20367 - Batch 144/963 - Train loss: 0.33642440  - Train acc: 0.8713 - Val loss: 1.09704602\n",
      "(4.16 min) Epoch 22/300 -- Iteration 20376 - Batch 153/963 - Train loss: 0.33561915  - Train acc: 0.8715 - Val loss: 1.09704602\n",
      "(4.16 min) Epoch 22/300 -- Iteration 20385 - Batch 162/963 - Train loss: 0.33512323  - Train acc: 0.8713 - Val loss: 1.09704602\n",
      "(4.16 min) Epoch 22/300 -- Iteration 20394 - Batch 171/963 - Train loss: 0.33666861  - Train acc: 0.8710 - Val loss: 1.09704602\n",
      "(4.17 min) Epoch 22/300 -- Iteration 20403 - Batch 180/963 - Train loss: 0.33476523  - Train acc: 0.8718 - Val loss: 1.09704602\n",
      "(4.17 min) Epoch 22/300 -- Iteration 20412 - Batch 189/963 - Train loss: 0.33546411  - Train acc: 0.8721 - Val loss: 1.09704602\n",
      "(4.17 min) Epoch 22/300 -- Iteration 20421 - Batch 198/963 - Train loss: 0.33637017  - Train acc: 0.8720 - Val loss: 1.09704602\n",
      "(4.17 min) Epoch 22/300 -- Iteration 20430 - Batch 207/963 - Train loss: 0.33514926  - Train acc: 0.8729 - Val loss: 1.09704602\n",
      "(4.17 min) Epoch 22/300 -- Iteration 20439 - Batch 216/963 - Train loss: 0.33418208  - Train acc: 0.8738 - Val loss: 1.09704602\n",
      "(4.18 min) Epoch 22/300 -- Iteration 20448 - Batch 225/963 - Train loss: 0.33438904  - Train acc: 0.8734 - Val loss: 1.09704602\n",
      "(4.18 min) Epoch 22/300 -- Iteration 20457 - Batch 234/963 - Train loss: 0.33384787  - Train acc: 0.8737 - Val loss: 1.09704602\n",
      "(4.18 min) Epoch 22/300 -- Iteration 20466 - Batch 243/963 - Train loss: 0.33360301  - Train acc: 0.8738 - Val loss: 1.09704602\n",
      "(4.18 min) Epoch 22/300 -- Iteration 20475 - Batch 252/963 - Train loss: 0.33354934  - Train acc: 0.8737 - Val loss: 1.09704602\n",
      "(4.18 min) Epoch 22/300 -- Iteration 20484 - Batch 261/963 - Train loss: 0.33374468  - Train acc: 0.8739 - Val loss: 1.09704602\n",
      "(4.18 min) Epoch 22/300 -- Iteration 20493 - Batch 270/963 - Train loss: 0.33386141  - Train acc: 0.8738 - Val loss: 1.09704602\n",
      "(4.19 min) Epoch 22/300 -- Iteration 20502 - Batch 279/963 - Train loss: 0.33233173  - Train acc: 0.8743 - Val loss: 1.09704602\n",
      "(4.19 min) Epoch 22/300 -- Iteration 20511 - Batch 288/963 - Train loss: 0.33152849  - Train acc: 0.8748 - Val loss: 1.09704602\n",
      "(4.19 min) Epoch 22/300 -- Iteration 20520 - Batch 297/963 - Train loss: 0.33220482  - Train acc: 0.8746 - Val loss: 1.09704602\n",
      "(4.19 min) Epoch 22/300 -- Iteration 20529 - Batch 306/963 - Train loss: 0.33203950  - Train acc: 0.8745 - Val loss: 1.09704602\n",
      "(4.19 min) Epoch 22/300 -- Iteration 20538 - Batch 315/963 - Train loss: 0.33270022  - Train acc: 0.8743 - Val loss: 1.09704602\n",
      "(4.20 min) Epoch 22/300 -- Iteration 20547 - Batch 324/963 - Train loss: 0.33215231  - Train acc: 0.8747 - Val loss: 1.09704602\n",
      "(4.20 min) Epoch 22/300 -- Iteration 20556 - Batch 333/963 - Train loss: 0.33211056  - Train acc: 0.8746 - Val loss: 1.09704602\n",
      "(4.20 min) Epoch 22/300 -- Iteration 20565 - Batch 342/963 - Train loss: 0.33182359  - Train acc: 0.8748 - Val loss: 1.09704602\n",
      "(4.20 min) Epoch 22/300 -- Iteration 20574 - Batch 351/963 - Train loss: 0.33123733  - Train acc: 0.8753 - Val loss: 1.09704602\n",
      "(4.20 min) Epoch 22/300 -- Iteration 20583 - Batch 360/963 - Train loss: 0.33059472  - Train acc: 0.8756 - Val loss: 1.09704602\n",
      "(4.20 min) Epoch 22/300 -- Iteration 20592 - Batch 369/963 - Train loss: 0.33056823  - Train acc: 0.8755 - Val loss: 1.09704602\n",
      "(4.21 min) Epoch 22/300 -- Iteration 20601 - Batch 378/963 - Train loss: 0.33091402  - Train acc: 0.8753 - Val loss: 1.09704602\n",
      "(4.21 min) Epoch 22/300 -- Iteration 20610 - Batch 387/963 - Train loss: 0.33124071  - Train acc: 0.8752 - Val loss: 1.09704602\n",
      "(4.21 min) Epoch 22/300 -- Iteration 20619 - Batch 396/963 - Train loss: 0.33068043  - Train acc: 0.8756 - Val loss: 1.09704602\n",
      "(4.21 min) Epoch 22/300 -- Iteration 20628 - Batch 405/963 - Train loss: 0.33058324  - Train acc: 0.8755 - Val loss: 1.09704602\n",
      "(4.21 min) Epoch 22/300 -- Iteration 20637 - Batch 414/963 - Train loss: 0.33105227  - Train acc: 0.8751 - Val loss: 1.09704602\n",
      "(4.22 min) Epoch 22/300 -- Iteration 20646 - Batch 423/963 - Train loss: 0.33177935  - Train acc: 0.8750 - Val loss: 1.09704602\n",
      "(4.22 min) Epoch 22/300 -- Iteration 20655 - Batch 432/963 - Train loss: 0.33103867  - Train acc: 0.8751 - Val loss: 1.09704602\n",
      "(4.22 min) Epoch 22/300 -- Iteration 20664 - Batch 441/963 - Train loss: 0.33161618  - Train acc: 0.8748 - Val loss: 1.09704602\n",
      "(4.22 min) Epoch 22/300 -- Iteration 20673 - Batch 450/963 - Train loss: 0.33142846  - Train acc: 0.8748 - Val loss: 1.09704602\n",
      "(4.22 min) Epoch 22/300 -- Iteration 20682 - Batch 459/963 - Train loss: 0.33065187  - Train acc: 0.8752 - Val loss: 1.09704602\n",
      "(4.22 min) Epoch 22/300 -- Iteration 20691 - Batch 468/963 - Train loss: 0.33061692  - Train acc: 0.8751 - Val loss: 1.09704602\n",
      "(4.23 min) Epoch 22/300 -- Iteration 20700 - Batch 477/963 - Train loss: 0.33055343  - Train acc: 0.8752 - Val loss: 1.09704602\n",
      "(4.23 min) Epoch 22/300 -- Iteration 20709 - Batch 486/963 - Train loss: 0.33050402  - Train acc: 0.8753 - Val loss: 1.09704602\n",
      "(4.23 min) Epoch 22/300 -- Iteration 20718 - Batch 495/963 - Train loss: 0.33001469  - Train acc: 0.8755 - Val loss: 1.09704602\n",
      "(4.23 min) Epoch 22/300 -- Iteration 20727 - Batch 504/963 - Train loss: 0.33032334  - Train acc: 0.8753 - Val loss: 1.09704602\n",
      "(4.23 min) Epoch 22/300 -- Iteration 20736 - Batch 513/963 - Train loss: 0.33040185  - Train acc: 0.8750 - Val loss: 1.09704602\n",
      "(4.24 min) Epoch 22/300 -- Iteration 20745 - Batch 522/963 - Train loss: 0.32968670  - Train acc: 0.8753 - Val loss: 1.09704602\n",
      "(4.24 min) Epoch 22/300 -- Iteration 20754 - Batch 531/963 - Train loss: 0.32924164  - Train acc: 0.8756 - Val loss: 1.09704602\n",
      "(4.24 min) Epoch 22/300 -- Iteration 20763 - Batch 540/963 - Train loss: 0.32875986  - Train acc: 0.8758 - Val loss: 1.09704602\n",
      "(4.24 min) Epoch 22/300 -- Iteration 20772 - Batch 549/963 - Train loss: 0.32886683  - Train acc: 0.8757 - Val loss: 1.09704602\n",
      "(4.24 min) Epoch 22/300 -- Iteration 20781 - Batch 558/963 - Train loss: 0.32881184  - Train acc: 0.8757 - Val loss: 1.09704602\n",
      "(4.24 min) Epoch 22/300 -- Iteration 20790 - Batch 567/963 - Train loss: 0.32876719  - Train acc: 0.8759 - Val loss: 1.09704602\n",
      "(4.25 min) Epoch 22/300 -- Iteration 20799 - Batch 576/963 - Train loss: 0.32802140  - Train acc: 0.8762 - Val loss: 1.09704602\n",
      "(4.25 min) Epoch 22/300 -- Iteration 20808 - Batch 585/963 - Train loss: 0.32736418  - Train acc: 0.8765 - Val loss: 1.09704602\n",
      "(4.25 min) Epoch 22/300 -- Iteration 20817 - Batch 594/963 - Train loss: 0.32702396  - Train acc: 0.8766 - Val loss: 1.09704602\n",
      "(4.25 min) Epoch 22/300 -- Iteration 20826 - Batch 603/963 - Train loss: 0.32726420  - Train acc: 0.8763 - Val loss: 1.09704602\n",
      "(4.25 min) Epoch 22/300 -- Iteration 20835 - Batch 612/963 - Train loss: 0.32703703  - Train acc: 0.8765 - Val loss: 1.09704602\n",
      "(4.26 min) Epoch 22/300 -- Iteration 20844 - Batch 621/963 - Train loss: 0.32741020  - Train acc: 0.8763 - Val loss: 1.09704602\n",
      "(4.26 min) Epoch 22/300 -- Iteration 20853 - Batch 630/963 - Train loss: 0.32756893  - Train acc: 0.8762 - Val loss: 1.09704602\n",
      "(4.26 min) Epoch 22/300 -- Iteration 20862 - Batch 639/963 - Train loss: 0.32772466  - Train acc: 0.8761 - Val loss: 1.09704602\n",
      "(4.26 min) Epoch 22/300 -- Iteration 20871 - Batch 648/963 - Train loss: 0.32738328  - Train acc: 0.8763 - Val loss: 1.09704602\n",
      "(4.26 min) Epoch 22/300 -- Iteration 20880 - Batch 657/963 - Train loss: 0.32749179  - Train acc: 0.8763 - Val loss: 1.09704602\n",
      "(4.27 min) Epoch 22/300 -- Iteration 20889 - Batch 666/963 - Train loss: 0.32758113  - Train acc: 0.8763 - Val loss: 1.09704602\n",
      "(4.27 min) Epoch 22/300 -- Iteration 20898 - Batch 675/963 - Train loss: 0.32728386  - Train acc: 0.8763 - Val loss: 1.09704602\n",
      "(4.27 min) Epoch 22/300 -- Iteration 20907 - Batch 684/963 - Train loss: 0.32704110  - Train acc: 0.8764 - Val loss: 1.09704602\n",
      "(4.27 min) Epoch 22/300 -- Iteration 20916 - Batch 693/963 - Train loss: 0.32665136  - Train acc: 0.8767 - Val loss: 1.09704602\n",
      "(4.27 min) Epoch 22/300 -- Iteration 20925 - Batch 702/963 - Train loss: 0.32658996  - Train acc: 0.8766 - Val loss: 1.09704602\n",
      "(4.27 min) Epoch 22/300 -- Iteration 20934 - Batch 711/963 - Train loss: 0.32675652  - Train acc: 0.8766 - Val loss: 1.09704602\n",
      "(4.28 min) Epoch 22/300 -- Iteration 20943 - Batch 720/963 - Train loss: 0.32651399  - Train acc: 0.8767 - Val loss: 1.09704602\n",
      "(4.28 min) Epoch 22/300 -- Iteration 20952 - Batch 729/963 - Train loss: 0.32605554  - Train acc: 0.8770 - Val loss: 1.09704602\n",
      "(4.28 min) Epoch 22/300 -- Iteration 20961 - Batch 738/963 - Train loss: 0.32627994  - Train acc: 0.8769 - Val loss: 1.09704602\n",
      "(4.28 min) Epoch 22/300 -- Iteration 20970 - Batch 747/963 - Train loss: 0.32603192  - Train acc: 0.8770 - Val loss: 1.09704602\n",
      "(4.28 min) Epoch 22/300 -- Iteration 20979 - Batch 756/963 - Train loss: 0.32578184  - Train acc: 0.8772 - Val loss: 1.09704602\n",
      "(4.29 min) Epoch 22/300 -- Iteration 20988 - Batch 765/963 - Train loss: 0.32574512  - Train acc: 0.8771 - Val loss: 1.09704602\n",
      "(4.29 min) Epoch 22/300 -- Iteration 20997 - Batch 774/963 - Train loss: 0.32574243  - Train acc: 0.8771 - Val loss: 1.09704602\n",
      "(4.29 min) Epoch 22/300 -- Iteration 21006 - Batch 783/963 - Train loss: 0.32594473  - Train acc: 0.8771 - Val loss: 1.09704602\n",
      "(4.29 min) Epoch 22/300 -- Iteration 21015 - Batch 792/963 - Train loss: 0.32581592  - Train acc: 0.8772 - Val loss: 1.09704602\n",
      "(4.29 min) Epoch 22/300 -- Iteration 21024 - Batch 801/963 - Train loss: 0.32600954  - Train acc: 0.8772 - Val loss: 1.09704602\n",
      "(4.29 min) Epoch 22/300 -- Iteration 21033 - Batch 810/963 - Train loss: 0.32606461  - Train acc: 0.8771 - Val loss: 1.09704602\n",
      "(4.30 min) Epoch 22/300 -- Iteration 21042 - Batch 819/963 - Train loss: 0.32596159  - Train acc: 0.8772 - Val loss: 1.09704602\n",
      "(4.30 min) Epoch 22/300 -- Iteration 21051 - Batch 828/963 - Train loss: 0.32620533  - Train acc: 0.8770 - Val loss: 1.09704602\n",
      "(4.30 min) Epoch 22/300 -- Iteration 21060 - Batch 837/963 - Train loss: 0.32674811  - Train acc: 0.8768 - Val loss: 1.09704602\n",
      "(4.30 min) Epoch 22/300 -- Iteration 21069 - Batch 846/963 - Train loss: 0.32694473  - Train acc: 0.8766 - Val loss: 1.09704602\n",
      "(4.30 min) Epoch 22/300 -- Iteration 21078 - Batch 855/963 - Train loss: 0.32693984  - Train acc: 0.8765 - Val loss: 1.09704602\n",
      "(4.31 min) Epoch 22/300 -- Iteration 21087 - Batch 864/963 - Train loss: 0.32678273  - Train acc: 0.8766 - Val loss: 1.09704602\n",
      "(4.31 min) Epoch 22/300 -- Iteration 21096 - Batch 873/963 - Train loss: 0.32707173  - Train acc: 0.8765 - Val loss: 1.09704602\n",
      "(4.31 min) Epoch 22/300 -- Iteration 21105 - Batch 882/963 - Train loss: 0.32726646  - Train acc: 0.8764 - Val loss: 1.09704602\n",
      "(4.31 min) Epoch 22/300 -- Iteration 21114 - Batch 891/963 - Train loss: 0.32723369  - Train acc: 0.8765 - Val loss: 1.09704602\n",
      "(4.31 min) Epoch 22/300 -- Iteration 21123 - Batch 900/963 - Train loss: 0.32714849  - Train acc: 0.8764 - Val loss: 1.09704602\n",
      "(4.31 min) Epoch 22/300 -- Iteration 21132 - Batch 909/963 - Train loss: 0.32700873  - Train acc: 0.8765 - Val loss: 1.09704602\n",
      "(4.32 min) Epoch 22/300 -- Iteration 21141 - Batch 918/963 - Train loss: 0.32693960  - Train acc: 0.8765 - Val loss: 1.09704602\n",
      "(4.32 min) Epoch 22/300 -- Iteration 21150 - Batch 927/963 - Train loss: 0.32684473  - Train acc: 0.8766 - Val loss: 1.09704602\n",
      "(4.32 min) Epoch 22/300 -- Iteration 21159 - Batch 936/963 - Train loss: 0.32688522  - Train acc: 0.8766 - Val loss: 1.09704602\n",
      "(4.32 min) Epoch 22/300 -- Iteration 21168 - Batch 945/963 - Train loss: 0.32701109  - Train acc: 0.8765 - Val loss: 1.09704602\n",
      "(4.32 min) Epoch 22/300 -- Iteration 21177 - Batch 954/963 - Train loss: 0.32703187  - Train acc: 0.8766 - Val loss: 1.09704602\n",
      "(4.33 min) Epoch 22/300 -- Iteration 21186 - Batch 962/963 - Train loss: 0.32710754  - Train acc: 0.8765 - Val loss: 1.07695878 - Val acc: 0.6083\n",
      "(4.33 min) Epoch 23/300 -- Iteration 21195 - Batch 9/963 - Train loss: 0.34303757  - Train acc: 0.8703 - Val loss: 1.07695878\n",
      "(4.33 min) Epoch 23/300 -- Iteration 21204 - Batch 18/963 - Train loss: 0.32020717  - Train acc: 0.8758 - Val loss: 1.07695878\n",
      "(4.33 min) Epoch 23/300 -- Iteration 21213 - Batch 27/963 - Train loss: 0.32281433  - Train acc: 0.8775 - Val loss: 1.07695878\n",
      "(4.33 min) Epoch 23/300 -- Iteration 21222 - Batch 36/963 - Train loss: 0.32844765  - Train acc: 0.8756 - Val loss: 1.07695878\n",
      "(4.34 min) Epoch 23/300 -- Iteration 21231 - Batch 45/963 - Train loss: 0.32424627  - Train acc: 0.8770 - Val loss: 1.07695878\n",
      "(4.34 min) Epoch 23/300 -- Iteration 21240 - Batch 54/963 - Train loss: 0.32426398  - Train acc: 0.8780 - Val loss: 1.07695878\n",
      "(4.34 min) Epoch 23/300 -- Iteration 21249 - Batch 63/963 - Train loss: 0.32451071  - Train acc: 0.8760 - Val loss: 1.07695878\n",
      "(4.34 min) Epoch 23/300 -- Iteration 21258 - Batch 72/963 - Train loss: 0.32319858  - Train acc: 0.8764 - Val loss: 1.07695878\n",
      "(4.34 min) Epoch 23/300 -- Iteration 21267 - Batch 81/963 - Train loss: 0.32300791  - Train acc: 0.8769 - Val loss: 1.07695878\n",
      "(4.34 min) Epoch 23/300 -- Iteration 21276 - Batch 90/963 - Train loss: 0.32439402  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.35 min) Epoch 23/300 -- Iteration 21285 - Batch 99/963 - Train loss: 0.32585592  - Train acc: 0.8751 - Val loss: 1.07695878\n",
      "(4.35 min) Epoch 23/300 -- Iteration 21294 - Batch 108/963 - Train loss: 0.32471885  - Train acc: 0.8760 - Val loss: 1.07695878\n",
      "(4.35 min) Epoch 23/300 -- Iteration 21303 - Batch 117/963 - Train loss: 0.32532511  - Train acc: 0.8757 - Val loss: 1.07695878\n",
      "(4.35 min) Epoch 23/300 -- Iteration 21312 - Batch 126/963 - Train loss: 0.32696877  - Train acc: 0.8745 - Val loss: 1.07695878\n",
      "(4.35 min) Epoch 23/300 -- Iteration 21321 - Batch 135/963 - Train loss: 0.32397816  - Train acc: 0.8762 - Val loss: 1.07695878\n",
      "(4.36 min) Epoch 23/300 -- Iteration 21330 - Batch 144/963 - Train loss: 0.32320510  - Train acc: 0.8772 - Val loss: 1.07695878\n",
      "(4.36 min) Epoch 23/300 -- Iteration 21339 - Batch 153/963 - Train loss: 0.32414409  - Train acc: 0.8764 - Val loss: 1.07695878\n",
      "(4.36 min) Epoch 23/300 -- Iteration 21348 - Batch 162/963 - Train loss: 0.32389074  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.36 min) Epoch 23/300 -- Iteration 21357 - Batch 171/963 - Train loss: 0.32359450  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.36 min) Epoch 23/300 -- Iteration 21366 - Batch 180/963 - Train loss: 0.32314354  - Train acc: 0.8774 - Val loss: 1.07695878\n",
      "(4.36 min) Epoch 23/300 -- Iteration 21375 - Batch 189/963 - Train loss: 0.32409057  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.37 min) Epoch 23/300 -- Iteration 21384 - Batch 198/963 - Train loss: 0.32211284  - Train acc: 0.8780 - Val loss: 1.07695878\n",
      "(4.37 min) Epoch 23/300 -- Iteration 21393 - Batch 207/963 - Train loss: 0.32207431  - Train acc: 0.8780 - Val loss: 1.07695878\n",
      "(4.37 min) Epoch 23/300 -- Iteration 21402 - Batch 216/963 - Train loss: 0.32142517  - Train acc: 0.8782 - Val loss: 1.07695878\n",
      "(4.37 min) Epoch 23/300 -- Iteration 21411 - Batch 225/963 - Train loss: 0.32121777  - Train acc: 0.8785 - Val loss: 1.07695878\n",
      "(4.37 min) Epoch 23/300 -- Iteration 21420 - Batch 234/963 - Train loss: 0.32187579  - Train acc: 0.8781 - Val loss: 1.07695878\n",
      "(4.38 min) Epoch 23/300 -- Iteration 21429 - Batch 243/963 - Train loss: 0.32182958  - Train acc: 0.8784 - Val loss: 1.07695878\n",
      "(4.38 min) Epoch 23/300 -- Iteration 21438 - Batch 252/963 - Train loss: 0.32201324  - Train acc: 0.8780 - Val loss: 1.07695878\n",
      "(4.38 min) Epoch 23/300 -- Iteration 21447 - Batch 261/963 - Train loss: 0.32303749  - Train acc: 0.8779 - Val loss: 1.07695878\n",
      "(4.38 min) Epoch 23/300 -- Iteration 21456 - Batch 270/963 - Train loss: 0.32285188  - Train acc: 0.8779 - Val loss: 1.07695878\n",
      "(4.38 min) Epoch 23/300 -- Iteration 21465 - Batch 279/963 - Train loss: 0.32338947  - Train acc: 0.8779 - Val loss: 1.07695878\n",
      "(4.38 min) Epoch 23/300 -- Iteration 21474 - Batch 288/963 - Train loss: 0.32354643  - Train acc: 0.8781 - Val loss: 1.07695878\n",
      "(4.39 min) Epoch 23/300 -- Iteration 21483 - Batch 297/963 - Train loss: 0.32284260  - Train acc: 0.8782 - Val loss: 1.07695878\n",
      "(4.39 min) Epoch 23/300 -- Iteration 21492 - Batch 306/963 - Train loss: 0.32239759  - Train acc: 0.8783 - Val loss: 1.07695878\n",
      "(4.39 min) Epoch 23/300 -- Iteration 21501 - Batch 315/963 - Train loss: 0.32166238  - Train acc: 0.8784 - Val loss: 1.07695878\n",
      "(4.39 min) Epoch 23/300 -- Iteration 21510 - Batch 324/963 - Train loss: 0.32178349  - Train acc: 0.8784 - Val loss: 1.07695878\n",
      "(4.39 min) Epoch 23/300 -- Iteration 21519 - Batch 333/963 - Train loss: 0.32224093  - Train acc: 0.8783 - Val loss: 1.07695878\n",
      "(4.40 min) Epoch 23/300 -- Iteration 21528 - Batch 342/963 - Train loss: 0.32273444  - Train acc: 0.8778 - Val loss: 1.07695878\n",
      "(4.40 min) Epoch 23/300 -- Iteration 21537 - Batch 351/963 - Train loss: 0.32273178  - Train acc: 0.8780 - Val loss: 1.07695878\n",
      "(4.40 min) Epoch 23/300 -- Iteration 21546 - Batch 360/963 - Train loss: 0.32299350  - Train acc: 0.8779 - Val loss: 1.07695878\n",
      "(4.40 min) Epoch 23/300 -- Iteration 21555 - Batch 369/963 - Train loss: 0.32259824  - Train acc: 0.8782 - Val loss: 1.07695878\n",
      "(4.40 min) Epoch 23/300 -- Iteration 21564 - Batch 378/963 - Train loss: 0.32239866  - Train acc: 0.8783 - Val loss: 1.07695878\n",
      "(4.40 min) Epoch 23/300 -- Iteration 21573 - Batch 387/963 - Train loss: 0.32295847  - Train acc: 0.8781 - Val loss: 1.07695878\n",
      "(4.41 min) Epoch 23/300 -- Iteration 21582 - Batch 396/963 - Train loss: 0.32369674  - Train acc: 0.8780 - Val loss: 1.07695878\n",
      "(4.41 min) Epoch 23/300 -- Iteration 21591 - Batch 405/963 - Train loss: 0.32332382  - Train acc: 0.8779 - Val loss: 1.07695878\n",
      "(4.41 min) Epoch 23/300 -- Iteration 21600 - Batch 414/963 - Train loss: 0.32380989  - Train acc: 0.8778 - Val loss: 1.07695878\n",
      "(4.41 min) Epoch 23/300 -- Iteration 21609 - Batch 423/963 - Train loss: 0.32385925  - Train acc: 0.8778 - Val loss: 1.07695878\n",
      "(4.41 min) Epoch 23/300 -- Iteration 21618 - Batch 432/963 - Train loss: 0.32434830  - Train acc: 0.8775 - Val loss: 1.07695878\n",
      "(4.42 min) Epoch 23/300 -- Iteration 21627 - Batch 441/963 - Train loss: 0.32451226  - Train acc: 0.8774 - Val loss: 1.07695878\n",
      "(4.42 min) Epoch 23/300 -- Iteration 21636 - Batch 450/963 - Train loss: 0.32467788  - Train acc: 0.8775 - Val loss: 1.07695878\n",
      "(4.42 min) Epoch 23/300 -- Iteration 21645 - Batch 459/963 - Train loss: 0.32513312  - Train acc: 0.8772 - Val loss: 1.07695878\n",
      "(4.42 min) Epoch 23/300 -- Iteration 21654 - Batch 468/963 - Train loss: 0.32512830  - Train acc: 0.8770 - Val loss: 1.07695878\n",
      "(4.42 min) Epoch 23/300 -- Iteration 21663 - Batch 477/963 - Train loss: 0.32595413  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.42 min) Epoch 23/300 -- Iteration 21672 - Batch 486/963 - Train loss: 0.32596208  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.43 min) Epoch 23/300 -- Iteration 21681 - Batch 495/963 - Train loss: 0.32661733  - Train acc: 0.8762 - Val loss: 1.07695878\n",
      "(4.43 min) Epoch 23/300 -- Iteration 21690 - Batch 504/963 - Train loss: 0.32629416  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.43 min) Epoch 23/300 -- Iteration 21699 - Batch 513/963 - Train loss: 0.32576211  - Train acc: 0.8768 - Val loss: 1.07695878\n",
      "(4.43 min) Epoch 23/300 -- Iteration 21708 - Batch 522/963 - Train loss: 0.32542967  - Train acc: 0.8769 - Val loss: 1.07695878\n",
      "(4.43 min) Epoch 23/300 -- Iteration 21717 - Batch 531/963 - Train loss: 0.32498064  - Train acc: 0.8772 - Val loss: 1.07695878\n",
      "(4.44 min) Epoch 23/300 -- Iteration 21726 - Batch 540/963 - Train loss: 0.32592355  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.44 min) Epoch 23/300 -- Iteration 21735 - Batch 549/963 - Train loss: 0.32594836  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.44 min) Epoch 23/300 -- Iteration 21744 - Batch 558/963 - Train loss: 0.32617962  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.44 min) Epoch 23/300 -- Iteration 21753 - Batch 567/963 - Train loss: 0.32630401  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.44 min) Epoch 23/300 -- Iteration 21762 - Batch 576/963 - Train loss: 0.32682831  - Train acc: 0.8762 - Val loss: 1.07695878\n",
      "(4.45 min) Epoch 23/300 -- Iteration 21771 - Batch 585/963 - Train loss: 0.32691530  - Train acc: 0.8762 - Val loss: 1.07695878\n",
      "(4.45 min) Epoch 23/300 -- Iteration 21780 - Batch 594/963 - Train loss: 0.32664901  - Train acc: 0.8763 - Val loss: 1.07695878\n",
      "(4.45 min) Epoch 23/300 -- Iteration 21789 - Batch 603/963 - Train loss: 0.32671193  - Train acc: 0.8762 - Val loss: 1.07695878\n",
      "(4.45 min) Epoch 23/300 -- Iteration 21798 - Batch 612/963 - Train loss: 0.32670753  - Train acc: 0.8762 - Val loss: 1.07695878\n",
      "(4.45 min) Epoch 23/300 -- Iteration 21807 - Batch 621/963 - Train loss: 0.32646424  - Train acc: 0.8763 - Val loss: 1.07695878\n",
      "(4.45 min) Epoch 23/300 -- Iteration 21816 - Batch 630/963 - Train loss: 0.32603932  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.46 min) Epoch 23/300 -- Iteration 21825 - Batch 639/963 - Train loss: 0.32593466  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.46 min) Epoch 23/300 -- Iteration 21834 - Batch 648/963 - Train loss: 0.32560342  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.46 min) Epoch 23/300 -- Iteration 21843 - Batch 657/963 - Train loss: 0.32551638  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.46 min) Epoch 23/300 -- Iteration 21852 - Batch 666/963 - Train loss: 0.32558719  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.46 min) Epoch 23/300 -- Iteration 21861 - Batch 675/963 - Train loss: 0.32562032  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.47 min) Epoch 23/300 -- Iteration 21870 - Batch 684/963 - Train loss: 0.32557150  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.47 min) Epoch 23/300 -- Iteration 21879 - Batch 693/963 - Train loss: 0.32558899  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.47 min) Epoch 23/300 -- Iteration 21888 - Batch 702/963 - Train loss: 0.32543299  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.47 min) Epoch 23/300 -- Iteration 21897 - Batch 711/963 - Train loss: 0.32508819  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.47 min) Epoch 23/300 -- Iteration 21906 - Batch 720/963 - Train loss: 0.32486961  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.47 min) Epoch 23/300 -- Iteration 21915 - Batch 729/963 - Train loss: 0.32482036  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.48 min) Epoch 23/300 -- Iteration 21924 - Batch 738/963 - Train loss: 0.32507713  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.48 min) Epoch 23/300 -- Iteration 21933 - Batch 747/963 - Train loss: 0.32523198  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.48 min) Epoch 23/300 -- Iteration 21942 - Batch 756/963 - Train loss: 0.32511021  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.48 min) Epoch 23/300 -- Iteration 21951 - Batch 765/963 - Train loss: 0.32532846  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.48 min) Epoch 23/300 -- Iteration 21960 - Batch 774/963 - Train loss: 0.32512677  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.49 min) Epoch 23/300 -- Iteration 21969 - Batch 783/963 - Train loss: 0.32506746  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.49 min) Epoch 23/300 -- Iteration 21978 - Batch 792/963 - Train loss: 0.32472084  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.49 min) Epoch 23/300 -- Iteration 21987 - Batch 801/963 - Train loss: 0.32453886  - Train acc: 0.8768 - Val loss: 1.07695878\n",
      "(4.49 min) Epoch 23/300 -- Iteration 21996 - Batch 810/963 - Train loss: 0.32468907  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.49 min) Epoch 23/300 -- Iteration 22005 - Batch 819/963 - Train loss: 0.32453057  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.49 min) Epoch 23/300 -- Iteration 22014 - Batch 828/963 - Train loss: 0.32502616  - Train acc: 0.8765 - Val loss: 1.07695878\n",
      "(4.50 min) Epoch 23/300 -- Iteration 22023 - Batch 837/963 - Train loss: 0.32480248  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.50 min) Epoch 23/300 -- Iteration 22032 - Batch 846/963 - Train loss: 0.32511177  - Train acc: 0.8766 - Val loss: 1.07695878\n",
      "(4.50 min) Epoch 23/300 -- Iteration 22041 - Batch 855/963 - Train loss: 0.32477301  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.50 min) Epoch 23/300 -- Iteration 22050 - Batch 864/963 - Train loss: 0.32493459  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.50 min) Epoch 23/300 -- Iteration 22059 - Batch 873/963 - Train loss: 0.32477446  - Train acc: 0.8767 - Val loss: 1.07695878\n",
      "(4.51 min) Epoch 23/300 -- Iteration 22068 - Batch 882/963 - Train loss: 0.32443668  - Train acc: 0.8769 - Val loss: 1.07695878\n",
      "(4.51 min) Epoch 23/300 -- Iteration 22077 - Batch 891/963 - Train loss: 0.32467719  - Train acc: 0.8768 - Val loss: 1.07695878\n",
      "(4.51 min) Epoch 23/300 -- Iteration 22086 - Batch 900/963 - Train loss: 0.32467147  - Train acc: 0.8768 - Val loss: 1.07695878\n",
      "(4.51 min) Epoch 23/300 -- Iteration 22095 - Batch 909/963 - Train loss: 0.32457383  - Train acc: 0.8769 - Val loss: 1.07695878\n",
      "(4.51 min) Epoch 23/300 -- Iteration 22104 - Batch 918/963 - Train loss: 0.32438332  - Train acc: 0.8770 - Val loss: 1.07695878\n",
      "(4.51 min) Epoch 23/300 -- Iteration 22113 - Batch 927/963 - Train loss: 0.32419024  - Train acc: 0.8771 - Val loss: 1.07695878\n",
      "(4.52 min) Epoch 23/300 -- Iteration 22122 - Batch 936/963 - Train loss: 0.32434080  - Train acc: 0.8771 - Val loss: 1.07695878\n",
      "(4.52 min) Epoch 23/300 -- Iteration 22131 - Batch 945/963 - Train loss: 0.32445815  - Train acc: 0.8771 - Val loss: 1.07695878\n",
      "(4.52 min) Epoch 23/300 -- Iteration 22140 - Batch 954/963 - Train loss: 0.32442572  - Train acc: 0.8770 - Val loss: 1.07695878\n",
      "(4.52 min) Epoch 23/300 -- Iteration 22149 - Batch 962/963 - Train loss: 0.32435073  - Train acc: 0.8771 - Val loss: 1.09525084 - Val acc: 0.6033\n",
      "(4.52 min) Epoch 24/300 -- Iteration 22158 - Batch 9/963 - Train loss: 0.33425743  - Train acc: 0.8758 - Val loss: 1.09525084\n",
      "(4.53 min) Epoch 24/300 -- Iteration 22167 - Batch 18/963 - Train loss: 0.31908042  - Train acc: 0.8816 - Val loss: 1.09525084\n",
      "(4.53 min) Epoch 24/300 -- Iteration 22176 - Batch 27/963 - Train loss: 0.32183332  - Train acc: 0.8797 - Val loss: 1.09525084\n",
      "(4.53 min) Epoch 24/300 -- Iteration 22185 - Batch 36/963 - Train loss: 0.32703074  - Train acc: 0.8763 - Val loss: 1.09525084\n",
      "(4.53 min) Epoch 24/300 -- Iteration 22194 - Batch 45/963 - Train loss: 0.32226963  - Train acc: 0.8781 - Val loss: 1.09525084\n",
      "(4.53 min) Epoch 24/300 -- Iteration 22203 - Batch 54/963 - Train loss: 0.32047167  - Train acc: 0.8787 - Val loss: 1.09525084\n",
      "(4.53 min) Epoch 24/300 -- Iteration 22212 - Batch 63/963 - Train loss: 0.32244573  - Train acc: 0.8782 - Val loss: 1.09525084\n",
      "(4.54 min) Epoch 24/300 -- Iteration 22221 - Batch 72/963 - Train loss: 0.32173552  - Train acc: 0.8781 - Val loss: 1.09525084\n",
      "(4.54 min) Epoch 24/300 -- Iteration 22230 - Batch 81/963 - Train loss: 0.32136311  - Train acc: 0.8786 - Val loss: 1.09525084\n",
      "(4.54 min) Epoch 24/300 -- Iteration 22239 - Batch 90/963 - Train loss: 0.32159351  - Train acc: 0.8783 - Val loss: 1.09525084\n",
      "(4.54 min) Epoch 24/300 -- Iteration 22248 - Batch 99/963 - Train loss: 0.32020860  - Train acc: 0.8790 - Val loss: 1.09525084\n",
      "(4.54 min) Epoch 24/300 -- Iteration 22257 - Batch 108/963 - Train loss: 0.32164186  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.55 min) Epoch 24/300 -- Iteration 22266 - Batch 117/963 - Train loss: 0.32018330  - Train acc: 0.8803 - Val loss: 1.09525084\n",
      "(4.55 min) Epoch 24/300 -- Iteration 22275 - Batch 126/963 - Train loss: 0.31860670  - Train acc: 0.8805 - Val loss: 1.09525084\n",
      "(4.55 min) Epoch 24/300 -- Iteration 22284 - Batch 135/963 - Train loss: 0.31927523  - Train acc: 0.8802 - Val loss: 1.09525084\n",
      "(4.55 min) Epoch 24/300 -- Iteration 22293 - Batch 144/963 - Train loss: 0.31917137  - Train acc: 0.8801 - Val loss: 1.09525084\n",
      "(4.55 min) Epoch 24/300 -- Iteration 22302 - Batch 153/963 - Train loss: 0.31988478  - Train acc: 0.8799 - Val loss: 1.09525084\n",
      "(4.55 min) Epoch 24/300 -- Iteration 22311 - Batch 162/963 - Train loss: 0.32073625  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.56 min) Epoch 24/300 -- Iteration 22320 - Batch 171/963 - Train loss: 0.32070491  - Train acc: 0.8800 - Val loss: 1.09525084\n",
      "(4.56 min) Epoch 24/300 -- Iteration 22329 - Batch 180/963 - Train loss: 0.31966123  - Train acc: 0.8806 - Val loss: 1.09525084\n",
      "(4.56 min) Epoch 24/300 -- Iteration 22338 - Batch 189/963 - Train loss: 0.31916438  - Train acc: 0.8807 - Val loss: 1.09525084\n",
      "(4.56 min) Epoch 24/300 -- Iteration 22347 - Batch 198/963 - Train loss: 0.32166821  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.56 min) Epoch 24/300 -- Iteration 22356 - Batch 207/963 - Train loss: 0.32220361  - Train acc: 0.8790 - Val loss: 1.09525084\n",
      "(4.57 min) Epoch 24/300 -- Iteration 22365 - Batch 216/963 - Train loss: 0.32037361  - Train acc: 0.8799 - Val loss: 1.09525084\n",
      "(4.57 min) Epoch 24/300 -- Iteration 22374 - Batch 225/963 - Train loss: 0.31955668  - Train acc: 0.8800 - Val loss: 1.09525084\n",
      "(4.57 min) Epoch 24/300 -- Iteration 22383 - Batch 234/963 - Train loss: 0.31831096  - Train acc: 0.8805 - Val loss: 1.09525084\n",
      "(4.57 min) Epoch 24/300 -- Iteration 22392 - Batch 243/963 - Train loss: 0.31948445  - Train acc: 0.8800 - Val loss: 1.09525084\n",
      "(4.57 min) Epoch 24/300 -- Iteration 22401 - Batch 252/963 - Train loss: 0.32174055  - Train acc: 0.8791 - Val loss: 1.09525084\n",
      "(4.57 min) Epoch 24/300 -- Iteration 22410 - Batch 261/963 - Train loss: 0.32053034  - Train acc: 0.8797 - Val loss: 1.09525084\n",
      "(4.58 min) Epoch 24/300 -- Iteration 22419 - Batch 270/963 - Train loss: 0.32108161  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.58 min) Epoch 24/300 -- Iteration 22428 - Batch 279/963 - Train loss: 0.32164651  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.58 min) Epoch 24/300 -- Iteration 22437 - Batch 288/963 - Train loss: 0.32238373  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.58 min) Epoch 24/300 -- Iteration 22446 - Batch 297/963 - Train loss: 0.32174616  - Train acc: 0.8797 - Val loss: 1.09525084\n",
      "(4.58 min) Epoch 24/300 -- Iteration 22455 - Batch 306/963 - Train loss: 0.32186003  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.59 min) Epoch 24/300 -- Iteration 22464 - Batch 315/963 - Train loss: 0.32166547  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.59 min) Epoch 24/300 -- Iteration 22473 - Batch 324/963 - Train loss: 0.32155632  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.59 min) Epoch 24/300 -- Iteration 22482 - Batch 333/963 - Train loss: 0.32158598  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.59 min) Epoch 24/300 -- Iteration 22491 - Batch 342/963 - Train loss: 0.32156142  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.59 min) Epoch 24/300 -- Iteration 22500 - Batch 351/963 - Train loss: 0.32164173  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.59 min) Epoch 24/300 -- Iteration 22509 - Batch 360/963 - Train loss: 0.32193742  - Train acc: 0.8791 - Val loss: 1.09525084\n",
      "(4.60 min) Epoch 24/300 -- Iteration 22518 - Batch 369/963 - Train loss: 0.32184208  - Train acc: 0.8791 - Val loss: 1.09525084\n",
      "(4.60 min) Epoch 24/300 -- Iteration 22527 - Batch 378/963 - Train loss: 0.32146719  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.60 min) Epoch 24/300 -- Iteration 22536 - Batch 387/963 - Train loss: 0.32140712  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.60 min) Epoch 24/300 -- Iteration 22545 - Batch 396/963 - Train loss: 0.32155657  - Train acc: 0.8789 - Val loss: 1.09525084\n",
      "(4.60 min) Epoch 24/300 -- Iteration 22554 - Batch 405/963 - Train loss: 0.32120754  - Train acc: 0.8791 - Val loss: 1.09525084\n",
      "(4.61 min) Epoch 24/300 -- Iteration 22563 - Batch 414/963 - Train loss: 0.32079156  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.61 min) Epoch 24/300 -- Iteration 22572 - Batch 423/963 - Train loss: 0.32033450  - Train acc: 0.8796 - Val loss: 1.09525084\n",
      "(4.61 min) Epoch 24/300 -- Iteration 22581 - Batch 432/963 - Train loss: 0.31992902  - Train acc: 0.8798 - Val loss: 1.09525084\n",
      "(4.61 min) Epoch 24/300 -- Iteration 22590 - Batch 441/963 - Train loss: 0.32056057  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.61 min) Epoch 24/300 -- Iteration 22599 - Batch 450/963 - Train loss: 0.32103037  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.61 min) Epoch 24/300 -- Iteration 22608 - Batch 459/963 - Train loss: 0.32068992  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.62 min) Epoch 24/300 -- Iteration 22617 - Batch 468/963 - Train loss: 0.32082642  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.62 min) Epoch 24/300 -- Iteration 22626 - Batch 477/963 - Train loss: 0.32108254  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.62 min) Epoch 24/300 -- Iteration 22635 - Batch 486/963 - Train loss: 0.32121289  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.62 min) Epoch 24/300 -- Iteration 22644 - Batch 495/963 - Train loss: 0.32167185  - Train acc: 0.8789 - Val loss: 1.09525084\n",
      "(4.62 min) Epoch 24/300 -- Iteration 22653 - Batch 504/963 - Train loss: 0.32115839  - Train acc: 0.8790 - Val loss: 1.09525084\n",
      "(4.63 min) Epoch 24/300 -- Iteration 22662 - Batch 513/963 - Train loss: 0.32079645  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.63 min) Epoch 24/300 -- Iteration 22671 - Batch 522/963 - Train loss: 0.32066907  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.63 min) Epoch 24/300 -- Iteration 22680 - Batch 531/963 - Train loss: 0.32051934  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.63 min) Epoch 24/300 -- Iteration 22689 - Batch 540/963 - Train loss: 0.32046566  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.63 min) Epoch 24/300 -- Iteration 22698 - Batch 549/963 - Train loss: 0.32093195  - Train acc: 0.8791 - Val loss: 1.09525084\n",
      "(4.64 min) Epoch 24/300 -- Iteration 22707 - Batch 558/963 - Train loss: 0.32068487  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.64 min) Epoch 24/300 -- Iteration 22716 - Batch 567/963 - Train loss: 0.32047027  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.64 min) Epoch 24/300 -- Iteration 22725 - Batch 576/963 - Train loss: 0.32117467  - Train acc: 0.8790 - Val loss: 1.09525084\n",
      "(4.64 min) Epoch 24/300 -- Iteration 22734 - Batch 585/963 - Train loss: 0.32108107  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.64 min) Epoch 24/300 -- Iteration 22743 - Batch 594/963 - Train loss: 0.32051309  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.64 min) Epoch 24/300 -- Iteration 22752 - Batch 603/963 - Train loss: 0.32022338  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.65 min) Epoch 24/300 -- Iteration 22761 - Batch 612/963 - Train loss: 0.32002047  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.65 min) Epoch 24/300 -- Iteration 22770 - Batch 621/963 - Train loss: 0.32033396  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.65 min) Epoch 24/300 -- Iteration 22779 - Batch 630/963 - Train loss: 0.32013499  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.65 min) Epoch 24/300 -- Iteration 22788 - Batch 639/963 - Train loss: 0.32002147  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.65 min) Epoch 24/300 -- Iteration 22797 - Batch 648/963 - Train loss: 0.31983061  - Train acc: 0.8796 - Val loss: 1.09525084\n",
      "(4.66 min) Epoch 24/300 -- Iteration 22806 - Batch 657/963 - Train loss: 0.32029914  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.66 min) Epoch 24/300 -- Iteration 22815 - Batch 666/963 - Train loss: 0.32042773  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.66 min) Epoch 24/300 -- Iteration 22824 - Batch 675/963 - Train loss: 0.32064909  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.66 min) Epoch 24/300 -- Iteration 22833 - Batch 684/963 - Train loss: 0.32047336  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.66 min) Epoch 24/300 -- Iteration 22842 - Batch 693/963 - Train loss: 0.32040077  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.67 min) Epoch 24/300 -- Iteration 22851 - Batch 702/963 - Train loss: 0.32016776  - Train acc: 0.8796 - Val loss: 1.09525084\n",
      "(4.67 min) Epoch 24/300 -- Iteration 22860 - Batch 711/963 - Train loss: 0.32055195  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.67 min) Epoch 24/300 -- Iteration 22869 - Batch 720/963 - Train loss: 0.32026625  - Train acc: 0.8796 - Val loss: 1.09525084\n",
      "(4.67 min) Epoch 24/300 -- Iteration 22878 - Batch 729/963 - Train loss: 0.32055645  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.67 min) Epoch 24/300 -- Iteration 22887 - Batch 738/963 - Train loss: 0.32028089  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.67 min) Epoch 24/300 -- Iteration 22896 - Batch 747/963 - Train loss: 0.32039277  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.68 min) Epoch 24/300 -- Iteration 22905 - Batch 756/963 - Train loss: 0.32067303  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.68 min) Epoch 24/300 -- Iteration 22914 - Batch 765/963 - Train loss: 0.32095657  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.68 min) Epoch 24/300 -- Iteration 22923 - Batch 774/963 - Train loss: 0.32098463  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.68 min) Epoch 24/300 -- Iteration 22932 - Batch 783/963 - Train loss: 0.32102405  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.68 min) Epoch 24/300 -- Iteration 22941 - Batch 792/963 - Train loss: 0.32100462  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.69 min) Epoch 24/300 -- Iteration 22950 - Batch 801/963 - Train loss: 0.32079843  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.69 min) Epoch 24/300 -- Iteration 22959 - Batch 810/963 - Train loss: 0.32054721  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.69 min) Epoch 24/300 -- Iteration 22968 - Batch 819/963 - Train loss: 0.32024343  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.69 min) Epoch 24/300 -- Iteration 22977 - Batch 828/963 - Train loss: 0.32011634  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.69 min) Epoch 24/300 -- Iteration 22986 - Batch 837/963 - Train loss: 0.32003092  - Train acc: 0.8796 - Val loss: 1.09525084\n",
      "(4.69 min) Epoch 24/300 -- Iteration 22995 - Batch 846/963 - Train loss: 0.32012852  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.70 min) Epoch 24/300 -- Iteration 23004 - Batch 855/963 - Train loss: 0.32031044  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.70 min) Epoch 24/300 -- Iteration 23013 - Batch 864/963 - Train loss: 0.32037845  - Train acc: 0.8792 - Val loss: 1.09525084\n",
      "(4.70 min) Epoch 24/300 -- Iteration 23022 - Batch 873/963 - Train loss: 0.32030446  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.70 min) Epoch 24/300 -- Iteration 23031 - Batch 882/963 - Train loss: 0.32027234  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.70 min) Epoch 24/300 -- Iteration 23040 - Batch 891/963 - Train loss: 0.32007566  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.71 min) Epoch 24/300 -- Iteration 23049 - Batch 900/963 - Train loss: 0.32013393  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.71 min) Epoch 24/300 -- Iteration 23058 - Batch 909/963 - Train loss: 0.32006754  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.71 min) Epoch 24/300 -- Iteration 23067 - Batch 918/963 - Train loss: 0.31986910  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.71 min) Epoch 24/300 -- Iteration 23076 - Batch 927/963 - Train loss: 0.31978631  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.71 min) Epoch 24/300 -- Iteration 23085 - Batch 936/963 - Train loss: 0.32000538  - Train acc: 0.8793 - Val loss: 1.09525084\n",
      "(4.71 min) Epoch 24/300 -- Iteration 23094 - Batch 945/963 - Train loss: 0.31959840  - Train acc: 0.8795 - Val loss: 1.09525084\n",
      "(4.72 min) Epoch 24/300 -- Iteration 23103 - Batch 954/963 - Train loss: 0.31975562  - Train acc: 0.8794 - Val loss: 1.09525084\n",
      "(4.72 min) Epoch 24/300 -- Iteration 23112 - Batch 962/963 - Train loss: 0.31978203  - Train acc: 0.8794 - Val loss: 1.08156776 - Val acc: 0.6033\n",
      "(4.72 min) Epoch 25/300 -- Iteration 23121 - Batch 9/963 - Train loss: 0.32678047  - Train acc: 0.8719 - Val loss: 1.08156776\n",
      "(4.72 min) Epoch 25/300 -- Iteration 23130 - Batch 18/963 - Train loss: 0.31581636  - Train acc: 0.8824 - Val loss: 1.08156776\n",
      "(4.72 min) Epoch 25/300 -- Iteration 23139 - Batch 27/963 - Train loss: 0.31769040  - Train acc: 0.8800 - Val loss: 1.08156776\n",
      "(4.73 min) Epoch 25/300 -- Iteration 23148 - Batch 36/963 - Train loss: 0.31886407  - Train acc: 0.8809 - Val loss: 1.08156776\n",
      "(4.73 min) Epoch 25/300 -- Iteration 23157 - Batch 45/963 - Train loss: 0.32005842  - Train acc: 0.8801 - Val loss: 1.08156776\n",
      "(4.73 min) Epoch 25/300 -- Iteration 23166 - Batch 54/963 - Train loss: 0.31852313  - Train acc: 0.8805 - Val loss: 1.08156776\n",
      "(4.73 min) Epoch 25/300 -- Iteration 23175 - Batch 63/963 - Train loss: 0.31871592  - Train acc: 0.8798 - Val loss: 1.08156776\n",
      "(4.73 min) Epoch 25/300 -- Iteration 23184 - Batch 72/963 - Train loss: 0.31840929  - Train acc: 0.8808 - Val loss: 1.08156776\n",
      "(4.74 min) Epoch 25/300 -- Iteration 23193 - Batch 81/963 - Train loss: 0.31773973  - Train acc: 0.8797 - Val loss: 1.08156776\n",
      "(4.74 min) Epoch 25/300 -- Iteration 23202 - Batch 90/963 - Train loss: 0.31771395  - Train acc: 0.8795 - Val loss: 1.08156776\n",
      "(4.74 min) Epoch 25/300 -- Iteration 23211 - Batch 99/963 - Train loss: 0.31754029  - Train acc: 0.8790 - Val loss: 1.08156776\n",
      "(4.74 min) Epoch 25/300 -- Iteration 23220 - Batch 108/963 - Train loss: 0.31863363  - Train acc: 0.8769 - Val loss: 1.08156776\n",
      "(4.74 min) Epoch 25/300 -- Iteration 23229 - Batch 117/963 - Train loss: 0.31769146  - Train acc: 0.8777 - Val loss: 1.08156776\n",
      "(4.74 min) Epoch 25/300 -- Iteration 23238 - Batch 126/963 - Train loss: 0.31607750  - Train acc: 0.8786 - Val loss: 1.08156776\n",
      "(4.75 min) Epoch 25/300 -- Iteration 23247 - Batch 135/963 - Train loss: 0.31529308  - Train acc: 0.8791 - Val loss: 1.08156776\n",
      "(4.75 min) Epoch 25/300 -- Iteration 23256 - Batch 144/963 - Train loss: 0.31509144  - Train acc: 0.8793 - Val loss: 1.08156776\n",
      "(4.75 min) Epoch 25/300 -- Iteration 23265 - Batch 153/963 - Train loss: 0.31547154  - Train acc: 0.8795 - Val loss: 1.08156776\n",
      "(4.75 min) Epoch 25/300 -- Iteration 23274 - Batch 162/963 - Train loss: 0.31544944  - Train acc: 0.8796 - Val loss: 1.08156776\n",
      "(4.75 min) Epoch 25/300 -- Iteration 23283 - Batch 171/963 - Train loss: 0.31632885  - Train acc: 0.8794 - Val loss: 1.08156776\n",
      "(4.76 min) Epoch 25/300 -- Iteration 23292 - Batch 180/963 - Train loss: 0.31659103  - Train acc: 0.8797 - Val loss: 1.08156776\n",
      "(4.76 min) Epoch 25/300 -- Iteration 23301 - Batch 189/963 - Train loss: 0.31802104  - Train acc: 0.8789 - Val loss: 1.08156776\n",
      "(4.76 min) Epoch 25/300 -- Iteration 23310 - Batch 198/963 - Train loss: 0.31860779  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.76 min) Epoch 25/300 -- Iteration 23319 - Batch 207/963 - Train loss: 0.31857174  - Train acc: 0.8784 - Val loss: 1.08156776\n",
      "(4.76 min) Epoch 25/300 -- Iteration 23328 - Batch 216/963 - Train loss: 0.31812376  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.77 min) Epoch 25/300 -- Iteration 23337 - Batch 225/963 - Train loss: 0.31841776  - Train acc: 0.8790 - Val loss: 1.08156776\n",
      "(4.77 min) Epoch 25/300 -- Iteration 23346 - Batch 234/963 - Train loss: 0.31957759  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.77 min) Epoch 25/300 -- Iteration 23355 - Batch 243/963 - Train loss: 0.31955774  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.77 min) Epoch 25/300 -- Iteration 23364 - Batch 252/963 - Train loss: 0.31990874  - Train acc: 0.8785 - Val loss: 1.08156776\n",
      "(4.77 min) Epoch 25/300 -- Iteration 23373 - Batch 261/963 - Train loss: 0.32005607  - Train acc: 0.8782 - Val loss: 1.08156776\n",
      "(4.77 min) Epoch 25/300 -- Iteration 23382 - Batch 270/963 - Train loss: 0.32018895  - Train acc: 0.8780 - Val loss: 1.08156776\n",
      "(4.78 min) Epoch 25/300 -- Iteration 23391 - Batch 279/963 - Train loss: 0.32013770  - Train acc: 0.8781 - Val loss: 1.08156776\n",
      "(4.78 min) Epoch 25/300 -- Iteration 23400 - Batch 288/963 - Train loss: 0.31987006  - Train acc: 0.8782 - Val loss: 1.08156776\n",
      "(4.78 min) Epoch 25/300 -- Iteration 23409 - Batch 297/963 - Train loss: 0.31951795  - Train acc: 0.8786 - Val loss: 1.08156776\n",
      "(4.78 min) Epoch 25/300 -- Iteration 23418 - Batch 306/963 - Train loss: 0.32006999  - Train acc: 0.8783 - Val loss: 1.08156776\n",
      "(4.78 min) Epoch 25/300 -- Iteration 23427 - Batch 315/963 - Train loss: 0.32032916  - Train acc: 0.8784 - Val loss: 1.08156776\n",
      "(4.79 min) Epoch 25/300 -- Iteration 23436 - Batch 324/963 - Train loss: 0.32002396  - Train acc: 0.8786 - Val loss: 1.08156776\n",
      "(4.79 min) Epoch 25/300 -- Iteration 23445 - Batch 333/963 - Train loss: 0.32000300  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.79 min) Epoch 25/300 -- Iteration 23454 - Batch 342/963 - Train loss: 0.32021545  - Train acc: 0.8786 - Val loss: 1.08156776\n",
      "(4.79 min) Epoch 25/300 -- Iteration 23463 - Batch 351/963 - Train loss: 0.31963931  - Train acc: 0.8788 - Val loss: 1.08156776\n",
      "(4.79 min) Epoch 25/300 -- Iteration 23472 - Batch 360/963 - Train loss: 0.32014805  - Train acc: 0.8785 - Val loss: 1.08156776\n",
      "(4.79 min) Epoch 25/300 -- Iteration 23481 - Batch 369/963 - Train loss: 0.31932651  - Train acc: 0.8789 - Val loss: 1.08156776\n",
      "(4.80 min) Epoch 25/300 -- Iteration 23490 - Batch 378/963 - Train loss: 0.31946243  - Train acc: 0.8790 - Val loss: 1.08156776\n",
      "(4.80 min) Epoch 25/300 -- Iteration 23499 - Batch 387/963 - Train loss: 0.31936890  - Train acc: 0.8790 - Val loss: 1.08156776\n",
      "(4.80 min) Epoch 25/300 -- Iteration 23508 - Batch 396/963 - Train loss: 0.31963304  - Train acc: 0.8788 - Val loss: 1.08156776\n",
      "(4.80 min) Epoch 25/300 -- Iteration 23517 - Batch 405/963 - Train loss: 0.32020525  - Train acc: 0.8783 - Val loss: 1.08156776\n",
      "(4.80 min) Epoch 25/300 -- Iteration 23526 - Batch 414/963 - Train loss: 0.31965785  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.81 min) Epoch 25/300 -- Iteration 23535 - Batch 423/963 - Train loss: 0.31940678  - Train acc: 0.8789 - Val loss: 1.08156776\n",
      "(4.81 min) Epoch 25/300 -- Iteration 23544 - Batch 432/963 - Train loss: 0.31915466  - Train acc: 0.8789 - Val loss: 1.08156776\n",
      "(4.81 min) Epoch 25/300 -- Iteration 23553 - Batch 441/963 - Train loss: 0.31878878  - Train acc: 0.8792 - Val loss: 1.08156776\n",
      "(4.81 min) Epoch 25/300 -- Iteration 23562 - Batch 450/963 - Train loss: 0.31890030  - Train acc: 0.8791 - Val loss: 1.08156776\n",
      "(4.81 min) Epoch 25/300 -- Iteration 23571 - Batch 459/963 - Train loss: 0.31903595  - Train acc: 0.8790 - Val loss: 1.08156776\n",
      "(4.81 min) Epoch 25/300 -- Iteration 23580 - Batch 468/963 - Train loss: 0.31952680  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.82 min) Epoch 25/300 -- Iteration 23589 - Batch 477/963 - Train loss: 0.31933621  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.82 min) Epoch 25/300 -- Iteration 23598 - Batch 486/963 - Train loss: 0.32021138  - Train acc: 0.8783 - Val loss: 1.08156776\n",
      "(4.82 min) Epoch 25/300 -- Iteration 23607 - Batch 495/963 - Train loss: 0.32053796  - Train acc: 0.8781 - Val loss: 1.08156776\n",
      "(4.82 min) Epoch 25/300 -- Iteration 23616 - Batch 504/963 - Train loss: 0.32027284  - Train acc: 0.8783 - Val loss: 1.08156776\n",
      "(4.82 min) Epoch 25/300 -- Iteration 23625 - Batch 513/963 - Train loss: 0.32034598  - Train acc: 0.8783 - Val loss: 1.08156776\n",
      "(4.83 min) Epoch 25/300 -- Iteration 23634 - Batch 522/963 - Train loss: 0.32066928  - Train acc: 0.8781 - Val loss: 1.08156776\n",
      "(4.83 min) Epoch 25/300 -- Iteration 23643 - Batch 531/963 - Train loss: 0.32101210  - Train acc: 0.8780 - Val loss: 1.08156776\n",
      "(4.83 min) Epoch 25/300 -- Iteration 23652 - Batch 540/963 - Train loss: 0.32073691  - Train acc: 0.8783 - Val loss: 1.08156776\n",
      "(4.83 min) Epoch 25/300 -- Iteration 23661 - Batch 549/963 - Train loss: 0.32055771  - Train acc: 0.8783 - Val loss: 1.08156776\n",
      "(4.83 min) Epoch 25/300 -- Iteration 23670 - Batch 558/963 - Train loss: 0.32037912  - Train acc: 0.8784 - Val loss: 1.08156776\n",
      "(4.83 min) Epoch 25/300 -- Iteration 23679 - Batch 567/963 - Train loss: 0.32030694  - Train acc: 0.8784 - Val loss: 1.08156776\n",
      "(4.84 min) Epoch 25/300 -- Iteration 23688 - Batch 576/963 - Train loss: 0.32014633  - Train acc: 0.8784 - Val loss: 1.08156776\n",
      "(4.84 min) Epoch 25/300 -- Iteration 23697 - Batch 585/963 - Train loss: 0.32026773  - Train acc: 0.8784 - Val loss: 1.08156776\n",
      "(4.84 min) Epoch 25/300 -- Iteration 23706 - Batch 594/963 - Train loss: 0.31985460  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.84 min) Epoch 25/300 -- Iteration 23715 - Batch 603/963 - Train loss: 0.31995272  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.84 min) Epoch 25/300 -- Iteration 23724 - Batch 612/963 - Train loss: 0.31984183  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.85 min) Epoch 25/300 -- Iteration 23733 - Batch 621/963 - Train loss: 0.32012697  - Train acc: 0.8784 - Val loss: 1.08156776\n",
      "(4.85 min) Epoch 25/300 -- Iteration 23742 - Batch 630/963 - Train loss: 0.32007506  - Train acc: 0.8784 - Val loss: 1.08156776\n",
      "(4.85 min) Epoch 25/300 -- Iteration 23751 - Batch 639/963 - Train loss: 0.31935046  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.85 min) Epoch 25/300 -- Iteration 23760 - Batch 648/963 - Train loss: 0.31907925  - Train acc: 0.8790 - Val loss: 1.08156776\n",
      "(4.85 min) Epoch 25/300 -- Iteration 23769 - Batch 657/963 - Train loss: 0.31915812  - Train acc: 0.8789 - Val loss: 1.08156776\n",
      "(4.85 min) Epoch 25/300 -- Iteration 23778 - Batch 666/963 - Train loss: 0.31876133  - Train acc: 0.8791 - Val loss: 1.08156776\n",
      "(4.86 min) Epoch 25/300 -- Iteration 23787 - Batch 675/963 - Train loss: 0.31863664  - Train acc: 0.8792 - Val loss: 1.08156776\n",
      "(4.86 min) Epoch 25/300 -- Iteration 23796 - Batch 684/963 - Train loss: 0.31850520  - Train acc: 0.8792 - Val loss: 1.08156776\n",
      "(4.86 min) Epoch 25/300 -- Iteration 23805 - Batch 693/963 - Train loss: 0.31846209  - Train acc: 0.8793 - Val loss: 1.08156776\n",
      "(4.86 min) Epoch 25/300 -- Iteration 23814 - Batch 702/963 - Train loss: 0.31815992  - Train acc: 0.8795 - Val loss: 1.08156776\n",
      "(4.86 min) Epoch 25/300 -- Iteration 23823 - Batch 711/963 - Train loss: 0.31824039  - Train acc: 0.8794 - Val loss: 1.08156776\n",
      "(4.87 min) Epoch 25/300 -- Iteration 23832 - Batch 720/963 - Train loss: 0.31819920  - Train acc: 0.8794 - Val loss: 1.08156776\n",
      "(4.87 min) Epoch 25/300 -- Iteration 23841 - Batch 729/963 - Train loss: 0.31793806  - Train acc: 0.8795 - Val loss: 1.08156776\n",
      "(4.87 min) Epoch 25/300 -- Iteration 23850 - Batch 738/963 - Train loss: 0.31797049  - Train acc: 0.8794 - Val loss: 1.08156776\n",
      "(4.87 min) Epoch 25/300 -- Iteration 23859 - Batch 747/963 - Train loss: 0.31839704  - Train acc: 0.8792 - Val loss: 1.08156776\n",
      "(4.87 min) Epoch 25/300 -- Iteration 23868 - Batch 756/963 - Train loss: 0.31858121  - Train acc: 0.8791 - Val loss: 1.08156776\n",
      "(4.87 min) Epoch 25/300 -- Iteration 23877 - Batch 765/963 - Train loss: 0.31878695  - Train acc: 0.8790 - Val loss: 1.08156776\n",
      "(4.88 min) Epoch 25/300 -- Iteration 23886 - Batch 774/963 - Train loss: 0.31877175  - Train acc: 0.8790 - Val loss: 1.08156776\n",
      "(4.88 min) Epoch 25/300 -- Iteration 23895 - Batch 783/963 - Train loss: 0.31919247  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.88 min) Epoch 25/300 -- Iteration 23904 - Batch 792/963 - Train loss: 0.31956165  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.88 min) Epoch 25/300 -- Iteration 23913 - Batch 801/963 - Train loss: 0.31952531  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.88 min) Epoch 25/300 -- Iteration 23922 - Batch 810/963 - Train loss: 0.31964255  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.89 min) Epoch 25/300 -- Iteration 23931 - Batch 819/963 - Train loss: 0.31951561  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.89 min) Epoch 25/300 -- Iteration 23940 - Batch 828/963 - Train loss: 0.31982392  - Train acc: 0.8785 - Val loss: 1.08156776\n",
      "(4.89 min) Epoch 25/300 -- Iteration 23949 - Batch 837/963 - Train loss: 0.31977318  - Train acc: 0.8786 - Val loss: 1.08156776\n",
      "(4.89 min) Epoch 25/300 -- Iteration 23958 - Batch 846/963 - Train loss: 0.32005510  - Train acc: 0.8785 - Val loss: 1.08156776\n",
      "(4.89 min) Epoch 25/300 -- Iteration 23967 - Batch 855/963 - Train loss: 0.31983268  - Train acc: 0.8786 - Val loss: 1.08156776\n",
      "(4.89 min) Epoch 25/300 -- Iteration 23976 - Batch 864/963 - Train loss: 0.31958799  - Train acc: 0.8786 - Val loss: 1.08156776\n",
      "(4.90 min) Epoch 25/300 -- Iteration 23985 - Batch 873/963 - Train loss: 0.31981794  - Train acc: 0.8785 - Val loss: 1.08156776\n",
      "(4.90 min) Epoch 25/300 -- Iteration 23994 - Batch 882/963 - Train loss: 0.31956666  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.90 min) Epoch 25/300 -- Iteration 24003 - Batch 891/963 - Train loss: 0.31950801  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.90 min) Epoch 25/300 -- Iteration 24012 - Batch 900/963 - Train loss: 0.31957186  - Train acc: 0.8787 - Val loss: 1.08156776\n",
      "(4.90 min) Epoch 25/300 -- Iteration 24021 - Batch 909/963 - Train loss: 0.31944426  - Train acc: 0.8789 - Val loss: 1.08156776\n",
      "(4.91 min) Epoch 25/300 -- Iteration 24030 - Batch 918/963 - Train loss: 0.31981561  - Train acc: 0.8788 - Val loss: 1.08156776\n",
      "(4.91 min) Epoch 25/300 -- Iteration 24039 - Batch 927/963 - Train loss: 0.32025441  - Train acc: 0.8786 - Val loss: 1.08156776\n",
      "(4.91 min) Epoch 25/300 -- Iteration 24048 - Batch 936/963 - Train loss: 0.32023624  - Train acc: 0.8786 - Val loss: 1.08156776\n",
      "(4.91 min) Epoch 25/300 -- Iteration 24057 - Batch 945/963 - Train loss: 0.32038488  - Train acc: 0.8785 - Val loss: 1.08156776\n",
      "(4.91 min) Epoch 25/300 -- Iteration 24066 - Batch 954/963 - Train loss: 0.32061394  - Train acc: 0.8785 - Val loss: 1.08156776\n",
      "(4.91 min) Epoch 25/300 -- Iteration 24075 - Batch 962/963 - Train loss: 0.32028125  - Train acc: 0.8787 - Val loss: 1.05357862 - Val acc: 0.6100\n",
      "(4.92 min) Epoch 26/300 -- Iteration 24084 - Batch 9/963 - Train loss: 0.30851848  - Train acc: 0.8766 - Val loss: 1.05357862\n",
      "(4.92 min) Epoch 26/300 -- Iteration 24093 - Batch 18/963 - Train loss: 0.31514211  - Train acc: 0.8795 - Val loss: 1.05357862\n",
      "(4.92 min) Epoch 26/300 -- Iteration 24102 - Batch 27/963 - Train loss: 0.32238546  - Train acc: 0.8742 - Val loss: 1.05357862\n",
      "(4.92 min) Epoch 26/300 -- Iteration 24111 - Batch 36/963 - Train loss: 0.32191042  - Train acc: 0.8748 - Val loss: 1.05357862\n",
      "(4.92 min) Epoch 26/300 -- Iteration 24120 - Batch 45/963 - Train loss: 0.32067263  - Train acc: 0.8758 - Val loss: 1.05357862\n",
      "(4.93 min) Epoch 26/300 -- Iteration 24129 - Batch 54/963 - Train loss: 0.32040586  - Train acc: 0.8763 - Val loss: 1.05357862\n",
      "(4.93 min) Epoch 26/300 -- Iteration 24138 - Batch 63/963 - Train loss: 0.32572573  - Train acc: 0.8741 - Val loss: 1.05357862\n",
      "(4.93 min) Epoch 26/300 -- Iteration 24147 - Batch 72/963 - Train loss: 0.32426342  - Train acc: 0.8749 - Val loss: 1.05357862\n",
      "(4.93 min) Epoch 26/300 -- Iteration 24156 - Batch 81/963 - Train loss: 0.32363538  - Train acc: 0.8753 - Val loss: 1.05357862\n",
      "(4.93 min) Epoch 26/300 -- Iteration 24165 - Batch 90/963 - Train loss: 0.32006509  - Train acc: 0.8777 - Val loss: 1.05357862\n",
      "(4.94 min) Epoch 26/300 -- Iteration 24174 - Batch 99/963 - Train loss: 0.31935499  - Train acc: 0.8779 - Val loss: 1.05357862\n",
      "(4.94 min) Epoch 26/300 -- Iteration 24183 - Batch 108/963 - Train loss: 0.32099489  - Train acc: 0.8774 - Val loss: 1.05357862\n",
      "(4.94 min) Epoch 26/300 -- Iteration 24192 - Batch 117/963 - Train loss: 0.32131000  - Train acc: 0.8776 - Val loss: 1.05357862\n",
      "(4.94 min) Epoch 26/300 -- Iteration 24201 - Batch 126/963 - Train loss: 0.32352484  - Train acc: 0.8773 - Val loss: 1.05357862\n",
      "(4.94 min) Epoch 26/300 -- Iteration 24210 - Batch 135/963 - Train loss: 0.32372907  - Train acc: 0.8772 - Val loss: 1.05357862\n",
      "(4.94 min) Epoch 26/300 -- Iteration 24219 - Batch 144/963 - Train loss: 0.32179811  - Train acc: 0.8779 - Val loss: 1.05357862\n",
      "(4.95 min) Epoch 26/300 -- Iteration 24228 - Batch 153/963 - Train loss: 0.31952804  - Train acc: 0.8786 - Val loss: 1.05357862\n",
      "(4.95 min) Epoch 26/300 -- Iteration 24237 - Batch 162/963 - Train loss: 0.32085057  - Train acc: 0.8786 - Val loss: 1.05357862\n",
      "(4.95 min) Epoch 26/300 -- Iteration 24246 - Batch 171/963 - Train loss: 0.32115128  - Train acc: 0.8787 - Val loss: 1.05357862\n",
      "(4.95 min) Epoch 26/300 -- Iteration 24255 - Batch 180/963 - Train loss: 0.32110705  - Train acc: 0.8787 - Val loss: 1.05357862\n",
      "(4.95 min) Epoch 26/300 -- Iteration 24264 - Batch 189/963 - Train loss: 0.32253126  - Train acc: 0.8780 - Val loss: 1.05357862\n",
      "(4.96 min) Epoch 26/300 -- Iteration 24273 - Batch 198/963 - Train loss: 0.32247003  - Train acc: 0.8779 - Val loss: 1.05357862\n",
      "(4.96 min) Epoch 26/300 -- Iteration 24282 - Batch 207/963 - Train loss: 0.32177793  - Train acc: 0.8780 - Val loss: 1.05357862\n",
      "(4.96 min) Epoch 26/300 -- Iteration 24291 - Batch 216/963 - Train loss: 0.32120598  - Train acc: 0.8781 - Val loss: 1.05357862\n",
      "(4.96 min) Epoch 26/300 -- Iteration 24300 - Batch 225/963 - Train loss: 0.31970147  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(4.96 min) Epoch 26/300 -- Iteration 24309 - Batch 234/963 - Train loss: 0.31824044  - Train acc: 0.8796 - Val loss: 1.05357862\n",
      "(4.96 min) Epoch 26/300 -- Iteration 24318 - Batch 243/963 - Train loss: 0.31777214  - Train acc: 0.8796 - Val loss: 1.05357862\n",
      "(4.97 min) Epoch 26/300 -- Iteration 24327 - Batch 252/963 - Train loss: 0.31827597  - Train acc: 0.8794 - Val loss: 1.05357862\n",
      "(4.97 min) Epoch 26/300 -- Iteration 24336 - Batch 261/963 - Train loss: 0.31885636  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(4.97 min) Epoch 26/300 -- Iteration 24345 - Batch 270/963 - Train loss: 0.31882497  - Train acc: 0.8793 - Val loss: 1.05357862\n",
      "(4.97 min) Epoch 26/300 -- Iteration 24354 - Batch 279/963 - Train loss: 0.31868054  - Train acc: 0.8795 - Val loss: 1.05357862\n",
      "(4.97 min) Epoch 26/300 -- Iteration 24363 - Batch 288/963 - Train loss: 0.31935525  - Train acc: 0.8793 - Val loss: 1.05357862\n",
      "(4.98 min) Epoch 26/300 -- Iteration 24372 - Batch 297/963 - Train loss: 0.31900113  - Train acc: 0.8794 - Val loss: 1.05357862\n",
      "(4.98 min) Epoch 26/300 -- Iteration 24381 - Batch 306/963 - Train loss: 0.31873084  - Train acc: 0.8795 - Val loss: 1.05357862\n",
      "(4.98 min) Epoch 26/300 -- Iteration 24390 - Batch 315/963 - Train loss: 0.31939448  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(4.98 min) Epoch 26/300 -- Iteration 24399 - Batch 324/963 - Train loss: 0.31873923  - Train acc: 0.8797 - Val loss: 1.05357862\n",
      "(4.98 min) Epoch 26/300 -- Iteration 24408 - Batch 333/963 - Train loss: 0.31853809  - Train acc: 0.8799 - Val loss: 1.05357862\n",
      "(4.99 min) Epoch 26/300 -- Iteration 24417 - Batch 342/963 - Train loss: 0.31899784  - Train acc: 0.8795 - Val loss: 1.05357862\n",
      "(4.99 min) Epoch 26/300 -- Iteration 24426 - Batch 351/963 - Train loss: 0.31928302  - Train acc: 0.8797 - Val loss: 1.05357862\n",
      "(4.99 min) Epoch 26/300 -- Iteration 24435 - Batch 360/963 - Train loss: 0.31894858  - Train acc: 0.8798 - Val loss: 1.05357862\n",
      "(4.99 min) Epoch 26/300 -- Iteration 24444 - Batch 369/963 - Train loss: 0.31888986  - Train acc: 0.8797 - Val loss: 1.05357862\n",
      "(4.99 min) Epoch 26/300 -- Iteration 24453 - Batch 378/963 - Train loss: 0.31815977  - Train acc: 0.8797 - Val loss: 1.05357862\n",
      "(4.99 min) Epoch 26/300 -- Iteration 24462 - Batch 387/963 - Train loss: 0.31741242  - Train acc: 0.8799 - Val loss: 1.05357862\n",
      "(5.00 min) Epoch 26/300 -- Iteration 24471 - Batch 396/963 - Train loss: 0.31727845  - Train acc: 0.8800 - Val loss: 1.05357862\n",
      "(5.00 min) Epoch 26/300 -- Iteration 24480 - Batch 405/963 - Train loss: 0.31787032  - Train acc: 0.8798 - Val loss: 1.05357862\n",
      "(5.00 min) Epoch 26/300 -- Iteration 24489 - Batch 414/963 - Train loss: 0.31739328  - Train acc: 0.8800 - Val loss: 1.05357862\n",
      "(5.00 min) Epoch 26/300 -- Iteration 24498 - Batch 423/963 - Train loss: 0.31731801  - Train acc: 0.8800 - Val loss: 1.05357862\n",
      "(5.00 min) Epoch 26/300 -- Iteration 24507 - Batch 432/963 - Train loss: 0.31704119  - Train acc: 0.8800 - Val loss: 1.05357862\n",
      "(5.01 min) Epoch 26/300 -- Iteration 24516 - Batch 441/963 - Train loss: 0.31734257  - Train acc: 0.8797 - Val loss: 1.05357862\n",
      "(5.01 min) Epoch 26/300 -- Iteration 24525 - Batch 450/963 - Train loss: 0.31721220  - Train acc: 0.8798 - Val loss: 1.05357862\n",
      "(5.01 min) Epoch 26/300 -- Iteration 24534 - Batch 459/963 - Train loss: 0.31822800  - Train acc: 0.8793 - Val loss: 1.05357862\n",
      "(5.01 min) Epoch 26/300 -- Iteration 24543 - Batch 468/963 - Train loss: 0.31772047  - Train acc: 0.8795 - Val loss: 1.05357862\n",
      "(5.01 min) Epoch 26/300 -- Iteration 24552 - Batch 477/963 - Train loss: 0.31777727  - Train acc: 0.8794 - Val loss: 1.05357862\n",
      "(5.01 min) Epoch 26/300 -- Iteration 24561 - Batch 486/963 - Train loss: 0.31821809  - Train acc: 0.8793 - Val loss: 1.05357862\n",
      "(5.02 min) Epoch 26/300 -- Iteration 24570 - Batch 495/963 - Train loss: 0.31813195  - Train acc: 0.8793 - Val loss: 1.05357862\n",
      "(5.02 min) Epoch 26/300 -- Iteration 24579 - Batch 504/963 - Train loss: 0.31790201  - Train acc: 0.8794 - Val loss: 1.05357862\n",
      "(5.02 min) Epoch 26/300 -- Iteration 24588 - Batch 513/963 - Train loss: 0.31801593  - Train acc: 0.8793 - Val loss: 1.05357862\n",
      "(5.02 min) Epoch 26/300 -- Iteration 24597 - Batch 522/963 - Train loss: 0.31782017  - Train acc: 0.8794 - Val loss: 1.05357862\n",
      "(5.02 min) Epoch 26/300 -- Iteration 24606 - Batch 531/963 - Train loss: 0.31773378  - Train acc: 0.8794 - Val loss: 1.05357862\n",
      "(5.03 min) Epoch 26/300 -- Iteration 24615 - Batch 540/963 - Train loss: 0.31809640  - Train acc: 0.8792 - Val loss: 1.05357862\n",
      "(5.03 min) Epoch 26/300 -- Iteration 24624 - Batch 549/963 - Train loss: 0.31794996  - Train acc: 0.8792 - Val loss: 1.05357862\n",
      "(5.03 min) Epoch 26/300 -- Iteration 24633 - Batch 558/963 - Train loss: 0.31807569  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.03 min) Epoch 26/300 -- Iteration 24642 - Batch 567/963 - Train loss: 0.31821567  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.03 min) Epoch 26/300 -- Iteration 24651 - Batch 576/963 - Train loss: 0.31813996  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.03 min) Epoch 26/300 -- Iteration 24660 - Batch 585/963 - Train loss: 0.31827554  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.04 min) Epoch 26/300 -- Iteration 24669 - Batch 594/963 - Train loss: 0.31836117  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.04 min) Epoch 26/300 -- Iteration 24678 - Batch 603/963 - Train loss: 0.31861783  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.04 min) Epoch 26/300 -- Iteration 24687 - Batch 612/963 - Train loss: 0.31900386  - Train acc: 0.8788 - Val loss: 1.05357862\n",
      "(5.04 min) Epoch 26/300 -- Iteration 24696 - Batch 621/963 - Train loss: 0.31922548  - Train acc: 0.8787 - Val loss: 1.05357862\n",
      "(5.04 min) Epoch 26/300 -- Iteration 24705 - Batch 630/963 - Train loss: 0.31878350  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(5.05 min) Epoch 26/300 -- Iteration 24714 - Batch 639/963 - Train loss: 0.31908706  - Train acc: 0.8787 - Val loss: 1.05357862\n",
      "(5.05 min) Epoch 26/300 -- Iteration 24723 - Batch 648/963 - Train loss: 0.31924940  - Train acc: 0.8786 - Val loss: 1.05357862\n",
      "(5.05 min) Epoch 26/300 -- Iteration 24732 - Batch 657/963 - Train loss: 0.31912632  - Train acc: 0.8787 - Val loss: 1.05357862\n",
      "(5.05 min) Epoch 26/300 -- Iteration 24741 - Batch 666/963 - Train loss: 0.31862499  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(5.05 min) Epoch 26/300 -- Iteration 24750 - Batch 675/963 - Train loss: 0.31862022  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(5.05 min) Epoch 26/300 -- Iteration 24759 - Batch 684/963 - Train loss: 0.31918051  - Train acc: 0.8786 - Val loss: 1.05357862\n",
      "(5.06 min) Epoch 26/300 -- Iteration 24768 - Batch 693/963 - Train loss: 0.31893014  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(5.06 min) Epoch 26/300 -- Iteration 24777 - Batch 702/963 - Train loss: 0.31885606  - Train acc: 0.8788 - Val loss: 1.05357862\n",
      "(5.06 min) Epoch 26/300 -- Iteration 24786 - Batch 711/963 - Train loss: 0.31879708  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(5.06 min) Epoch 26/300 -- Iteration 24795 - Batch 720/963 - Train loss: 0.31869100  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(5.06 min) Epoch 26/300 -- Iteration 24804 - Batch 729/963 - Train loss: 0.31868915  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.07 min) Epoch 26/300 -- Iteration 24813 - Batch 738/963 - Train loss: 0.31873889  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(5.07 min) Epoch 26/300 -- Iteration 24822 - Batch 747/963 - Train loss: 0.31845091  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.07 min) Epoch 26/300 -- Iteration 24831 - Batch 756/963 - Train loss: 0.31840132  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.07 min) Epoch 26/300 -- Iteration 24840 - Batch 765/963 - Train loss: 0.31853387  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(5.07 min) Epoch 26/300 -- Iteration 24849 - Batch 774/963 - Train loss: 0.31838527  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.08 min) Epoch 26/300 -- Iteration 24858 - Batch 783/963 - Train loss: 0.31845855  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.08 min) Epoch 26/300 -- Iteration 24867 - Batch 792/963 - Train loss: 0.31851841  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.08 min) Epoch 26/300 -- Iteration 24876 - Batch 801/963 - Train loss: 0.31856975  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.08 min) Epoch 26/300 -- Iteration 24885 - Batch 810/963 - Train loss: 0.31823739  - Train acc: 0.8792 - Val loss: 1.05357862\n",
      "(5.08 min) Epoch 26/300 -- Iteration 24894 - Batch 819/963 - Train loss: 0.31863983  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.08 min) Epoch 26/300 -- Iteration 24903 - Batch 828/963 - Train loss: 0.31865691  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.09 min) Epoch 26/300 -- Iteration 24912 - Batch 837/963 - Train loss: 0.31853832  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.09 min) Epoch 26/300 -- Iteration 24921 - Batch 846/963 - Train loss: 0.31853240  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.09 min) Epoch 26/300 -- Iteration 24930 - Batch 855/963 - Train loss: 0.31846959  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.09 min) Epoch 26/300 -- Iteration 24939 - Batch 864/963 - Train loss: 0.31829002  - Train acc: 0.8792 - Val loss: 1.05357862\n",
      "(5.09 min) Epoch 26/300 -- Iteration 24948 - Batch 873/963 - Train loss: 0.31815316  - Train acc: 0.8793 - Val loss: 1.05357862\n",
      "(5.10 min) Epoch 26/300 -- Iteration 24957 - Batch 882/963 - Train loss: 0.31833789  - Train acc: 0.8792 - Val loss: 1.05357862\n",
      "(5.10 min) Epoch 26/300 -- Iteration 24966 - Batch 891/963 - Train loss: 0.31833385  - Train acc: 0.8792 - Val loss: 1.05357862\n",
      "(5.10 min) Epoch 26/300 -- Iteration 24975 - Batch 900/963 - Train loss: 0.31856146  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.10 min) Epoch 26/300 -- Iteration 24984 - Batch 909/963 - Train loss: 0.31858696  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.10 min) Epoch 26/300 -- Iteration 24993 - Batch 918/963 - Train loss: 0.31870822  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.10 min) Epoch 26/300 -- Iteration 25002 - Batch 927/963 - Train loss: 0.31881509  - Train acc: 0.8789 - Val loss: 1.05357862\n",
      "(5.11 min) Epoch 26/300 -- Iteration 25011 - Batch 936/963 - Train loss: 0.31877641  - Train acc: 0.8790 - Val loss: 1.05357862\n",
      "(5.11 min) Epoch 26/300 -- Iteration 25020 - Batch 945/963 - Train loss: 0.31863010  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.11 min) Epoch 26/300 -- Iteration 25029 - Batch 954/963 - Train loss: 0.31850966  - Train acc: 0.8791 - Val loss: 1.05357862\n",
      "(5.11 min) Epoch 26/300 -- Iteration 25038 - Batch 962/963 - Train loss: 0.31829564  - Train acc: 0.8792 - Val loss: 1.06673455 - Val acc: 0.6050\n",
      "(5.11 min) Epoch 27/300 -- Iteration 25047 - Batch 9/963 - Train loss: 0.30779116  - Train acc: 0.8875 - Val loss: 1.06673455\n",
      "(5.12 min) Epoch 27/300 -- Iteration 25056 - Batch 18/963 - Train loss: 0.30532275  - Train acc: 0.8836 - Val loss: 1.06673455\n",
      "(5.12 min) Epoch 27/300 -- Iteration 25065 - Batch 27/963 - Train loss: 0.30298860  - Train acc: 0.8864 - Val loss: 1.06673455\n",
      "(5.12 min) Epoch 27/300 -- Iteration 25074 - Batch 36/963 - Train loss: 0.30909130  - Train acc: 0.8809 - Val loss: 1.06673455\n",
      "(5.12 min) Epoch 27/300 -- Iteration 25083 - Batch 45/963 - Train loss: 0.31732620  - Train acc: 0.8769 - Val loss: 1.06673455\n",
      "(5.12 min) Epoch 27/300 -- Iteration 25092 - Batch 54/963 - Train loss: 0.31873291  - Train acc: 0.8767 - Val loss: 1.06673455\n",
      "(5.13 min) Epoch 27/300 -- Iteration 25101 - Batch 63/963 - Train loss: 0.32039043  - Train acc: 0.8772 - Val loss: 1.06673455\n",
      "(5.13 min) Epoch 27/300 -- Iteration 25110 - Batch 72/963 - Train loss: 0.32053819  - Train acc: 0.8766 - Val loss: 1.06673455\n",
      "(5.13 min) Epoch 27/300 -- Iteration 25119 - Batch 81/963 - Train loss: 0.31746986  - Train acc: 0.8779 - Val loss: 1.06673455\n",
      "(5.13 min) Epoch 27/300 -- Iteration 25128 - Batch 90/963 - Train loss: 0.31542720  - Train acc: 0.8790 - Val loss: 1.06673455\n",
      "(5.13 min) Epoch 27/300 -- Iteration 25137 - Batch 99/963 - Train loss: 0.31581434  - Train acc: 0.8798 - Val loss: 1.06673455\n",
      "(5.13 min) Epoch 27/300 -- Iteration 25146 - Batch 108/963 - Train loss: 0.31569397  - Train acc: 0.8796 - Val loss: 1.06673455\n",
      "(5.14 min) Epoch 27/300 -- Iteration 25155 - Batch 117/963 - Train loss: 0.31309111  - Train acc: 0.8806 - Val loss: 1.06673455\n",
      "(5.14 min) Epoch 27/300 -- Iteration 25164 - Batch 126/963 - Train loss: 0.31430687  - Train acc: 0.8807 - Val loss: 1.06673455\n",
      "(5.14 min) Epoch 27/300 -- Iteration 25173 - Batch 135/963 - Train loss: 0.31523437  - Train acc: 0.8798 - Val loss: 1.06673455\n",
      "(5.14 min) Epoch 27/300 -- Iteration 25182 - Batch 144/963 - Train loss: 0.31651940  - Train acc: 0.8795 - Val loss: 1.06673455\n",
      "(5.14 min) Epoch 27/300 -- Iteration 25191 - Batch 153/963 - Train loss: 0.31601829  - Train acc: 0.8798 - Val loss: 1.06673455\n",
      "(5.15 min) Epoch 27/300 -- Iteration 25200 - Batch 162/963 - Train loss: 0.31748476  - Train acc: 0.8791 - Val loss: 1.06673455\n",
      "(5.15 min) Epoch 27/300 -- Iteration 25209 - Batch 171/963 - Train loss: 0.31610220  - Train acc: 0.8795 - Val loss: 1.06673455\n",
      "(5.15 min) Epoch 27/300 -- Iteration 25218 - Batch 180/963 - Train loss: 0.31762708  - Train acc: 0.8781 - Val loss: 1.06673455\n",
      "(5.15 min) Epoch 27/300 -- Iteration 25227 - Batch 189/963 - Train loss: 0.31718580  - Train acc: 0.8787 - Val loss: 1.06673455\n",
      "(5.15 min) Epoch 27/300 -- Iteration 25236 - Batch 198/963 - Train loss: 0.31750698  - Train acc: 0.8786 - Val loss: 1.06673455\n",
      "(5.15 min) Epoch 27/300 -- Iteration 25245 - Batch 207/963 - Train loss: 0.31876312  - Train acc: 0.8781 - Val loss: 1.06673455\n",
      "(5.16 min) Epoch 27/300 -- Iteration 25254 - Batch 216/963 - Train loss: 0.31930418  - Train acc: 0.8776 - Val loss: 1.06673455\n",
      "(5.16 min) Epoch 27/300 -- Iteration 25263 - Batch 225/963 - Train loss: 0.31982879  - Train acc: 0.8772 - Val loss: 1.06673455\n",
      "(5.16 min) Epoch 27/300 -- Iteration 25272 - Batch 234/963 - Train loss: 0.31953519  - Train acc: 0.8776 - Val loss: 1.06673455\n",
      "(5.16 min) Epoch 27/300 -- Iteration 25281 - Batch 243/963 - Train loss: 0.31789879  - Train acc: 0.8784 - Val loss: 1.06673455\n",
      "(5.16 min) Epoch 27/300 -- Iteration 25290 - Batch 252/963 - Train loss: 0.31750145  - Train acc: 0.8786 - Val loss: 1.06673455\n",
      "(5.17 min) Epoch 27/300 -- Iteration 25299 - Batch 261/963 - Train loss: 0.31757665  - Train acc: 0.8787 - Val loss: 1.06673455\n",
      "(5.17 min) Epoch 27/300 -- Iteration 25308 - Batch 270/963 - Train loss: 0.31794434  - Train acc: 0.8789 - Val loss: 1.06673455\n",
      "(5.17 min) Epoch 27/300 -- Iteration 25317 - Batch 279/963 - Train loss: 0.31788548  - Train acc: 0.8789 - Val loss: 1.06673455\n",
      "(5.17 min) Epoch 27/300 -- Iteration 25326 - Batch 288/963 - Train loss: 0.31766064  - Train acc: 0.8788 - Val loss: 1.06673455\n",
      "(5.17 min) Epoch 27/300 -- Iteration 25335 - Batch 297/963 - Train loss: 0.31658184  - Train acc: 0.8793 - Val loss: 1.06673455\n",
      "(5.18 min) Epoch 27/300 -- Iteration 25344 - Batch 306/963 - Train loss: 0.31680545  - Train acc: 0.8795 - Val loss: 1.06673455\n",
      "(5.18 min) Epoch 27/300 -- Iteration 25353 - Batch 315/963 - Train loss: 0.31603044  - Train acc: 0.8798 - Val loss: 1.06673455\n",
      "(5.18 min) Epoch 27/300 -- Iteration 25362 - Batch 324/963 - Train loss: 0.31582235  - Train acc: 0.8798 - Val loss: 1.06673455\n",
      "(5.18 min) Epoch 27/300 -- Iteration 25371 - Batch 333/963 - Train loss: 0.31542253  - Train acc: 0.8800 - Val loss: 1.06673455\n",
      "(5.18 min) Epoch 27/300 -- Iteration 25380 - Batch 342/963 - Train loss: 0.31614332  - Train acc: 0.8797 - Val loss: 1.06673455\n",
      "(5.18 min) Epoch 27/300 -- Iteration 25389 - Batch 351/963 - Train loss: 0.31666105  - Train acc: 0.8794 - Val loss: 1.06673455\n",
      "(5.19 min) Epoch 27/300 -- Iteration 25398 - Batch 360/963 - Train loss: 0.31765509  - Train acc: 0.8791 - Val loss: 1.06673455\n",
      "(5.19 min) Epoch 27/300 -- Iteration 25407 - Batch 369/963 - Train loss: 0.31765895  - Train acc: 0.8791 - Val loss: 1.06673455\n",
      "(5.19 min) Epoch 27/300 -- Iteration 25416 - Batch 378/963 - Train loss: 0.31714108  - Train acc: 0.8795 - Val loss: 1.06673455\n",
      "(5.19 min) Epoch 27/300 -- Iteration 25425 - Batch 387/963 - Train loss: 0.31737987  - Train acc: 0.8792 - Val loss: 1.06673455\n",
      "(5.19 min) Epoch 27/300 -- Iteration 25434 - Batch 396/963 - Train loss: 0.31696133  - Train acc: 0.8796 - Val loss: 1.06673455\n",
      "(5.20 min) Epoch 27/300 -- Iteration 25443 - Batch 405/963 - Train loss: 0.31649376  - Train acc: 0.8797 - Val loss: 1.06673455\n",
      "(5.20 min) Epoch 27/300 -- Iteration 25452 - Batch 414/963 - Train loss: 0.31688771  - Train acc: 0.8797 - Val loss: 1.06673455\n",
      "(5.20 min) Epoch 27/300 -- Iteration 25461 - Batch 423/963 - Train loss: 0.31631495  - Train acc: 0.8799 - Val loss: 1.06673455\n",
      "(5.20 min) Epoch 27/300 -- Iteration 25470 - Batch 432/963 - Train loss: 0.31601135  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.20 min) Epoch 27/300 -- Iteration 25479 - Batch 441/963 - Train loss: 0.31570293  - Train acc: 0.8804 - Val loss: 1.06673455\n",
      "(5.20 min) Epoch 27/300 -- Iteration 25488 - Batch 450/963 - Train loss: 0.31557244  - Train acc: 0.8806 - Val loss: 1.06673455\n",
      "(5.21 min) Epoch 27/300 -- Iteration 25497 - Batch 459/963 - Train loss: 0.31535710  - Train acc: 0.8807 - Val loss: 1.06673455\n",
      "(5.21 min) Epoch 27/300 -- Iteration 25506 - Batch 468/963 - Train loss: 0.31569448  - Train acc: 0.8804 - Val loss: 1.06673455\n",
      "(5.21 min) Epoch 27/300 -- Iteration 25515 - Batch 477/963 - Train loss: 0.31573989  - Train acc: 0.8804 - Val loss: 1.06673455\n",
      "(5.21 min) Epoch 27/300 -- Iteration 25524 - Batch 486/963 - Train loss: 0.31576395  - Train acc: 0.8805 - Val loss: 1.06673455\n",
      "(5.21 min) Epoch 27/300 -- Iteration 25533 - Batch 495/963 - Train loss: 0.31587408  - Train acc: 0.8807 - Val loss: 1.06673455\n",
      "(5.22 min) Epoch 27/300 -- Iteration 25542 - Batch 504/963 - Train loss: 0.31588117  - Train acc: 0.8807 - Val loss: 1.06673455\n",
      "(5.22 min) Epoch 27/300 -- Iteration 25551 - Batch 513/963 - Train loss: 0.31629633  - Train acc: 0.8807 - Val loss: 1.06673455\n",
      "(5.22 min) Epoch 27/300 -- Iteration 25560 - Batch 522/963 - Train loss: 0.31647536  - Train acc: 0.8805 - Val loss: 1.06673455\n",
      "(5.22 min) Epoch 27/300 -- Iteration 25569 - Batch 531/963 - Train loss: 0.31663431  - Train acc: 0.8804 - Val loss: 1.06673455\n",
      "(5.22 min) Epoch 27/300 -- Iteration 25578 - Batch 540/963 - Train loss: 0.31662830  - Train acc: 0.8805 - Val loss: 1.06673455\n",
      "(5.22 min) Epoch 27/300 -- Iteration 25587 - Batch 549/963 - Train loss: 0.31656525  - Train acc: 0.8806 - Val loss: 1.06673455\n",
      "(5.23 min) Epoch 27/300 -- Iteration 25596 - Batch 558/963 - Train loss: 0.31679071  - Train acc: 0.8807 - Val loss: 1.06673455\n",
      "(5.23 min) Epoch 27/300 -- Iteration 25605 - Batch 567/963 - Train loss: 0.31682934  - Train acc: 0.8806 - Val loss: 1.06673455\n",
      "(5.23 min) Epoch 27/300 -- Iteration 25614 - Batch 576/963 - Train loss: 0.31636945  - Train acc: 0.8807 - Val loss: 1.06673455\n",
      "(5.23 min) Epoch 27/300 -- Iteration 25623 - Batch 585/963 - Train loss: 0.31605856  - Train acc: 0.8808 - Val loss: 1.06673455\n",
      "(5.23 min) Epoch 27/300 -- Iteration 25632 - Batch 594/963 - Train loss: 0.31576254  - Train acc: 0.8808 - Val loss: 1.06673455\n",
      "(5.24 min) Epoch 27/300 -- Iteration 25641 - Batch 603/963 - Train loss: 0.31593335  - Train acc: 0.8807 - Val loss: 1.06673455\n",
      "(5.24 min) Epoch 27/300 -- Iteration 25650 - Batch 612/963 - Train loss: 0.31571494  - Train acc: 0.8807 - Val loss: 1.06673455\n",
      "(5.24 min) Epoch 27/300 -- Iteration 25659 - Batch 621/963 - Train loss: 0.31547215  - Train acc: 0.8808 - Val loss: 1.06673455\n",
      "(5.24 min) Epoch 27/300 -- Iteration 25668 - Batch 630/963 - Train loss: 0.31526968  - Train acc: 0.8808 - Val loss: 1.06673455\n",
      "(5.24 min) Epoch 27/300 -- Iteration 25677 - Batch 639/963 - Train loss: 0.31556821  - Train acc: 0.8805 - Val loss: 1.06673455\n",
      "(5.24 min) Epoch 27/300 -- Iteration 25686 - Batch 648/963 - Train loss: 0.31550160  - Train acc: 0.8805 - Val loss: 1.06673455\n",
      "(5.25 min) Epoch 27/300 -- Iteration 25695 - Batch 657/963 - Train loss: 0.31567940  - Train acc: 0.8803 - Val loss: 1.06673455\n",
      "(5.25 min) Epoch 27/300 -- Iteration 25704 - Batch 666/963 - Train loss: 0.31542891  - Train acc: 0.8804 - Val loss: 1.06673455\n",
      "(5.25 min) Epoch 27/300 -- Iteration 25713 - Batch 675/963 - Train loss: 0.31518866  - Train acc: 0.8804 - Val loss: 1.06673455\n",
      "(5.25 min) Epoch 27/300 -- Iteration 25722 - Batch 684/963 - Train loss: 0.31519915  - Train acc: 0.8805 - Val loss: 1.06673455\n",
      "(5.25 min) Epoch 27/300 -- Iteration 25731 - Batch 693/963 - Train loss: 0.31541572  - Train acc: 0.8804 - Val loss: 1.06673455\n",
      "(5.26 min) Epoch 27/300 -- Iteration 25740 - Batch 702/963 - Train loss: 0.31497686  - Train acc: 0.8806 - Val loss: 1.06673455\n",
      "(5.26 min) Epoch 27/300 -- Iteration 25749 - Batch 711/963 - Train loss: 0.31501163  - Train acc: 0.8806 - Val loss: 1.06673455\n",
      "(5.26 min) Epoch 27/300 -- Iteration 25758 - Batch 720/963 - Train loss: 0.31551364  - Train acc: 0.8804 - Val loss: 1.06673455\n",
      "(5.26 min) Epoch 27/300 -- Iteration 25767 - Batch 729/963 - Train loss: 0.31581786  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.26 min) Epoch 27/300 -- Iteration 25776 - Batch 738/963 - Train loss: 0.31587664  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.27 min) Epoch 27/300 -- Iteration 25785 - Batch 747/963 - Train loss: 0.31599858  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.27 min) Epoch 27/300 -- Iteration 25794 - Batch 756/963 - Train loss: 0.31604840  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.27 min) Epoch 27/300 -- Iteration 25803 - Batch 765/963 - Train loss: 0.31609903  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.27 min) Epoch 27/300 -- Iteration 25812 - Batch 774/963 - Train loss: 0.31621389  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.27 min) Epoch 27/300 -- Iteration 25821 - Batch 783/963 - Train loss: 0.31613594  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.27 min) Epoch 27/300 -- Iteration 25830 - Batch 792/963 - Train loss: 0.31619961  - Train acc: 0.8801 - Val loss: 1.06673455\n",
      "(5.28 min) Epoch 27/300 -- Iteration 25839 - Batch 801/963 - Train loss: 0.31606921  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.28 min) Epoch 27/300 -- Iteration 25848 - Batch 810/963 - Train loss: 0.31588215  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.28 min) Epoch 27/300 -- Iteration 25857 - Batch 819/963 - Train loss: 0.31581716  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.28 min) Epoch 27/300 -- Iteration 25866 - Batch 828/963 - Train loss: 0.31582279  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.28 min) Epoch 27/300 -- Iteration 25875 - Batch 837/963 - Train loss: 0.31548395  - Train acc: 0.8803 - Val loss: 1.06673455\n",
      "(5.29 min) Epoch 27/300 -- Iteration 25884 - Batch 846/963 - Train loss: 0.31594885  - Train acc: 0.8801 - Val loss: 1.06673455\n",
      "(5.29 min) Epoch 27/300 -- Iteration 25893 - Batch 855/963 - Train loss: 0.31597537  - Train acc: 0.8801 - Val loss: 1.06673455\n",
      "(5.29 min) Epoch 27/300 -- Iteration 25902 - Batch 864/963 - Train loss: 0.31575833  - Train acc: 0.8801 - Val loss: 1.06673455\n",
      "(5.29 min) Epoch 27/300 -- Iteration 25911 - Batch 873/963 - Train loss: 0.31556615  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.29 min) Epoch 27/300 -- Iteration 25920 - Batch 882/963 - Train loss: 0.31550931  - Train acc: 0.8803 - Val loss: 1.06673455\n",
      "(5.29 min) Epoch 27/300 -- Iteration 25929 - Batch 891/963 - Train loss: 0.31547866  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.30 min) Epoch 27/300 -- Iteration 25938 - Batch 900/963 - Train loss: 0.31556972  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.30 min) Epoch 27/300 -- Iteration 25947 - Batch 909/963 - Train loss: 0.31582244  - Train acc: 0.8801 - Val loss: 1.06673455\n",
      "(5.30 min) Epoch 27/300 -- Iteration 25956 - Batch 918/963 - Train loss: 0.31553265  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.30 min) Epoch 27/300 -- Iteration 25965 - Batch 927/963 - Train loss: 0.31542702  - Train acc: 0.8802 - Val loss: 1.06673455\n",
      "(5.30 min) Epoch 27/300 -- Iteration 25974 - Batch 936/963 - Train loss: 0.31529230  - Train acc: 0.8803 - Val loss: 1.06673455\n",
      "(5.31 min) Epoch 27/300 -- Iteration 25983 - Batch 945/963 - Train loss: 0.31504198  - Train acc: 0.8804 - Val loss: 1.06673455\n",
      "(5.31 min) Epoch 27/300 -- Iteration 25992 - Batch 954/963 - Train loss: 0.31471598  - Train acc: 0.8806 - Val loss: 1.06673455\n",
      "(5.31 min) Epoch 27/300 -- Iteration 26001 - Batch 962/963 - Train loss: 0.31474473  - Train acc: 0.8806 - Val loss: 1.07385814 - Val acc: 0.6050\n",
      "(5.31 min) Epoch 28/300 -- Iteration 26010 - Batch 9/963 - Train loss: 0.30714748  - Train acc: 0.8781 - Val loss: 1.07385814\n",
      "(5.31 min) Epoch 28/300 -- Iteration 26019 - Batch 18/963 - Train loss: 0.32611291  - Train acc: 0.8742 - Val loss: 1.07385814\n",
      "(5.32 min) Epoch 28/300 -- Iteration 26028 - Batch 27/963 - Train loss: 0.31433884  - Train acc: 0.8797 - Val loss: 1.07385814\n",
      "(5.32 min) Epoch 28/300 -- Iteration 26037 - Batch 36/963 - Train loss: 0.31018027  - Train acc: 0.8815 - Val loss: 1.07385814\n",
      "(5.32 min) Epoch 28/300 -- Iteration 26046 - Batch 45/963 - Train loss: 0.30815758  - Train acc: 0.8830 - Val loss: 1.07385814\n",
      "(5.32 min) Epoch 28/300 -- Iteration 26055 - Batch 54/963 - Train loss: 0.30862380  - Train acc: 0.8812 - Val loss: 1.07385814\n",
      "(5.32 min) Epoch 28/300 -- Iteration 26064 - Batch 63/963 - Train loss: 0.31008151  - Train acc: 0.8795 - Val loss: 1.07385814\n",
      "(5.32 min) Epoch 28/300 -- Iteration 26073 - Batch 72/963 - Train loss: 0.31224705  - Train acc: 0.8786 - Val loss: 1.07385814\n",
      "(5.33 min) Epoch 28/300 -- Iteration 26082 - Batch 81/963 - Train loss: 0.30937493  - Train acc: 0.8805 - Val loss: 1.07385814\n",
      "(5.33 min) Epoch 28/300 -- Iteration 26091 - Batch 90/963 - Train loss: 0.31160196  - Train acc: 0.8792 - Val loss: 1.07385814\n",
      "(5.33 min) Epoch 28/300 -- Iteration 26100 - Batch 99/963 - Train loss: 0.31361384  - Train acc: 0.8779 - Val loss: 1.07385814\n",
      "(5.33 min) Epoch 28/300 -- Iteration 26109 - Batch 108/963 - Train loss: 0.31113143  - Train acc: 0.8791 - Val loss: 1.07385814\n",
      "(5.33 min) Epoch 28/300 -- Iteration 26118 - Batch 117/963 - Train loss: 0.31136975  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.34 min) Epoch 28/300 -- Iteration 26127 - Batch 126/963 - Train loss: 0.31228209  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.34 min) Epoch 28/300 -- Iteration 26136 - Batch 135/963 - Train loss: 0.31075052  - Train acc: 0.8794 - Val loss: 1.07385814\n",
      "(5.34 min) Epoch 28/300 -- Iteration 26145 - Batch 144/963 - Train loss: 0.31279209  - Train acc: 0.8787 - Val loss: 1.07385814\n",
      "(5.34 min) Epoch 28/300 -- Iteration 26154 - Batch 153/963 - Train loss: 0.31113170  - Train acc: 0.8792 - Val loss: 1.07385814\n",
      "(5.34 min) Epoch 28/300 -- Iteration 26163 - Batch 162/963 - Train loss: 0.30949265  - Train acc: 0.8805 - Val loss: 1.07385814\n",
      "(5.34 min) Epoch 28/300 -- Iteration 26172 - Batch 171/963 - Train loss: 0.31164999  - Train acc: 0.8804 - Val loss: 1.07385814\n",
      "(5.35 min) Epoch 28/300 -- Iteration 26181 - Batch 180/963 - Train loss: 0.31117147  - Train acc: 0.8804 - Val loss: 1.07385814\n",
      "(5.35 min) Epoch 28/300 -- Iteration 26190 - Batch 189/963 - Train loss: 0.31201573  - Train acc: 0.8800 - Val loss: 1.07385814\n",
      "(5.35 min) Epoch 28/300 -- Iteration 26199 - Batch 198/963 - Train loss: 0.31284011  - Train acc: 0.8795 - Val loss: 1.07385814\n",
      "(5.35 min) Epoch 28/300 -- Iteration 26208 - Batch 207/963 - Train loss: 0.31279484  - Train acc: 0.8796 - Val loss: 1.07385814\n",
      "(5.35 min) Epoch 28/300 -- Iteration 26217 - Batch 216/963 - Train loss: 0.31264032  - Train acc: 0.8797 - Val loss: 1.07385814\n",
      "(5.36 min) Epoch 28/300 -- Iteration 26226 - Batch 225/963 - Train loss: 0.31252972  - Train acc: 0.8800 - Val loss: 1.07385814\n",
      "(5.36 min) Epoch 28/300 -- Iteration 26235 - Batch 234/963 - Train loss: 0.31310507  - Train acc: 0.8796 - Val loss: 1.07385814\n",
      "(5.36 min) Epoch 28/300 -- Iteration 26244 - Batch 243/963 - Train loss: 0.31405438  - Train acc: 0.8795 - Val loss: 1.07385814\n",
      "(5.36 min) Epoch 28/300 -- Iteration 26253 - Batch 252/963 - Train loss: 0.31314293  - Train acc: 0.8798 - Val loss: 1.07385814\n",
      "(5.36 min) Epoch 28/300 -- Iteration 26262 - Batch 261/963 - Train loss: 0.31327616  - Train acc: 0.8800 - Val loss: 1.07385814\n",
      "(5.36 min) Epoch 28/300 -- Iteration 26271 - Batch 270/963 - Train loss: 0.31323655  - Train acc: 0.8797 - Val loss: 1.07385814\n",
      "(5.37 min) Epoch 28/300 -- Iteration 26280 - Batch 279/963 - Train loss: 0.31387135  - Train acc: 0.8794 - Val loss: 1.07385814\n",
      "(5.37 min) Epoch 28/300 -- Iteration 26289 - Batch 288/963 - Train loss: 0.31464592  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.37 min) Epoch 28/300 -- Iteration 26298 - Batch 297/963 - Train loss: 0.31355401  - Train acc: 0.8794 - Val loss: 1.07385814\n",
      "(5.37 min) Epoch 28/300 -- Iteration 26307 - Batch 306/963 - Train loss: 0.31388582  - Train acc: 0.8791 - Val loss: 1.07385814\n",
      "(5.37 min) Epoch 28/300 -- Iteration 26316 - Batch 315/963 - Train loss: 0.31457159  - Train acc: 0.8785 - Val loss: 1.07385814\n",
      "(5.38 min) Epoch 28/300 -- Iteration 26325 - Batch 324/963 - Train loss: 0.31502729  - Train acc: 0.8784 - Val loss: 1.07385814\n",
      "(5.38 min) Epoch 28/300 -- Iteration 26334 - Batch 333/963 - Train loss: 0.31527412  - Train acc: 0.8784 - Val loss: 1.07385814\n",
      "(5.38 min) Epoch 28/300 -- Iteration 26343 - Batch 342/963 - Train loss: 0.31502355  - Train acc: 0.8783 - Val loss: 1.07385814\n",
      "(5.38 min) Epoch 28/300 -- Iteration 26352 - Batch 351/963 - Train loss: 0.31524756  - Train acc: 0.8785 - Val loss: 1.07385814\n",
      "(5.38 min) Epoch 28/300 -- Iteration 26361 - Batch 360/963 - Train loss: 0.31528936  - Train acc: 0.8783 - Val loss: 1.07385814\n",
      "(5.38 min) Epoch 28/300 -- Iteration 26370 - Batch 369/963 - Train loss: 0.31510470  - Train acc: 0.8784 - Val loss: 1.07385814\n",
      "(5.39 min) Epoch 28/300 -- Iteration 26379 - Batch 378/963 - Train loss: 0.31518029  - Train acc: 0.8784 - Val loss: 1.07385814\n",
      "(5.39 min) Epoch 28/300 -- Iteration 26388 - Batch 387/963 - Train loss: 0.31561231  - Train acc: 0.8780 - Val loss: 1.07385814\n",
      "(5.39 min) Epoch 28/300 -- Iteration 26397 - Batch 396/963 - Train loss: 0.31607728  - Train acc: 0.8777 - Val loss: 1.07385814\n",
      "(5.39 min) Epoch 28/300 -- Iteration 26406 - Batch 405/963 - Train loss: 0.31530097  - Train acc: 0.8780 - Val loss: 1.07385814\n",
      "(5.39 min) Epoch 28/300 -- Iteration 26415 - Batch 414/963 - Train loss: 0.31584472  - Train acc: 0.8778 - Val loss: 1.07385814\n",
      "(5.40 min) Epoch 28/300 -- Iteration 26424 - Batch 423/963 - Train loss: 0.31594111  - Train acc: 0.8778 - Val loss: 1.07385814\n",
      "(5.40 min) Epoch 28/300 -- Iteration 26433 - Batch 432/963 - Train loss: 0.31630961  - Train acc: 0.8776 - Val loss: 1.07385814\n",
      "(5.40 min) Epoch 28/300 -- Iteration 26442 - Batch 441/963 - Train loss: 0.31614662  - Train acc: 0.8779 - Val loss: 1.07385814\n",
      "(5.40 min) Epoch 28/300 -- Iteration 26451 - Batch 450/963 - Train loss: 0.31531128  - Train acc: 0.8783 - Val loss: 1.07385814\n",
      "(5.40 min) Epoch 28/300 -- Iteration 26460 - Batch 459/963 - Train loss: 0.31543847  - Train acc: 0.8782 - Val loss: 1.07385814\n",
      "(5.40 min) Epoch 28/300 -- Iteration 26469 - Batch 468/963 - Train loss: 0.31571442  - Train acc: 0.8782 - Val loss: 1.07385814\n",
      "(5.41 min) Epoch 28/300 -- Iteration 26478 - Batch 477/963 - Train loss: 0.31580601  - Train acc: 0.8781 - Val loss: 1.07385814\n",
      "(5.41 min) Epoch 28/300 -- Iteration 26487 - Batch 486/963 - Train loss: 0.31578729  - Train acc: 0.8782 - Val loss: 1.07385814\n",
      "(5.41 min) Epoch 28/300 -- Iteration 26496 - Batch 495/963 - Train loss: 0.31601810  - Train acc: 0.8784 - Val loss: 1.07385814\n",
      "(5.41 min) Epoch 28/300 -- Iteration 26505 - Batch 504/963 - Train loss: 0.31625243  - Train acc: 0.8784 - Val loss: 1.07385814\n",
      "(5.41 min) Epoch 28/300 -- Iteration 26514 - Batch 513/963 - Train loss: 0.31595696  - Train acc: 0.8785 - Val loss: 1.07385814\n",
      "(5.42 min) Epoch 28/300 -- Iteration 26523 - Batch 522/963 - Train loss: 0.31596094  - Train acc: 0.8784 - Val loss: 1.07385814\n",
      "(5.42 min) Epoch 28/300 -- Iteration 26532 - Batch 531/963 - Train loss: 0.31612906  - Train acc: 0.8783 - Val loss: 1.07385814\n",
      "(5.42 min) Epoch 28/300 -- Iteration 26541 - Batch 540/963 - Train loss: 0.31586294  - Train acc: 0.8785 - Val loss: 1.07385814\n",
      "(5.42 min) Epoch 28/300 -- Iteration 26550 - Batch 549/963 - Train loss: 0.31568522  - Train acc: 0.8787 - Val loss: 1.07385814\n",
      "(5.42 min) Epoch 28/300 -- Iteration 26559 - Batch 558/963 - Train loss: 0.31589265  - Train acc: 0.8786 - Val loss: 1.07385814\n",
      "(5.42 min) Epoch 28/300 -- Iteration 26568 - Batch 567/963 - Train loss: 0.31601974  - Train acc: 0.8785 - Val loss: 1.07385814\n",
      "(5.43 min) Epoch 28/300 -- Iteration 26577 - Batch 576/963 - Train loss: 0.31583462  - Train acc: 0.8786 - Val loss: 1.07385814\n",
      "(5.43 min) Epoch 28/300 -- Iteration 26586 - Batch 585/963 - Train loss: 0.31608286  - Train acc: 0.8785 - Val loss: 1.07385814\n",
      "(5.43 min) Epoch 28/300 -- Iteration 26595 - Batch 594/963 - Train loss: 0.31574947  - Train acc: 0.8787 - Val loss: 1.07385814\n",
      "(5.43 min) Epoch 28/300 -- Iteration 26604 - Batch 603/963 - Train loss: 0.31538934  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.43 min) Epoch 28/300 -- Iteration 26613 - Batch 612/963 - Train loss: 0.31560602  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.44 min) Epoch 28/300 -- Iteration 26622 - Batch 621/963 - Train loss: 0.31587328  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.44 min) Epoch 28/300 -- Iteration 26631 - Batch 630/963 - Train loss: 0.31610180  - Train acc: 0.8787 - Val loss: 1.07385814\n",
      "(5.44 min) Epoch 28/300 -- Iteration 26640 - Batch 639/963 - Train loss: 0.31601509  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.44 min) Epoch 28/300 -- Iteration 26649 - Batch 648/963 - Train loss: 0.31571813  - Train acc: 0.8790 - Val loss: 1.07385814\n",
      "(5.44 min) Epoch 28/300 -- Iteration 26658 - Batch 657/963 - Train loss: 0.31550790  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.44 min) Epoch 28/300 -- Iteration 26667 - Batch 666/963 - Train loss: 0.31571656  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.45 min) Epoch 28/300 -- Iteration 26676 - Batch 675/963 - Train loss: 0.31596643  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.45 min) Epoch 28/300 -- Iteration 26685 - Batch 684/963 - Train loss: 0.31625392  - Train acc: 0.8787 - Val loss: 1.07385814\n",
      "(5.45 min) Epoch 28/300 -- Iteration 26694 - Batch 693/963 - Train loss: 0.31613768  - Train acc: 0.8787 - Val loss: 1.07385814\n",
      "(5.45 min) Epoch 28/300 -- Iteration 26703 - Batch 702/963 - Train loss: 0.31589913  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.45 min) Epoch 28/300 -- Iteration 26712 - Batch 711/963 - Train loss: 0.31578857  - Train acc: 0.8787 - Val loss: 1.07385814\n",
      "(5.46 min) Epoch 28/300 -- Iteration 26721 - Batch 720/963 - Train loss: 0.31576922  - Train acc: 0.8787 - Val loss: 1.07385814\n",
      "(5.46 min) Epoch 28/300 -- Iteration 26730 - Batch 729/963 - Train loss: 0.31548025  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.46 min) Epoch 28/300 -- Iteration 26739 - Batch 738/963 - Train loss: 0.31520951  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.46 min) Epoch 28/300 -- Iteration 26748 - Batch 747/963 - Train loss: 0.31498440  - Train acc: 0.8790 - Val loss: 1.07385814\n",
      "(5.46 min) Epoch 28/300 -- Iteration 26757 - Batch 756/963 - Train loss: 0.31505828  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.47 min) Epoch 28/300 -- Iteration 26766 - Batch 765/963 - Train loss: 0.31539042  - Train acc: 0.8786 - Val loss: 1.07385814\n",
      "(5.47 min) Epoch 28/300 -- Iteration 26775 - Batch 774/963 - Train loss: 0.31567311  - Train acc: 0.8786 - Val loss: 1.07385814\n",
      "(5.47 min) Epoch 28/300 -- Iteration 26784 - Batch 783/963 - Train loss: 0.31528884  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.47 min) Epoch 28/300 -- Iteration 26793 - Batch 792/963 - Train loss: 0.31542861  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.47 min) Epoch 28/300 -- Iteration 26802 - Batch 801/963 - Train loss: 0.31528678  - Train acc: 0.8790 - Val loss: 1.07385814\n",
      "(5.47 min) Epoch 28/300 -- Iteration 26811 - Batch 810/963 - Train loss: 0.31559605  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.48 min) Epoch 28/300 -- Iteration 26820 - Batch 819/963 - Train loss: 0.31606717  - Train acc: 0.8788 - Val loss: 1.07385814\n",
      "(5.48 min) Epoch 28/300 -- Iteration 26829 - Batch 828/963 - Train loss: 0.31588290  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.48 min) Epoch 28/300 -- Iteration 26838 - Batch 837/963 - Train loss: 0.31608563  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.48 min) Epoch 28/300 -- Iteration 26847 - Batch 846/963 - Train loss: 0.31593294  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.48 min) Epoch 28/300 -- Iteration 26856 - Batch 855/963 - Train loss: 0.31596160  - Train acc: 0.8790 - Val loss: 1.07385814\n",
      "(5.49 min) Epoch 28/300 -- Iteration 26865 - Batch 864/963 - Train loss: 0.31573195  - Train acc: 0.8791 - Val loss: 1.07385814\n",
      "(5.49 min) Epoch 28/300 -- Iteration 26874 - Batch 873/963 - Train loss: 0.31585218  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.49 min) Epoch 28/300 -- Iteration 26883 - Batch 882/963 - Train loss: 0.31578036  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.49 min) Epoch 28/300 -- Iteration 26892 - Batch 891/963 - Train loss: 0.31580114  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.49 min) Epoch 28/300 -- Iteration 26901 - Batch 900/963 - Train loss: 0.31573557  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.49 min) Epoch 28/300 -- Iteration 26910 - Batch 909/963 - Train loss: 0.31569046  - Train acc: 0.8790 - Val loss: 1.07385814\n",
      "(5.50 min) Epoch 28/300 -- Iteration 26919 - Batch 918/963 - Train loss: 0.31583532  - Train acc: 0.8789 - Val loss: 1.07385814\n",
      "(5.50 min) Epoch 28/300 -- Iteration 26928 - Batch 927/963 - Train loss: 0.31549280  - Train acc: 0.8792 - Val loss: 1.07385814\n",
      "(5.50 min) Epoch 28/300 -- Iteration 26937 - Batch 936/963 - Train loss: 0.31524305  - Train acc: 0.8794 - Val loss: 1.07385814\n",
      "(5.50 min) Epoch 28/300 -- Iteration 26946 - Batch 945/963 - Train loss: 0.31517763  - Train acc: 0.8794 - Val loss: 1.07385814\n",
      "(5.50 min) Epoch 28/300 -- Iteration 26955 - Batch 954/963 - Train loss: 0.31511525  - Train acc: 0.8795 - Val loss: 1.07385814\n",
      "(5.51 min) Epoch 28/300 -- Iteration 26964 - Batch 962/963 - Train loss: 0.31504735  - Train acc: 0.8796 - Val loss: 1.06839228 - Val acc: 0.6050\n",
      "(5.51 min) Epoch 29/300 -- Iteration 26973 - Batch 9/963 - Train loss: 0.27416477  - Train acc: 0.9016 - Val loss: 1.06839228\n",
      "(5.51 min) Epoch 29/300 -- Iteration 26982 - Batch 18/963 - Train loss: 0.28780901  - Train acc: 0.8935 - Val loss: 1.06839228\n",
      "(5.51 min) Epoch 29/300 -- Iteration 26991 - Batch 27/963 - Train loss: 0.29379860  - Train acc: 0.8895 - Val loss: 1.06839228\n",
      "(5.51 min) Epoch 29/300 -- Iteration 27000 - Batch 36/963 - Train loss: 0.28977152  - Train acc: 0.8919 - Val loss: 1.06839228\n",
      "(5.51 min) Epoch 29/300 -- Iteration 27009 - Batch 45/963 - Train loss: 0.29928730  - Train acc: 0.8867 - Val loss: 1.06839228\n",
      "(5.52 min) Epoch 29/300 -- Iteration 27018 - Batch 54/963 - Train loss: 0.30533769  - Train acc: 0.8818 - Val loss: 1.06839228\n",
      "(5.52 min) Epoch 29/300 -- Iteration 27027 - Batch 63/963 - Train loss: 0.30914336  - Train acc: 0.8811 - Val loss: 1.06839228\n",
      "(5.52 min) Epoch 29/300 -- Iteration 27036 - Batch 72/963 - Train loss: 0.30756470  - Train acc: 0.8817 - Val loss: 1.06839228\n",
      "(5.52 min) Epoch 29/300 -- Iteration 27045 - Batch 81/963 - Train loss: 0.30491845  - Train acc: 0.8841 - Val loss: 1.06839228\n",
      "(5.52 min) Epoch 29/300 -- Iteration 27054 - Batch 90/963 - Train loss: 0.30430494  - Train acc: 0.8841 - Val loss: 1.06839228\n",
      "(5.53 min) Epoch 29/300 -- Iteration 27063 - Batch 99/963 - Train loss: 0.30263212  - Train acc: 0.8852 - Val loss: 1.06839228\n",
      "(5.53 min) Epoch 29/300 -- Iteration 27072 - Batch 108/963 - Train loss: 0.30493864  - Train acc: 0.8840 - Val loss: 1.06839228\n",
      "(5.53 min) Epoch 29/300 -- Iteration 27081 - Batch 117/963 - Train loss: 0.30613757  - Train acc: 0.8837 - Val loss: 1.06839228\n",
      "(5.53 min) Epoch 29/300 -- Iteration 27090 - Batch 126/963 - Train loss: 0.30652478  - Train acc: 0.8834 - Val loss: 1.06839228\n",
      "(5.53 min) Epoch 29/300 -- Iteration 27099 - Batch 135/963 - Train loss: 0.30918733  - Train acc: 0.8817 - Val loss: 1.06839228\n",
      "(5.54 min) Epoch 29/300 -- Iteration 27108 - Batch 144/963 - Train loss: 0.30842122  - Train acc: 0.8818 - Val loss: 1.06839228\n",
      "(5.54 min) Epoch 29/300 -- Iteration 27117 - Batch 153/963 - Train loss: 0.31172519  - Train acc: 0.8807 - Val loss: 1.06839228\n",
      "(5.54 min) Epoch 29/300 -- Iteration 27126 - Batch 162/963 - Train loss: 0.31103047  - Train acc: 0.8810 - Val loss: 1.06839228\n",
      "(5.54 min) Epoch 29/300 -- Iteration 27135 - Batch 171/963 - Train loss: 0.31068572  - Train acc: 0.8812 - Val loss: 1.06839228\n",
      "(5.54 min) Epoch 29/300 -- Iteration 27144 - Batch 180/963 - Train loss: 0.31047996  - Train acc: 0.8814 - Val loss: 1.06839228\n",
      "(5.54 min) Epoch 29/300 -- Iteration 27153 - Batch 189/963 - Train loss: 0.31081767  - Train acc: 0.8810 - Val loss: 1.06839228\n",
      "(5.55 min) Epoch 29/300 -- Iteration 27162 - Batch 198/963 - Train loss: 0.31005630  - Train acc: 0.8815 - Val loss: 1.06839228\n",
      "(5.55 min) Epoch 29/300 -- Iteration 27171 - Batch 207/963 - Train loss: 0.30882572  - Train acc: 0.8818 - Val loss: 1.06839228\n",
      "(5.55 min) Epoch 29/300 -- Iteration 27180 - Batch 216/963 - Train loss: 0.31028823  - Train acc: 0.8813 - Val loss: 1.06839228\n",
      "(5.55 min) Epoch 29/300 -- Iteration 27189 - Batch 225/963 - Train loss: 0.31072514  - Train acc: 0.8810 - Val loss: 1.06839228\n",
      "(5.55 min) Epoch 29/300 -- Iteration 27198 - Batch 234/963 - Train loss: 0.31052463  - Train acc: 0.8811 - Val loss: 1.06839228\n",
      "(5.56 min) Epoch 29/300 -- Iteration 27207 - Batch 243/963 - Train loss: 0.31005392  - Train acc: 0.8815 - Val loss: 1.06839228\n",
      "(5.56 min) Epoch 29/300 -- Iteration 27216 - Batch 252/963 - Train loss: 0.31041168  - Train acc: 0.8811 - Val loss: 1.06839228\n",
      "(5.56 min) Epoch 29/300 -- Iteration 27225 - Batch 261/963 - Train loss: 0.30999647  - Train acc: 0.8813 - Val loss: 1.06839228\n",
      "(5.56 min) Epoch 29/300 -- Iteration 27234 - Batch 270/963 - Train loss: 0.30996455  - Train acc: 0.8814 - Val loss: 1.06839228\n",
      "(5.56 min) Epoch 29/300 -- Iteration 27243 - Batch 279/963 - Train loss: 0.30935105  - Train acc: 0.8819 - Val loss: 1.06839228\n",
      "(5.56 min) Epoch 29/300 -- Iteration 27252 - Batch 288/963 - Train loss: 0.30879230  - Train acc: 0.8818 - Val loss: 1.06839228\n",
      "(5.57 min) Epoch 29/300 -- Iteration 27261 - Batch 297/963 - Train loss: 0.30886970  - Train acc: 0.8819 - Val loss: 1.06839228\n",
      "(5.57 min) Epoch 29/300 -- Iteration 27270 - Batch 306/963 - Train loss: 0.30915759  - Train acc: 0.8819 - Val loss: 1.06839228\n",
      "(5.57 min) Epoch 29/300 -- Iteration 27279 - Batch 315/963 - Train loss: 0.31021610  - Train acc: 0.8815 - Val loss: 1.06839228\n",
      "(5.57 min) Epoch 29/300 -- Iteration 27288 - Batch 324/963 - Train loss: 0.31026638  - Train acc: 0.8814 - Val loss: 1.06839228\n",
      "(5.57 min) Epoch 29/300 -- Iteration 27297 - Batch 333/963 - Train loss: 0.31035443  - Train acc: 0.8816 - Val loss: 1.06839228\n",
      "(5.58 min) Epoch 29/300 -- Iteration 27306 - Batch 342/963 - Train loss: 0.31002506  - Train acc: 0.8818 - Val loss: 1.06839228\n",
      "(5.58 min) Epoch 29/300 -- Iteration 27315 - Batch 351/963 - Train loss: 0.30956993  - Train acc: 0.8820 - Val loss: 1.06839228\n",
      "(5.58 min) Epoch 29/300 -- Iteration 27324 - Batch 360/963 - Train loss: 0.30989165  - Train acc: 0.8819 - Val loss: 1.06839228\n",
      "(5.58 min) Epoch 29/300 -- Iteration 27333 - Batch 369/963 - Train loss: 0.30975185  - Train acc: 0.8821 - Val loss: 1.06839228\n",
      "(5.58 min) Epoch 29/300 -- Iteration 27342 - Batch 378/963 - Train loss: 0.30906574  - Train acc: 0.8825 - Val loss: 1.06839228\n",
      "(5.58 min) Epoch 29/300 -- Iteration 27351 - Batch 387/963 - Train loss: 0.30884512  - Train acc: 0.8828 - Val loss: 1.06839228\n",
      "(5.59 min) Epoch 29/300 -- Iteration 27360 - Batch 396/963 - Train loss: 0.30858811  - Train acc: 0.8828 - Val loss: 1.06839228\n",
      "(5.59 min) Epoch 29/300 -- Iteration 27369 - Batch 405/963 - Train loss: 0.30812540  - Train acc: 0.8832 - Val loss: 1.06839228\n",
      "(5.59 min) Epoch 29/300 -- Iteration 27378 - Batch 414/963 - Train loss: 0.30725639  - Train acc: 0.8837 - Val loss: 1.06839228\n",
      "(5.59 min) Epoch 29/300 -- Iteration 27387 - Batch 423/963 - Train loss: 0.30770662  - Train acc: 0.8834 - Val loss: 1.06839228\n",
      "(5.59 min) Epoch 29/300 -- Iteration 27396 - Batch 432/963 - Train loss: 0.30796834  - Train acc: 0.8831 - Val loss: 1.06839228\n",
      "(5.60 min) Epoch 29/300 -- Iteration 27405 - Batch 441/963 - Train loss: 0.30783578  - Train acc: 0.8832 - Val loss: 1.06839228\n",
      "(5.60 min) Epoch 29/300 -- Iteration 27414 - Batch 450/963 - Train loss: 0.30815887  - Train acc: 0.8831 - Val loss: 1.06839228\n",
      "(5.60 min) Epoch 29/300 -- Iteration 27423 - Batch 459/963 - Train loss: 0.30846426  - Train acc: 0.8829 - Val loss: 1.06839228\n",
      "(5.60 min) Epoch 29/300 -- Iteration 27432 - Batch 468/963 - Train loss: 0.30794180  - Train acc: 0.8832 - Val loss: 1.06839228\n",
      "(5.60 min) Epoch 29/300 -- Iteration 27441 - Batch 477/963 - Train loss: 0.30831912  - Train acc: 0.8831 - Val loss: 1.06839228\n",
      "(5.60 min) Epoch 29/300 -- Iteration 27450 - Batch 486/963 - Train loss: 0.30895940  - Train acc: 0.8829 - Val loss: 1.06839228\n",
      "(5.61 min) Epoch 29/300 -- Iteration 27459 - Batch 495/963 - Train loss: 0.30891860  - Train acc: 0.8828 - Val loss: 1.06839228\n",
      "(5.61 min) Epoch 29/300 -- Iteration 27468 - Batch 504/963 - Train loss: 0.30915796  - Train acc: 0.8826 - Val loss: 1.06839228\n",
      "(5.61 min) Epoch 29/300 -- Iteration 27477 - Batch 513/963 - Train loss: 0.30912386  - Train acc: 0.8825 - Val loss: 1.06839228\n",
      "(5.61 min) Epoch 29/300 -- Iteration 27486 - Batch 522/963 - Train loss: 0.30915116  - Train acc: 0.8825 - Val loss: 1.06839228\n",
      "(5.61 min) Epoch 29/300 -- Iteration 27495 - Batch 531/963 - Train loss: 0.30848071  - Train acc: 0.8828 - Val loss: 1.06839228\n",
      "(5.62 min) Epoch 29/300 -- Iteration 27504 - Batch 540/963 - Train loss: 0.30863643  - Train acc: 0.8828 - Val loss: 1.06839228\n",
      "(5.62 min) Epoch 29/300 -- Iteration 27513 - Batch 549/963 - Train loss: 0.30866986  - Train acc: 0.8827 - Val loss: 1.06839228\n",
      "(5.62 min) Epoch 29/300 -- Iteration 27522 - Batch 558/963 - Train loss: 0.30845928  - Train acc: 0.8828 - Val loss: 1.06839228\n",
      "(5.62 min) Epoch 29/300 -- Iteration 27531 - Batch 567/963 - Train loss: 0.30770411  - Train acc: 0.8831 - Val loss: 1.06839228\n",
      "(5.62 min) Epoch 29/300 -- Iteration 27540 - Batch 576/963 - Train loss: 0.30755829  - Train acc: 0.8831 - Val loss: 1.06839228\n",
      "(5.62 min) Epoch 29/300 -- Iteration 27549 - Batch 585/963 - Train loss: 0.30772903  - Train acc: 0.8829 - Val loss: 1.06839228\n",
      "(5.63 min) Epoch 29/300 -- Iteration 27558 - Batch 594/963 - Train loss: 0.30806330  - Train acc: 0.8827 - Val loss: 1.06839228\n",
      "(5.63 min) Epoch 29/300 -- Iteration 27567 - Batch 603/963 - Train loss: 0.30830225  - Train acc: 0.8824 - Val loss: 1.06839228\n",
      "(5.63 min) Epoch 29/300 -- Iteration 27576 - Batch 612/963 - Train loss: 0.30826560  - Train acc: 0.8826 - Val loss: 1.06839228\n",
      "(5.63 min) Epoch 29/300 -- Iteration 27585 - Batch 621/963 - Train loss: 0.30778776  - Train acc: 0.8827 - Val loss: 1.06839228\n",
      "(5.63 min) Epoch 29/300 -- Iteration 27594 - Batch 630/963 - Train loss: 0.30784638  - Train acc: 0.8827 - Val loss: 1.06839228\n",
      "(5.64 min) Epoch 29/300 -- Iteration 27603 - Batch 639/963 - Train loss: 0.30753395  - Train acc: 0.8829 - Val loss: 1.06839228\n",
      "(5.64 min) Epoch 29/300 -- Iteration 27612 - Batch 648/963 - Train loss: 0.30780341  - Train acc: 0.8828 - Val loss: 1.06839228\n",
      "(5.64 min) Epoch 29/300 -- Iteration 27621 - Batch 657/963 - Train loss: 0.30800631  - Train acc: 0.8827 - Val loss: 1.06839228\n",
      "(5.64 min) Epoch 29/300 -- Iteration 27630 - Batch 666/963 - Train loss: 0.30827059  - Train acc: 0.8826 - Val loss: 1.06839228\n",
      "(5.64 min) Epoch 29/300 -- Iteration 27639 - Batch 675/963 - Train loss: 0.30875394  - Train acc: 0.8825 - Val loss: 1.06839228\n",
      "(5.65 min) Epoch 29/300 -- Iteration 27648 - Batch 684/963 - Train loss: 0.30883786  - Train acc: 0.8823 - Val loss: 1.06839228\n",
      "(5.65 min) Epoch 29/300 -- Iteration 27657 - Batch 693/963 - Train loss: 0.30947175  - Train acc: 0.8821 - Val loss: 1.06839228\n",
      "(5.65 min) Epoch 29/300 -- Iteration 27666 - Batch 702/963 - Train loss: 0.30937160  - Train acc: 0.8822 - Val loss: 1.06839228\n",
      "(5.65 min) Epoch 29/300 -- Iteration 27675 - Batch 711/963 - Train loss: 0.31017372  - Train acc: 0.8820 - Val loss: 1.06839228\n",
      "(5.65 min) Epoch 29/300 -- Iteration 27684 - Batch 720/963 - Train loss: 0.31061631  - Train acc: 0.8817 - Val loss: 1.06839228\n",
      "(5.65 min) Epoch 29/300 -- Iteration 27693 - Batch 729/963 - Train loss: 0.31102120  - Train acc: 0.8815 - Val loss: 1.06839228\n",
      "(5.66 min) Epoch 29/300 -- Iteration 27702 - Batch 738/963 - Train loss: 0.31121124  - Train acc: 0.8814 - Val loss: 1.06839228\n",
      "(5.66 min) Epoch 29/300 -- Iteration 27711 - Batch 747/963 - Train loss: 0.31097671  - Train acc: 0.8816 - Val loss: 1.06839228\n",
      "(5.66 min) Epoch 29/300 -- Iteration 27720 - Batch 756/963 - Train loss: 0.31100447  - Train acc: 0.8816 - Val loss: 1.06839228\n",
      "(5.66 min) Epoch 29/300 -- Iteration 27729 - Batch 765/963 - Train loss: 0.31096456  - Train acc: 0.8817 - Val loss: 1.06839228\n",
      "(5.66 min) Epoch 29/300 -- Iteration 27738 - Batch 774/963 - Train loss: 0.31089261  - Train acc: 0.8817 - Val loss: 1.06839228\n",
      "(5.67 min) Epoch 29/300 -- Iteration 27747 - Batch 783/963 - Train loss: 0.31067309  - Train acc: 0.8819 - Val loss: 1.06839228\n",
      "(5.67 min) Epoch 29/300 -- Iteration 27756 - Batch 792/963 - Train loss: 0.31031143  - Train acc: 0.8820 - Val loss: 1.06839228\n",
      "(5.67 min) Epoch 29/300 -- Iteration 27765 - Batch 801/963 - Train loss: 0.31051696  - Train acc: 0.8820 - Val loss: 1.06839228\n",
      "(5.67 min) Epoch 29/300 -- Iteration 27774 - Batch 810/963 - Train loss: 0.31015457  - Train acc: 0.8821 - Val loss: 1.06839228\n",
      "(5.67 min) Epoch 29/300 -- Iteration 27783 - Batch 819/963 - Train loss: 0.31020247  - Train acc: 0.8822 - Val loss: 1.06839228\n",
      "(5.68 min) Epoch 29/300 -- Iteration 27792 - Batch 828/963 - Train loss: 0.31039266  - Train acc: 0.8821 - Val loss: 1.06839228\n",
      "(5.68 min) Epoch 29/300 -- Iteration 27801 - Batch 837/963 - Train loss: 0.31061357  - Train acc: 0.8819 - Val loss: 1.06839228\n",
      "(5.68 min) Epoch 29/300 -- Iteration 27810 - Batch 846/963 - Train loss: 0.31058396  - Train acc: 0.8820 - Val loss: 1.06839228\n",
      "(5.68 min) Epoch 29/300 -- Iteration 27819 - Batch 855/963 - Train loss: 0.31039032  - Train acc: 0.8820 - Val loss: 1.06839228\n",
      "(5.68 min) Epoch 29/300 -- Iteration 27828 - Batch 864/963 - Train loss: 0.31055223  - Train acc: 0.8820 - Val loss: 1.06839228\n",
      "(5.68 min) Epoch 29/300 -- Iteration 27837 - Batch 873/963 - Train loss: 0.31047112  - Train acc: 0.8820 - Val loss: 1.06839228\n",
      "(5.69 min) Epoch 29/300 -- Iteration 27846 - Batch 882/963 - Train loss: 0.31045389  - Train acc: 0.8821 - Val loss: 1.06839228\n",
      "(5.69 min) Epoch 29/300 -- Iteration 27855 - Batch 891/963 - Train loss: 0.31018885  - Train acc: 0.8822 - Val loss: 1.06839228\n",
      "(5.69 min) Epoch 29/300 -- Iteration 27864 - Batch 900/963 - Train loss: 0.31040557  - Train acc: 0.8822 - Val loss: 1.06839228\n",
      "(5.69 min) Epoch 29/300 -- Iteration 27873 - Batch 909/963 - Train loss: 0.31051121  - Train acc: 0.8821 - Val loss: 1.06839228\n",
      "(5.69 min) Epoch 29/300 -- Iteration 27882 - Batch 918/963 - Train loss: 0.31041955  - Train acc: 0.8821 - Val loss: 1.06839228\n",
      "(5.69 min) Epoch 29/300 -- Iteration 27891 - Batch 927/963 - Train loss: 0.31049521  - Train acc: 0.8822 - Val loss: 1.06839228\n",
      "(5.70 min) Epoch 29/300 -- Iteration 27900 - Batch 936/963 - Train loss: 0.31029644  - Train acc: 0.8823 - Val loss: 1.06839228\n",
      "(5.70 min) Epoch 29/300 -- Iteration 27909 - Batch 945/963 - Train loss: 0.31040799  - Train acc: 0.8822 - Val loss: 1.06839228\n",
      "(5.70 min) Epoch 29/300 -- Iteration 27918 - Batch 954/963 - Train loss: 0.31028444  - Train acc: 0.8823 - Val loss: 1.06839228\n",
      "(5.70 min) Epoch 29/300 -- Iteration 27927 - Batch 962/963 - Train loss: 0.31047142  - Train acc: 0.8822 - Val loss: 1.07303226 - Val acc: 0.5983\n",
      "(5.70 min) Epoch 30/300 -- Iteration 27936 - Batch 9/963 - Train loss: 0.33577223  - Train acc: 0.8734 - Val loss: 1.07303226\n",
      "(5.71 min) Epoch 30/300 -- Iteration 27945 - Batch 18/963 - Train loss: 0.32183004  - Train acc: 0.8808 - Val loss: 1.07303226\n",
      "(5.71 min) Epoch 30/300 -- Iteration 27954 - Batch 27/963 - Train loss: 0.31710190  - Train acc: 0.8823 - Val loss: 1.07303226\n",
      "(5.71 min) Epoch 30/300 -- Iteration 27963 - Batch 36/963 - Train loss: 0.31486805  - Train acc: 0.8811 - Val loss: 1.07303226\n",
      "(5.71 min) Epoch 30/300 -- Iteration 27972 - Batch 45/963 - Train loss: 0.31451341  - Train acc: 0.8808 - Val loss: 1.07303226\n",
      "(5.71 min) Epoch 30/300 -- Iteration 27981 - Batch 54/963 - Train loss: 0.31184271  - Train acc: 0.8821 - Val loss: 1.07303226\n",
      "(5.72 min) Epoch 30/300 -- Iteration 27990 - Batch 63/963 - Train loss: 0.30746309  - Train acc: 0.8859 - Val loss: 1.07303226\n",
      "(5.72 min) Epoch 30/300 -- Iteration 27999 - Batch 72/963 - Train loss: 0.30691755  - Train acc: 0.8859 - Val loss: 1.07303226\n",
      "(5.72 min) Epoch 30/300 -- Iteration 28008 - Batch 81/963 - Train loss: 0.30987351  - Train acc: 0.8844 - Val loss: 1.07303226\n",
      "(5.72 min) Epoch 30/300 -- Iteration 28017 - Batch 90/963 - Train loss: 0.31073332  - Train acc: 0.8837 - Val loss: 1.07303226\n",
      "(5.72 min) Epoch 30/300 -- Iteration 28026 - Batch 99/963 - Train loss: 0.30888512  - Train acc: 0.8840 - Val loss: 1.07303226\n",
      "(5.73 min) Epoch 30/300 -- Iteration 28035 - Batch 108/963 - Train loss: 0.30826812  - Train acc: 0.8837 - Val loss: 1.07303226\n",
      "(5.73 min) Epoch 30/300 -- Iteration 28044 - Batch 117/963 - Train loss: 0.30654035  - Train acc: 0.8853 - Val loss: 1.07303226\n",
      "(5.73 min) Epoch 30/300 -- Iteration 28053 - Batch 126/963 - Train loss: 0.30684764  - Train acc: 0.8848 - Val loss: 1.07303226\n",
      "(5.73 min) Epoch 30/300 -- Iteration 28062 - Batch 135/963 - Train loss: 0.30804792  - Train acc: 0.8840 - Val loss: 1.07303226\n",
      "(5.73 min) Epoch 30/300 -- Iteration 28071 - Batch 144/963 - Train loss: 0.30672076  - Train acc: 0.8849 - Val loss: 1.07303226\n",
      "(5.73 min) Epoch 30/300 -- Iteration 28080 - Batch 153/963 - Train loss: 0.30730446  - Train acc: 0.8841 - Val loss: 1.07303226\n",
      "(5.74 min) Epoch 30/300 -- Iteration 28089 - Batch 162/963 - Train loss: 0.30974049  - Train acc: 0.8831 - Val loss: 1.07303226\n",
      "(5.74 min) Epoch 30/300 -- Iteration 28098 - Batch 171/963 - Train loss: 0.31076043  - Train acc: 0.8826 - Val loss: 1.07303226\n",
      "(5.74 min) Epoch 30/300 -- Iteration 28107 - Batch 180/963 - Train loss: 0.31160582  - Train acc: 0.8823 - Val loss: 1.07303226\n",
      "(5.74 min) Epoch 30/300 -- Iteration 28116 - Batch 189/963 - Train loss: 0.31174575  - Train acc: 0.8824 - Val loss: 1.07303226\n",
      "(5.74 min) Epoch 30/300 -- Iteration 28125 - Batch 198/963 - Train loss: 0.31139459  - Train acc: 0.8825 - Val loss: 1.07303226\n",
      "(5.75 min) Epoch 30/300 -- Iteration 28134 - Batch 207/963 - Train loss: 0.31111340  - Train acc: 0.8823 - Val loss: 1.07303226\n",
      "(5.75 min) Epoch 30/300 -- Iteration 28143 - Batch 216/963 - Train loss: 0.31129566  - Train acc: 0.8822 - Val loss: 1.07303226\n",
      "(5.75 min) Epoch 30/300 -- Iteration 28152 - Batch 225/963 - Train loss: 0.31066313  - Train acc: 0.8825 - Val loss: 1.07303226\n",
      "(5.75 min) Epoch 30/300 -- Iteration 28161 - Batch 234/963 - Train loss: 0.31101044  - Train acc: 0.8826 - Val loss: 1.07303226\n",
      "(5.75 min) Epoch 30/300 -- Iteration 28170 - Batch 243/963 - Train loss: 0.31121862  - Train acc: 0.8822 - Val loss: 1.07303226\n",
      "(5.75 min) Epoch 30/300 -- Iteration 28179 - Batch 252/963 - Train loss: 0.31197534  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.76 min) Epoch 30/300 -- Iteration 28188 - Batch 261/963 - Train loss: 0.31089727  - Train acc: 0.8823 - Val loss: 1.07303226\n",
      "(5.76 min) Epoch 30/300 -- Iteration 28197 - Batch 270/963 - Train loss: 0.31014612  - Train acc: 0.8824 - Val loss: 1.07303226\n",
      "(5.76 min) Epoch 30/300 -- Iteration 28206 - Batch 279/963 - Train loss: 0.31068908  - Train acc: 0.8819 - Val loss: 1.07303226\n",
      "(5.76 min) Epoch 30/300 -- Iteration 28215 - Batch 288/963 - Train loss: 0.30961682  - Train acc: 0.8824 - Val loss: 1.07303226\n",
      "(5.76 min) Epoch 30/300 -- Iteration 28224 - Batch 297/963 - Train loss: 0.30920047  - Train acc: 0.8829 - Val loss: 1.07303226\n",
      "(5.77 min) Epoch 30/300 -- Iteration 28233 - Batch 306/963 - Train loss: 0.30864704  - Train acc: 0.8832 - Val loss: 1.07303226\n",
      "(5.77 min) Epoch 30/300 -- Iteration 28242 - Batch 315/963 - Train loss: 0.30906841  - Train acc: 0.8832 - Val loss: 1.07303226\n",
      "(5.77 min) Epoch 30/300 -- Iteration 28251 - Batch 324/963 - Train loss: 0.30967767  - Train acc: 0.8828 - Val loss: 1.07303226\n",
      "(5.77 min) Epoch 30/300 -- Iteration 28260 - Batch 333/963 - Train loss: 0.30988796  - Train acc: 0.8826 - Val loss: 1.07303226\n",
      "(5.77 min) Epoch 30/300 -- Iteration 28269 - Batch 342/963 - Train loss: 0.31038602  - Train acc: 0.8821 - Val loss: 1.07303226\n",
      "(5.77 min) Epoch 30/300 -- Iteration 28278 - Batch 351/963 - Train loss: 0.31130611  - Train acc: 0.8815 - Val loss: 1.07303226\n",
      "(5.78 min) Epoch 30/300 -- Iteration 28287 - Batch 360/963 - Train loss: 0.31095492  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.78 min) Epoch 30/300 -- Iteration 28296 - Batch 369/963 - Train loss: 0.31146832  - Train acc: 0.8813 - Val loss: 1.07303226\n",
      "(5.78 min) Epoch 30/300 -- Iteration 28305 - Batch 378/963 - Train loss: 0.31100157  - Train acc: 0.8815 - Val loss: 1.07303226\n",
      "(5.78 min) Epoch 30/300 -- Iteration 28314 - Batch 387/963 - Train loss: 0.31088828  - Train acc: 0.8817 - Val loss: 1.07303226\n",
      "(5.78 min) Epoch 30/300 -- Iteration 28323 - Batch 396/963 - Train loss: 0.31042130  - Train acc: 0.8819 - Val loss: 1.07303226\n",
      "(5.79 min) Epoch 30/300 -- Iteration 28332 - Batch 405/963 - Train loss: 0.30989009  - Train acc: 0.8823 - Val loss: 1.07303226\n",
      "(5.79 min) Epoch 30/300 -- Iteration 28341 - Batch 414/963 - Train loss: 0.31072545  - Train acc: 0.8821 - Val loss: 1.07303226\n",
      "(5.79 min) Epoch 30/300 -- Iteration 28350 - Batch 423/963 - Train loss: 0.31086478  - Train acc: 0.8822 - Val loss: 1.07303226\n",
      "(5.79 min) Epoch 30/300 -- Iteration 28359 - Batch 432/963 - Train loss: 0.31057059  - Train acc: 0.8824 - Val loss: 1.07303226\n",
      "(5.79 min) Epoch 30/300 -- Iteration 28368 - Batch 441/963 - Train loss: 0.31075383  - Train acc: 0.8823 - Val loss: 1.07303226\n",
      "(5.79 min) Epoch 30/300 -- Iteration 28377 - Batch 450/963 - Train loss: 0.31088042  - Train acc: 0.8819 - Val loss: 1.07303226\n",
      "(5.80 min) Epoch 30/300 -- Iteration 28386 - Batch 459/963 - Train loss: 0.31130898  - Train acc: 0.8820 - Val loss: 1.07303226\n",
      "(5.80 min) Epoch 30/300 -- Iteration 28395 - Batch 468/963 - Train loss: 0.31149473  - Train acc: 0.8819 - Val loss: 1.07303226\n",
      "(5.80 min) Epoch 30/300 -- Iteration 28404 - Batch 477/963 - Train loss: 0.31153560  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.80 min) Epoch 30/300 -- Iteration 28413 - Batch 486/963 - Train loss: 0.31142996  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.80 min) Epoch 30/300 -- Iteration 28422 - Batch 495/963 - Train loss: 0.31107578  - Train acc: 0.8819 - Val loss: 1.07303226\n",
      "(5.81 min) Epoch 30/300 -- Iteration 28431 - Batch 504/963 - Train loss: 0.31101847  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.81 min) Epoch 30/300 -- Iteration 28440 - Batch 513/963 - Train loss: 0.31118017  - Train acc: 0.8817 - Val loss: 1.07303226\n",
      "(5.81 min) Epoch 30/300 -- Iteration 28449 - Batch 522/963 - Train loss: 0.31085785  - Train acc: 0.8817 - Val loss: 1.07303226\n",
      "(5.81 min) Epoch 30/300 -- Iteration 28458 - Batch 531/963 - Train loss: 0.31081530  - Train acc: 0.8817 - Val loss: 1.07303226\n",
      "(5.81 min) Epoch 30/300 -- Iteration 28467 - Batch 540/963 - Train loss: 0.31084574  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.81 min) Epoch 30/300 -- Iteration 28476 - Batch 549/963 - Train loss: 0.31042858  - Train acc: 0.8820 - Val loss: 1.07303226\n",
      "(5.82 min) Epoch 30/300 -- Iteration 28485 - Batch 558/963 - Train loss: 0.31018544  - Train acc: 0.8821 - Val loss: 1.07303226\n",
      "(5.82 min) Epoch 30/300 -- Iteration 28494 - Batch 567/963 - Train loss: 0.31032691  - Train acc: 0.8819 - Val loss: 1.07303226\n",
      "(5.82 min) Epoch 30/300 -- Iteration 28503 - Batch 576/963 - Train loss: 0.31036776  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.82 min) Epoch 30/300 -- Iteration 28512 - Batch 585/963 - Train loss: 0.31014510  - Train acc: 0.8819 - Val loss: 1.07303226\n",
      "(5.82 min) Epoch 30/300 -- Iteration 28521 - Batch 594/963 - Train loss: 0.30983392  - Train acc: 0.8821 - Val loss: 1.07303226\n",
      "(5.83 min) Epoch 30/300 -- Iteration 28530 - Batch 603/963 - Train loss: 0.31021565  - Train acc: 0.8820 - Val loss: 1.07303226\n",
      "(5.83 min) Epoch 30/300 -- Iteration 28539 - Batch 612/963 - Train loss: 0.31022123  - Train acc: 0.8820 - Val loss: 1.07303226\n",
      "(5.83 min) Epoch 30/300 -- Iteration 28548 - Batch 621/963 - Train loss: 0.31007707  - Train acc: 0.8820 - Val loss: 1.07303226\n",
      "(5.83 min) Epoch 30/300 -- Iteration 28557 - Batch 630/963 - Train loss: 0.31037999  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.83 min) Epoch 30/300 -- Iteration 28566 - Batch 639/963 - Train loss: 0.31046930  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.84 min) Epoch 30/300 -- Iteration 28575 - Batch 648/963 - Train loss: 0.31037329  - Train acc: 0.8817 - Val loss: 1.07303226\n",
      "(5.84 min) Epoch 30/300 -- Iteration 28584 - Batch 657/963 - Train loss: 0.31028666  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.84 min) Epoch 30/300 -- Iteration 28593 - Batch 666/963 - Train loss: 0.30994635  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.84 min) Epoch 30/300 -- Iteration 28602 - Batch 675/963 - Train loss: 0.31033007  - Train acc: 0.8817 - Val loss: 1.07303226\n",
      "(5.84 min) Epoch 30/300 -- Iteration 28611 - Batch 684/963 - Train loss: 0.31040183  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.85 min) Epoch 30/300 -- Iteration 28620 - Batch 693/963 - Train loss: 0.31079331  - Train acc: 0.8814 - Val loss: 1.07303226\n",
      "(5.85 min) Epoch 30/300 -- Iteration 28629 - Batch 702/963 - Train loss: 0.31038380  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.85 min) Epoch 30/300 -- Iteration 28638 - Batch 711/963 - Train loss: 0.31021095  - Train acc: 0.8817 - Val loss: 1.07303226\n",
      "(5.85 min) Epoch 30/300 -- Iteration 28647 - Batch 720/963 - Train loss: 0.31023968  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.85 min) Epoch 30/300 -- Iteration 28656 - Batch 729/963 - Train loss: 0.31021196  - Train acc: 0.8817 - Val loss: 1.07303226\n",
      "(5.85 min) Epoch 30/300 -- Iteration 28665 - Batch 738/963 - Train loss: 0.31039004  - Train acc: 0.8817 - Val loss: 1.07303226\n",
      "(5.86 min) Epoch 30/300 -- Iteration 28674 - Batch 747/963 - Train loss: 0.31066565  - Train acc: 0.8815 - Val loss: 1.07303226\n",
      "(5.86 min) Epoch 30/300 -- Iteration 28683 - Batch 756/963 - Train loss: 0.31059251  - Train acc: 0.8815 - Val loss: 1.07303226\n",
      "(5.86 min) Epoch 30/300 -- Iteration 28692 - Batch 765/963 - Train loss: 0.31090386  - Train acc: 0.8813 - Val loss: 1.07303226\n",
      "(5.86 min) Epoch 30/300 -- Iteration 28701 - Batch 774/963 - Train loss: 0.31081360  - Train acc: 0.8814 - Val loss: 1.07303226\n",
      "(5.86 min) Epoch 30/300 -- Iteration 28710 - Batch 783/963 - Train loss: 0.31092386  - Train acc: 0.8814 - Val loss: 1.07303226\n",
      "(5.87 min) Epoch 30/300 -- Iteration 28719 - Batch 792/963 - Train loss: 0.31084437  - Train acc: 0.8814 - Val loss: 1.07303226\n",
      "(5.87 min) Epoch 30/300 -- Iteration 28728 - Batch 801/963 - Train loss: 0.31087593  - Train acc: 0.8814 - Val loss: 1.07303226\n",
      "(5.87 min) Epoch 30/300 -- Iteration 28737 - Batch 810/963 - Train loss: 0.31056103  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.87 min) Epoch 30/300 -- Iteration 28746 - Batch 819/963 - Train loss: 0.31052574  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.87 min) Epoch 30/300 -- Iteration 28755 - Batch 828/963 - Train loss: 0.31073325  - Train acc: 0.8815 - Val loss: 1.07303226\n",
      "(5.88 min) Epoch 30/300 -- Iteration 28764 - Batch 837/963 - Train loss: 0.31048387  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.88 min) Epoch 30/300 -- Iteration 28773 - Batch 846/963 - Train loss: 0.31041413  - Train acc: 0.8818 - Val loss: 1.07303226\n",
      "(5.88 min) Epoch 30/300 -- Iteration 28782 - Batch 855/963 - Train loss: 0.31087736  - Train acc: 0.8815 - Val loss: 1.07303226\n",
      "(5.88 min) Epoch 30/300 -- Iteration 28791 - Batch 864/963 - Train loss: 0.31097181  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.88 min) Epoch 30/300 -- Iteration 28800 - Batch 873/963 - Train loss: 0.31113995  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.89 min) Epoch 30/300 -- Iteration 28809 - Batch 882/963 - Train loss: 0.31138732  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.89 min) Epoch 30/300 -- Iteration 28818 - Batch 891/963 - Train loss: 0.31150869  - Train acc: 0.8815 - Val loss: 1.07303226\n",
      "(5.89 min) Epoch 30/300 -- Iteration 28827 - Batch 900/963 - Train loss: 0.31139693  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.89 min) Epoch 30/300 -- Iteration 28836 - Batch 909/963 - Train loss: 0.31140862  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.89 min) Epoch 30/300 -- Iteration 28845 - Batch 918/963 - Train loss: 0.31159925  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.90 min) Epoch 30/300 -- Iteration 28854 - Batch 927/963 - Train loss: 0.31161605  - Train acc: 0.8815 - Val loss: 1.07303226\n",
      "(5.90 min) Epoch 30/300 -- Iteration 28863 - Batch 936/963 - Train loss: 0.31166194  - Train acc: 0.8814 - Val loss: 1.07303226\n",
      "(5.90 min) Epoch 30/300 -- Iteration 28872 - Batch 945/963 - Train loss: 0.31131645  - Train acc: 0.8816 - Val loss: 1.07303226\n",
      "(5.90 min) Epoch 30/300 -- Iteration 28881 - Batch 954/963 - Train loss: 0.31131013  - Train acc: 0.8815 - Val loss: 1.07303226\n",
      "(5.90 min) Epoch 30/300 -- Iteration 28890 - Batch 962/963 - Train loss: 0.31140490  - Train acc: 0.8815 - Val loss: 1.04404569 - Val acc: 0.6050\n",
      "(5.91 min) Epoch 31/300 -- Iteration 28899 - Batch 9/963 - Train loss: 0.30916902  - Train acc: 0.8766 - Val loss: 1.04404569\n",
      "(5.91 min) Epoch 31/300 -- Iteration 28908 - Batch 18/963 - Train loss: 0.31533354  - Train acc: 0.8758 - Val loss: 1.04404569\n",
      "(5.91 min) Epoch 31/300 -- Iteration 28917 - Batch 27/963 - Train loss: 0.31375734  - Train acc: 0.8764 - Val loss: 1.04404569\n",
      "(5.91 min) Epoch 31/300 -- Iteration 28926 - Batch 36/963 - Train loss: 0.31024573  - Train acc: 0.8784 - Val loss: 1.04404569\n",
      "(5.91 min) Epoch 31/300 -- Iteration 28935 - Batch 45/963 - Train loss: 0.30989121  - Train acc: 0.8794 - Val loss: 1.04404569\n",
      "(5.92 min) Epoch 31/300 -- Iteration 28944 - Batch 54/963 - Train loss: 0.31370234  - Train acc: 0.8788 - Val loss: 1.04404569\n",
      "(5.92 min) Epoch 31/300 -- Iteration 28953 - Batch 63/963 - Train loss: 0.31367107  - Train acc: 0.8787 - Val loss: 1.04404569\n",
      "(5.92 min) Epoch 31/300 -- Iteration 28962 - Batch 72/963 - Train loss: 0.31248644  - Train acc: 0.8796 - Val loss: 1.04404569\n",
      "(5.92 min) Epoch 31/300 -- Iteration 28971 - Batch 81/963 - Train loss: 0.31460828  - Train acc: 0.8784 - Val loss: 1.04404569\n",
      "(5.92 min) Epoch 31/300 -- Iteration 28980 - Batch 90/963 - Train loss: 0.31248817  - Train acc: 0.8792 - Val loss: 1.04404569\n",
      "(5.93 min) Epoch 31/300 -- Iteration 28989 - Batch 99/963 - Train loss: 0.31294215  - Train acc: 0.8787 - Val loss: 1.04404569\n",
      "(5.93 min) Epoch 31/300 -- Iteration 28998 - Batch 108/963 - Train loss: 0.31106018  - Train acc: 0.8799 - Val loss: 1.04404569\n",
      "(5.93 min) Epoch 31/300 -- Iteration 29007 - Batch 117/963 - Train loss: 0.31017156  - Train acc: 0.8802 - Val loss: 1.04404569\n",
      "(5.93 min) Epoch 31/300 -- Iteration 29016 - Batch 126/963 - Train loss: 0.31146735  - Train acc: 0.8798 - Val loss: 1.04404569\n",
      "(5.93 min) Epoch 31/300 -- Iteration 29025 - Batch 135/963 - Train loss: 0.31209958  - Train acc: 0.8791 - Val loss: 1.04404569\n",
      "(5.93 min) Epoch 31/300 -- Iteration 29034 - Batch 144/963 - Train loss: 0.31141866  - Train acc: 0.8791 - Val loss: 1.04404569\n",
      "(5.94 min) Epoch 31/300 -- Iteration 29043 - Batch 153/963 - Train loss: 0.31132245  - Train acc: 0.8795 - Val loss: 1.04404569\n",
      "(5.94 min) Epoch 31/300 -- Iteration 29052 - Batch 162/963 - Train loss: 0.31051275  - Train acc: 0.8803 - Val loss: 1.04404569\n",
      "(5.94 min) Epoch 31/300 -- Iteration 29061 - Batch 171/963 - Train loss: 0.31031408  - Train acc: 0.8805 - Val loss: 1.04404569\n",
      "(5.94 min) Epoch 31/300 -- Iteration 29070 - Batch 180/963 - Train loss: 0.31016661  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(5.94 min) Epoch 31/300 -- Iteration 29079 - Batch 189/963 - Train loss: 0.30956042  - Train acc: 0.8812 - Val loss: 1.04404569\n",
      "(5.95 min) Epoch 31/300 -- Iteration 29088 - Batch 198/963 - Train loss: 0.30951179  - Train acc: 0.8812 - Val loss: 1.04404569\n",
      "(5.95 min) Epoch 31/300 -- Iteration 29097 - Batch 207/963 - Train loss: 0.30868479  - Train acc: 0.8818 - Val loss: 1.04404569\n",
      "(5.95 min) Epoch 31/300 -- Iteration 29106 - Batch 216/963 - Train loss: 0.30781215  - Train acc: 0.8826 - Val loss: 1.04404569\n",
      "(5.95 min) Epoch 31/300 -- Iteration 29115 - Batch 225/963 - Train loss: 0.30724255  - Train acc: 0.8828 - Val loss: 1.04404569\n",
      "(5.95 min) Epoch 31/300 -- Iteration 29124 - Batch 234/963 - Train loss: 0.30778865  - Train acc: 0.8827 - Val loss: 1.04404569\n",
      "(5.96 min) Epoch 31/300 -- Iteration 29133 - Batch 243/963 - Train loss: 0.30907363  - Train acc: 0.8819 - Val loss: 1.04404569\n",
      "(5.96 min) Epoch 31/300 -- Iteration 29142 - Batch 252/963 - Train loss: 0.30984739  - Train acc: 0.8816 - Val loss: 1.04404569\n",
      "(5.96 min) Epoch 31/300 -- Iteration 29151 - Batch 261/963 - Train loss: 0.30979406  - Train acc: 0.8816 - Val loss: 1.04404569\n",
      "(5.96 min) Epoch 31/300 -- Iteration 29160 - Batch 270/963 - Train loss: 0.30981511  - Train acc: 0.8818 - Val loss: 1.04404569\n",
      "(5.97 min) Epoch 31/300 -- Iteration 29169 - Batch 279/963 - Train loss: 0.30977700  - Train acc: 0.8820 - Val loss: 1.04404569\n",
      "(5.97 min) Epoch 31/300 -- Iteration 29178 - Batch 288/963 - Train loss: 0.30868924  - Train acc: 0.8824 - Val loss: 1.04404569\n",
      "(5.97 min) Epoch 31/300 -- Iteration 29187 - Batch 297/963 - Train loss: 0.30928671  - Train acc: 0.8821 - Val loss: 1.04404569\n",
      "(5.97 min) Epoch 31/300 -- Iteration 29196 - Batch 306/963 - Train loss: 0.30889171  - Train acc: 0.8822 - Val loss: 1.04404569\n",
      "(5.97 min) Epoch 31/300 -- Iteration 29205 - Batch 315/963 - Train loss: 0.30781678  - Train acc: 0.8827 - Val loss: 1.04404569\n",
      "(5.98 min) Epoch 31/300 -- Iteration 29214 - Batch 324/963 - Train loss: 0.30867725  - Train acc: 0.8825 - Val loss: 1.04404569\n",
      "(5.98 min) Epoch 31/300 -- Iteration 29223 - Batch 333/963 - Train loss: 0.30852984  - Train acc: 0.8827 - Val loss: 1.04404569\n",
      "(5.98 min) Epoch 31/300 -- Iteration 29232 - Batch 342/963 - Train loss: 0.30795340  - Train acc: 0.8827 - Val loss: 1.04404569\n",
      "(5.98 min) Epoch 31/300 -- Iteration 29241 - Batch 351/963 - Train loss: 0.30771360  - Train acc: 0.8827 - Val loss: 1.04404569\n",
      "(5.98 min) Epoch 31/300 -- Iteration 29250 - Batch 360/963 - Train loss: 0.30802110  - Train acc: 0.8825 - Val loss: 1.04404569\n",
      "(5.99 min) Epoch 31/300 -- Iteration 29259 - Batch 369/963 - Train loss: 0.30843950  - Train acc: 0.8824 - Val loss: 1.04404569\n",
      "(5.99 min) Epoch 31/300 -- Iteration 29268 - Batch 378/963 - Train loss: 0.30849927  - Train acc: 0.8825 - Val loss: 1.04404569\n",
      "(5.99 min) Epoch 31/300 -- Iteration 29277 - Batch 387/963 - Train loss: 0.30837197  - Train acc: 0.8826 - Val loss: 1.04404569\n",
      "(5.99 min) Epoch 31/300 -- Iteration 29286 - Batch 396/963 - Train loss: 0.30861119  - Train acc: 0.8824 - Val loss: 1.04404569\n",
      "(5.99 min) Epoch 31/300 -- Iteration 29295 - Batch 405/963 - Train loss: 0.30861555  - Train acc: 0.8823 - Val loss: 1.04404569\n",
      "(6.00 min) Epoch 31/300 -- Iteration 29304 - Batch 414/963 - Train loss: 0.30894266  - Train acc: 0.8821 - Val loss: 1.04404569\n",
      "(6.00 min) Epoch 31/300 -- Iteration 29313 - Batch 423/963 - Train loss: 0.30877990  - Train acc: 0.8822 - Val loss: 1.04404569\n",
      "(6.00 min) Epoch 31/300 -- Iteration 29322 - Batch 432/963 - Train loss: 0.30854388  - Train acc: 0.8822 - Val loss: 1.04404569\n",
      "(6.00 min) Epoch 31/300 -- Iteration 29331 - Batch 441/963 - Train loss: 0.30839158  - Train acc: 0.8820 - Val loss: 1.04404569\n",
      "(6.00 min) Epoch 31/300 -- Iteration 29340 - Batch 450/963 - Train loss: 0.30896551  - Train acc: 0.8817 - Val loss: 1.04404569\n",
      "(6.01 min) Epoch 31/300 -- Iteration 29349 - Batch 459/963 - Train loss: 0.30938631  - Train acc: 0.8813 - Val loss: 1.04404569\n",
      "(6.01 min) Epoch 31/300 -- Iteration 29358 - Batch 468/963 - Train loss: 0.30883185  - Train acc: 0.8814 - Val loss: 1.04404569\n",
      "(6.01 min) Epoch 31/300 -- Iteration 29367 - Batch 477/963 - Train loss: 0.30913256  - Train acc: 0.8812 - Val loss: 1.04404569\n",
      "(6.01 min) Epoch 31/300 -- Iteration 29376 - Batch 486/963 - Train loss: 0.30888052  - Train acc: 0.8811 - Val loss: 1.04404569\n",
      "(6.01 min) Epoch 31/300 -- Iteration 29385 - Batch 495/963 - Train loss: 0.30887332  - Train acc: 0.8811 - Val loss: 1.04404569\n",
      "(6.02 min) Epoch 31/300 -- Iteration 29394 - Batch 504/963 - Train loss: 0.30930943  - Train acc: 0.8810 - Val loss: 1.04404569\n",
      "(6.02 min) Epoch 31/300 -- Iteration 29403 - Batch 513/963 - Train loss: 0.30901748  - Train acc: 0.8811 - Val loss: 1.04404569\n",
      "(6.02 min) Epoch 31/300 -- Iteration 29412 - Batch 522/963 - Train loss: 0.30976684  - Train acc: 0.8810 - Val loss: 1.04404569\n",
      "(6.02 min) Epoch 31/300 -- Iteration 29421 - Batch 531/963 - Train loss: 0.31001016  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.02 min) Epoch 31/300 -- Iteration 29430 - Batch 540/963 - Train loss: 0.31008927  - Train acc: 0.8808 - Val loss: 1.04404569\n",
      "(6.02 min) Epoch 31/300 -- Iteration 29439 - Batch 549/963 - Train loss: 0.30980148  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.03 min) Epoch 31/300 -- Iteration 29448 - Batch 558/963 - Train loss: 0.30963562  - Train acc: 0.8810 - Val loss: 1.04404569\n",
      "(6.03 min) Epoch 31/300 -- Iteration 29457 - Batch 567/963 - Train loss: 0.30974560  - Train acc: 0.8810 - Val loss: 1.04404569\n",
      "(6.03 min) Epoch 31/300 -- Iteration 29466 - Batch 576/963 - Train loss: 0.30954017  - Train acc: 0.8811 - Val loss: 1.04404569\n",
      "(6.03 min) Epoch 31/300 -- Iteration 29475 - Batch 585/963 - Train loss: 0.30963501  - Train acc: 0.8811 - Val loss: 1.04404569\n",
      "(6.03 min) Epoch 31/300 -- Iteration 29484 - Batch 594/963 - Train loss: 0.30985318  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.04 min) Epoch 31/300 -- Iteration 29493 - Batch 603/963 - Train loss: 0.30988856  - Train acc: 0.8808 - Val loss: 1.04404569\n",
      "(6.04 min) Epoch 31/300 -- Iteration 29502 - Batch 612/963 - Train loss: 0.30970978  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.04 min) Epoch 31/300 -- Iteration 29511 - Batch 621/963 - Train loss: 0.30944717  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.04 min) Epoch 31/300 -- Iteration 29520 - Batch 630/963 - Train loss: 0.30982100  - Train acc: 0.8808 - Val loss: 1.04404569\n",
      "(6.04 min) Epoch 31/300 -- Iteration 29529 - Batch 639/963 - Train loss: 0.30965378  - Train acc: 0.8808 - Val loss: 1.04404569\n",
      "(6.05 min) Epoch 31/300 -- Iteration 29538 - Batch 648/963 - Train loss: 0.31006592  - Train acc: 0.8806 - Val loss: 1.04404569\n",
      "(6.05 min) Epoch 31/300 -- Iteration 29547 - Batch 657/963 - Train loss: 0.30995670  - Train acc: 0.8807 - Val loss: 1.04404569\n",
      "(6.05 min) Epoch 31/300 -- Iteration 29556 - Batch 666/963 - Train loss: 0.30973732  - Train acc: 0.8807 - Val loss: 1.04404569\n",
      "(6.05 min) Epoch 31/300 -- Iteration 29565 - Batch 675/963 - Train loss: 0.30961774  - Train acc: 0.8807 - Val loss: 1.04404569\n",
      "(6.05 min) Epoch 31/300 -- Iteration 29574 - Batch 684/963 - Train loss: 0.30945016  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.06 min) Epoch 31/300 -- Iteration 29583 - Batch 693/963 - Train loss: 0.30936864  - Train acc: 0.8808 - Val loss: 1.04404569\n",
      "(6.06 min) Epoch 31/300 -- Iteration 29592 - Batch 702/963 - Train loss: 0.30938804  - Train acc: 0.8807 - Val loss: 1.04404569\n",
      "(6.06 min) Epoch 31/300 -- Iteration 29601 - Batch 711/963 - Train loss: 0.30958818  - Train acc: 0.8807 - Val loss: 1.04404569\n",
      "(6.06 min) Epoch 31/300 -- Iteration 29610 - Batch 720/963 - Train loss: 0.30977486  - Train acc: 0.8806 - Val loss: 1.04404569\n",
      "(6.06 min) Epoch 31/300 -- Iteration 29619 - Batch 729/963 - Train loss: 0.31010279  - Train acc: 0.8804 - Val loss: 1.04404569\n",
      "(6.07 min) Epoch 31/300 -- Iteration 29628 - Batch 738/963 - Train loss: 0.31020559  - Train acc: 0.8804 - Val loss: 1.04404569\n",
      "(6.07 min) Epoch 31/300 -- Iteration 29637 - Batch 747/963 - Train loss: 0.31058457  - Train acc: 0.8802 - Val loss: 1.04404569\n",
      "(6.07 min) Epoch 31/300 -- Iteration 29646 - Batch 756/963 - Train loss: 0.31027676  - Train acc: 0.8804 - Val loss: 1.04404569\n",
      "(6.07 min) Epoch 31/300 -- Iteration 29655 - Batch 765/963 - Train loss: 0.31022141  - Train acc: 0.8803 - Val loss: 1.04404569\n",
      "(6.07 min) Epoch 31/300 -- Iteration 29664 - Batch 774/963 - Train loss: 0.31045595  - Train acc: 0.8803 - Val loss: 1.04404569\n",
      "(6.08 min) Epoch 31/300 -- Iteration 29673 - Batch 783/963 - Train loss: 0.31020401  - Train acc: 0.8804 - Val loss: 1.04404569\n",
      "(6.08 min) Epoch 31/300 -- Iteration 29682 - Batch 792/963 - Train loss: 0.31021788  - Train acc: 0.8805 - Val loss: 1.04404569\n",
      "(6.08 min) Epoch 31/300 -- Iteration 29691 - Batch 801/963 - Train loss: 0.31029682  - Train acc: 0.8804 - Val loss: 1.04404569\n",
      "(6.08 min) Epoch 31/300 -- Iteration 29700 - Batch 810/963 - Train loss: 0.31041018  - Train acc: 0.8805 - Val loss: 1.04404569\n",
      "(6.08 min) Epoch 31/300 -- Iteration 29709 - Batch 819/963 - Train loss: 0.31026865  - Train acc: 0.8806 - Val loss: 1.04404569\n",
      "(6.09 min) Epoch 31/300 -- Iteration 29718 - Batch 828/963 - Train loss: 0.31028920  - Train acc: 0.8806 - Val loss: 1.04404569\n",
      "(6.09 min) Epoch 31/300 -- Iteration 29727 - Batch 837/963 - Train loss: 0.31022496  - Train acc: 0.8806 - Val loss: 1.04404569\n",
      "(6.09 min) Epoch 31/300 -- Iteration 29736 - Batch 846/963 - Train loss: 0.31029747  - Train acc: 0.8806 - Val loss: 1.04404569\n",
      "(6.09 min) Epoch 31/300 -- Iteration 29745 - Batch 855/963 - Train loss: 0.31021305  - Train acc: 0.8806 - Val loss: 1.04404569\n",
      "(6.09 min) Epoch 31/300 -- Iteration 29754 - Batch 864/963 - Train loss: 0.31009963  - Train acc: 0.8806 - Val loss: 1.04404569\n",
      "(6.10 min) Epoch 31/300 -- Iteration 29763 - Batch 873/963 - Train loss: 0.30958750  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.10 min) Epoch 31/300 -- Iteration 29772 - Batch 882/963 - Train loss: 0.30974097  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.10 min) Epoch 31/300 -- Iteration 29781 - Batch 891/963 - Train loss: 0.30964621  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.10 min) Epoch 31/300 -- Iteration 29790 - Batch 900/963 - Train loss: 0.30959552  - Train acc: 0.8809 - Val loss: 1.04404569\n",
      "(6.10 min) Epoch 31/300 -- Iteration 29799 - Batch 909/963 - Train loss: 0.30977216  - Train acc: 0.8807 - Val loss: 1.04404569\n",
      "(6.11 min) Epoch 31/300 -- Iteration 29808 - Batch 918/963 - Train loss: 0.30968849  - Train acc: 0.8807 - Val loss: 1.04404569\n",
      "(6.11 min) Epoch 31/300 -- Iteration 29817 - Batch 927/963 - Train loss: 0.30971404  - Train acc: 0.8807 - Val loss: 1.04404569\n",
      "(6.11 min) Epoch 31/300 -- Iteration 29826 - Batch 936/963 - Train loss: 0.30961657  - Train acc: 0.8808 - Val loss: 1.04404569\n",
      "(6.11 min) Epoch 31/300 -- Iteration 29835 - Batch 945/963 - Train loss: 0.30972685  - Train acc: 0.8808 - Val loss: 1.04404569\n",
      "(6.12 min) Epoch 31/300 -- Iteration 29844 - Batch 954/963 - Train loss: 0.30965382  - Train acc: 0.8808 - Val loss: 1.04404569\n",
      "(6.12 min) Epoch 31/300 -- Iteration 29853 - Batch 962/963 - Train loss: 0.30957472  - Train acc: 0.8809 - Val loss: 1.05303299 - Val acc: 0.6017\n",
      "(6.12 min) Epoch 32/300 -- Iteration 29862 - Batch 9/963 - Train loss: 0.28950406  - Train acc: 0.8961 - Val loss: 1.05303299\n",
      "(6.12 min) Epoch 32/300 -- Iteration 29871 - Batch 18/963 - Train loss: 0.31892009  - Train acc: 0.8799 - Val loss: 1.05303299\n",
      "(6.12 min) Epoch 32/300 -- Iteration 29880 - Batch 27/963 - Train loss: 0.31199288  - Train acc: 0.8853 - Val loss: 1.05303299\n",
      "(6.13 min) Epoch 32/300 -- Iteration 29889 - Batch 36/963 - Train loss: 0.31191454  - Train acc: 0.8858 - Val loss: 1.05303299\n",
      "(6.13 min) Epoch 32/300 -- Iteration 29898 - Batch 45/963 - Train loss: 0.31097583  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.13 min) Epoch 32/300 -- Iteration 29907 - Batch 54/963 - Train loss: 0.30889882  - Train acc: 0.8837 - Val loss: 1.05303299\n",
      "(6.13 min) Epoch 32/300 -- Iteration 29916 - Batch 63/963 - Train loss: 0.30284408  - Train acc: 0.8846 - Val loss: 1.05303299\n",
      "(6.13 min) Epoch 32/300 -- Iteration 29925 - Batch 72/963 - Train loss: 0.30097899  - Train acc: 0.8850 - Val loss: 1.05303299\n",
      "(6.14 min) Epoch 32/300 -- Iteration 29934 - Batch 81/963 - Train loss: 0.29949165  - Train acc: 0.8857 - Val loss: 1.05303299\n",
      "(6.14 min) Epoch 32/300 -- Iteration 29943 - Batch 90/963 - Train loss: 0.30027591  - Train acc: 0.8857 - Val loss: 1.05303299\n",
      "(6.14 min) Epoch 32/300 -- Iteration 29952 - Batch 99/963 - Train loss: 0.30247384  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.14 min) Epoch 32/300 -- Iteration 29961 - Batch 108/963 - Train loss: 0.30436665  - Train acc: 0.8840 - Val loss: 1.05303299\n",
      "(6.15 min) Epoch 32/300 -- Iteration 29970 - Batch 117/963 - Train loss: 0.30323053  - Train acc: 0.8847 - Val loss: 1.05303299\n",
      "(6.15 min) Epoch 32/300 -- Iteration 29979 - Batch 126/963 - Train loss: 0.30402879  - Train acc: 0.8841 - Val loss: 1.05303299\n",
      "(6.15 min) Epoch 32/300 -- Iteration 29988 - Batch 135/963 - Train loss: 0.30255832  - Train acc: 0.8849 - Val loss: 1.05303299\n",
      "(6.15 min) Epoch 32/300 -- Iteration 29997 - Batch 144/963 - Train loss: 0.30189676  - Train acc: 0.8851 - Val loss: 1.05303299\n",
      "(6.15 min) Epoch 32/300 -- Iteration 30006 - Batch 153/963 - Train loss: 0.30071136  - Train acc: 0.8860 - Val loss: 1.05303299\n",
      "(6.16 min) Epoch 32/300 -- Iteration 30015 - Batch 162/963 - Train loss: 0.30018689  - Train acc: 0.8859 - Val loss: 1.05303299\n",
      "(6.16 min) Epoch 32/300 -- Iteration 30024 - Batch 171/963 - Train loss: 0.30211545  - Train acc: 0.8848 - Val loss: 1.05303299\n",
      "(6.16 min) Epoch 32/300 -- Iteration 30033 - Batch 180/963 - Train loss: 0.30186818  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.17 min) Epoch 32/300 -- Iteration 30042 - Batch 189/963 - Train loss: 0.30290286  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.17 min) Epoch 32/300 -- Iteration 30051 - Batch 198/963 - Train loss: 0.30408597  - Train acc: 0.8840 - Val loss: 1.05303299\n",
      "(6.17 min) Epoch 32/300 -- Iteration 30060 - Batch 207/963 - Train loss: 0.30414338  - Train acc: 0.8842 - Val loss: 1.05303299\n",
      "(6.17 min) Epoch 32/300 -- Iteration 30069 - Batch 216/963 - Train loss: 0.30376291  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.17 min) Epoch 32/300 -- Iteration 30078 - Batch 225/963 - Train loss: 0.30509514  - Train acc: 0.8840 - Val loss: 1.05303299\n",
      "(6.18 min) Epoch 32/300 -- Iteration 30087 - Batch 234/963 - Train loss: 0.30531989  - Train acc: 0.8840 - Val loss: 1.05303299\n",
      "(6.18 min) Epoch 32/300 -- Iteration 30096 - Batch 243/963 - Train loss: 0.30413454  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.18 min) Epoch 32/300 -- Iteration 30105 - Batch 252/963 - Train loss: 0.30352121  - Train acc: 0.8846 - Val loss: 1.05303299\n",
      "(6.18 min) Epoch 32/300 -- Iteration 30114 - Batch 261/963 - Train loss: 0.30349579  - Train acc: 0.8850 - Val loss: 1.05303299\n",
      "(6.18 min) Epoch 32/300 -- Iteration 30123 - Batch 270/963 - Train loss: 0.30384324  - Train acc: 0.8849 - Val loss: 1.05303299\n",
      "(6.19 min) Epoch 32/300 -- Iteration 30132 - Batch 279/963 - Train loss: 0.30413433  - Train acc: 0.8850 - Val loss: 1.05303299\n",
      "(6.19 min) Epoch 32/300 -- Iteration 30141 - Batch 288/963 - Train loss: 0.30485369  - Train acc: 0.8849 - Val loss: 1.05303299\n",
      "(6.19 min) Epoch 32/300 -- Iteration 30150 - Batch 297/963 - Train loss: 0.30564478  - Train acc: 0.8847 - Val loss: 1.05303299\n",
      "(6.19 min) Epoch 32/300 -- Iteration 30159 - Batch 306/963 - Train loss: 0.30513536  - Train acc: 0.8851 - Val loss: 1.05303299\n",
      "(6.19 min) Epoch 32/300 -- Iteration 30168 - Batch 315/963 - Train loss: 0.30552309  - Train acc: 0.8850 - Val loss: 1.05303299\n",
      "(6.19 min) Epoch 32/300 -- Iteration 30177 - Batch 324/963 - Train loss: 0.30626416  - Train acc: 0.8848 - Val loss: 1.05303299\n",
      "(6.20 min) Epoch 32/300 -- Iteration 30186 - Batch 333/963 - Train loss: 0.30551338  - Train acc: 0.8849 - Val loss: 1.05303299\n",
      "(6.20 min) Epoch 32/300 -- Iteration 30195 - Batch 342/963 - Train loss: 0.30453530  - Train acc: 0.8853 - Val loss: 1.05303299\n",
      "(6.20 min) Epoch 32/300 -- Iteration 30204 - Batch 351/963 - Train loss: 0.30458843  - Train acc: 0.8853 - Val loss: 1.05303299\n",
      "(6.20 min) Epoch 32/300 -- Iteration 30213 - Batch 360/963 - Train loss: 0.30481496  - Train acc: 0.8851 - Val loss: 1.05303299\n",
      "(6.20 min) Epoch 32/300 -- Iteration 30222 - Batch 369/963 - Train loss: 0.30490557  - Train acc: 0.8851 - Val loss: 1.05303299\n",
      "(6.21 min) Epoch 32/300 -- Iteration 30231 - Batch 378/963 - Train loss: 0.30498842  - Train acc: 0.8850 - Val loss: 1.05303299\n",
      "(6.21 min) Epoch 32/300 -- Iteration 30240 - Batch 387/963 - Train loss: 0.30486488  - Train acc: 0.8852 - Val loss: 1.05303299\n",
      "(6.21 min) Epoch 32/300 -- Iteration 30249 - Batch 396/963 - Train loss: 0.30491684  - Train acc: 0.8852 - Val loss: 1.05303299\n",
      "(6.21 min) Epoch 32/300 -- Iteration 30258 - Batch 405/963 - Train loss: 0.30493921  - Train acc: 0.8850 - Val loss: 1.05303299\n",
      "(6.21 min) Epoch 32/300 -- Iteration 30267 - Batch 414/963 - Train loss: 0.30466757  - Train acc: 0.8850 - Val loss: 1.05303299\n",
      "(6.21 min) Epoch 32/300 -- Iteration 30276 - Batch 423/963 - Train loss: 0.30555715  - Train acc: 0.8846 - Val loss: 1.05303299\n",
      "(6.22 min) Epoch 32/300 -- Iteration 30285 - Batch 432/963 - Train loss: 0.30510780  - Train acc: 0.8848 - Val loss: 1.05303299\n",
      "(6.22 min) Epoch 32/300 -- Iteration 30294 - Batch 441/963 - Train loss: 0.30543704  - Train acc: 0.8847 - Val loss: 1.05303299\n",
      "(6.22 min) Epoch 32/300 -- Iteration 30303 - Batch 450/963 - Train loss: 0.30530549  - Train acc: 0.8848 - Val loss: 1.05303299\n",
      "(6.22 min) Epoch 32/300 -- Iteration 30312 - Batch 459/963 - Train loss: 0.30580079  - Train acc: 0.8847 - Val loss: 1.05303299\n",
      "(6.22 min) Epoch 32/300 -- Iteration 30321 - Batch 468/963 - Train loss: 0.30584291  - Train acc: 0.8847 - Val loss: 1.05303299\n",
      "(6.23 min) Epoch 32/300 -- Iteration 30330 - Batch 477/963 - Train loss: 0.30547385  - Train acc: 0.8849 - Val loss: 1.05303299\n",
      "(6.23 min) Epoch 32/300 -- Iteration 30339 - Batch 486/963 - Train loss: 0.30552840  - Train acc: 0.8849 - Val loss: 1.05303299\n",
      "(6.23 min) Epoch 32/300 -- Iteration 30348 - Batch 495/963 - Train loss: 0.30590492  - Train acc: 0.8847 - Val loss: 1.05303299\n",
      "(6.23 min) Epoch 32/300 -- Iteration 30357 - Batch 504/963 - Train loss: 0.30573421  - Train acc: 0.8846 - Val loss: 1.05303299\n",
      "(6.23 min) Epoch 32/300 -- Iteration 30366 - Batch 513/963 - Train loss: 0.30567212  - Train acc: 0.8847 - Val loss: 1.05303299\n",
      "(6.24 min) Epoch 32/300 -- Iteration 30375 - Batch 522/963 - Train loss: 0.30596149  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.24 min) Epoch 32/300 -- Iteration 30384 - Batch 531/963 - Train loss: 0.30589424  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.24 min) Epoch 32/300 -- Iteration 30393 - Batch 540/963 - Train loss: 0.30586971  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.24 min) Epoch 32/300 -- Iteration 30402 - Batch 549/963 - Train loss: 0.30529952  - Train acc: 0.8846 - Val loss: 1.05303299\n",
      "(6.24 min) Epoch 32/300 -- Iteration 30411 - Batch 558/963 - Train loss: 0.30600711  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.25 min) Epoch 32/300 -- Iteration 30420 - Batch 567/963 - Train loss: 0.30580156  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.25 min) Epoch 32/300 -- Iteration 30429 - Batch 576/963 - Train loss: 0.30534859  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.25 min) Epoch 32/300 -- Iteration 30438 - Batch 585/963 - Train loss: 0.30522446  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.25 min) Epoch 32/300 -- Iteration 30447 - Batch 594/963 - Train loss: 0.30540365  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.25 min) Epoch 32/300 -- Iteration 30456 - Batch 603/963 - Train loss: 0.30516850  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.25 min) Epoch 32/300 -- Iteration 30465 - Batch 612/963 - Train loss: 0.30499331  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.26 min) Epoch 32/300 -- Iteration 30474 - Batch 621/963 - Train loss: 0.30547047  - Train acc: 0.8841 - Val loss: 1.05303299\n",
      "(6.26 min) Epoch 32/300 -- Iteration 30483 - Batch 630/963 - Train loss: 0.30579032  - Train acc: 0.8841 - Val loss: 1.05303299\n",
      "(6.26 min) Epoch 32/300 -- Iteration 30492 - Batch 639/963 - Train loss: 0.30554593  - Train acc: 0.8842 - Val loss: 1.05303299\n",
      "(6.26 min) Epoch 32/300 -- Iteration 30501 - Batch 648/963 - Train loss: 0.30533304  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.26 min) Epoch 32/300 -- Iteration 30510 - Batch 657/963 - Train loss: 0.30557015  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.27 min) Epoch 32/300 -- Iteration 30519 - Batch 666/963 - Train loss: 0.30628580  - Train acc: 0.8840 - Val loss: 1.05303299\n",
      "(6.27 min) Epoch 32/300 -- Iteration 30528 - Batch 675/963 - Train loss: 0.30601158  - Train acc: 0.8842 - Val loss: 1.05303299\n",
      "(6.27 min) Epoch 32/300 -- Iteration 30537 - Batch 684/963 - Train loss: 0.30599021  - Train acc: 0.8841 - Val loss: 1.05303299\n",
      "(6.27 min) Epoch 32/300 -- Iteration 30546 - Batch 693/963 - Train loss: 0.30604981  - Train acc: 0.8841 - Val loss: 1.05303299\n",
      "(6.27 min) Epoch 32/300 -- Iteration 30555 - Batch 702/963 - Train loss: 0.30583911  - Train acc: 0.8842 - Val loss: 1.05303299\n",
      "(6.28 min) Epoch 32/300 -- Iteration 30564 - Batch 711/963 - Train loss: 0.30610475  - Train acc: 0.8841 - Val loss: 1.05303299\n",
      "(6.28 min) Epoch 32/300 -- Iteration 30573 - Batch 720/963 - Train loss: 0.30597186  - Train acc: 0.8841 - Val loss: 1.05303299\n",
      "(6.28 min) Epoch 32/300 -- Iteration 30582 - Batch 729/963 - Train loss: 0.30599272  - Train acc: 0.8842 - Val loss: 1.05303299\n",
      "(6.28 min) Epoch 32/300 -- Iteration 30591 - Batch 738/963 - Train loss: 0.30588214  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.28 min) Epoch 32/300 -- Iteration 30600 - Batch 747/963 - Train loss: 0.30585327  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.29 min) Epoch 32/300 -- Iteration 30609 - Batch 756/963 - Train loss: 0.30594695  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.29 min) Epoch 32/300 -- Iteration 30618 - Batch 765/963 - Train loss: 0.30602602  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.29 min) Epoch 32/300 -- Iteration 30627 - Batch 774/963 - Train loss: 0.30634210  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.29 min) Epoch 32/300 -- Iteration 30636 - Batch 783/963 - Train loss: 0.30626280  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.29 min) Epoch 32/300 -- Iteration 30645 - Batch 792/963 - Train loss: 0.30632836  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.30 min) Epoch 32/300 -- Iteration 30654 - Batch 801/963 - Train loss: 0.30634150  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.30 min) Epoch 32/300 -- Iteration 30663 - Batch 810/963 - Train loss: 0.30624154  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.30 min) Epoch 32/300 -- Iteration 30672 - Batch 819/963 - Train loss: 0.30656172  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.30 min) Epoch 32/300 -- Iteration 30681 - Batch 828/963 - Train loss: 0.30660447  - Train acc: 0.8842 - Val loss: 1.05303299\n",
      "(6.31 min) Epoch 32/300 -- Iteration 30690 - Batch 837/963 - Train loss: 0.30628708  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.31 min) Epoch 32/300 -- Iteration 30699 - Batch 846/963 - Train loss: 0.30658885  - Train acc: 0.8843 - Val loss: 1.05303299\n",
      "(6.31 min) Epoch 32/300 -- Iteration 30708 - Batch 855/963 - Train loss: 0.30629830  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.31 min) Epoch 32/300 -- Iteration 30717 - Batch 864/963 - Train loss: 0.30646080  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.31 min) Epoch 32/300 -- Iteration 30726 - Batch 873/963 - Train loss: 0.30633702  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.31 min) Epoch 32/300 -- Iteration 30735 - Batch 882/963 - Train loss: 0.30607828  - Train acc: 0.8846 - Val loss: 1.05303299\n",
      "(6.32 min) Epoch 32/300 -- Iteration 30744 - Batch 891/963 - Train loss: 0.30612293  - Train acc: 0.8847 - Val loss: 1.05303299\n",
      "(6.32 min) Epoch 32/300 -- Iteration 30753 - Batch 900/963 - Train loss: 0.30630642  - Train acc: 0.8846 - Val loss: 1.05303299\n",
      "(6.32 min) Epoch 32/300 -- Iteration 30762 - Batch 909/963 - Train loss: 0.30678332  - Train acc: 0.8844 - Val loss: 1.05303299\n",
      "(6.32 min) Epoch 32/300 -- Iteration 30771 - Batch 918/963 - Train loss: 0.30672661  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.32 min) Epoch 32/300 -- Iteration 30780 - Batch 927/963 - Train loss: 0.30660837  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.33 min) Epoch 32/300 -- Iteration 30789 - Batch 936/963 - Train loss: 0.30661212  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.33 min) Epoch 32/300 -- Iteration 30798 - Batch 945/963 - Train loss: 0.30667658  - Train acc: 0.8845 - Val loss: 1.05303299\n",
      "(6.33 min) Epoch 32/300 -- Iteration 30807 - Batch 954/963 - Train loss: 0.30651282  - Train acc: 0.8846 - Val loss: 1.05303299\n",
      "(6.33 min) Epoch 32/300 -- Iteration 30816 - Batch 962/963 - Train loss: 0.30651932  - Train acc: 0.8846 - Val loss: 1.03950953 - Val acc: 0.6033\n",
      "(6.33 min) Epoch 33/300 -- Iteration 30825 - Batch 9/963 - Train loss: 0.27156204  - Train acc: 0.8914 - Val loss: 1.03950953\n",
      "(6.34 min) Epoch 33/300 -- Iteration 30834 - Batch 18/963 - Train loss: 0.28104884  - Train acc: 0.8890 - Val loss: 1.03950953\n",
      "(6.34 min) Epoch 33/300 -- Iteration 30843 - Batch 27/963 - Train loss: 0.28901085  - Train acc: 0.8903 - Val loss: 1.03950953\n",
      "(6.34 min) Epoch 33/300 -- Iteration 30852 - Batch 36/963 - Train loss: 0.28892213  - Train acc: 0.8879 - Val loss: 1.03950953\n",
      "(6.34 min) Epoch 33/300 -- Iteration 30861 - Batch 45/963 - Train loss: 0.29149954  - Train acc: 0.8869 - Val loss: 1.03950953\n",
      "(6.34 min) Epoch 33/300 -- Iteration 30870 - Batch 54/963 - Train loss: 0.29385471  - Train acc: 0.8852 - Val loss: 1.03950953\n",
      "(6.35 min) Epoch 33/300 -- Iteration 30879 - Batch 63/963 - Train loss: 0.29424456  - Train acc: 0.8860 - Val loss: 1.03950953\n",
      "(6.35 min) Epoch 33/300 -- Iteration 30888 - Batch 72/963 - Train loss: 0.29419439  - Train acc: 0.8863 - Val loss: 1.03950953\n",
      "(6.35 min) Epoch 33/300 -- Iteration 30897 - Batch 81/963 - Train loss: 0.29549342  - Train acc: 0.8859 - Val loss: 1.03950953\n",
      "(6.35 min) Epoch 33/300 -- Iteration 30906 - Batch 90/963 - Train loss: 0.29588504  - Train acc: 0.8856 - Val loss: 1.03950953\n",
      "(6.35 min) Epoch 33/300 -- Iteration 30915 - Batch 99/963 - Train loss: 0.29647959  - Train acc: 0.8857 - Val loss: 1.03950953\n",
      "(6.36 min) Epoch 33/300 -- Iteration 30924 - Batch 108/963 - Train loss: 0.29734757  - Train acc: 0.8856 - Val loss: 1.03950953\n",
      "(6.36 min) Epoch 33/300 -- Iteration 30933 - Batch 117/963 - Train loss: 0.29738781  - Train acc: 0.8857 - Val loss: 1.03950953\n",
      "(6.36 min) Epoch 33/300 -- Iteration 30942 - Batch 126/963 - Train loss: 0.30115427  - Train acc: 0.8844 - Val loss: 1.03950953\n",
      "(6.36 min) Epoch 33/300 -- Iteration 30951 - Batch 135/963 - Train loss: 0.30003274  - Train acc: 0.8847 - Val loss: 1.03950953\n",
      "(6.36 min) Epoch 33/300 -- Iteration 30960 - Batch 144/963 - Train loss: 0.30042431  - Train acc: 0.8847 - Val loss: 1.03950953\n",
      "(6.37 min) Epoch 33/300 -- Iteration 30969 - Batch 153/963 - Train loss: 0.30039308  - Train acc: 0.8847 - Val loss: 1.03950953\n",
      "(6.37 min) Epoch 33/300 -- Iteration 30978 - Batch 162/963 - Train loss: 0.30103127  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.37 min) Epoch 33/300 -- Iteration 30987 - Batch 171/963 - Train loss: 0.30156922  - Train acc: 0.8843 - Val loss: 1.03950953\n",
      "(6.37 min) Epoch 33/300 -- Iteration 30996 - Batch 180/963 - Train loss: 0.30285766  - Train acc: 0.8837 - Val loss: 1.03950953\n",
      "(6.37 min) Epoch 33/300 -- Iteration 31005 - Batch 189/963 - Train loss: 0.30140468  - Train acc: 0.8844 - Val loss: 1.03950953\n",
      "(6.38 min) Epoch 33/300 -- Iteration 31014 - Batch 198/963 - Train loss: 0.30153704  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.38 min) Epoch 33/300 -- Iteration 31023 - Batch 207/963 - Train loss: 0.30261344  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.38 min) Epoch 33/300 -- Iteration 31032 - Batch 216/963 - Train loss: 0.30132267  - Train acc: 0.8847 - Val loss: 1.03950953\n",
      "(6.38 min) Epoch 33/300 -- Iteration 31041 - Batch 225/963 - Train loss: 0.30132160  - Train acc: 0.8846 - Val loss: 1.03950953\n",
      "(6.38 min) Epoch 33/300 -- Iteration 31050 - Batch 234/963 - Train loss: 0.30344004  - Train acc: 0.8835 - Val loss: 1.03950953\n",
      "(6.39 min) Epoch 33/300 -- Iteration 31059 - Batch 243/963 - Train loss: 0.30318550  - Train acc: 0.8838 - Val loss: 1.03950953\n",
      "(6.39 min) Epoch 33/300 -- Iteration 31068 - Batch 252/963 - Train loss: 0.30409457  - Train acc: 0.8834 - Val loss: 1.03950953\n",
      "(6.39 min) Epoch 33/300 -- Iteration 31077 - Batch 261/963 - Train loss: 0.30356075  - Train acc: 0.8835 - Val loss: 1.03950953\n",
      "(6.39 min) Epoch 33/300 -- Iteration 31086 - Batch 270/963 - Train loss: 0.30440145  - Train acc: 0.8832 - Val loss: 1.03950953\n",
      "(6.39 min) Epoch 33/300 -- Iteration 31095 - Batch 279/963 - Train loss: 0.30489817  - Train acc: 0.8832 - Val loss: 1.03950953\n",
      "(6.40 min) Epoch 33/300 -- Iteration 31104 - Batch 288/963 - Train loss: 0.30539316  - Train acc: 0.8830 - Val loss: 1.03950953\n",
      "(6.40 min) Epoch 33/300 -- Iteration 31113 - Batch 297/963 - Train loss: 0.30533377  - Train acc: 0.8831 - Val loss: 1.03950953\n",
      "(6.40 min) Epoch 33/300 -- Iteration 31122 - Batch 306/963 - Train loss: 0.30616364  - Train acc: 0.8828 - Val loss: 1.03950953\n",
      "(6.40 min) Epoch 33/300 -- Iteration 31131 - Batch 315/963 - Train loss: 0.30549181  - Train acc: 0.8830 - Val loss: 1.03950953\n",
      "(6.40 min) Epoch 33/300 -- Iteration 31140 - Batch 324/963 - Train loss: 0.30540709  - Train acc: 0.8830 - Val loss: 1.03950953\n",
      "(6.40 min) Epoch 33/300 -- Iteration 31149 - Batch 333/963 - Train loss: 0.30513874  - Train acc: 0.8830 - Val loss: 1.03950953\n",
      "(6.41 min) Epoch 33/300 -- Iteration 31158 - Batch 342/963 - Train loss: 0.30536289  - Train acc: 0.8832 - Val loss: 1.03950953\n",
      "(6.41 min) Epoch 33/300 -- Iteration 31167 - Batch 351/963 - Train loss: 0.30565352  - Train acc: 0.8833 - Val loss: 1.03950953\n",
      "(6.41 min) Epoch 33/300 -- Iteration 31176 - Batch 360/963 - Train loss: 0.30511143  - Train acc: 0.8834 - Val loss: 1.03950953\n",
      "(6.41 min) Epoch 33/300 -- Iteration 31185 - Batch 369/963 - Train loss: 0.30543389  - Train acc: 0.8832 - Val loss: 1.03950953\n",
      "(6.42 min) Epoch 33/300 -- Iteration 31194 - Batch 378/963 - Train loss: 0.30504227  - Train acc: 0.8835 - Val loss: 1.03950953\n",
      "(6.42 min) Epoch 33/300 -- Iteration 31203 - Batch 387/963 - Train loss: 0.30532840  - Train acc: 0.8833 - Val loss: 1.03950953\n",
      "(6.42 min) Epoch 33/300 -- Iteration 31212 - Batch 396/963 - Train loss: 0.30474196  - Train acc: 0.8837 - Val loss: 1.03950953\n",
      "(6.42 min) Epoch 33/300 -- Iteration 31221 - Batch 405/963 - Train loss: 0.30474532  - Train acc: 0.8838 - Val loss: 1.03950953\n",
      "(6.42 min) Epoch 33/300 -- Iteration 31230 - Batch 414/963 - Train loss: 0.30393467  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.42 min) Epoch 33/300 -- Iteration 31239 - Batch 423/963 - Train loss: 0.30388143  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.43 min) Epoch 33/300 -- Iteration 31248 - Batch 432/963 - Train loss: 0.30395360  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.43 min) Epoch 33/300 -- Iteration 31257 - Batch 441/963 - Train loss: 0.30390702  - Train acc: 0.8838 - Val loss: 1.03950953\n",
      "(6.43 min) Epoch 33/300 -- Iteration 31266 - Batch 450/963 - Train loss: 0.30436512  - Train acc: 0.8836 - Val loss: 1.03950953\n",
      "(6.43 min) Epoch 33/300 -- Iteration 31275 - Batch 459/963 - Train loss: 0.30452463  - Train acc: 0.8835 - Val loss: 1.03950953\n",
      "(6.43 min) Epoch 33/300 -- Iteration 31284 - Batch 468/963 - Train loss: 0.30509290  - Train acc: 0.8832 - Val loss: 1.03950953\n",
      "(6.44 min) Epoch 33/300 -- Iteration 31293 - Batch 477/963 - Train loss: 0.30484358  - Train acc: 0.8833 - Val loss: 1.03950953\n",
      "(6.44 min) Epoch 33/300 -- Iteration 31302 - Batch 486/963 - Train loss: 0.30438982  - Train acc: 0.8834 - Val loss: 1.03950953\n",
      "(6.44 min) Epoch 33/300 -- Iteration 31311 - Batch 495/963 - Train loss: 0.30469388  - Train acc: 0.8832 - Val loss: 1.03950953\n",
      "(6.44 min) Epoch 33/300 -- Iteration 31320 - Batch 504/963 - Train loss: 0.30448228  - Train acc: 0.8834 - Val loss: 1.03950953\n",
      "(6.44 min) Epoch 33/300 -- Iteration 31329 - Batch 513/963 - Train loss: 0.30470186  - Train acc: 0.8833 - Val loss: 1.03950953\n",
      "(6.45 min) Epoch 33/300 -- Iteration 31338 - Batch 522/963 - Train loss: 0.30457305  - Train acc: 0.8833 - Val loss: 1.03950953\n",
      "(6.45 min) Epoch 33/300 -- Iteration 31347 - Batch 531/963 - Train loss: 0.30450010  - Train acc: 0.8834 - Val loss: 1.03950953\n",
      "(6.45 min) Epoch 33/300 -- Iteration 31356 - Batch 540/963 - Train loss: 0.30403038  - Train acc: 0.8837 - Val loss: 1.03950953\n",
      "(6.45 min) Epoch 33/300 -- Iteration 31365 - Batch 549/963 - Train loss: 0.30424574  - Train acc: 0.8836 - Val loss: 1.03950953\n",
      "(6.45 min) Epoch 33/300 -- Iteration 31374 - Batch 558/963 - Train loss: 0.30415365  - Train acc: 0.8837 - Val loss: 1.03950953\n",
      "(6.46 min) Epoch 33/300 -- Iteration 31383 - Batch 567/963 - Train loss: 0.30427907  - Train acc: 0.8837 - Val loss: 1.03950953\n",
      "(6.46 min) Epoch 33/300 -- Iteration 31392 - Batch 576/963 - Train loss: 0.30402802  - Train acc: 0.8839 - Val loss: 1.03950953\n",
      "(6.46 min) Epoch 33/300 -- Iteration 31401 - Batch 585/963 - Train loss: 0.30419774  - Train acc: 0.8837 - Val loss: 1.03950953\n",
      "(6.46 min) Epoch 33/300 -- Iteration 31410 - Batch 594/963 - Train loss: 0.30421719  - Train acc: 0.8839 - Val loss: 1.03950953\n",
      "(6.46 min) Epoch 33/300 -- Iteration 31419 - Batch 603/963 - Train loss: 0.30391522  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.47 min) Epoch 33/300 -- Iteration 31428 - Batch 612/963 - Train loss: 0.30387140  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.47 min) Epoch 33/300 -- Iteration 31437 - Batch 621/963 - Train loss: 0.30376227  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.47 min) Epoch 33/300 -- Iteration 31446 - Batch 630/963 - Train loss: 0.30407545  - Train acc: 0.8839 - Val loss: 1.03950953\n",
      "(6.47 min) Epoch 33/300 -- Iteration 31455 - Batch 639/963 - Train loss: 0.30400610  - Train acc: 0.8839 - Val loss: 1.03950953\n",
      "(6.47 min) Epoch 33/300 -- Iteration 31464 - Batch 648/963 - Train loss: 0.30397347  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.48 min) Epoch 33/300 -- Iteration 31473 - Batch 657/963 - Train loss: 0.30395897  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.48 min) Epoch 33/300 -- Iteration 31482 - Batch 666/963 - Train loss: 0.30380755  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.48 min) Epoch 33/300 -- Iteration 31491 - Batch 675/963 - Train loss: 0.30377725  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.48 min) Epoch 33/300 -- Iteration 31500 - Batch 684/963 - Train loss: 0.30372501  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.48 min) Epoch 33/300 -- Iteration 31509 - Batch 693/963 - Train loss: 0.30366338  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.49 min) Epoch 33/300 -- Iteration 31518 - Batch 702/963 - Train loss: 0.30376936  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.49 min) Epoch 33/300 -- Iteration 31527 - Batch 711/963 - Train loss: 0.30379674  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.49 min) Epoch 33/300 -- Iteration 31536 - Batch 720/963 - Train loss: 0.30396297  - Train acc: 0.8839 - Val loss: 1.03950953\n",
      "(6.49 min) Epoch 33/300 -- Iteration 31545 - Batch 729/963 - Train loss: 0.30412982  - Train acc: 0.8839 - Val loss: 1.03950953\n",
      "(6.49 min) Epoch 33/300 -- Iteration 31554 - Batch 738/963 - Train loss: 0.30424628  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.50 min) Epoch 33/300 -- Iteration 31563 - Batch 747/963 - Train loss: 0.30414509  - Train acc: 0.8842 - Val loss: 1.03950953\n",
      "(6.50 min) Epoch 33/300 -- Iteration 31572 - Batch 756/963 - Train loss: 0.30411130  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.50 min) Epoch 33/300 -- Iteration 31581 - Batch 765/963 - Train loss: 0.30426805  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.50 min) Epoch 33/300 -- Iteration 31590 - Batch 774/963 - Train loss: 0.30417648  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.50 min) Epoch 33/300 -- Iteration 31599 - Batch 783/963 - Train loss: 0.30465948  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.51 min) Epoch 33/300 -- Iteration 31608 - Batch 792/963 - Train loss: 0.30461689  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.51 min) Epoch 33/300 -- Iteration 31617 - Batch 801/963 - Train loss: 0.30419951  - Train acc: 0.8842 - Val loss: 1.03950953\n",
      "(6.51 min) Epoch 33/300 -- Iteration 31626 - Batch 810/963 - Train loss: 0.30430455  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.51 min) Epoch 33/300 -- Iteration 31635 - Batch 819/963 - Train loss: 0.30423398  - Train acc: 0.8842 - Val loss: 1.03950953\n",
      "(6.51 min) Epoch 33/300 -- Iteration 31644 - Batch 828/963 - Train loss: 0.30394689  - Train acc: 0.8842 - Val loss: 1.03950953\n",
      "(6.52 min) Epoch 33/300 -- Iteration 31653 - Batch 837/963 - Train loss: 0.30404690  - Train acc: 0.8842 - Val loss: 1.03950953\n",
      "(6.52 min) Epoch 33/300 -- Iteration 31662 - Batch 846/963 - Train loss: 0.30419488  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.52 min) Epoch 33/300 -- Iteration 31671 - Batch 855/963 - Train loss: 0.30430953  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.52 min) Epoch 33/300 -- Iteration 31680 - Batch 864/963 - Train loss: 0.30432833  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.52 min) Epoch 33/300 -- Iteration 31689 - Batch 873/963 - Train loss: 0.30443524  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.53 min) Epoch 33/300 -- Iteration 31698 - Batch 882/963 - Train loss: 0.30458966  - Train acc: 0.8839 - Val loss: 1.03950953\n",
      "(6.53 min) Epoch 33/300 -- Iteration 31707 - Batch 891/963 - Train loss: 0.30458275  - Train acc: 0.8839 - Val loss: 1.03950953\n",
      "(6.53 min) Epoch 33/300 -- Iteration 31716 - Batch 900/963 - Train loss: 0.30450507  - Train acc: 0.8840 - Val loss: 1.03950953\n",
      "(6.53 min) Epoch 33/300 -- Iteration 31725 - Batch 909/963 - Train loss: 0.30422732  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.53 min) Epoch 33/300 -- Iteration 31734 - Batch 918/963 - Train loss: 0.30434537  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.53 min) Epoch 33/300 -- Iteration 31743 - Batch 927/963 - Train loss: 0.30435347  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.54 min) Epoch 33/300 -- Iteration 31752 - Batch 936/963 - Train loss: 0.30440377  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.54 min) Epoch 33/300 -- Iteration 31761 - Batch 945/963 - Train loss: 0.30427466  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.54 min) Epoch 33/300 -- Iteration 31770 - Batch 954/963 - Train loss: 0.30424692  - Train acc: 0.8841 - Val loss: 1.03950953\n",
      "(6.54 min) Epoch 33/300 -- Iteration 31779 - Batch 962/963 - Train loss: 0.30441871  - Train acc: 0.8841 - Val loss: 1.04436505 - Val acc: 0.6000\n",
      "(6.54 min) Epoch 34/300 -- Iteration 31788 - Batch 9/963 - Train loss: 0.27708512  - Train acc: 0.9039 - Val loss: 1.04436505\n",
      "(6.55 min) Epoch 34/300 -- Iteration 31797 - Batch 18/963 - Train loss: 0.29374686  - Train acc: 0.8972 - Val loss: 1.04436505\n",
      "(6.55 min) Epoch 34/300 -- Iteration 31806 - Batch 27/963 - Train loss: 0.30279027  - Train acc: 0.8929 - Val loss: 1.04436505\n",
      "(6.55 min) Epoch 34/300 -- Iteration 31815 - Batch 36/963 - Train loss: 0.30437817  - Train acc: 0.8881 - Val loss: 1.04436505\n",
      "(6.55 min) Epoch 34/300 -- Iteration 31824 - Batch 45/963 - Train loss: 0.30525493  - Train acc: 0.8857 - Val loss: 1.04436505\n",
      "(6.55 min) Epoch 34/300 -- Iteration 31833 - Batch 54/963 - Train loss: 0.30673160  - Train acc: 0.8865 - Val loss: 1.04436505\n",
      "(6.56 min) Epoch 34/300 -- Iteration 31842 - Batch 63/963 - Train loss: 0.30252017  - Train acc: 0.8883 - Val loss: 1.04436505\n",
      "(6.56 min) Epoch 34/300 -- Iteration 31851 - Batch 72/963 - Train loss: 0.30635740  - Train acc: 0.8852 - Val loss: 1.04436505\n",
      "(6.56 min) Epoch 34/300 -- Iteration 31860 - Batch 81/963 - Train loss: 0.30924989  - Train acc: 0.8844 - Val loss: 1.04436505\n",
      "(6.56 min) Epoch 34/300 -- Iteration 31869 - Batch 90/963 - Train loss: 0.31000741  - Train acc: 0.8844 - Val loss: 1.04436505\n",
      "(6.56 min) Epoch 34/300 -- Iteration 31878 - Batch 99/963 - Train loss: 0.31088754  - Train acc: 0.8837 - Val loss: 1.04436505\n",
      "(6.57 min) Epoch 34/300 -- Iteration 31887 - Batch 108/963 - Train loss: 0.30783631  - Train acc: 0.8853 - Val loss: 1.04436505\n",
      "(6.57 min) Epoch 34/300 -- Iteration 31896 - Batch 117/963 - Train loss: 0.30869174  - Train acc: 0.8849 - Val loss: 1.04436505\n",
      "(6.57 min) Epoch 34/300 -- Iteration 31905 - Batch 126/963 - Train loss: 0.30981119  - Train acc: 0.8841 - Val loss: 1.04436505\n",
      "(6.57 min) Epoch 34/300 -- Iteration 31914 - Batch 135/963 - Train loss: 0.30986140  - Train acc: 0.8838 - Val loss: 1.04436505\n",
      "(6.57 min) Epoch 34/300 -- Iteration 31923 - Batch 144/963 - Train loss: 0.30865201  - Train acc: 0.8844 - Val loss: 1.04436505\n",
      "(6.58 min) Epoch 34/300 -- Iteration 31932 - Batch 153/963 - Train loss: 0.30725907  - Train acc: 0.8844 - Val loss: 1.04436505\n",
      "(6.58 min) Epoch 34/300 -- Iteration 31941 - Batch 162/963 - Train loss: 0.30563002  - Train acc: 0.8848 - Val loss: 1.04436505\n",
      "(6.58 min) Epoch 34/300 -- Iteration 31950 - Batch 171/963 - Train loss: 0.30582237  - Train acc: 0.8850 - Val loss: 1.04436505\n",
      "(6.58 min) Epoch 34/300 -- Iteration 31959 - Batch 180/963 - Train loss: 0.30555114  - Train acc: 0.8850 - Val loss: 1.04436505\n",
      "(6.58 min) Epoch 34/300 -- Iteration 31968 - Batch 189/963 - Train loss: 0.30589851  - Train acc: 0.8850 - Val loss: 1.04436505\n",
      "(6.59 min) Epoch 34/300 -- Iteration 31977 - Batch 198/963 - Train loss: 0.30598667  - Train acc: 0.8846 - Val loss: 1.04436505\n",
      "(6.59 min) Epoch 34/300 -- Iteration 31986 - Batch 207/963 - Train loss: 0.30695497  - Train acc: 0.8842 - Val loss: 1.04436505\n",
      "(6.59 min) Epoch 34/300 -- Iteration 31995 - Batch 216/963 - Train loss: 0.30733910  - Train acc: 0.8840 - Val loss: 1.04436505\n",
      "(6.59 min) Epoch 34/300 -- Iteration 32004 - Batch 225/963 - Train loss: 0.30789616  - Train acc: 0.8837 - Val loss: 1.04436505\n",
      "(6.59 min) Epoch 34/300 -- Iteration 32013 - Batch 234/963 - Train loss: 0.30686464  - Train acc: 0.8840 - Val loss: 1.04436505\n",
      "(6.60 min) Epoch 34/300 -- Iteration 32022 - Batch 243/963 - Train loss: 0.30722938  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.60 min) Epoch 34/300 -- Iteration 32031 - Batch 252/963 - Train loss: 0.30648456  - Train acc: 0.8842 - Val loss: 1.04436505\n",
      "(6.60 min) Epoch 34/300 -- Iteration 32040 - Batch 261/963 - Train loss: 0.30712180  - Train acc: 0.8838 - Val loss: 1.04436505\n",
      "(6.60 min) Epoch 34/300 -- Iteration 32049 - Batch 270/963 - Train loss: 0.30652126  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.60 min) Epoch 34/300 -- Iteration 32058 - Batch 279/963 - Train loss: 0.30657205  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.61 min) Epoch 34/300 -- Iteration 32067 - Batch 288/963 - Train loss: 0.30649402  - Train acc: 0.8840 - Val loss: 1.04436505\n",
      "(6.61 min) Epoch 34/300 -- Iteration 32076 - Batch 297/963 - Train loss: 0.30571308  - Train acc: 0.8843 - Val loss: 1.04436505\n",
      "(6.61 min) Epoch 34/300 -- Iteration 32085 - Batch 306/963 - Train loss: 0.30540270  - Train acc: 0.8844 - Val loss: 1.04436505\n",
      "(6.61 min) Epoch 34/300 -- Iteration 32094 - Batch 315/963 - Train loss: 0.30570337  - Train acc: 0.8843 - Val loss: 1.04436505\n",
      "(6.61 min) Epoch 34/300 -- Iteration 32103 - Batch 324/963 - Train loss: 0.30589053  - Train acc: 0.8841 - Val loss: 1.04436505\n",
      "(6.61 min) Epoch 34/300 -- Iteration 32112 - Batch 333/963 - Train loss: 0.30554938  - Train acc: 0.8841 - Val loss: 1.04436505\n",
      "(6.62 min) Epoch 34/300 -- Iteration 32121 - Batch 342/963 - Train loss: 0.30606940  - Train acc: 0.8840 - Val loss: 1.04436505\n",
      "(6.62 min) Epoch 34/300 -- Iteration 32130 - Batch 351/963 - Train loss: 0.30668391  - Train acc: 0.8837 - Val loss: 1.04436505\n",
      "(6.62 min) Epoch 34/300 -- Iteration 32139 - Batch 360/963 - Train loss: 0.30676555  - Train acc: 0.8837 - Val loss: 1.04436505\n",
      "(6.62 min) Epoch 34/300 -- Iteration 32148 - Batch 369/963 - Train loss: 0.30643360  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.62 min) Epoch 34/300 -- Iteration 32157 - Batch 378/963 - Train loss: 0.30636488  - Train acc: 0.8840 - Val loss: 1.04436505\n",
      "(6.63 min) Epoch 34/300 -- Iteration 32166 - Batch 387/963 - Train loss: 0.30592706  - Train acc: 0.8842 - Val loss: 1.04436505\n",
      "(6.63 min) Epoch 34/300 -- Iteration 32175 - Batch 396/963 - Train loss: 0.30628064  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.63 min) Epoch 34/300 -- Iteration 32184 - Batch 405/963 - Train loss: 0.30632778  - Train acc: 0.8838 - Val loss: 1.04436505\n",
      "(6.63 min) Epoch 34/300 -- Iteration 32193 - Batch 414/963 - Train loss: 0.30600193  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.63 min) Epoch 34/300 -- Iteration 32202 - Batch 423/963 - Train loss: 0.30559745  - Train acc: 0.8843 - Val loss: 1.04436505\n",
      "(6.64 min) Epoch 34/300 -- Iteration 32211 - Batch 432/963 - Train loss: 0.30549426  - Train acc: 0.8845 - Val loss: 1.04436505\n",
      "(6.64 min) Epoch 34/300 -- Iteration 32220 - Batch 441/963 - Train loss: 0.30514440  - Train acc: 0.8847 - Val loss: 1.04436505\n",
      "(6.64 min) Epoch 34/300 -- Iteration 32229 - Batch 450/963 - Train loss: 0.30494450  - Train acc: 0.8847 - Val loss: 1.04436505\n",
      "(6.64 min) Epoch 34/300 -- Iteration 32238 - Batch 459/963 - Train loss: 0.30504198  - Train acc: 0.8847 - Val loss: 1.04436505\n",
      "(6.64 min) Epoch 34/300 -- Iteration 32247 - Batch 468/963 - Train loss: 0.30520596  - Train acc: 0.8847 - Val loss: 1.04436505\n",
      "(6.65 min) Epoch 34/300 -- Iteration 32256 - Batch 477/963 - Train loss: 0.30497853  - Train acc: 0.8849 - Val loss: 1.04436505\n",
      "(6.65 min) Epoch 34/300 -- Iteration 32265 - Batch 486/963 - Train loss: 0.30462837  - Train acc: 0.8850 - Val loss: 1.04436505\n",
      "(6.65 min) Epoch 34/300 -- Iteration 32274 - Batch 495/963 - Train loss: 0.30461914  - Train acc: 0.8850 - Val loss: 1.04436505\n",
      "(6.65 min) Epoch 34/300 -- Iteration 32283 - Batch 504/963 - Train loss: 0.30482168  - Train acc: 0.8849 - Val loss: 1.04436505\n",
      "(6.65 min) Epoch 34/300 -- Iteration 32292 - Batch 513/963 - Train loss: 0.30474940  - Train acc: 0.8850 - Val loss: 1.04436505\n",
      "(6.66 min) Epoch 34/300 -- Iteration 32301 - Batch 522/963 - Train loss: 0.30485538  - Train acc: 0.8849 - Val loss: 1.04436505\n",
      "(6.66 min) Epoch 34/300 -- Iteration 32310 - Batch 531/963 - Train loss: 0.30487061  - Train acc: 0.8850 - Val loss: 1.04436505\n",
      "(6.66 min) Epoch 34/300 -- Iteration 32319 - Batch 540/963 - Train loss: 0.30528064  - Train acc: 0.8849 - Val loss: 1.04436505\n",
      "(6.66 min) Epoch 34/300 -- Iteration 32328 - Batch 549/963 - Train loss: 0.30540265  - Train acc: 0.8847 - Val loss: 1.04436505\n",
      "(6.67 min) Epoch 34/300 -- Iteration 32337 - Batch 558/963 - Train loss: 0.30568471  - Train acc: 0.8845 - Val loss: 1.04436505\n",
      "(6.67 min) Epoch 34/300 -- Iteration 32346 - Batch 567/963 - Train loss: 0.30547883  - Train acc: 0.8845 - Val loss: 1.04436505\n",
      "(6.67 min) Epoch 34/300 -- Iteration 32355 - Batch 576/963 - Train loss: 0.30599275  - Train acc: 0.8843 - Val loss: 1.04436505\n",
      "(6.67 min) Epoch 34/300 -- Iteration 32364 - Batch 585/963 - Train loss: 0.30587184  - Train acc: 0.8842 - Val loss: 1.04436505\n",
      "(6.67 min) Epoch 34/300 -- Iteration 32373 - Batch 594/963 - Train loss: 0.30547912  - Train acc: 0.8845 - Val loss: 1.04436505\n",
      "(6.68 min) Epoch 34/300 -- Iteration 32382 - Batch 603/963 - Train loss: 0.30541254  - Train acc: 0.8845 - Val loss: 1.04436505\n",
      "(6.68 min) Epoch 34/300 -- Iteration 32391 - Batch 612/963 - Train loss: 0.30535533  - Train acc: 0.8843 - Val loss: 1.04436505\n",
      "(6.68 min) Epoch 34/300 -- Iteration 32400 - Batch 621/963 - Train loss: 0.30552779  - Train acc: 0.8844 - Val loss: 1.04436505\n",
      "(6.68 min) Epoch 34/300 -- Iteration 32409 - Batch 630/963 - Train loss: 0.30513267  - Train acc: 0.8846 - Val loss: 1.04436505\n",
      "(6.68 min) Epoch 34/300 -- Iteration 32418 - Batch 639/963 - Train loss: 0.30489421  - Train acc: 0.8846 - Val loss: 1.04436505\n",
      "(6.68 min) Epoch 34/300 -- Iteration 32427 - Batch 648/963 - Train loss: 0.30536808  - Train acc: 0.8845 - Val loss: 1.04436505\n",
      "(6.69 min) Epoch 34/300 -- Iteration 32436 - Batch 657/963 - Train loss: 0.30558308  - Train acc: 0.8845 - Val loss: 1.04436505\n",
      "(6.69 min) Epoch 34/300 -- Iteration 32445 - Batch 666/963 - Train loss: 0.30568132  - Train acc: 0.8844 - Val loss: 1.04436505\n",
      "(6.69 min) Epoch 34/300 -- Iteration 32454 - Batch 675/963 - Train loss: 0.30564184  - Train acc: 0.8843 - Val loss: 1.04436505\n",
      "(6.69 min) Epoch 34/300 -- Iteration 32463 - Batch 684/963 - Train loss: 0.30574840  - Train acc: 0.8842 - Val loss: 1.04436505\n",
      "(6.69 min) Epoch 34/300 -- Iteration 32472 - Batch 693/963 - Train loss: 0.30576408  - Train acc: 0.8843 - Val loss: 1.04436505\n",
      "(6.70 min) Epoch 34/300 -- Iteration 32481 - Batch 702/963 - Train loss: 0.30575139  - Train acc: 0.8843 - Val loss: 1.04436505\n",
      "(6.70 min) Epoch 34/300 -- Iteration 32490 - Batch 711/963 - Train loss: 0.30581455  - Train acc: 0.8843 - Val loss: 1.04436505\n",
      "(6.70 min) Epoch 34/300 -- Iteration 32499 - Batch 720/963 - Train loss: 0.30645783  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.70 min) Epoch 34/300 -- Iteration 32508 - Batch 729/963 - Train loss: 0.30626267  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.70 min) Epoch 34/300 -- Iteration 32517 - Batch 738/963 - Train loss: 0.30607726  - Train acc: 0.8840 - Val loss: 1.04436505\n",
      "(6.71 min) Epoch 34/300 -- Iteration 32526 - Batch 747/963 - Train loss: 0.30613378  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.71 min) Epoch 34/300 -- Iteration 32535 - Batch 756/963 - Train loss: 0.30629198  - Train acc: 0.8839 - Val loss: 1.04436505\n",
      "(6.71 min) Epoch 34/300 -- Iteration 32544 - Batch 765/963 - Train loss: 0.30643881  - Train acc: 0.8838 - Val loss: 1.04436505\n",
      "(6.71 min) Epoch 34/300 -- Iteration 32553 - Batch 774/963 - Train loss: 0.30650875  - Train acc: 0.8837 - Val loss: 1.04436505\n",
      "(6.71 min) Epoch 34/300 -- Iteration 32562 - Batch 783/963 - Train loss: 0.30662867  - Train acc: 0.8836 - Val loss: 1.04436505\n",
      "(6.71 min) Epoch 34/300 -- Iteration 32571 - Batch 792/963 - Train loss: 0.30640432  - Train acc: 0.8837 - Val loss: 1.04436505\n",
      "(6.72 min) Epoch 34/300 -- Iteration 32580 - Batch 801/963 - Train loss: 0.30625388  - Train acc: 0.8837 - Val loss: 1.04436505\n",
      "(6.72 min) Epoch 34/300 -- Iteration 32589 - Batch 810/963 - Train loss: 0.30640405  - Train acc: 0.8836 - Val loss: 1.04436505\n",
      "(6.72 min) Epoch 34/300 -- Iteration 32598 - Batch 819/963 - Train loss: 0.30618553  - Train acc: 0.8837 - Val loss: 1.04436505\n",
      "(6.72 min) Epoch 34/300 -- Iteration 32607 - Batch 828/963 - Train loss: 0.30570506  - Train acc: 0.8838 - Val loss: 1.04436505\n",
      "(6.73 min) Epoch 34/300 -- Iteration 32616 - Batch 837/963 - Train loss: 0.30567287  - Train acc: 0.8838 - Val loss: 1.04436505\n",
      "(6.73 min) Epoch 34/300 -- Iteration 32625 - Batch 846/963 - Train loss: 0.30601282  - Train acc: 0.8835 - Val loss: 1.04436505\n",
      "(6.73 min) Epoch 34/300 -- Iteration 32634 - Batch 855/963 - Train loss: 0.30597987  - Train acc: 0.8835 - Val loss: 1.04436505\n",
      "(6.73 min) Epoch 34/300 -- Iteration 32643 - Batch 864/963 - Train loss: 0.30584825  - Train acc: 0.8835 - Val loss: 1.04436505\n",
      "(6.73 min) Epoch 34/300 -- Iteration 32652 - Batch 873/963 - Train loss: 0.30591294  - Train acc: 0.8835 - Val loss: 1.04436505\n",
      "(6.74 min) Epoch 34/300 -- Iteration 32661 - Batch 882/963 - Train loss: 0.30583440  - Train acc: 0.8836 - Val loss: 1.04436505\n",
      "(6.74 min) Epoch 34/300 -- Iteration 32670 - Batch 891/963 - Train loss: 0.30547696  - Train acc: 0.8837 - Val loss: 1.04436505\n",
      "(6.74 min) Epoch 34/300 -- Iteration 32679 - Batch 900/963 - Train loss: 0.30515488  - Train acc: 0.8838 - Val loss: 1.04436505\n",
      "(6.74 min) Epoch 34/300 -- Iteration 32688 - Batch 909/963 - Train loss: 0.30538559  - Train acc: 0.8836 - Val loss: 1.04436505\n",
      "(6.74 min) Epoch 34/300 -- Iteration 32697 - Batch 918/963 - Train loss: 0.30529123  - Train acc: 0.8835 - Val loss: 1.04436505\n",
      "(6.75 min) Epoch 34/300 -- Iteration 32706 - Batch 927/963 - Train loss: 0.30546779  - Train acc: 0.8834 - Val loss: 1.04436505\n",
      "(6.75 min) Epoch 34/300 -- Iteration 32715 - Batch 936/963 - Train loss: 0.30561598  - Train acc: 0.8834 - Val loss: 1.04436505\n",
      "(6.75 min) Epoch 34/300 -- Iteration 32724 - Batch 945/963 - Train loss: 0.30546882  - Train acc: 0.8835 - Val loss: 1.04436505\n",
      "(6.75 min) Epoch 34/300 -- Iteration 32733 - Batch 954/963 - Train loss: 0.30549929  - Train acc: 0.8834 - Val loss: 1.04436505\n",
      "(6.75 min) Epoch 34/300 -- Iteration 32742 - Batch 962/963 - Train loss: 0.30566269  - Train acc: 0.8834 - Val loss: 1.04349244 - Val acc: 0.5933\n",
      "(6.76 min) Epoch 35/300 -- Iteration 32751 - Batch 9/963 - Train loss: 0.31462474  - Train acc: 0.8844 - Val loss: 1.04349244\n",
      "(6.76 min) Epoch 35/300 -- Iteration 32760 - Batch 18/963 - Train loss: 0.31265206  - Train acc: 0.8820 - Val loss: 1.04349244\n",
      "(6.76 min) Epoch 35/300 -- Iteration 32769 - Batch 27/963 - Train loss: 0.30884574  - Train acc: 0.8803 - Val loss: 1.04349244\n",
      "(6.76 min) Epoch 35/300 -- Iteration 32778 - Batch 36/963 - Train loss: 0.30764166  - Train acc: 0.8807 - Val loss: 1.04349244\n",
      "(6.76 min) Epoch 35/300 -- Iteration 32787 - Batch 45/963 - Train loss: 0.31138730  - Train acc: 0.8804 - Val loss: 1.04349244\n",
      "(6.77 min) Epoch 35/300 -- Iteration 32796 - Batch 54/963 - Train loss: 0.31711797  - Train acc: 0.8764 - Val loss: 1.04349244\n",
      "(6.77 min) Epoch 35/300 -- Iteration 32805 - Batch 63/963 - Train loss: 0.31649357  - Train acc: 0.8774 - Val loss: 1.04349244\n",
      "(6.77 min) Epoch 35/300 -- Iteration 32814 - Batch 72/963 - Train loss: 0.31886045  - Train acc: 0.8761 - Val loss: 1.04349244\n",
      "(6.77 min) Epoch 35/300 -- Iteration 32823 - Batch 81/963 - Train loss: 0.31953945  - Train acc: 0.8754 - Val loss: 1.04349244\n",
      "(6.77 min) Epoch 35/300 -- Iteration 32832 - Batch 90/963 - Train loss: 0.31530609  - Train acc: 0.8765 - Val loss: 1.04349244\n",
      "(6.78 min) Epoch 35/300 -- Iteration 32841 - Batch 99/963 - Train loss: 0.31336291  - Train acc: 0.8778 - Val loss: 1.04349244\n",
      "(6.78 min) Epoch 35/300 -- Iteration 32850 - Batch 108/963 - Train loss: 0.31441024  - Train acc: 0.8777 - Val loss: 1.04349244\n",
      "(6.78 min) Epoch 35/300 -- Iteration 32859 - Batch 117/963 - Train loss: 0.31310998  - Train acc: 0.8788 - Val loss: 1.04349244\n",
      "(6.78 min) Epoch 35/300 -- Iteration 32868 - Batch 126/963 - Train loss: 0.31259781  - Train acc: 0.8789 - Val loss: 1.04349244\n",
      "(6.78 min) Epoch 35/300 -- Iteration 32877 - Batch 135/963 - Train loss: 0.31248676  - Train acc: 0.8790 - Val loss: 1.04349244\n",
      "(6.79 min) Epoch 35/300 -- Iteration 32886 - Batch 144/963 - Train loss: 0.31155674  - Train acc: 0.8791 - Val loss: 1.04349244\n",
      "(6.79 min) Epoch 35/300 -- Iteration 32895 - Batch 153/963 - Train loss: 0.31173031  - Train acc: 0.8785 - Val loss: 1.04349244\n",
      "(6.79 min) Epoch 35/300 -- Iteration 32904 - Batch 162/963 - Train loss: 0.31054306  - Train acc: 0.8791 - Val loss: 1.04349244\n",
      "(6.79 min) Epoch 35/300 -- Iteration 32913 - Batch 171/963 - Train loss: 0.31147804  - Train acc: 0.8794 - Val loss: 1.04349244\n",
      "(6.79 min) Epoch 35/300 -- Iteration 32922 - Batch 180/963 - Train loss: 0.31171382  - Train acc: 0.8794 - Val loss: 1.04349244\n",
      "(6.80 min) Epoch 35/300 -- Iteration 32931 - Batch 189/963 - Train loss: 0.31143266  - Train acc: 0.8798 - Val loss: 1.04349244\n",
      "(6.80 min) Epoch 35/300 -- Iteration 32940 - Batch 198/963 - Train loss: 0.31093741  - Train acc: 0.8798 - Val loss: 1.04349244\n",
      "(6.80 min) Epoch 35/300 -- Iteration 32949 - Batch 207/963 - Train loss: 0.31009930  - Train acc: 0.8802 - Val loss: 1.04349244\n",
      "(6.80 min) Epoch 35/300 -- Iteration 32958 - Batch 216/963 - Train loss: 0.31024619  - Train acc: 0.8802 - Val loss: 1.04349244\n",
      "(6.80 min) Epoch 35/300 -- Iteration 32967 - Batch 225/963 - Train loss: 0.30917630  - Train acc: 0.8807 - Val loss: 1.04349244\n",
      "(6.81 min) Epoch 35/300 -- Iteration 32976 - Batch 234/963 - Train loss: 0.30976806  - Train acc: 0.8805 - Val loss: 1.04349244\n",
      "(6.81 min) Epoch 35/300 -- Iteration 32985 - Batch 243/963 - Train loss: 0.30724271  - Train acc: 0.8815 - Val loss: 1.04349244\n",
      "(6.81 min) Epoch 35/300 -- Iteration 32994 - Batch 252/963 - Train loss: 0.30580417  - Train acc: 0.8818 - Val loss: 1.04349244\n",
      "(6.81 min) Epoch 35/300 -- Iteration 33003 - Batch 261/963 - Train loss: 0.30558754  - Train acc: 0.8821 - Val loss: 1.04349244\n",
      "(6.81 min) Epoch 35/300 -- Iteration 33012 - Batch 270/963 - Train loss: 0.30592379  - Train acc: 0.8821 - Val loss: 1.04349244\n",
      "(6.82 min) Epoch 35/300 -- Iteration 33021 - Batch 279/963 - Train loss: 0.30563072  - Train acc: 0.8824 - Val loss: 1.04349244\n",
      "(6.82 min) Epoch 35/300 -- Iteration 33030 - Batch 288/963 - Train loss: 0.30655846  - Train acc: 0.8818 - Val loss: 1.04349244\n",
      "(6.82 min) Epoch 35/300 -- Iteration 33039 - Batch 297/963 - Train loss: 0.30681093  - Train acc: 0.8816 - Val loss: 1.04349244\n",
      "(6.82 min) Epoch 35/300 -- Iteration 33048 - Batch 306/963 - Train loss: 0.30633893  - Train acc: 0.8820 - Val loss: 1.04349244\n",
      "(6.83 min) Epoch 35/300 -- Iteration 33057 - Batch 315/963 - Train loss: 0.30622974  - Train acc: 0.8823 - Val loss: 1.04349244\n",
      "(6.83 min) Epoch 35/300 -- Iteration 33066 - Batch 324/963 - Train loss: 0.30629659  - Train acc: 0.8822 - Val loss: 1.04349244\n",
      "(6.83 min) Epoch 35/300 -- Iteration 33075 - Batch 333/963 - Train loss: 0.30631695  - Train acc: 0.8821 - Val loss: 1.04349244\n",
      "(6.83 min) Epoch 35/300 -- Iteration 33084 - Batch 342/963 - Train loss: 0.30632414  - Train acc: 0.8821 - Val loss: 1.04349244\n",
      "(6.83 min) Epoch 35/300 -- Iteration 33093 - Batch 351/963 - Train loss: 0.30672773  - Train acc: 0.8818 - Val loss: 1.04349244\n",
      "(6.84 min) Epoch 35/300 -- Iteration 33102 - Batch 360/963 - Train loss: 0.30650478  - Train acc: 0.8821 - Val loss: 1.04349244\n",
      "(6.84 min) Epoch 35/300 -- Iteration 33111 - Batch 369/963 - Train loss: 0.30721052  - Train acc: 0.8818 - Val loss: 1.04349244\n",
      "(6.84 min) Epoch 35/300 -- Iteration 33120 - Batch 378/963 - Train loss: 0.30624802  - Train acc: 0.8823 - Val loss: 1.04349244\n",
      "(6.84 min) Epoch 35/300 -- Iteration 33129 - Batch 387/963 - Train loss: 0.30607573  - Train acc: 0.8825 - Val loss: 1.04349244\n",
      "(6.84 min) Epoch 35/300 -- Iteration 33138 - Batch 396/963 - Train loss: 0.30605664  - Train acc: 0.8827 - Val loss: 1.04349244\n",
      "(6.85 min) Epoch 35/300 -- Iteration 33147 - Batch 405/963 - Train loss: 0.30631752  - Train acc: 0.8823 - Val loss: 1.04349244\n",
      "(6.85 min) Epoch 35/300 -- Iteration 33156 - Batch 414/963 - Train loss: 0.30698480  - Train acc: 0.8819 - Val loss: 1.04349244\n",
      "(6.85 min) Epoch 35/300 -- Iteration 33165 - Batch 423/963 - Train loss: 0.30683433  - Train acc: 0.8821 - Val loss: 1.04349244\n",
      "(6.85 min) Epoch 35/300 -- Iteration 33174 - Batch 432/963 - Train loss: 0.30690177  - Train acc: 0.8822 - Val loss: 1.04349244\n",
      "(6.85 min) Epoch 35/300 -- Iteration 33183 - Batch 441/963 - Train loss: 0.30676809  - Train acc: 0.8823 - Val loss: 1.04349244\n",
      "(6.85 min) Epoch 35/300 -- Iteration 33192 - Batch 450/963 - Train loss: 0.30693048  - Train acc: 0.8825 - Val loss: 1.04349244\n",
      "(6.86 min) Epoch 35/300 -- Iteration 33201 - Batch 459/963 - Train loss: 0.30680598  - Train acc: 0.8826 - Val loss: 1.04349244\n",
      "(6.86 min) Epoch 35/300 -- Iteration 33210 - Batch 468/963 - Train loss: 0.30730153  - Train acc: 0.8824 - Val loss: 1.04349244\n",
      "(6.86 min) Epoch 35/300 -- Iteration 33219 - Batch 477/963 - Train loss: 0.30759664  - Train acc: 0.8825 - Val loss: 1.04349244\n",
      "(6.86 min) Epoch 35/300 -- Iteration 33228 - Batch 486/963 - Train loss: 0.30731906  - Train acc: 0.8825 - Val loss: 1.04349244\n",
      "(6.87 min) Epoch 35/300 -- Iteration 33237 - Batch 495/963 - Train loss: 0.30748632  - Train acc: 0.8826 - Val loss: 1.04349244\n",
      "(6.87 min) Epoch 35/300 -- Iteration 33246 - Batch 504/963 - Train loss: 0.30744235  - Train acc: 0.8827 - Val loss: 1.04349244\n",
      "(6.87 min) Epoch 35/300 -- Iteration 33255 - Batch 513/963 - Train loss: 0.30739736  - Train acc: 0.8826 - Val loss: 1.04349244\n",
      "(6.87 min) Epoch 35/300 -- Iteration 33264 - Batch 522/963 - Train loss: 0.30708689  - Train acc: 0.8826 - Val loss: 1.04349244\n",
      "(6.87 min) Epoch 35/300 -- Iteration 33273 - Batch 531/963 - Train loss: 0.30707924  - Train acc: 0.8827 - Val loss: 1.04349244\n",
      "(6.88 min) Epoch 35/300 -- Iteration 33282 - Batch 540/963 - Train loss: 0.30705309  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.88 min) Epoch 35/300 -- Iteration 33291 - Batch 549/963 - Train loss: 0.30705919  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.88 min) Epoch 35/300 -- Iteration 33300 - Batch 558/963 - Train loss: 0.30700241  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.88 min) Epoch 35/300 -- Iteration 33309 - Batch 567/963 - Train loss: 0.30703585  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.88 min) Epoch 35/300 -- Iteration 33318 - Batch 576/963 - Train loss: 0.30674163  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.89 min) Epoch 35/300 -- Iteration 33327 - Batch 585/963 - Train loss: 0.30611842  - Train acc: 0.8831 - Val loss: 1.04349244\n",
      "(6.89 min) Epoch 35/300 -- Iteration 33336 - Batch 594/963 - Train loss: 0.30616213  - Train acc: 0.8830 - Val loss: 1.04349244\n",
      "(6.89 min) Epoch 35/300 -- Iteration 33345 - Batch 603/963 - Train loss: 0.30607750  - Train acc: 0.8829 - Val loss: 1.04349244\n",
      "(6.89 min) Epoch 35/300 -- Iteration 33354 - Batch 612/963 - Train loss: 0.30636295  - Train acc: 0.8827 - Val loss: 1.04349244\n",
      "(6.89 min) Epoch 35/300 -- Iteration 33363 - Batch 621/963 - Train loss: 0.30639013  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.89 min) Epoch 35/300 -- Iteration 33372 - Batch 630/963 - Train loss: 0.30662542  - Train acc: 0.8826 - Val loss: 1.04349244\n",
      "(6.90 min) Epoch 35/300 -- Iteration 33381 - Batch 639/963 - Train loss: 0.30673783  - Train acc: 0.8825 - Val loss: 1.04349244\n",
      "(6.90 min) Epoch 35/300 -- Iteration 33390 - Batch 648/963 - Train loss: 0.30671649  - Train acc: 0.8827 - Val loss: 1.04349244\n",
      "(6.90 min) Epoch 35/300 -- Iteration 33399 - Batch 657/963 - Train loss: 0.30638377  - Train acc: 0.8829 - Val loss: 1.04349244\n",
      "(6.90 min) Epoch 35/300 -- Iteration 33408 - Batch 666/963 - Train loss: 0.30646272  - Train acc: 0.8829 - Val loss: 1.04349244\n",
      "(6.90 min) Epoch 35/300 -- Iteration 33417 - Batch 675/963 - Train loss: 0.30685969  - Train acc: 0.8827 - Val loss: 1.04349244\n",
      "(6.91 min) Epoch 35/300 -- Iteration 33426 - Batch 684/963 - Train loss: 0.30690948  - Train acc: 0.8827 - Val loss: 1.04349244\n",
      "(6.91 min) Epoch 35/300 -- Iteration 33435 - Batch 693/963 - Train loss: 0.30668887  - Train acc: 0.8829 - Val loss: 1.04349244\n",
      "(6.91 min) Epoch 35/300 -- Iteration 33444 - Batch 702/963 - Train loss: 0.30633126  - Train acc: 0.8831 - Val loss: 1.04349244\n",
      "(6.91 min) Epoch 35/300 -- Iteration 33453 - Batch 711/963 - Train loss: 0.30666955  - Train acc: 0.8829 - Val loss: 1.04349244\n",
      "(6.91 min) Epoch 35/300 -- Iteration 33462 - Batch 720/963 - Train loss: 0.30675218  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.92 min) Epoch 35/300 -- Iteration 33471 - Batch 729/963 - Train loss: 0.30702246  - Train acc: 0.8826 - Val loss: 1.04349244\n",
      "(6.92 min) Epoch 35/300 -- Iteration 33480 - Batch 738/963 - Train loss: 0.30721677  - Train acc: 0.8826 - Val loss: 1.04349244\n",
      "(6.92 min) Epoch 35/300 -- Iteration 33489 - Batch 747/963 - Train loss: 0.30733358  - Train acc: 0.8825 - Val loss: 1.04349244\n",
      "(6.92 min) Epoch 35/300 -- Iteration 33498 - Batch 756/963 - Train loss: 0.30685964  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.92 min) Epoch 35/300 -- Iteration 33507 - Batch 765/963 - Train loss: 0.30675737  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.92 min) Epoch 35/300 -- Iteration 33516 - Batch 774/963 - Train loss: 0.30691635  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.93 min) Epoch 35/300 -- Iteration 33525 - Batch 783/963 - Train loss: 0.30660358  - Train acc: 0.8830 - Val loss: 1.04349244\n",
      "(6.93 min) Epoch 35/300 -- Iteration 33534 - Batch 792/963 - Train loss: 0.30654949  - Train acc: 0.8829 - Val loss: 1.04349244\n",
      "(6.93 min) Epoch 35/300 -- Iteration 33543 - Batch 801/963 - Train loss: 0.30646295  - Train acc: 0.8830 - Val loss: 1.04349244\n",
      "(6.93 min) Epoch 35/300 -- Iteration 33552 - Batch 810/963 - Train loss: 0.30665592  - Train acc: 0.8829 - Val loss: 1.04349244\n",
      "(6.93 min) Epoch 35/300 -- Iteration 33561 - Batch 819/963 - Train loss: 0.30667340  - Train acc: 0.8828 - Val loss: 1.04349244\n",
      "(6.94 min) Epoch 35/300 -- Iteration 33570 - Batch 828/963 - Train loss: 0.30660839  - Train acc: 0.8829 - Val loss: 1.04349244\n",
      "(6.94 min) Epoch 35/300 -- Iteration 33579 - Batch 837/963 - Train loss: 0.30638320  - Train acc: 0.8829 - Val loss: 1.04349244\n",
      "(6.94 min) Epoch 35/300 -- Iteration 33588 - Batch 846/963 - Train loss: 0.30603088  - Train acc: 0.8831 - Val loss: 1.04349244\n",
      "(6.94 min) Epoch 35/300 -- Iteration 33597 - Batch 855/963 - Train loss: 0.30600273  - Train acc: 0.8832 - Val loss: 1.04349244\n",
      "(6.94 min) Epoch 35/300 -- Iteration 33606 - Batch 864/963 - Train loss: 0.30615265  - Train acc: 0.8831 - Val loss: 1.04349244\n",
      "(6.95 min) Epoch 35/300 -- Iteration 33615 - Batch 873/963 - Train loss: 0.30602814  - Train acc: 0.8832 - Val loss: 1.04349244\n",
      "(6.95 min) Epoch 35/300 -- Iteration 33624 - Batch 882/963 - Train loss: 0.30580380  - Train acc: 0.8833 - Val loss: 1.04349244\n",
      "(6.95 min) Epoch 35/300 -- Iteration 33633 - Batch 891/963 - Train loss: 0.30589645  - Train acc: 0.8832 - Val loss: 1.04349244\n",
      "(6.95 min) Epoch 35/300 -- Iteration 33642 - Batch 900/963 - Train loss: 0.30606885  - Train acc: 0.8831 - Val loss: 1.04349244\n",
      "(6.95 min) Epoch 35/300 -- Iteration 33651 - Batch 909/963 - Train loss: 0.30577993  - Train acc: 0.8832 - Val loss: 1.04349244\n",
      "(6.96 min) Epoch 35/300 -- Iteration 33660 - Batch 918/963 - Train loss: 0.30576656  - Train acc: 0.8832 - Val loss: 1.04349244\n",
      "(6.96 min) Epoch 35/300 -- Iteration 33669 - Batch 927/963 - Train loss: 0.30582100  - Train acc: 0.8832 - Val loss: 1.04349244\n",
      "(6.96 min) Epoch 35/300 -- Iteration 33678 - Batch 936/963 - Train loss: 0.30552274  - Train acc: 0.8833 - Val loss: 1.04349244\n",
      "(6.96 min) Epoch 35/300 -- Iteration 33687 - Batch 945/963 - Train loss: 0.30550675  - Train acc: 0.8833 - Val loss: 1.04349244\n",
      "(6.96 min) Epoch 35/300 -- Iteration 33696 - Batch 954/963 - Train loss: 0.30540377  - Train acc: 0.8833 - Val loss: 1.04349244\n",
      "(6.97 min) Epoch 35/300 -- Iteration 33705 - Batch 962/963 - Train loss: 0.30538036  - Train acc: 0.8834 - Val loss: 1.04577553 - Val acc: 0.5933\n",
      "(6.97 min) Epoch 36/300 -- Iteration 33714 - Batch 9/963 - Train loss: 0.28557286  - Train acc: 0.8969 - Val loss: 1.04577553\n",
      "(6.97 min) Epoch 36/300 -- Iteration 33723 - Batch 18/963 - Train loss: 0.29980497  - Train acc: 0.8857 - Val loss: 1.04577553\n",
      "(6.97 min) Epoch 36/300 -- Iteration 33732 - Batch 27/963 - Train loss: 0.29405231  - Train acc: 0.8881 - Val loss: 1.04577553\n",
      "(6.97 min) Epoch 36/300 -- Iteration 33741 - Batch 36/963 - Train loss: 0.30287071  - Train acc: 0.8847 - Val loss: 1.04577553\n",
      "(6.98 min) Epoch 36/300 -- Iteration 33750 - Batch 45/963 - Train loss: 0.29501389  - Train acc: 0.8891 - Val loss: 1.04577553\n",
      "(6.98 min) Epoch 36/300 -- Iteration 33759 - Batch 54/963 - Train loss: 0.29655892  - Train acc: 0.8874 - Val loss: 1.04577553\n",
      "(6.98 min) Epoch 36/300 -- Iteration 33768 - Batch 63/963 - Train loss: 0.29954033  - Train acc: 0.8838 - Val loss: 1.04577553\n",
      "(6.98 min) Epoch 36/300 -- Iteration 33777 - Batch 72/963 - Train loss: 0.30118652  - Train acc: 0.8835 - Val loss: 1.04577553\n",
      "(6.98 min) Epoch 36/300 -- Iteration 33786 - Batch 81/963 - Train loss: 0.30334761  - Train acc: 0.8817 - Val loss: 1.04577553\n",
      "(6.99 min) Epoch 36/300 -- Iteration 33795 - Batch 90/963 - Train loss: 0.30449055  - Train acc: 0.8824 - Val loss: 1.04577553\n",
      "(6.99 min) Epoch 36/300 -- Iteration 33804 - Batch 99/963 - Train loss: 0.30268145  - Train acc: 0.8835 - Val loss: 1.04577553\n",
      "(6.99 min) Epoch 36/300 -- Iteration 33813 - Batch 108/963 - Train loss: 0.30278868  - Train acc: 0.8834 - Val loss: 1.04577553\n",
      "(6.99 min) Epoch 36/300 -- Iteration 33822 - Batch 117/963 - Train loss: 0.30289032  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(6.99 min) Epoch 36/300 -- Iteration 33831 - Batch 126/963 - Train loss: 0.30546697  - Train acc: 0.8819 - Val loss: 1.04577553\n",
      "(7.00 min) Epoch 36/300 -- Iteration 33840 - Batch 135/963 - Train loss: 0.30653952  - Train acc: 0.8813 - Val loss: 1.04577553\n",
      "(7.00 min) Epoch 36/300 -- Iteration 33849 - Batch 144/963 - Train loss: 0.30556000  - Train acc: 0.8817 - Val loss: 1.04577553\n",
      "(7.00 min) Epoch 36/300 -- Iteration 33858 - Batch 153/963 - Train loss: 0.30442484  - Train acc: 0.8823 - Val loss: 1.04577553\n",
      "(7.00 min) Epoch 36/300 -- Iteration 33867 - Batch 162/963 - Train loss: 0.30411612  - Train acc: 0.8822 - Val loss: 1.04577553\n",
      "(7.00 min) Epoch 36/300 -- Iteration 33876 - Batch 171/963 - Train loss: 0.30290787  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.01 min) Epoch 36/300 -- Iteration 33885 - Batch 180/963 - Train loss: 0.30181409  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.01 min) Epoch 36/300 -- Iteration 33894 - Batch 189/963 - Train loss: 0.30267531  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.01 min) Epoch 36/300 -- Iteration 33903 - Batch 198/963 - Train loss: 0.30261708  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.01 min) Epoch 36/300 -- Iteration 33912 - Batch 207/963 - Train loss: 0.30314654  - Train acc: 0.8824 - Val loss: 1.04577553\n",
      "(7.01 min) Epoch 36/300 -- Iteration 33921 - Batch 216/963 - Train loss: 0.30333771  - Train acc: 0.8827 - Val loss: 1.04577553\n",
      "(7.02 min) Epoch 36/300 -- Iteration 33930 - Batch 225/963 - Train loss: 0.30300218  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.02 min) Epoch 36/300 -- Iteration 33939 - Batch 234/963 - Train loss: 0.30224412  - Train acc: 0.8833 - Val loss: 1.04577553\n",
      "(7.02 min) Epoch 36/300 -- Iteration 33948 - Batch 243/963 - Train loss: 0.30213027  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.02 min) Epoch 36/300 -- Iteration 33957 - Batch 252/963 - Train loss: 0.30172444  - Train acc: 0.8831 - Val loss: 1.04577553\n",
      "(7.02 min) Epoch 36/300 -- Iteration 33966 - Batch 261/963 - Train loss: 0.30272011  - Train acc: 0.8825 - Val loss: 1.04577553\n",
      "(7.03 min) Epoch 36/300 -- Iteration 33975 - Batch 270/963 - Train loss: 0.30244850  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.03 min) Epoch 36/300 -- Iteration 33984 - Batch 279/963 - Train loss: 0.30260443  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.03 min) Epoch 36/300 -- Iteration 33993 - Batch 288/963 - Train loss: 0.30264702  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.03 min) Epoch 36/300 -- Iteration 34002 - Batch 297/963 - Train loss: 0.30205210  - Train acc: 0.8831 - Val loss: 1.04577553\n",
      "(7.03 min) Epoch 36/300 -- Iteration 34011 - Batch 306/963 - Train loss: 0.30095302  - Train acc: 0.8837 - Val loss: 1.04577553\n",
      "(7.03 min) Epoch 36/300 -- Iteration 34020 - Batch 315/963 - Train loss: 0.30108580  - Train acc: 0.8837 - Val loss: 1.04577553\n",
      "(7.04 min) Epoch 36/300 -- Iteration 34029 - Batch 324/963 - Train loss: 0.30208893  - Train acc: 0.8831 - Val loss: 1.04577553\n",
      "(7.04 min) Epoch 36/300 -- Iteration 34038 - Batch 333/963 - Train loss: 0.30174857  - Train acc: 0.8835 - Val loss: 1.04577553\n",
      "(7.04 min) Epoch 36/300 -- Iteration 34047 - Batch 342/963 - Train loss: 0.30204333  - Train acc: 0.8834 - Val loss: 1.04577553\n",
      "(7.04 min) Epoch 36/300 -- Iteration 34056 - Batch 351/963 - Train loss: 0.30099252  - Train acc: 0.8837 - Val loss: 1.04577553\n",
      "(7.04 min) Epoch 36/300 -- Iteration 34065 - Batch 360/963 - Train loss: 0.30116936  - Train acc: 0.8836 - Val loss: 1.04577553\n",
      "(7.05 min) Epoch 36/300 -- Iteration 34074 - Batch 369/963 - Train loss: 0.30165422  - Train acc: 0.8836 - Val loss: 1.04577553\n",
      "(7.05 min) Epoch 36/300 -- Iteration 34083 - Batch 378/963 - Train loss: 0.30223352  - Train acc: 0.8833 - Val loss: 1.04577553\n",
      "(7.05 min) Epoch 36/300 -- Iteration 34092 - Batch 387/963 - Train loss: 0.30184901  - Train acc: 0.8833 - Val loss: 1.04577553\n",
      "(7.05 min) Epoch 36/300 -- Iteration 34101 - Batch 396/963 - Train loss: 0.30178405  - Train acc: 0.8831 - Val loss: 1.04577553\n",
      "(7.05 min) Epoch 36/300 -- Iteration 34110 - Batch 405/963 - Train loss: 0.30194012  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.06 min) Epoch 36/300 -- Iteration 34119 - Batch 414/963 - Train loss: 0.30228728  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.06 min) Epoch 36/300 -- Iteration 34128 - Batch 423/963 - Train loss: 0.30260893  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.06 min) Epoch 36/300 -- Iteration 34137 - Batch 432/963 - Train loss: 0.30275495  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.06 min) Epoch 36/300 -- Iteration 34146 - Batch 441/963 - Train loss: 0.30265768  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.06 min) Epoch 36/300 -- Iteration 34155 - Batch 450/963 - Train loss: 0.30336656  - Train acc: 0.8823 - Val loss: 1.04577553\n",
      "(7.06 min) Epoch 36/300 -- Iteration 34164 - Batch 459/963 - Train loss: 0.30374025  - Train acc: 0.8823 - Val loss: 1.04577553\n",
      "(7.07 min) Epoch 36/300 -- Iteration 34173 - Batch 468/963 - Train loss: 0.30339508  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.07 min) Epoch 36/300 -- Iteration 34182 - Batch 477/963 - Train loss: 0.30372430  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.07 min) Epoch 36/300 -- Iteration 34191 - Batch 486/963 - Train loss: 0.30403178  - Train acc: 0.8825 - Val loss: 1.04577553\n",
      "(7.07 min) Epoch 36/300 -- Iteration 34200 - Batch 495/963 - Train loss: 0.30402785  - Train acc: 0.8825 - Val loss: 1.04577553\n",
      "(7.07 min) Epoch 36/300 -- Iteration 34209 - Batch 504/963 - Train loss: 0.30376856  - Train acc: 0.8825 - Val loss: 1.04577553\n",
      "(7.08 min) Epoch 36/300 -- Iteration 34218 - Batch 513/963 - Train loss: 0.30441437  - Train acc: 0.8822 - Val loss: 1.04577553\n",
      "(7.08 min) Epoch 36/300 -- Iteration 34227 - Batch 522/963 - Train loss: 0.30430926  - Train acc: 0.8823 - Val loss: 1.04577553\n",
      "(7.08 min) Epoch 36/300 -- Iteration 34236 - Batch 531/963 - Train loss: 0.30428303  - Train acc: 0.8824 - Val loss: 1.04577553\n",
      "(7.08 min) Epoch 36/300 -- Iteration 34245 - Batch 540/963 - Train loss: 0.30425670  - Train acc: 0.8825 - Val loss: 1.04577553\n",
      "(7.08 min) Epoch 36/300 -- Iteration 34254 - Batch 549/963 - Train loss: 0.30398830  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.08 min) Epoch 36/300 -- Iteration 34263 - Batch 558/963 - Train loss: 0.30451416  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.09 min) Epoch 36/300 -- Iteration 34272 - Batch 567/963 - Train loss: 0.30457411  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.09 min) Epoch 36/300 -- Iteration 34281 - Batch 576/963 - Train loss: 0.30448108  - Train acc: 0.8827 - Val loss: 1.04577553\n",
      "(7.09 min) Epoch 36/300 -- Iteration 34290 - Batch 585/963 - Train loss: 0.30455781  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.09 min) Epoch 36/300 -- Iteration 34299 - Batch 594/963 - Train loss: 0.30468118  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.10 min) Epoch 36/300 -- Iteration 34308 - Batch 603/963 - Train loss: 0.30477355  - Train acc: 0.8825 - Val loss: 1.04577553\n",
      "(7.10 min) Epoch 36/300 -- Iteration 34317 - Batch 612/963 - Train loss: 0.30470731  - Train acc: 0.8827 - Val loss: 1.04577553\n",
      "(7.10 min) Epoch 36/300 -- Iteration 34326 - Batch 621/963 - Train loss: 0.30420052  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.10 min) Epoch 36/300 -- Iteration 34335 - Batch 630/963 - Train loss: 0.30424667  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.10 min) Epoch 36/300 -- Iteration 34344 - Batch 639/963 - Train loss: 0.30471159  - Train acc: 0.8827 - Val loss: 1.04577553\n",
      "(7.10 min) Epoch 36/300 -- Iteration 34353 - Batch 648/963 - Train loss: 0.30450375  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.11 min) Epoch 36/300 -- Iteration 34362 - Batch 657/963 - Train loss: 0.30471343  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.11 min) Epoch 36/300 -- Iteration 34371 - Batch 666/963 - Train loss: 0.30469911  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.11 min) Epoch 36/300 -- Iteration 34380 - Batch 675/963 - Train loss: 0.30453974  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.11 min) Epoch 36/300 -- Iteration 34389 - Batch 684/963 - Train loss: 0.30407904  - Train acc: 0.8831 - Val loss: 1.04577553\n",
      "(7.11 min) Epoch 36/300 -- Iteration 34398 - Batch 693/963 - Train loss: 0.30427323  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.12 min) Epoch 36/300 -- Iteration 34407 - Batch 702/963 - Train loss: 0.30459853  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.12 min) Epoch 36/300 -- Iteration 34416 - Batch 711/963 - Train loss: 0.30455043  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.12 min) Epoch 36/300 -- Iteration 34425 - Batch 720/963 - Train loss: 0.30398819  - Train acc: 0.8831 - Val loss: 1.04577553\n",
      "(7.12 min) Epoch 36/300 -- Iteration 34434 - Batch 729/963 - Train loss: 0.30405110  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.12 min) Epoch 36/300 -- Iteration 34443 - Batch 738/963 - Train loss: 0.30431116  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.12 min) Epoch 36/300 -- Iteration 34452 - Batch 747/963 - Train loss: 0.30477567  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.13 min) Epoch 36/300 -- Iteration 34461 - Batch 756/963 - Train loss: 0.30502466  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.13 min) Epoch 36/300 -- Iteration 34470 - Batch 765/963 - Train loss: 0.30514344  - Train acc: 0.8825 - Val loss: 1.04577553\n",
      "(7.13 min) Epoch 36/300 -- Iteration 34479 - Batch 774/963 - Train loss: 0.30500945  - Train acc: 0.8826 - Val loss: 1.04577553\n",
      "(7.13 min) Epoch 36/300 -- Iteration 34488 - Batch 783/963 - Train loss: 0.30499316  - Train acc: 0.8827 - Val loss: 1.04577553\n",
      "(7.13 min) Epoch 36/300 -- Iteration 34497 - Batch 792/963 - Train loss: 0.30503918  - Train acc: 0.8827 - Val loss: 1.04577553\n",
      "(7.14 min) Epoch 36/300 -- Iteration 34506 - Batch 801/963 - Train loss: 0.30486965  - Train acc: 0.8828 - Val loss: 1.04577553\n",
      "(7.14 min) Epoch 36/300 -- Iteration 34515 - Batch 810/963 - Train loss: 0.30452425  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.14 min) Epoch 36/300 -- Iteration 34524 - Batch 819/963 - Train loss: 0.30470334  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.14 min) Epoch 36/300 -- Iteration 34533 - Batch 828/963 - Train loss: 0.30455585  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.14 min) Epoch 36/300 -- Iteration 34542 - Batch 837/963 - Train loss: 0.30478581  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.14 min) Epoch 36/300 -- Iteration 34551 - Batch 846/963 - Train loss: 0.30488151  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.15 min) Epoch 36/300 -- Iteration 34560 - Batch 855/963 - Train loss: 0.30496095  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.15 min) Epoch 36/300 -- Iteration 34569 - Batch 864/963 - Train loss: 0.30466609  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.15 min) Epoch 36/300 -- Iteration 34578 - Batch 873/963 - Train loss: 0.30481227  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.15 min) Epoch 36/300 -- Iteration 34587 - Batch 882/963 - Train loss: 0.30453174  - Train acc: 0.8831 - Val loss: 1.04577553\n",
      "(7.15 min) Epoch 36/300 -- Iteration 34596 - Batch 891/963 - Train loss: 0.30433694  - Train acc: 0.8832 - Val loss: 1.04577553\n",
      "(7.16 min) Epoch 36/300 -- Iteration 34605 - Batch 900/963 - Train loss: 0.30429323  - Train acc: 0.8832 - Val loss: 1.04577553\n",
      "(7.16 min) Epoch 36/300 -- Iteration 34614 - Batch 909/963 - Train loss: 0.30453494  - Train acc: 0.8831 - Val loss: 1.04577553\n",
      "(7.16 min) Epoch 36/300 -- Iteration 34623 - Batch 918/963 - Train loss: 0.30476046  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.16 min) Epoch 36/300 -- Iteration 34632 - Batch 927/963 - Train loss: 0.30500573  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.16 min) Epoch 36/300 -- Iteration 34641 - Batch 936/963 - Train loss: 0.30508928  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.17 min) Epoch 36/300 -- Iteration 34650 - Batch 945/963 - Train loss: 0.30501587  - Train acc: 0.8830 - Val loss: 1.04577553\n",
      "(7.17 min) Epoch 36/300 -- Iteration 34659 - Batch 954/963 - Train loss: 0.30512867  - Train acc: 0.8829 - Val loss: 1.04577553\n",
      "(7.17 min) Epoch 36/300 -- Iteration 34668 - Batch 962/963 - Train loss: 0.30496884  - Train acc: 0.8830 - Val loss: 1.04222727 - Val acc: 0.5917\n",
      "(7.17 min) Epoch 37/300 -- Iteration 34677 - Batch 9/963 - Train loss: 0.31500087  - Train acc: 0.8781 - Val loss: 1.04222727\n",
      "(7.17 min) Epoch 37/300 -- Iteration 34686 - Batch 18/963 - Train loss: 0.30375767  - Train acc: 0.8840 - Val loss: 1.04222727\n",
      "(7.18 min) Epoch 37/300 -- Iteration 34695 - Batch 27/963 - Train loss: 0.31198769  - Train acc: 0.8800 - Val loss: 1.04222727\n",
      "(7.18 min) Epoch 37/300 -- Iteration 34704 - Batch 36/963 - Train loss: 0.31751897  - Train acc: 0.8775 - Val loss: 1.04222727\n",
      "(7.18 min) Epoch 37/300 -- Iteration 34713 - Batch 45/963 - Train loss: 0.31911101  - Train acc: 0.8786 - Val loss: 1.04222727\n",
      "(7.18 min) Epoch 37/300 -- Iteration 34722 - Batch 54/963 - Train loss: 0.31446975  - Train acc: 0.8795 - Val loss: 1.04222727\n",
      "(7.18 min) Epoch 37/300 -- Iteration 34731 - Batch 63/963 - Train loss: 0.30978108  - Train acc: 0.8821 - Val loss: 1.04222727\n",
      "(7.19 min) Epoch 37/300 -- Iteration 34740 - Batch 72/963 - Train loss: 0.30830746  - Train acc: 0.8817 - Val loss: 1.04222727\n",
      "(7.19 min) Epoch 37/300 -- Iteration 34749 - Batch 81/963 - Train loss: 0.30345966  - Train acc: 0.8836 - Val loss: 1.04222727\n",
      "(7.19 min) Epoch 37/300 -- Iteration 34758 - Batch 90/963 - Train loss: 0.30505630  - Train acc: 0.8829 - Val loss: 1.04222727\n",
      "(7.19 min) Epoch 37/300 -- Iteration 34767 - Batch 99/963 - Train loss: 0.30574236  - Train acc: 0.8829 - Val loss: 1.04222727\n",
      "(7.19 min) Epoch 37/300 -- Iteration 34776 - Batch 108/963 - Train loss: 0.30959515  - Train acc: 0.8816 - Val loss: 1.04222727\n",
      "(7.19 min) Epoch 37/300 -- Iteration 34785 - Batch 117/963 - Train loss: 0.30918487  - Train acc: 0.8818 - Val loss: 1.04222727\n",
      "(7.20 min) Epoch 37/300 -- Iteration 34794 - Batch 126/963 - Train loss: 0.30819353  - Train acc: 0.8820 - Val loss: 1.04222727\n",
      "(7.20 min) Epoch 37/300 -- Iteration 34803 - Batch 135/963 - Train loss: 0.30910971  - Train acc: 0.8811 - Val loss: 1.04222727\n",
      "(7.20 min) Epoch 37/300 -- Iteration 34812 - Batch 144/963 - Train loss: 0.30868698  - Train acc: 0.8811 - Val loss: 1.04222727\n",
      "(7.20 min) Epoch 37/300 -- Iteration 34821 - Batch 153/963 - Train loss: 0.30952136  - Train acc: 0.8805 - Val loss: 1.04222727\n",
      "(7.20 min) Epoch 37/300 -- Iteration 34830 - Batch 162/963 - Train loss: 0.30920595  - Train acc: 0.8802 - Val loss: 1.04222727\n",
      "(7.21 min) Epoch 37/300 -- Iteration 34839 - Batch 171/963 - Train loss: 0.30762752  - Train acc: 0.8812 - Val loss: 1.04222727\n",
      "(7.21 min) Epoch 37/300 -- Iteration 34848 - Batch 180/963 - Train loss: 0.30779582  - Train acc: 0.8811 - Val loss: 1.04222727\n",
      "(7.21 min) Epoch 37/300 -- Iteration 34857 - Batch 189/963 - Train loss: 0.30768356  - Train acc: 0.8810 - Val loss: 1.04222727\n",
      "(7.21 min) Epoch 37/300 -- Iteration 34866 - Batch 198/963 - Train loss: 0.30638499  - Train acc: 0.8820 - Val loss: 1.04222727\n",
      "(7.21 min) Epoch 37/300 -- Iteration 34875 - Batch 207/963 - Train loss: 0.30584407  - Train acc: 0.8821 - Val loss: 1.04222727\n",
      "(7.22 min) Epoch 37/300 -- Iteration 34884 - Batch 216/963 - Train loss: 0.30552443  - Train acc: 0.8825 - Val loss: 1.04222727\n",
      "(7.22 min) Epoch 37/300 -- Iteration 34893 - Batch 225/963 - Train loss: 0.30507769  - Train acc: 0.8825 - Val loss: 1.04222727\n",
      "(7.22 min) Epoch 37/300 -- Iteration 34902 - Batch 234/963 - Train loss: 0.30428462  - Train acc: 0.8831 - Val loss: 1.04222727\n",
      "(7.22 min) Epoch 37/300 -- Iteration 34911 - Batch 243/963 - Train loss: 0.30397558  - Train acc: 0.8835 - Val loss: 1.04222727\n",
      "(7.22 min) Epoch 37/300 -- Iteration 34920 - Batch 252/963 - Train loss: 0.30434695  - Train acc: 0.8834 - Val loss: 1.04222727\n",
      "(7.22 min) Epoch 37/300 -- Iteration 34929 - Batch 261/963 - Train loss: 0.30435752  - Train acc: 0.8831 - Val loss: 1.04222727\n",
      "(7.23 min) Epoch 37/300 -- Iteration 34938 - Batch 270/963 - Train loss: 0.30343070  - Train acc: 0.8835 - Val loss: 1.04222727\n",
      "(7.23 min) Epoch 37/300 -- Iteration 34947 - Batch 279/963 - Train loss: 0.30242659  - Train acc: 0.8840 - Val loss: 1.04222727\n",
      "(7.23 min) Epoch 37/300 -- Iteration 34956 - Batch 288/963 - Train loss: 0.30294540  - Train acc: 0.8836 - Val loss: 1.04222727\n",
      "(7.23 min) Epoch 37/300 -- Iteration 34965 - Batch 297/963 - Train loss: 0.30208773  - Train acc: 0.8840 - Val loss: 1.04222727\n",
      "(7.23 min) Epoch 37/300 -- Iteration 34974 - Batch 306/963 - Train loss: 0.30266706  - Train acc: 0.8837 - Val loss: 1.04222727\n",
      "(7.24 min) Epoch 37/300 -- Iteration 34983 - Batch 315/963 - Train loss: 0.30326981  - Train acc: 0.8835 - Val loss: 1.04222727\n",
      "(7.24 min) Epoch 37/300 -- Iteration 34992 - Batch 324/963 - Train loss: 0.30292662  - Train acc: 0.8838 - Val loss: 1.04222727\n",
      "(7.24 min) Epoch 37/300 -- Iteration 35001 - Batch 333/963 - Train loss: 0.30239888  - Train acc: 0.8837 - Val loss: 1.04222727\n",
      "(7.24 min) Epoch 37/300 -- Iteration 35010 - Batch 342/963 - Train loss: 0.30230083  - Train acc: 0.8840 - Val loss: 1.04222727\n",
      "(7.24 min) Epoch 37/300 -- Iteration 35019 - Batch 351/963 - Train loss: 0.30235467  - Train acc: 0.8841 - Val loss: 1.04222727\n",
      "(7.25 min) Epoch 37/300 -- Iteration 35028 - Batch 360/963 - Train loss: 0.30193851  - Train acc: 0.8844 - Val loss: 1.04222727\n",
      "(7.25 min) Epoch 37/300 -- Iteration 35037 - Batch 369/963 - Train loss: 0.30173051  - Train acc: 0.8846 - Val loss: 1.04222727\n",
      "(7.25 min) Epoch 37/300 -- Iteration 35046 - Batch 378/963 - Train loss: 0.30119476  - Train acc: 0.8846 - Val loss: 1.04222727\n",
      "(7.25 min) Epoch 37/300 -- Iteration 35055 - Batch 387/963 - Train loss: 0.30191834  - Train acc: 0.8840 - Val loss: 1.04222727\n",
      "(7.25 min) Epoch 37/300 -- Iteration 35064 - Batch 396/963 - Train loss: 0.30202283  - Train acc: 0.8840 - Val loss: 1.04222727\n",
      "(7.26 min) Epoch 37/300 -- Iteration 35073 - Batch 405/963 - Train loss: 0.30271112  - Train acc: 0.8838 - Val loss: 1.04222727\n",
      "(7.26 min) Epoch 37/300 -- Iteration 35082 - Batch 414/963 - Train loss: 0.30269778  - Train acc: 0.8840 - Val loss: 1.04222727\n",
      "(7.26 min) Epoch 37/300 -- Iteration 35091 - Batch 423/963 - Train loss: 0.30205299  - Train acc: 0.8842 - Val loss: 1.04222727\n",
      "(7.26 min) Epoch 37/300 -- Iteration 35100 - Batch 432/963 - Train loss: 0.30170309  - Train acc: 0.8844 - Val loss: 1.04222727\n",
      "(7.26 min) Epoch 37/300 -- Iteration 35109 - Batch 441/963 - Train loss: 0.30165031  - Train acc: 0.8846 - Val loss: 1.04222727\n",
      "(7.27 min) Epoch 37/300 -- Iteration 35118 - Batch 450/963 - Train loss: 0.30134961  - Train acc: 0.8847 - Val loss: 1.04222727\n",
      "(7.27 min) Epoch 37/300 -- Iteration 35127 - Batch 459/963 - Train loss: 0.30123866  - Train acc: 0.8847 - Val loss: 1.04222727\n",
      "(7.27 min) Epoch 37/300 -- Iteration 35136 - Batch 468/963 - Train loss: 0.30122287  - Train acc: 0.8849 - Val loss: 1.04222727\n",
      "(7.27 min) Epoch 37/300 -- Iteration 35145 - Batch 477/963 - Train loss: 0.30135063  - Train acc: 0.8848 - Val loss: 1.04222727\n",
      "(7.27 min) Epoch 37/300 -- Iteration 35154 - Batch 486/963 - Train loss: 0.30117065  - Train acc: 0.8850 - Val loss: 1.04222727\n",
      "(7.27 min) Epoch 37/300 -- Iteration 35163 - Batch 495/963 - Train loss: 0.30104554  - Train acc: 0.8852 - Val loss: 1.04222727\n",
      "(7.28 min) Epoch 37/300 -- Iteration 35172 - Batch 504/963 - Train loss: 0.30048418  - Train acc: 0.8854 - Val loss: 1.04222727\n",
      "(7.28 min) Epoch 37/300 -- Iteration 35181 - Batch 513/963 - Train loss: 0.30083707  - Train acc: 0.8853 - Val loss: 1.04222727\n",
      "(7.28 min) Epoch 37/300 -- Iteration 35190 - Batch 522/963 - Train loss: 0.30079363  - Train acc: 0.8852 - Val loss: 1.04222727\n",
      "(7.28 min) Epoch 37/300 -- Iteration 35199 - Batch 531/963 - Train loss: 0.30068404  - Train acc: 0.8852 - Val loss: 1.04222727\n",
      "(7.29 min) Epoch 37/300 -- Iteration 35208 - Batch 540/963 - Train loss: 0.30046401  - Train acc: 0.8853 - Val loss: 1.04222727\n",
      "(7.29 min) Epoch 37/300 -- Iteration 35217 - Batch 549/963 - Train loss: 0.30069300  - Train acc: 0.8852 - Val loss: 1.04222727\n",
      "(7.29 min) Epoch 37/300 -- Iteration 35226 - Batch 558/963 - Train loss: 0.30052470  - Train acc: 0.8853 - Val loss: 1.04222727\n",
      "(7.29 min) Epoch 37/300 -- Iteration 35235 - Batch 567/963 - Train loss: 0.30015762  - Train acc: 0.8856 - Val loss: 1.04222727\n",
      "(7.30 min) Epoch 37/300 -- Iteration 35244 - Batch 576/963 - Train loss: 0.29985795  - Train acc: 0.8859 - Val loss: 1.04222727\n",
      "(7.30 min) Epoch 37/300 -- Iteration 35253 - Batch 585/963 - Train loss: 0.29980134  - Train acc: 0.8859 - Val loss: 1.04222727\n",
      "(7.30 min) Epoch 37/300 -- Iteration 35262 - Batch 594/963 - Train loss: 0.29953145  - Train acc: 0.8860 - Val loss: 1.04222727\n",
      "(7.30 min) Epoch 37/300 -- Iteration 35271 - Batch 603/963 - Train loss: 0.29921037  - Train acc: 0.8860 - Val loss: 1.04222727\n",
      "(7.30 min) Epoch 37/300 -- Iteration 35280 - Batch 612/963 - Train loss: 0.29960935  - Train acc: 0.8858 - Val loss: 1.04222727\n",
      "(7.31 min) Epoch 37/300 -- Iteration 35289 - Batch 621/963 - Train loss: 0.29962266  - Train acc: 0.8859 - Val loss: 1.04222727\n",
      "(7.31 min) Epoch 37/300 -- Iteration 35298 - Batch 630/963 - Train loss: 0.29913215  - Train acc: 0.8861 - Val loss: 1.04222727\n",
      "(7.31 min) Epoch 37/300 -- Iteration 35307 - Batch 639/963 - Train loss: 0.29898287  - Train acc: 0.8862 - Val loss: 1.04222727\n",
      "(7.31 min) Epoch 37/300 -- Iteration 35316 - Batch 648/963 - Train loss: 0.29949739  - Train acc: 0.8860 - Val loss: 1.04222727\n",
      "(7.31 min) Epoch 37/300 -- Iteration 35325 - Batch 657/963 - Train loss: 0.29956581  - Train acc: 0.8859 - Val loss: 1.04222727\n",
      "(7.32 min) Epoch 37/300 -- Iteration 35334 - Batch 666/963 - Train loss: 0.30018306  - Train acc: 0.8855 - Val loss: 1.04222727\n",
      "(7.32 min) Epoch 37/300 -- Iteration 35343 - Batch 675/963 - Train loss: 0.30044927  - Train acc: 0.8853 - Val loss: 1.04222727\n",
      "(7.32 min) Epoch 37/300 -- Iteration 35352 - Batch 684/963 - Train loss: 0.30044061  - Train acc: 0.8854 - Val loss: 1.04222727\n",
      "(7.32 min) Epoch 37/300 -- Iteration 35361 - Batch 693/963 - Train loss: 0.30021214  - Train acc: 0.8855 - Val loss: 1.04222727\n",
      "(7.33 min) Epoch 37/300 -- Iteration 35370 - Batch 702/963 - Train loss: 0.30067719  - Train acc: 0.8854 - Val loss: 1.04222727\n",
      "(7.33 min) Epoch 37/300 -- Iteration 35379 - Batch 711/963 - Train loss: 0.30059615  - Train acc: 0.8853 - Val loss: 1.04222727\n",
      "(7.33 min) Epoch 37/300 -- Iteration 35388 - Batch 720/963 - Train loss: 0.30047109  - Train acc: 0.8854 - Val loss: 1.04222727\n",
      "(7.33 min) Epoch 37/300 -- Iteration 35397 - Batch 729/963 - Train loss: 0.30080642  - Train acc: 0.8853 - Val loss: 1.04222727\n",
      "(7.33 min) Epoch 37/300 -- Iteration 35406 - Batch 738/963 - Train loss: 0.30117957  - Train acc: 0.8852 - Val loss: 1.04222727\n",
      "(7.34 min) Epoch 37/300 -- Iteration 35415 - Batch 747/963 - Train loss: 0.30100221  - Train acc: 0.8853 - Val loss: 1.04222727\n",
      "(7.34 min) Epoch 37/300 -- Iteration 35424 - Batch 756/963 - Train loss: 0.30094383  - Train acc: 0.8854 - Val loss: 1.04222727\n",
      "(7.34 min) Epoch 37/300 -- Iteration 35433 - Batch 765/963 - Train loss: 0.30079527  - Train acc: 0.8855 - Val loss: 1.04222727\n",
      "(7.34 min) Epoch 37/300 -- Iteration 35442 - Batch 774/963 - Train loss: 0.30084891  - Train acc: 0.8854 - Val loss: 1.04222727\n",
      "(7.34 min) Epoch 37/300 -- Iteration 35451 - Batch 783/963 - Train loss: 0.30063486  - Train acc: 0.8855 - Val loss: 1.04222727\n",
      "(7.35 min) Epoch 37/300 -- Iteration 35460 - Batch 792/963 - Train loss: 0.30058974  - Train acc: 0.8855 - Val loss: 1.04222727\n",
      "(7.35 min) Epoch 37/300 -- Iteration 35469 - Batch 801/963 - Train loss: 0.30012764  - Train acc: 0.8857 - Val loss: 1.04222727\n",
      "(7.35 min) Epoch 37/300 -- Iteration 35478 - Batch 810/963 - Train loss: 0.30021277  - Train acc: 0.8856 - Val loss: 1.04222727\n",
      "(7.35 min) Epoch 37/300 -- Iteration 35487 - Batch 819/963 - Train loss: 0.30017970  - Train acc: 0.8856 - Val loss: 1.04222727\n",
      "(7.35 min) Epoch 37/300 -- Iteration 35496 - Batch 828/963 - Train loss: 0.30038614  - Train acc: 0.8855 - Val loss: 1.04222727\n",
      "(7.36 min) Epoch 37/300 -- Iteration 35505 - Batch 837/963 - Train loss: 0.30039555  - Train acc: 0.8855 - Val loss: 1.04222727\n",
      "(7.36 min) Epoch 37/300 -- Iteration 35514 - Batch 846/963 - Train loss: 0.30006996  - Train acc: 0.8858 - Val loss: 1.04222727\n",
      "(7.36 min) Epoch 37/300 -- Iteration 35523 - Batch 855/963 - Train loss: 0.30048584  - Train acc: 0.8856 - Val loss: 1.04222727\n",
      "(7.36 min) Epoch 37/300 -- Iteration 35532 - Batch 864/963 - Train loss: 0.30048035  - Train acc: 0.8855 - Val loss: 1.04222727\n",
      "(7.36 min) Epoch 37/300 -- Iteration 35541 - Batch 873/963 - Train loss: 0.30060712  - Train acc: 0.8855 - Val loss: 1.04222727\n",
      "(7.37 min) Epoch 37/300 -- Iteration 35550 - Batch 882/963 - Train loss: 0.30081364  - Train acc: 0.8853 - Val loss: 1.04222727\n",
      "(7.37 min) Epoch 37/300 -- Iteration 35559 - Batch 891/963 - Train loss: 0.30117113  - Train acc: 0.8851 - Val loss: 1.04222727\n",
      "(7.37 min) Epoch 37/300 -- Iteration 35568 - Batch 900/963 - Train loss: 0.30109356  - Train acc: 0.8852 - Val loss: 1.04222727\n",
      "(7.37 min) Epoch 37/300 -- Iteration 35577 - Batch 909/963 - Train loss: 0.30101323  - Train acc: 0.8851 - Val loss: 1.04222727\n",
      "(7.37 min) Epoch 37/300 -- Iteration 35586 - Batch 918/963 - Train loss: 0.30092018  - Train acc: 0.8852 - Val loss: 1.04222727\n",
      "(7.38 min) Epoch 37/300 -- Iteration 35595 - Batch 927/963 - Train loss: 0.30065357  - Train acc: 0.8853 - Val loss: 1.04222727\n",
      "(7.38 min) Epoch 37/300 -- Iteration 35604 - Batch 936/963 - Train loss: 0.30085927  - Train acc: 0.8851 - Val loss: 1.04222727\n",
      "(7.38 min) Epoch 37/300 -- Iteration 35613 - Batch 945/963 - Train loss: 0.30092429  - Train acc: 0.8851 - Val loss: 1.04222727\n",
      "(7.38 min) Epoch 37/300 -- Iteration 35622 - Batch 954/963 - Train loss: 0.30098297  - Train acc: 0.8852 - Val loss: 1.04222727\n",
      "(7.39 min) Epoch 37/300 -- Iteration 35631 - Batch 962/963 - Train loss: 0.30085123  - Train acc: 0.8853 - Val loss: 1.02654493 - Val acc: 0.5933\n",
      "(7.39 min) Epoch 38/300 -- Iteration 35640 - Batch 9/963 - Train loss: 0.29609725  - Train acc: 0.8898 - Val loss: 1.02654493\n",
      "(7.39 min) Epoch 38/300 -- Iteration 35649 - Batch 18/963 - Train loss: 0.30788462  - Train acc: 0.8824 - Val loss: 1.02654493\n",
      "(7.39 min) Epoch 38/300 -- Iteration 35658 - Batch 27/963 - Train loss: 0.30374820  - Train acc: 0.8848 - Val loss: 1.02654493\n",
      "(7.40 min) Epoch 38/300 -- Iteration 35667 - Batch 36/963 - Train loss: 0.30093875  - Train acc: 0.8845 - Val loss: 1.02654493\n",
      "(7.40 min) Epoch 38/300 -- Iteration 35676 - Batch 45/963 - Train loss: 0.29900809  - Train acc: 0.8847 - Val loss: 1.02654493\n",
      "(7.40 min) Epoch 38/300 -- Iteration 35685 - Batch 54/963 - Train loss: 0.29862797  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.40 min) Epoch 38/300 -- Iteration 35694 - Batch 63/963 - Train loss: 0.30063268  - Train acc: 0.8849 - Val loss: 1.02654493\n",
      "(7.40 min) Epoch 38/300 -- Iteration 35703 - Batch 72/963 - Train loss: 0.29943543  - Train acc: 0.8845 - Val loss: 1.02654493\n",
      "(7.41 min) Epoch 38/300 -- Iteration 35712 - Batch 81/963 - Train loss: 0.29569696  - Train acc: 0.8861 - Val loss: 1.02654493\n",
      "(7.41 min) Epoch 38/300 -- Iteration 35721 - Batch 90/963 - Train loss: 0.29692765  - Train acc: 0.8861 - Val loss: 1.02654493\n",
      "(7.41 min) Epoch 38/300 -- Iteration 35730 - Batch 99/963 - Train loss: 0.29863165  - Train acc: 0.8874 - Val loss: 1.02654493\n",
      "(7.41 min) Epoch 38/300 -- Iteration 35739 - Batch 108/963 - Train loss: 0.29933327  - Train acc: 0.8873 - Val loss: 1.02654493\n",
      "(7.41 min) Epoch 38/300 -- Iteration 35748 - Batch 117/963 - Train loss: 0.29777454  - Train acc: 0.8873 - Val loss: 1.02654493\n",
      "(7.42 min) Epoch 38/300 -- Iteration 35757 - Batch 126/963 - Train loss: 0.29686785  - Train acc: 0.8876 - Val loss: 1.02654493\n",
      "(7.42 min) Epoch 38/300 -- Iteration 35766 - Batch 135/963 - Train loss: 0.29627549  - Train acc: 0.8879 - Val loss: 1.02654493\n",
      "(7.42 min) Epoch 38/300 -- Iteration 35775 - Batch 144/963 - Train loss: 0.29583325  - Train acc: 0.8883 - Val loss: 1.02654493\n",
      "(7.42 min) Epoch 38/300 -- Iteration 35784 - Batch 153/963 - Train loss: 0.29558515  - Train acc: 0.8886 - Val loss: 1.02654493\n",
      "(7.42 min) Epoch 38/300 -- Iteration 35793 - Batch 162/963 - Train loss: 0.29584227  - Train acc: 0.8881 - Val loss: 1.02654493\n",
      "(7.43 min) Epoch 38/300 -- Iteration 35802 - Batch 171/963 - Train loss: 0.29718580  - Train acc: 0.8877 - Val loss: 1.02654493\n",
      "(7.43 min) Epoch 38/300 -- Iteration 35811 - Batch 180/963 - Train loss: 0.29622729  - Train acc: 0.8886 - Val loss: 1.02654493\n",
      "(7.43 min) Epoch 38/300 -- Iteration 35820 - Batch 189/963 - Train loss: 0.29713213  - Train acc: 0.8882 - Val loss: 1.02654493\n",
      "(7.43 min) Epoch 38/300 -- Iteration 35829 - Batch 198/963 - Train loss: 0.29699194  - Train acc: 0.8882 - Val loss: 1.02654493\n",
      "(7.43 min) Epoch 38/300 -- Iteration 35838 - Batch 207/963 - Train loss: 0.29817193  - Train acc: 0.8878 - Val loss: 1.02654493\n",
      "(7.44 min) Epoch 38/300 -- Iteration 35847 - Batch 216/963 - Train loss: 0.30005635  - Train acc: 0.8869 - Val loss: 1.02654493\n",
      "(7.44 min) Epoch 38/300 -- Iteration 35856 - Batch 225/963 - Train loss: 0.29941974  - Train acc: 0.8873 - Val loss: 1.02654493\n",
      "(7.44 min) Epoch 38/300 -- Iteration 35865 - Batch 234/963 - Train loss: 0.30009155  - Train acc: 0.8869 - Val loss: 1.02654493\n",
      "(7.44 min) Epoch 38/300 -- Iteration 35874 - Batch 243/963 - Train loss: 0.30049610  - Train acc: 0.8867 - Val loss: 1.02654493\n",
      "(7.45 min) Epoch 38/300 -- Iteration 35883 - Batch 252/963 - Train loss: 0.30183903  - Train acc: 0.8861 - Val loss: 1.02654493\n",
      "(7.45 min) Epoch 38/300 -- Iteration 35892 - Batch 261/963 - Train loss: 0.30201598  - Train acc: 0.8861 - Val loss: 1.02654493\n",
      "(7.45 min) Epoch 38/300 -- Iteration 35901 - Batch 270/963 - Train loss: 0.30236307  - Train acc: 0.8858 - Val loss: 1.02654493\n",
      "(7.45 min) Epoch 38/300 -- Iteration 35910 - Batch 279/963 - Train loss: 0.30220853  - Train acc: 0.8857 - Val loss: 1.02654493\n",
      "(7.45 min) Epoch 38/300 -- Iteration 35919 - Batch 288/963 - Train loss: 0.30170817  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.45 min) Epoch 38/300 -- Iteration 35928 - Batch 297/963 - Train loss: 0.30194888  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.46 min) Epoch 38/300 -- Iteration 35937 - Batch 306/963 - Train loss: 0.30133706  - Train acc: 0.8859 - Val loss: 1.02654493\n",
      "(7.46 min) Epoch 38/300 -- Iteration 35946 - Batch 315/963 - Train loss: 0.30070110  - Train acc: 0.8861 - Val loss: 1.02654493\n",
      "(7.46 min) Epoch 38/300 -- Iteration 35955 - Batch 324/963 - Train loss: 0.30097861  - Train acc: 0.8860 - Val loss: 1.02654493\n",
      "(7.46 min) Epoch 38/300 -- Iteration 35964 - Batch 333/963 - Train loss: 0.30043148  - Train acc: 0.8859 - Val loss: 1.02654493\n",
      "(7.46 min) Epoch 38/300 -- Iteration 35973 - Batch 342/963 - Train loss: 0.29985636  - Train acc: 0.8861 - Val loss: 1.02654493\n",
      "(7.47 min) Epoch 38/300 -- Iteration 35982 - Batch 351/963 - Train loss: 0.29978910  - Train acc: 0.8861 - Val loss: 1.02654493\n",
      "(7.47 min) Epoch 38/300 -- Iteration 35991 - Batch 360/963 - Train loss: 0.30014223  - Train acc: 0.8858 - Val loss: 1.02654493\n",
      "(7.47 min) Epoch 38/300 -- Iteration 36000 - Batch 369/963 - Train loss: 0.30066860  - Train acc: 0.8854 - Val loss: 1.02654493\n",
      "(7.47 min) Epoch 38/300 -- Iteration 36009 - Batch 378/963 - Train loss: 0.30072861  - Train acc: 0.8854 - Val loss: 1.02654493\n",
      "(7.47 min) Epoch 38/300 -- Iteration 36018 - Batch 387/963 - Train loss: 0.30075674  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.48 min) Epoch 38/300 -- Iteration 36027 - Batch 396/963 - Train loss: 0.30042905  - Train acc: 0.8853 - Val loss: 1.02654493\n",
      "(7.48 min) Epoch 38/300 -- Iteration 36036 - Batch 405/963 - Train loss: 0.30084244  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.48 min) Epoch 38/300 -- Iteration 36045 - Batch 414/963 - Train loss: 0.30078451  - Train acc: 0.8850 - Val loss: 1.02654493\n",
      "(7.48 min) Epoch 38/300 -- Iteration 36054 - Batch 423/963 - Train loss: 0.30094268  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.48 min) Epoch 38/300 -- Iteration 36063 - Batch 432/963 - Train loss: 0.30054658  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.49 min) Epoch 38/300 -- Iteration 36072 - Batch 441/963 - Train loss: 0.30045506  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.49 min) Epoch 38/300 -- Iteration 36081 - Batch 450/963 - Train loss: 0.30038820  - Train acc: 0.8852 - Val loss: 1.02654493\n",
      "(7.49 min) Epoch 38/300 -- Iteration 36090 - Batch 459/963 - Train loss: 0.29992629  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.49 min) Epoch 38/300 -- Iteration 36099 - Batch 468/963 - Train loss: 0.30010966  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.49 min) Epoch 38/300 -- Iteration 36108 - Batch 477/963 - Train loss: 0.29975717  - Train acc: 0.8859 - Val loss: 1.02654493\n",
      "(7.50 min) Epoch 38/300 -- Iteration 36117 - Batch 486/963 - Train loss: 0.29963078  - Train acc: 0.8860 - Val loss: 1.02654493\n",
      "(7.50 min) Epoch 38/300 -- Iteration 36126 - Batch 495/963 - Train loss: 0.29992724  - Train acc: 0.8858 - Val loss: 1.02654493\n",
      "(7.50 min) Epoch 38/300 -- Iteration 36135 - Batch 504/963 - Train loss: 0.29961073  - Train acc: 0.8859 - Val loss: 1.02654493\n",
      "(7.50 min) Epoch 38/300 -- Iteration 36144 - Batch 513/963 - Train loss: 0.29979203  - Train acc: 0.8859 - Val loss: 1.02654493\n",
      "(7.50 min) Epoch 38/300 -- Iteration 36153 - Batch 522/963 - Train loss: 0.29991673  - Train acc: 0.8858 - Val loss: 1.02654493\n",
      "(7.51 min) Epoch 38/300 -- Iteration 36162 - Batch 531/963 - Train loss: 0.30020011  - Train acc: 0.8857 - Val loss: 1.02654493\n",
      "(7.51 min) Epoch 38/300 -- Iteration 36171 - Batch 540/963 - Train loss: 0.29997679  - Train acc: 0.8857 - Val loss: 1.02654493\n",
      "(7.51 min) Epoch 38/300 -- Iteration 36180 - Batch 549/963 - Train loss: 0.30016368  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.51 min) Epoch 38/300 -- Iteration 36189 - Batch 558/963 - Train loss: 0.30023592  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.51 min) Epoch 38/300 -- Iteration 36198 - Batch 567/963 - Train loss: 0.29997750  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.52 min) Epoch 38/300 -- Iteration 36207 - Batch 576/963 - Train loss: 0.30038412  - Train acc: 0.8853 - Val loss: 1.02654493\n",
      "(7.52 min) Epoch 38/300 -- Iteration 36216 - Batch 585/963 - Train loss: 0.30035846  - Train acc: 0.8852 - Val loss: 1.02654493\n",
      "(7.52 min) Epoch 38/300 -- Iteration 36225 - Batch 594/963 - Train loss: 0.30028631  - Train acc: 0.8852 - Val loss: 1.02654493\n",
      "(7.52 min) Epoch 38/300 -- Iteration 36234 - Batch 603/963 - Train loss: 0.30057540  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.52 min) Epoch 38/300 -- Iteration 36243 - Batch 612/963 - Train loss: 0.30077759  - Train acc: 0.8849 - Val loss: 1.02654493\n",
      "(7.52 min) Epoch 38/300 -- Iteration 36252 - Batch 621/963 - Train loss: 0.30032427  - Train acc: 0.8850 - Val loss: 1.02654493\n",
      "(7.53 min) Epoch 38/300 -- Iteration 36261 - Batch 630/963 - Train loss: 0.30003233  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.53 min) Epoch 38/300 -- Iteration 36270 - Batch 639/963 - Train loss: 0.30064655  - Train acc: 0.8849 - Val loss: 1.02654493\n",
      "(7.53 min) Epoch 38/300 -- Iteration 36279 - Batch 648/963 - Train loss: 0.30033231  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.53 min) Epoch 38/300 -- Iteration 36288 - Batch 657/963 - Train loss: 0.30034329  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.54 min) Epoch 38/300 -- Iteration 36297 - Batch 666/963 - Train loss: 0.30027435  - Train acc: 0.8852 - Val loss: 1.02654493\n",
      "(7.54 min) Epoch 38/300 -- Iteration 36306 - Batch 675/963 - Train loss: 0.30032429  - Train acc: 0.8852 - Val loss: 1.02654493\n",
      "(7.54 min) Epoch 38/300 -- Iteration 36315 - Batch 684/963 - Train loss: 0.30050763  - Train acc: 0.8851 - Val loss: 1.02654493\n",
      "(7.54 min) Epoch 38/300 -- Iteration 36324 - Batch 693/963 - Train loss: 0.30023119  - Train acc: 0.8852 - Val loss: 1.02654493\n",
      "(7.55 min) Epoch 38/300 -- Iteration 36333 - Batch 702/963 - Train loss: 0.30017148  - Train acc: 0.8853 - Val loss: 1.02654493\n",
      "(7.55 min) Epoch 38/300 -- Iteration 36342 - Batch 711/963 - Train loss: 0.30025728  - Train acc: 0.8853 - Val loss: 1.02654493\n",
      "(7.55 min) Epoch 38/300 -- Iteration 36351 - Batch 720/963 - Train loss: 0.30020497  - Train acc: 0.8854 - Val loss: 1.02654493\n",
      "(7.55 min) Epoch 38/300 -- Iteration 36360 - Batch 729/963 - Train loss: 0.29973366  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.55 min) Epoch 38/300 -- Iteration 36369 - Batch 738/963 - Train loss: 0.29964671  - Train acc: 0.8857 - Val loss: 1.02654493\n",
      "(7.56 min) Epoch 38/300 -- Iteration 36378 - Batch 747/963 - Train loss: 0.29961562  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.56 min) Epoch 38/300 -- Iteration 36387 - Batch 756/963 - Train loss: 0.29953433  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.56 min) Epoch 38/300 -- Iteration 36396 - Batch 765/963 - Train loss: 0.29949643  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.56 min) Epoch 38/300 -- Iteration 36405 - Batch 774/963 - Train loss: 0.29978192  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.57 min) Epoch 38/300 -- Iteration 36414 - Batch 783/963 - Train loss: 0.29959705  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.57 min) Epoch 38/300 -- Iteration 36423 - Batch 792/963 - Train loss: 0.29924056  - Train acc: 0.8857 - Val loss: 1.02654493\n",
      "(7.57 min) Epoch 38/300 -- Iteration 36432 - Batch 801/963 - Train loss: 0.29942957  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.57 min) Epoch 38/300 -- Iteration 36441 - Batch 810/963 - Train loss: 0.29952367  - Train acc: 0.8854 - Val loss: 1.02654493\n",
      "(7.58 min) Epoch 38/300 -- Iteration 36450 - Batch 819/963 - Train loss: 0.29953063  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.58 min) Epoch 38/300 -- Iteration 36459 - Batch 828/963 - Train loss: 0.29924968  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.58 min) Epoch 38/300 -- Iteration 36468 - Batch 837/963 - Train loss: 0.29920983  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.58 min) Epoch 38/300 -- Iteration 36477 - Batch 846/963 - Train loss: 0.29950275  - Train acc: 0.8856 - Val loss: 1.02654493\n",
      "(7.58 min) Epoch 38/300 -- Iteration 36486 - Batch 855/963 - Train loss: 0.29951729  - Train acc: 0.8857 - Val loss: 1.02654493\n",
      "(7.59 min) Epoch 38/300 -- Iteration 36495 - Batch 864/963 - Train loss: 0.29925305  - Train acc: 0.8859 - Val loss: 1.02654493\n",
      "(7.59 min) Epoch 38/300 -- Iteration 36504 - Batch 873/963 - Train loss: 0.29949244  - Train acc: 0.8859 - Val loss: 1.02654493\n",
      "(7.59 min) Epoch 38/300 -- Iteration 36513 - Batch 882/963 - Train loss: 0.29991597  - Train acc: 0.8857 - Val loss: 1.02654493\n",
      "(7.59 min) Epoch 38/300 -- Iteration 36522 - Batch 891/963 - Train loss: 0.30046873  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.59 min) Epoch 38/300 -- Iteration 36531 - Batch 900/963 - Train loss: 0.30051464  - Train acc: 0.8854 - Val loss: 1.02654493\n",
      "(7.60 min) Epoch 38/300 -- Iteration 36540 - Batch 909/963 - Train loss: 0.30048396  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.60 min) Epoch 38/300 -- Iteration 36549 - Batch 918/963 - Train loss: 0.30047216  - Train acc: 0.8855 - Val loss: 1.02654493\n",
      "(7.60 min) Epoch 38/300 -- Iteration 36558 - Batch 927/963 - Train loss: 0.30019248  - Train acc: 0.8857 - Val loss: 1.02654493\n",
      "(7.60 min) Epoch 38/300 -- Iteration 36567 - Batch 936/963 - Train loss: 0.30020284  - Train acc: 0.8857 - Val loss: 1.02654493\n",
      "(7.60 min) Epoch 38/300 -- Iteration 36576 - Batch 945/963 - Train loss: 0.30014079  - Train acc: 0.8858 - Val loss: 1.02654493\n",
      "(7.61 min) Epoch 38/300 -- Iteration 36585 - Batch 954/963 - Train loss: 0.29990908  - Train acc: 0.8858 - Val loss: 1.02654493\n",
      "(7.61 min) Epoch 38/300 -- Iteration 36594 - Batch 962/963 - Train loss: 0.29985378  - Train acc: 0.8858 - Val loss: 1.02696121 - Val acc: 0.5967\n",
      "(7.61 min) Epoch 39/300 -- Iteration 36603 - Batch 9/963 - Train loss: 0.30867756  - Train acc: 0.8859 - Val loss: 1.02696121\n",
      "(7.61 min) Epoch 39/300 -- Iteration 36612 - Batch 18/963 - Train loss: 0.29576505  - Train acc: 0.8894 - Val loss: 1.02696121\n",
      "(7.61 min) Epoch 39/300 -- Iteration 36621 - Batch 27/963 - Train loss: 0.29465569  - Train acc: 0.8901 - Val loss: 1.02696121\n",
      "(7.62 min) Epoch 39/300 -- Iteration 36630 - Batch 36/963 - Train loss: 0.29519854  - Train acc: 0.8904 - Val loss: 1.02696121\n",
      "(7.62 min) Epoch 39/300 -- Iteration 36639 - Batch 45/963 - Train loss: 0.29505988  - Train acc: 0.8893 - Val loss: 1.02696121\n",
      "(7.62 min) Epoch 39/300 -- Iteration 36648 - Batch 54/963 - Train loss: 0.29382538  - Train acc: 0.8891 - Val loss: 1.02696121\n",
      "(7.62 min) Epoch 39/300 -- Iteration 36657 - Batch 63/963 - Train loss: 0.29494937  - Train acc: 0.8898 - Val loss: 1.02696121\n",
      "(7.62 min) Epoch 39/300 -- Iteration 36666 - Batch 72/963 - Train loss: 0.29398444  - Train acc: 0.8898 - Val loss: 1.02696121\n",
      "(7.63 min) Epoch 39/300 -- Iteration 36675 - Batch 81/963 - Train loss: 0.29359519  - Train acc: 0.8894 - Val loss: 1.02696121\n",
      "(7.63 min) Epoch 39/300 -- Iteration 36684 - Batch 90/963 - Train loss: 0.29584257  - Train acc: 0.8880 - Val loss: 1.02696121\n",
      "(7.63 min) Epoch 39/300 -- Iteration 36693 - Batch 99/963 - Train loss: 0.29596926  - Train acc: 0.8877 - Val loss: 1.02696121\n",
      "(7.63 min) Epoch 39/300 -- Iteration 36702 - Batch 108/963 - Train loss: 0.29763867  - Train acc: 0.8868 - Val loss: 1.02696121\n",
      "(7.63 min) Epoch 39/300 -- Iteration 36711 - Batch 117/963 - Train loss: 0.29796865  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.63 min) Epoch 39/300 -- Iteration 36720 - Batch 126/963 - Train loss: 0.29670666  - Train acc: 0.8880 - Val loss: 1.02696121\n",
      "(7.64 min) Epoch 39/300 -- Iteration 36729 - Batch 135/963 - Train loss: 0.29570699  - Train acc: 0.8887 - Val loss: 1.02696121\n",
      "(7.64 min) Epoch 39/300 -- Iteration 36738 - Batch 144/963 - Train loss: 0.29698053  - Train acc: 0.8882 - Val loss: 1.02696121\n",
      "(7.64 min) Epoch 39/300 -- Iteration 36747 - Batch 153/963 - Train loss: 0.29668906  - Train acc: 0.8885 - Val loss: 1.02696121\n",
      "(7.64 min) Epoch 39/300 -- Iteration 36756 - Batch 162/963 - Train loss: 0.29668352  - Train acc: 0.8882 - Val loss: 1.02696121\n",
      "(7.65 min) Epoch 39/300 -- Iteration 36765 - Batch 171/963 - Train loss: 0.29655394  - Train acc: 0.8876 - Val loss: 1.02696121\n",
      "(7.65 min) Epoch 39/300 -- Iteration 36774 - Batch 180/963 - Train loss: 0.29744133  - Train acc: 0.8874 - Val loss: 1.02696121\n",
      "(7.65 min) Epoch 39/300 -- Iteration 36783 - Batch 189/963 - Train loss: 0.29813075  - Train acc: 0.8873 - Val loss: 1.02696121\n",
      "(7.65 min) Epoch 39/300 -- Iteration 36792 - Batch 198/963 - Train loss: 0.29733743  - Train acc: 0.8876 - Val loss: 1.02696121\n",
      "(7.65 min) Epoch 39/300 -- Iteration 36801 - Batch 207/963 - Train loss: 0.29804003  - Train acc: 0.8874 - Val loss: 1.02696121\n",
      "(7.65 min) Epoch 39/300 -- Iteration 36810 - Batch 216/963 - Train loss: 0.29777092  - Train acc: 0.8876 - Val loss: 1.02696121\n",
      "(7.66 min) Epoch 39/300 -- Iteration 36819 - Batch 225/963 - Train loss: 0.29730093  - Train acc: 0.8878 - Val loss: 1.02696121\n",
      "(7.66 min) Epoch 39/300 -- Iteration 36828 - Batch 234/963 - Train loss: 0.29668880  - Train acc: 0.8883 - Val loss: 1.02696121\n",
      "(7.66 min) Epoch 39/300 -- Iteration 36837 - Batch 243/963 - Train loss: 0.29654571  - Train acc: 0.8884 - Val loss: 1.02696121\n",
      "(7.66 min) Epoch 39/300 -- Iteration 36846 - Batch 252/963 - Train loss: 0.29716409  - Train acc: 0.8882 - Val loss: 1.02696121\n",
      "(7.66 min) Epoch 39/300 -- Iteration 36855 - Batch 261/963 - Train loss: 0.29701255  - Train acc: 0.8882 - Val loss: 1.02696121\n",
      "(7.67 min) Epoch 39/300 -- Iteration 36864 - Batch 270/963 - Train loss: 0.29725277  - Train acc: 0.8883 - Val loss: 1.02696121\n",
      "(7.67 min) Epoch 39/300 -- Iteration 36873 - Batch 279/963 - Train loss: 0.29705912  - Train acc: 0.8882 - Val loss: 1.02696121\n",
      "(7.67 min) Epoch 39/300 -- Iteration 36882 - Batch 288/963 - Train loss: 0.29661518  - Train acc: 0.8881 - Val loss: 1.02696121\n",
      "(7.67 min) Epoch 39/300 -- Iteration 36891 - Batch 297/963 - Train loss: 0.29634328  - Train acc: 0.8880 - Val loss: 1.02696121\n",
      "(7.67 min) Epoch 39/300 -- Iteration 36900 - Batch 306/963 - Train loss: 0.29646364  - Train acc: 0.8879 - Val loss: 1.02696121\n",
      "(7.68 min) Epoch 39/300 -- Iteration 36909 - Batch 315/963 - Train loss: 0.29703474  - Train acc: 0.8877 - Val loss: 1.02696121\n",
      "(7.68 min) Epoch 39/300 -- Iteration 36918 - Batch 324/963 - Train loss: 0.29705118  - Train acc: 0.8878 - Val loss: 1.02696121\n",
      "(7.68 min) Epoch 39/300 -- Iteration 36927 - Batch 333/963 - Train loss: 0.29750139  - Train acc: 0.8874 - Val loss: 1.02696121\n",
      "(7.68 min) Epoch 39/300 -- Iteration 36936 - Batch 342/963 - Train loss: 0.29769944  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.68 min) Epoch 39/300 -- Iteration 36945 - Batch 351/963 - Train loss: 0.29787662  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.68 min) Epoch 39/300 -- Iteration 36954 - Batch 360/963 - Train loss: 0.29910443  - Train acc: 0.8867 - Val loss: 1.02696121\n",
      "(7.69 min) Epoch 39/300 -- Iteration 36963 - Batch 369/963 - Train loss: 0.29920851  - Train acc: 0.8866 - Val loss: 1.02696121\n",
      "(7.69 min) Epoch 39/300 -- Iteration 36972 - Batch 378/963 - Train loss: 0.29875582  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.69 min) Epoch 39/300 -- Iteration 36981 - Batch 387/963 - Train loss: 0.29885422  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.69 min) Epoch 39/300 -- Iteration 36990 - Batch 396/963 - Train loss: 0.29825839  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.70 min) Epoch 39/300 -- Iteration 36999 - Batch 405/963 - Train loss: 0.29776693  - Train acc: 0.8874 - Val loss: 1.02696121\n",
      "(7.70 min) Epoch 39/300 -- Iteration 37008 - Batch 414/963 - Train loss: 0.29814541  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.70 min) Epoch 39/300 -- Iteration 37017 - Batch 423/963 - Train loss: 0.29811634  - Train acc: 0.8873 - Val loss: 1.02696121\n",
      "(7.70 min) Epoch 39/300 -- Iteration 37026 - Batch 432/963 - Train loss: 0.29869531  - Train acc: 0.8870 - Val loss: 1.02696121\n",
      "(7.70 min) Epoch 39/300 -- Iteration 37035 - Batch 441/963 - Train loss: 0.29838988  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.70 min) Epoch 39/300 -- Iteration 37044 - Batch 450/963 - Train loss: 0.29790133  - Train acc: 0.8874 - Val loss: 1.02696121\n",
      "(7.71 min) Epoch 39/300 -- Iteration 37053 - Batch 459/963 - Train loss: 0.29832297  - Train acc: 0.8873 - Val loss: 1.02696121\n",
      "(7.71 min) Epoch 39/300 -- Iteration 37062 - Batch 468/963 - Train loss: 0.29825099  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.71 min) Epoch 39/300 -- Iteration 37071 - Batch 477/963 - Train loss: 0.29846095  - Train acc: 0.8871 - Val loss: 1.02696121\n",
      "(7.71 min) Epoch 39/300 -- Iteration 37080 - Batch 486/963 - Train loss: 0.29816177  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.71 min) Epoch 39/300 -- Iteration 37089 - Batch 495/963 - Train loss: 0.29876455  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.72 min) Epoch 39/300 -- Iteration 37098 - Batch 504/963 - Train loss: 0.29887932  - Train acc: 0.8868 - Val loss: 1.02696121\n",
      "(7.72 min) Epoch 39/300 -- Iteration 37107 - Batch 513/963 - Train loss: 0.29867091  - Train acc: 0.8867 - Val loss: 1.02696121\n",
      "(7.72 min) Epoch 39/300 -- Iteration 37116 - Batch 522/963 - Train loss: 0.29882099  - Train acc: 0.8868 - Val loss: 1.02696121\n",
      "(7.72 min) Epoch 39/300 -- Iteration 37125 - Batch 531/963 - Train loss: 0.29862671  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.72 min) Epoch 39/300 -- Iteration 37134 - Batch 540/963 - Train loss: 0.29862275  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.73 min) Epoch 39/300 -- Iteration 37143 - Batch 549/963 - Train loss: 0.29838460  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.73 min) Epoch 39/300 -- Iteration 37152 - Batch 558/963 - Train loss: 0.29826876  - Train acc: 0.8870 - Val loss: 1.02696121\n",
      "(7.73 min) Epoch 39/300 -- Iteration 37161 - Batch 567/963 - Train loss: 0.29865093  - Train acc: 0.8868 - Val loss: 1.02696121\n",
      "(7.73 min) Epoch 39/300 -- Iteration 37170 - Batch 576/963 - Train loss: 0.29824932  - Train acc: 0.8870 - Val loss: 1.02696121\n",
      "(7.73 min) Epoch 39/300 -- Iteration 37179 - Batch 585/963 - Train loss: 0.29852522  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.73 min) Epoch 39/300 -- Iteration 37188 - Batch 594/963 - Train loss: 0.29851009  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.74 min) Epoch 39/300 -- Iteration 37197 - Batch 603/963 - Train loss: 0.29821626  - Train acc: 0.8870 - Val loss: 1.02696121\n",
      "(7.74 min) Epoch 39/300 -- Iteration 37206 - Batch 612/963 - Train loss: 0.29844943  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.74 min) Epoch 39/300 -- Iteration 37215 - Batch 621/963 - Train loss: 0.29867910  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.74 min) Epoch 39/300 -- Iteration 37224 - Batch 630/963 - Train loss: 0.29864030  - Train acc: 0.8870 - Val loss: 1.02696121\n",
      "(7.75 min) Epoch 39/300 -- Iteration 37233 - Batch 639/963 - Train loss: 0.29883646  - Train acc: 0.8867 - Val loss: 1.02696121\n",
      "(7.75 min) Epoch 39/300 -- Iteration 37242 - Batch 648/963 - Train loss: 0.29856180  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.75 min) Epoch 39/300 -- Iteration 37251 - Batch 657/963 - Train loss: 0.29922644  - Train acc: 0.8868 - Val loss: 1.02696121\n",
      "(7.75 min) Epoch 39/300 -- Iteration 37260 - Batch 666/963 - Train loss: 0.29945870  - Train acc: 0.8867 - Val loss: 1.02696121\n",
      "(7.75 min) Epoch 39/300 -- Iteration 37269 - Batch 675/963 - Train loss: 0.29924869  - Train acc: 0.8868 - Val loss: 1.02696121\n",
      "(7.75 min) Epoch 39/300 -- Iteration 37278 - Batch 684/963 - Train loss: 0.29910583  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.76 min) Epoch 39/300 -- Iteration 37287 - Batch 693/963 - Train loss: 0.29922698  - Train acc: 0.8868 - Val loss: 1.02696121\n",
      "(7.76 min) Epoch 39/300 -- Iteration 37296 - Batch 702/963 - Train loss: 0.29901949  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.76 min) Epoch 39/300 -- Iteration 37305 - Batch 711/963 - Train loss: 0.29852893  - Train acc: 0.8871 - Val loss: 1.02696121\n",
      "(7.76 min) Epoch 39/300 -- Iteration 37314 - Batch 720/963 - Train loss: 0.29872627  - Train acc: 0.8871 - Val loss: 1.02696121\n",
      "(7.76 min) Epoch 39/300 -- Iteration 37323 - Batch 729/963 - Train loss: 0.29861015  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.77 min) Epoch 39/300 -- Iteration 37332 - Batch 738/963 - Train loss: 0.29831036  - Train acc: 0.8873 - Val loss: 1.02696121\n",
      "(7.77 min) Epoch 39/300 -- Iteration 37341 - Batch 747/963 - Train loss: 0.29818964  - Train acc: 0.8874 - Val loss: 1.02696121\n",
      "(7.77 min) Epoch 39/300 -- Iteration 37350 - Batch 756/963 - Train loss: 0.29829494  - Train acc: 0.8873 - Val loss: 1.02696121\n",
      "(7.77 min) Epoch 39/300 -- Iteration 37359 - Batch 765/963 - Train loss: 0.29827028  - Train acc: 0.8873 - Val loss: 1.02696121\n",
      "(7.78 min) Epoch 39/300 -- Iteration 37368 - Batch 774/963 - Train loss: 0.29838748  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.78 min) Epoch 39/300 -- Iteration 37377 - Batch 783/963 - Train loss: 0.29805522  - Train acc: 0.8874 - Val loss: 1.02696121\n",
      "(7.78 min) Epoch 39/300 -- Iteration 37386 - Batch 792/963 - Train loss: 0.29811403  - Train acc: 0.8874 - Val loss: 1.02696121\n",
      "(7.78 min) Epoch 39/300 -- Iteration 37395 - Batch 801/963 - Train loss: 0.29830749  - Train acc: 0.8873 - Val loss: 1.02696121\n",
      "(7.79 min) Epoch 39/300 -- Iteration 37404 - Batch 810/963 - Train loss: 0.29856706  - Train acc: 0.8872 - Val loss: 1.02696121\n",
      "(7.79 min) Epoch 39/300 -- Iteration 37413 - Batch 819/963 - Train loss: 0.29867414  - Train acc: 0.8873 - Val loss: 1.02696121\n",
      "(7.79 min) Epoch 39/300 -- Iteration 37422 - Batch 828/963 - Train loss: 0.29895596  - Train acc: 0.8871 - Val loss: 1.02696121\n",
      "(7.79 min) Epoch 39/300 -- Iteration 37431 - Batch 837/963 - Train loss: 0.29903005  - Train acc: 0.8871 - Val loss: 1.02696121\n",
      "(7.79 min) Epoch 39/300 -- Iteration 37440 - Batch 846/963 - Train loss: 0.29928620  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.80 min) Epoch 39/300 -- Iteration 37449 - Batch 855/963 - Train loss: 0.29941041  - Train acc: 0.8868 - Val loss: 1.02696121\n",
      "(7.80 min) Epoch 39/300 -- Iteration 37458 - Batch 864/963 - Train loss: 0.29934716  - Train acc: 0.8869 - Val loss: 1.02696121\n",
      "(7.80 min) Epoch 39/300 -- Iteration 37467 - Batch 873/963 - Train loss: 0.29963970  - Train acc: 0.8867 - Val loss: 1.02696121\n",
      "(7.80 min) Epoch 39/300 -- Iteration 37476 - Batch 882/963 - Train loss: 0.29977783  - Train acc: 0.8866 - Val loss: 1.02696121\n",
      "(7.80 min) Epoch 39/300 -- Iteration 37485 - Batch 891/963 - Train loss: 0.29997309  - Train acc: 0.8865 - Val loss: 1.02696121\n",
      "(7.81 min) Epoch 39/300 -- Iteration 37494 - Batch 900/963 - Train loss: 0.29989701  - Train acc: 0.8865 - Val loss: 1.02696121\n",
      "(7.81 min) Epoch 39/300 -- Iteration 37503 - Batch 909/963 - Train loss: 0.30001635  - Train acc: 0.8865 - Val loss: 1.02696121\n",
      "(7.81 min) Epoch 39/300 -- Iteration 37512 - Batch 918/963 - Train loss: 0.29977848  - Train acc: 0.8866 - Val loss: 1.02696121\n",
      "(7.81 min) Epoch 39/300 -- Iteration 37521 - Batch 927/963 - Train loss: 0.29947183  - Train acc: 0.8867 - Val loss: 1.02696121\n",
      "(7.81 min) Epoch 39/300 -- Iteration 37530 - Batch 936/963 - Train loss: 0.29974817  - Train acc: 0.8866 - Val loss: 1.02696121\n",
      "(7.81 min) Epoch 39/300 -- Iteration 37539 - Batch 945/963 - Train loss: 0.30004721  - Train acc: 0.8864 - Val loss: 1.02696121\n",
      "(7.82 min) Epoch 39/300 -- Iteration 37548 - Batch 954/963 - Train loss: 0.30044037  - Train acc: 0.8863 - Val loss: 1.02696121\n",
      "(7.82 min) Epoch 39/300 -- Iteration 37557 - Batch 962/963 - Train loss: 0.30034551  - Train acc: 0.8864 - Val loss: 1.04304874 - Val acc: 0.5917\n",
      "(7.82 min) Epoch 40/300 -- Iteration 37566 - Batch 9/963 - Train loss: 0.31014838  - Train acc: 0.8828 - Val loss: 1.04304874\n",
      "(7.82 min) Epoch 40/300 -- Iteration 37575 - Batch 18/963 - Train loss: 0.29697328  - Train acc: 0.8882 - Val loss: 1.04304874\n",
      "(7.82 min) Epoch 40/300 -- Iteration 37584 - Batch 27/963 - Train loss: 0.29246911  - Train acc: 0.8892 - Val loss: 1.04304874\n",
      "(7.83 min) Epoch 40/300 -- Iteration 37593 - Batch 36/963 - Train loss: 0.28889028  - Train acc: 0.8904 - Val loss: 1.04304874\n",
      "(7.83 min) Epoch 40/300 -- Iteration 37602 - Batch 45/963 - Train loss: 0.29653736  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.83 min) Epoch 40/300 -- Iteration 37611 - Batch 54/963 - Train loss: 0.29833892  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.83 min) Epoch 40/300 -- Iteration 37620 - Batch 63/963 - Train loss: 0.29681338  - Train acc: 0.8875 - Val loss: 1.04304874\n",
      "(7.83 min) Epoch 40/300 -- Iteration 37629 - Batch 72/963 - Train loss: 0.29576118  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(7.84 min) Epoch 40/300 -- Iteration 37638 - Batch 81/963 - Train loss: 0.29557518  - Train acc: 0.8881 - Val loss: 1.04304874\n",
      "(7.84 min) Epoch 40/300 -- Iteration 37647 - Batch 90/963 - Train loss: 0.29721499  - Train acc: 0.8876 - Val loss: 1.04304874\n",
      "(7.84 min) Epoch 40/300 -- Iteration 37656 - Batch 99/963 - Train loss: 0.29636543  - Train acc: 0.8885 - Val loss: 1.04304874\n",
      "(7.84 min) Epoch 40/300 -- Iteration 37665 - Batch 108/963 - Train loss: 0.29585842  - Train acc: 0.8883 - Val loss: 1.04304874\n",
      "(7.84 min) Epoch 40/300 -- Iteration 37674 - Batch 117/963 - Train loss: 0.29509287  - Train acc: 0.8885 - Val loss: 1.04304874\n",
      "(7.85 min) Epoch 40/300 -- Iteration 37683 - Batch 126/963 - Train loss: 0.29483026  - Train acc: 0.8893 - Val loss: 1.04304874\n",
      "(7.85 min) Epoch 40/300 -- Iteration 37692 - Batch 135/963 - Train loss: 0.29440603  - Train acc: 0.8893 - Val loss: 1.04304874\n",
      "(7.85 min) Epoch 40/300 -- Iteration 37701 - Batch 144/963 - Train loss: 0.29613037  - Train acc: 0.8887 - Val loss: 1.04304874\n",
      "(7.85 min) Epoch 40/300 -- Iteration 37710 - Batch 153/963 - Train loss: 0.29670361  - Train acc: 0.8880 - Val loss: 1.04304874\n",
      "(7.85 min) Epoch 40/300 -- Iteration 37719 - Batch 162/963 - Train loss: 0.29688948  - Train acc: 0.8878 - Val loss: 1.04304874\n",
      "(7.86 min) Epoch 40/300 -- Iteration 37728 - Batch 171/963 - Train loss: 0.29729614  - Train acc: 0.8872 - Val loss: 1.04304874\n",
      "(7.86 min) Epoch 40/300 -- Iteration 37737 - Batch 180/963 - Train loss: 0.29693351  - Train acc: 0.8874 - Val loss: 1.04304874\n",
      "(7.86 min) Epoch 40/300 -- Iteration 37746 - Batch 189/963 - Train loss: 0.29646462  - Train acc: 0.8877 - Val loss: 1.04304874\n",
      "(7.86 min) Epoch 40/300 -- Iteration 37755 - Batch 198/963 - Train loss: 0.29726366  - Train acc: 0.8876 - Val loss: 1.04304874\n",
      "(7.86 min) Epoch 40/300 -- Iteration 37764 - Batch 207/963 - Train loss: 0.29585737  - Train acc: 0.8881 - Val loss: 1.04304874\n",
      "(7.87 min) Epoch 40/300 -- Iteration 37773 - Batch 216/963 - Train loss: 0.29616523  - Train acc: 0.8881 - Val loss: 1.04304874\n",
      "(7.87 min) Epoch 40/300 -- Iteration 37782 - Batch 225/963 - Train loss: 0.29634278  - Train acc: 0.8879 - Val loss: 1.04304874\n",
      "(7.87 min) Epoch 40/300 -- Iteration 37791 - Batch 234/963 - Train loss: 0.29692977  - Train acc: 0.8876 - Val loss: 1.04304874\n",
      "(7.87 min) Epoch 40/300 -- Iteration 37800 - Batch 243/963 - Train loss: 0.29707746  - Train acc: 0.8873 - Val loss: 1.04304874\n",
      "(7.87 min) Epoch 40/300 -- Iteration 37809 - Batch 252/963 - Train loss: 0.29654701  - Train acc: 0.8873 - Val loss: 1.04304874\n",
      "(7.87 min) Epoch 40/300 -- Iteration 37818 - Batch 261/963 - Train loss: 0.29670150  - Train acc: 0.8872 - Val loss: 1.04304874\n",
      "(7.88 min) Epoch 40/300 -- Iteration 37827 - Batch 270/963 - Train loss: 0.29671661  - Train acc: 0.8871 - Val loss: 1.04304874\n",
      "(7.88 min) Epoch 40/300 -- Iteration 37836 - Batch 279/963 - Train loss: 0.29684063  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.88 min) Epoch 40/300 -- Iteration 37845 - Batch 288/963 - Train loss: 0.29726575  - Train acc: 0.8867 - Val loss: 1.04304874\n",
      "(7.88 min) Epoch 40/300 -- Iteration 37854 - Batch 297/963 - Train loss: 0.29708218  - Train acc: 0.8867 - Val loss: 1.04304874\n",
      "(7.88 min) Epoch 40/300 -- Iteration 37863 - Batch 306/963 - Train loss: 0.29727157  - Train acc: 0.8865 - Val loss: 1.04304874\n",
      "(7.89 min) Epoch 40/300 -- Iteration 37872 - Batch 315/963 - Train loss: 0.29820803  - Train acc: 0.8860 - Val loss: 1.04304874\n",
      "(7.89 min) Epoch 40/300 -- Iteration 37881 - Batch 324/963 - Train loss: 0.29875009  - Train acc: 0.8860 - Val loss: 1.04304874\n",
      "(7.89 min) Epoch 40/300 -- Iteration 37890 - Batch 333/963 - Train loss: 0.29868066  - Train acc: 0.8861 - Val loss: 1.04304874\n",
      "(7.89 min) Epoch 40/300 -- Iteration 37899 - Batch 342/963 - Train loss: 0.29879964  - Train acc: 0.8858 - Val loss: 1.04304874\n",
      "(7.89 min) Epoch 40/300 -- Iteration 37908 - Batch 351/963 - Train loss: 0.29863067  - Train acc: 0.8860 - Val loss: 1.04304874\n",
      "(7.90 min) Epoch 40/300 -- Iteration 37917 - Batch 360/963 - Train loss: 0.29762995  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.90 min) Epoch 40/300 -- Iteration 37926 - Batch 369/963 - Train loss: 0.29727418  - Train acc: 0.8866 - Val loss: 1.04304874\n",
      "(7.90 min) Epoch 40/300 -- Iteration 37935 - Batch 378/963 - Train loss: 0.29726666  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.90 min) Epoch 40/300 -- Iteration 37944 - Batch 387/963 - Train loss: 0.29768882  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.90 min) Epoch 40/300 -- Iteration 37953 - Batch 396/963 - Train loss: 0.29730038  - Train acc: 0.8867 - Val loss: 1.04304874\n",
      "(7.90 min) Epoch 40/300 -- Iteration 37962 - Batch 405/963 - Train loss: 0.29731862  - Train acc: 0.8867 - Val loss: 1.04304874\n",
      "(7.91 min) Epoch 40/300 -- Iteration 37971 - Batch 414/963 - Train loss: 0.29682531  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.91 min) Epoch 40/300 -- Iteration 37980 - Batch 423/963 - Train loss: 0.29751094  - Train acc: 0.8868 - Val loss: 1.04304874\n",
      "(7.91 min) Epoch 40/300 -- Iteration 37989 - Batch 432/963 - Train loss: 0.29760773  - Train acc: 0.8867 - Val loss: 1.04304874\n",
      "(7.91 min) Epoch 40/300 -- Iteration 37998 - Batch 441/963 - Train loss: 0.29672413  - Train acc: 0.8872 - Val loss: 1.04304874\n",
      "(7.91 min) Epoch 40/300 -- Iteration 38007 - Batch 450/963 - Train loss: 0.29683458  - Train acc: 0.8872 - Val loss: 1.04304874\n",
      "(7.92 min) Epoch 40/300 -- Iteration 38016 - Batch 459/963 - Train loss: 0.29769203  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.92 min) Epoch 40/300 -- Iteration 38025 - Batch 468/963 - Train loss: 0.29700810  - Train acc: 0.8871 - Val loss: 1.04304874\n",
      "(7.92 min) Epoch 40/300 -- Iteration 38034 - Batch 477/963 - Train loss: 0.29768757  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(7.92 min) Epoch 40/300 -- Iteration 38043 - Batch 486/963 - Train loss: 0.29767470  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(7.92 min) Epoch 40/300 -- Iteration 38052 - Batch 495/963 - Train loss: 0.29782969  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.93 min) Epoch 40/300 -- Iteration 38061 - Batch 504/963 - Train loss: 0.29759822  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(7.93 min) Epoch 40/300 -- Iteration 38070 - Batch 513/963 - Train loss: 0.29790822  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.93 min) Epoch 40/300 -- Iteration 38079 - Batch 522/963 - Train loss: 0.29832536  - Train acc: 0.8868 - Val loss: 1.04304874\n",
      "(7.93 min) Epoch 40/300 -- Iteration 38088 - Batch 531/963 - Train loss: 0.29841674  - Train acc: 0.8867 - Val loss: 1.04304874\n",
      "(7.93 min) Epoch 40/300 -- Iteration 38097 - Batch 540/963 - Train loss: 0.29865226  - Train acc: 0.8867 - Val loss: 1.04304874\n",
      "(7.93 min) Epoch 40/300 -- Iteration 38106 - Batch 549/963 - Train loss: 0.29850953  - Train acc: 0.8867 - Val loss: 1.04304874\n",
      "(7.94 min) Epoch 40/300 -- Iteration 38115 - Batch 558/963 - Train loss: 0.29905417  - Train acc: 0.8866 - Val loss: 1.04304874\n",
      "(7.94 min) Epoch 40/300 -- Iteration 38124 - Batch 567/963 - Train loss: 0.29928848  - Train acc: 0.8866 - Val loss: 1.04304874\n",
      "(7.94 min) Epoch 40/300 -- Iteration 38133 - Batch 576/963 - Train loss: 0.29926498  - Train acc: 0.8865 - Val loss: 1.04304874\n",
      "(7.94 min) Epoch 40/300 -- Iteration 38142 - Batch 585/963 - Train loss: 0.29962683  - Train acc: 0.8863 - Val loss: 1.04304874\n",
      "(7.94 min) Epoch 40/300 -- Iteration 38151 - Batch 594/963 - Train loss: 0.29962144  - Train acc: 0.8862 - Val loss: 1.04304874\n",
      "(7.95 min) Epoch 40/300 -- Iteration 38160 - Batch 603/963 - Train loss: 0.29938019  - Train acc: 0.8863 - Val loss: 1.04304874\n",
      "(7.95 min) Epoch 40/300 -- Iteration 38169 - Batch 612/963 - Train loss: 0.29903607  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.95 min) Epoch 40/300 -- Iteration 38178 - Batch 621/963 - Train loss: 0.29928085  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.95 min) Epoch 40/300 -- Iteration 38187 - Batch 630/963 - Train loss: 0.29889525  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.95 min) Epoch 40/300 -- Iteration 38196 - Batch 639/963 - Train loss: 0.29888656  - Train acc: 0.8865 - Val loss: 1.04304874\n",
      "(7.96 min) Epoch 40/300 -- Iteration 38205 - Batch 648/963 - Train loss: 0.29891666  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.96 min) Epoch 40/300 -- Iteration 38214 - Batch 657/963 - Train loss: 0.29902627  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.96 min) Epoch 40/300 -- Iteration 38223 - Batch 666/963 - Train loss: 0.29921404  - Train acc: 0.8862 - Val loss: 1.04304874\n",
      "(7.96 min) Epoch 40/300 -- Iteration 38232 - Batch 675/963 - Train loss: 0.29924099  - Train acc: 0.8863 - Val loss: 1.04304874\n",
      "(7.96 min) Epoch 40/300 -- Iteration 38241 - Batch 684/963 - Train loss: 0.29939296  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.97 min) Epoch 40/300 -- Iteration 38250 - Batch 693/963 - Train loss: 0.29924321  - Train acc: 0.8864 - Val loss: 1.04304874\n",
      "(7.97 min) Epoch 40/300 -- Iteration 38259 - Batch 702/963 - Train loss: 0.29927231  - Train acc: 0.8865 - Val loss: 1.04304874\n",
      "(7.97 min) Epoch 40/300 -- Iteration 38268 - Batch 711/963 - Train loss: 0.29865607  - Train acc: 0.8868 - Val loss: 1.04304874\n",
      "(7.97 min) Epoch 40/300 -- Iteration 38277 - Batch 720/963 - Train loss: 0.29856265  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.97 min) Epoch 40/300 -- Iteration 38286 - Batch 729/963 - Train loss: 0.29878421  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.98 min) Epoch 40/300 -- Iteration 38295 - Batch 738/963 - Train loss: 0.29893137  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.98 min) Epoch 40/300 -- Iteration 38304 - Batch 747/963 - Train loss: 0.29891793  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(7.98 min) Epoch 40/300 -- Iteration 38313 - Batch 756/963 - Train loss: 0.29903247  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.98 min) Epoch 40/300 -- Iteration 38322 - Batch 765/963 - Train loss: 0.29894164  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.98 min) Epoch 40/300 -- Iteration 38331 - Batch 774/963 - Train loss: 0.29897529  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(7.99 min) Epoch 40/300 -- Iteration 38340 - Batch 783/963 - Train loss: 0.29896942  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(7.99 min) Epoch 40/300 -- Iteration 38349 - Batch 792/963 - Train loss: 0.29889426  - Train acc: 0.8871 - Val loss: 1.04304874\n",
      "(7.99 min) Epoch 40/300 -- Iteration 38358 - Batch 801/963 - Train loss: 0.29881489  - Train acc: 0.8871 - Val loss: 1.04304874\n",
      "(7.99 min) Epoch 40/300 -- Iteration 38367 - Batch 810/963 - Train loss: 0.29878132  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(7.99 min) Epoch 40/300 -- Iteration 38376 - Batch 819/963 - Train loss: 0.29893896  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(8.00 min) Epoch 40/300 -- Iteration 38385 - Batch 828/963 - Train loss: 0.29867621  - Train acc: 0.8871 - Val loss: 1.04304874\n",
      "(8.00 min) Epoch 40/300 -- Iteration 38394 - Batch 837/963 - Train loss: 0.29859556  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(8.00 min) Epoch 40/300 -- Iteration 38403 - Batch 846/963 - Train loss: 0.29877876  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(8.00 min) Epoch 40/300 -- Iteration 38412 - Batch 855/963 - Train loss: 0.29889196  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(8.00 min) Epoch 40/300 -- Iteration 38421 - Batch 864/963 - Train loss: 0.29866550  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(8.01 min) Epoch 40/300 -- Iteration 38430 - Batch 873/963 - Train loss: 0.29849825  - Train acc: 0.8870 - Val loss: 1.04304874\n",
      "(8.01 min) Epoch 40/300 -- Iteration 38439 - Batch 882/963 - Train loss: 0.29836966  - Train acc: 0.8871 - Val loss: 1.04304874\n",
      "(8.01 min) Epoch 40/300 -- Iteration 38448 - Batch 891/963 - Train loss: 0.29812738  - Train acc: 0.8871 - Val loss: 1.04304874\n",
      "(8.01 min) Epoch 40/300 -- Iteration 38457 - Batch 900/963 - Train loss: 0.29807800  - Train acc: 0.8872 - Val loss: 1.04304874\n",
      "(8.01 min) Epoch 40/300 -- Iteration 38466 - Batch 909/963 - Train loss: 0.29807469  - Train acc: 0.8872 - Val loss: 1.04304874\n",
      "(8.01 min) Epoch 40/300 -- Iteration 38475 - Batch 918/963 - Train loss: 0.29828358  - Train acc: 0.8871 - Val loss: 1.04304874\n",
      "(8.02 min) Epoch 40/300 -- Iteration 38484 - Batch 927/963 - Train loss: 0.29827807  - Train acc: 0.8872 - Val loss: 1.04304874\n",
      "(8.02 min) Epoch 40/300 -- Iteration 38493 - Batch 936/963 - Train loss: 0.29815890  - Train acc: 0.8872 - Val loss: 1.04304874\n",
      "(8.02 min) Epoch 40/300 -- Iteration 38502 - Batch 945/963 - Train loss: 0.29812431  - Train acc: 0.8871 - Val loss: 1.04304874\n",
      "(8.02 min) Epoch 40/300 -- Iteration 38511 - Batch 954/963 - Train loss: 0.29820008  - Train acc: 0.8869 - Val loss: 1.04304874\n",
      "(8.03 min) Epoch 40/300 -- Iteration 38520 - Batch 962/963 - Train loss: 0.29801310  - Train acc: 0.8870 - Val loss: 1.01195407 - Val acc: 0.5950\n",
      "(8.03 min) Epoch 41/300 -- Iteration 38529 - Batch 9/963 - Train loss: 0.28132372  - Train acc: 0.9008 - Val loss: 1.01195407\n",
      "(8.03 min) Epoch 41/300 -- Iteration 38538 - Batch 18/963 - Train loss: 0.28255239  - Train acc: 0.8980 - Val loss: 1.01195407\n",
      "(8.03 min) Epoch 41/300 -- Iteration 38547 - Batch 27/963 - Train loss: 0.29740220  - Train acc: 0.8909 - Val loss: 1.01195407\n",
      "(8.03 min) Epoch 41/300 -- Iteration 38556 - Batch 36/963 - Train loss: 0.30353638  - Train acc: 0.8877 - Val loss: 1.01195407\n",
      "(8.04 min) Epoch 41/300 -- Iteration 38565 - Batch 45/963 - Train loss: 0.30505644  - Train acc: 0.8860 - Val loss: 1.01195407\n",
      "(8.04 min) Epoch 41/300 -- Iteration 38574 - Batch 54/963 - Train loss: 0.30708928  - Train acc: 0.8854 - Val loss: 1.01195407\n",
      "(8.04 min) Epoch 41/300 -- Iteration 38583 - Batch 63/963 - Train loss: 0.30431370  - Train acc: 0.8850 - Val loss: 1.01195407\n",
      "(8.04 min) Epoch 41/300 -- Iteration 38592 - Batch 72/963 - Train loss: 0.30383290  - Train acc: 0.8853 - Val loss: 1.01195407\n",
      "(8.04 min) Epoch 41/300 -- Iteration 38601 - Batch 81/963 - Train loss: 0.30080787  - Train acc: 0.8874 - Val loss: 1.01195407\n",
      "(8.05 min) Epoch 41/300 -- Iteration 38610 - Batch 90/963 - Train loss: 0.30176750  - Train acc: 0.8875 - Val loss: 1.01195407\n",
      "(8.05 min) Epoch 41/300 -- Iteration 38619 - Batch 99/963 - Train loss: 0.30131378  - Train acc: 0.8870 - Val loss: 1.01195407\n",
      "(8.05 min) Epoch 41/300 -- Iteration 38628 - Batch 108/963 - Train loss: 0.30165481  - Train acc: 0.8868 - Val loss: 1.01195407\n",
      "(8.05 min) Epoch 41/300 -- Iteration 38637 - Batch 117/963 - Train loss: 0.30192315  - Train acc: 0.8873 - Val loss: 1.01195407\n",
      "(8.05 min) Epoch 41/300 -- Iteration 38646 - Batch 126/963 - Train loss: 0.30520862  - Train acc: 0.8855 - Val loss: 1.01195407\n",
      "(8.05 min) Epoch 41/300 -- Iteration 38655 - Batch 135/963 - Train loss: 0.30572628  - Train acc: 0.8853 - Val loss: 1.01195407\n",
      "(8.06 min) Epoch 41/300 -- Iteration 38664 - Batch 144/963 - Train loss: 0.30233559  - Train acc: 0.8864 - Val loss: 1.01195407\n",
      "(8.06 min) Epoch 41/300 -- Iteration 38673 - Batch 153/963 - Train loss: 0.30176377  - Train acc: 0.8863 - Val loss: 1.01195407\n",
      "(8.06 min) Epoch 41/300 -- Iteration 38682 - Batch 162/963 - Train loss: 0.29995714  - Train acc: 0.8869 - Val loss: 1.01195407\n",
      "(8.06 min) Epoch 41/300 -- Iteration 38691 - Batch 171/963 - Train loss: 0.30021086  - Train acc: 0.8870 - Val loss: 1.01195407\n",
      "(8.06 min) Epoch 41/300 -- Iteration 38700 - Batch 180/963 - Train loss: 0.29996810  - Train acc: 0.8870 - Val loss: 1.01195407\n",
      "(8.07 min) Epoch 41/300 -- Iteration 38709 - Batch 189/963 - Train loss: 0.29942679  - Train acc: 0.8873 - Val loss: 1.01195407\n",
      "(8.07 min) Epoch 41/300 -- Iteration 38718 - Batch 198/963 - Train loss: 0.29942330  - Train acc: 0.8874 - Val loss: 1.01195407\n",
      "(8.07 min) Epoch 41/300 -- Iteration 38727 - Batch 207/963 - Train loss: 0.30031811  - Train acc: 0.8871 - Val loss: 1.01195407\n",
      "(8.07 min) Epoch 41/300 -- Iteration 38736 - Batch 216/963 - Train loss: 0.29943746  - Train acc: 0.8875 - Val loss: 1.01195407\n",
      "(8.07 min) Epoch 41/300 -- Iteration 38745 - Batch 225/963 - Train loss: 0.30004699  - Train acc: 0.8873 - Val loss: 1.01195407\n",
      "(8.08 min) Epoch 41/300 -- Iteration 38754 - Batch 234/963 - Train loss: 0.29970594  - Train acc: 0.8874 - Val loss: 1.01195407\n",
      "(8.08 min) Epoch 41/300 -- Iteration 38763 - Batch 243/963 - Train loss: 0.30025160  - Train acc: 0.8872 - Val loss: 1.01195407\n",
      "(8.08 min) Epoch 41/300 -- Iteration 38772 - Batch 252/963 - Train loss: 0.30004394  - Train acc: 0.8873 - Val loss: 1.01195407\n",
      "(8.08 min) Epoch 41/300 -- Iteration 38781 - Batch 261/963 - Train loss: 0.29970787  - Train acc: 0.8877 - Val loss: 1.01195407\n",
      "(8.08 min) Epoch 41/300 -- Iteration 38790 - Batch 270/963 - Train loss: 0.29901390  - Train acc: 0.8879 - Val loss: 1.01195407\n",
      "(8.09 min) Epoch 41/300 -- Iteration 38799 - Batch 279/963 - Train loss: 0.29941411  - Train acc: 0.8879 - Val loss: 1.01195407\n",
      "(8.09 min) Epoch 41/300 -- Iteration 38808 - Batch 288/963 - Train loss: 0.30016161  - Train acc: 0.8872 - Val loss: 1.01195407\n",
      "(8.09 min) Epoch 41/300 -- Iteration 38817 - Batch 297/963 - Train loss: 0.30033766  - Train acc: 0.8870 - Val loss: 1.01195407\n",
      "(8.09 min) Epoch 41/300 -- Iteration 38826 - Batch 306/963 - Train loss: 0.30046658  - Train acc: 0.8872 - Val loss: 1.01195407\n",
      "(8.09 min) Epoch 41/300 -- Iteration 38835 - Batch 315/963 - Train loss: 0.30027464  - Train acc: 0.8873 - Val loss: 1.01195407\n",
      "(8.10 min) Epoch 41/300 -- Iteration 38844 - Batch 324/963 - Train loss: 0.29988606  - Train acc: 0.8874 - Val loss: 1.01195407\n",
      "(8.10 min) Epoch 41/300 -- Iteration 38853 - Batch 333/963 - Train loss: 0.29915256  - Train acc: 0.8874 - Val loss: 1.01195407\n",
      "(8.10 min) Epoch 41/300 -- Iteration 38862 - Batch 342/963 - Train loss: 0.29925427  - Train acc: 0.8873 - Val loss: 1.01195407\n",
      "(8.10 min) Epoch 41/300 -- Iteration 38871 - Batch 351/963 - Train loss: 0.29892419  - Train acc: 0.8875 - Val loss: 1.01195407\n",
      "(8.10 min) Epoch 41/300 -- Iteration 38880 - Batch 360/963 - Train loss: 0.29922490  - Train acc: 0.8871 - Val loss: 1.01195407\n",
      "(8.11 min) Epoch 41/300 -- Iteration 38889 - Batch 369/963 - Train loss: 0.29905870  - Train acc: 0.8870 - Val loss: 1.01195407\n",
      "(8.11 min) Epoch 41/300 -- Iteration 38898 - Batch 378/963 - Train loss: 0.29891369  - Train acc: 0.8869 - Val loss: 1.01195407\n",
      "(8.11 min) Epoch 41/300 -- Iteration 38907 - Batch 387/963 - Train loss: 0.29829285  - Train acc: 0.8869 - Val loss: 1.01195407\n",
      "(8.11 min) Epoch 41/300 -- Iteration 38916 - Batch 396/963 - Train loss: 0.29843723  - Train acc: 0.8868 - Val loss: 1.01195407\n",
      "(8.11 min) Epoch 41/300 -- Iteration 38925 - Batch 405/963 - Train loss: 0.29845822  - Train acc: 0.8869 - Val loss: 1.01195407\n",
      "(8.11 min) Epoch 41/300 -- Iteration 38934 - Batch 414/963 - Train loss: 0.29808683  - Train acc: 0.8869 - Val loss: 1.01195407\n",
      "(8.12 min) Epoch 41/300 -- Iteration 38943 - Batch 423/963 - Train loss: 0.29853299  - Train acc: 0.8866 - Val loss: 1.01195407\n",
      "(8.12 min) Epoch 41/300 -- Iteration 38952 - Batch 432/963 - Train loss: 0.29948936  - Train acc: 0.8862 - Val loss: 1.01195407\n",
      "(8.12 min) Epoch 41/300 -- Iteration 38961 - Batch 441/963 - Train loss: 0.29945048  - Train acc: 0.8860 - Val loss: 1.01195407\n",
      "(8.12 min) Epoch 41/300 -- Iteration 38970 - Batch 450/963 - Train loss: 0.29906614  - Train acc: 0.8863 - Val loss: 1.01195407\n",
      "(8.12 min) Epoch 41/300 -- Iteration 38979 - Batch 459/963 - Train loss: 0.29929779  - Train acc: 0.8863 - Val loss: 1.01195407\n",
      "(8.13 min) Epoch 41/300 -- Iteration 38988 - Batch 468/963 - Train loss: 0.29953462  - Train acc: 0.8862 - Val loss: 1.01195407\n",
      "(8.13 min) Epoch 41/300 -- Iteration 38997 - Batch 477/963 - Train loss: 0.29942256  - Train acc: 0.8864 - Val loss: 1.01195407\n",
      "(8.13 min) Epoch 41/300 -- Iteration 39006 - Batch 486/963 - Train loss: 0.29924790  - Train acc: 0.8864 - Val loss: 1.01195407\n",
      "(8.13 min) Epoch 41/300 -- Iteration 39015 - Batch 495/963 - Train loss: 0.29942862  - Train acc: 0.8862 - Val loss: 1.01195407\n",
      "(8.13 min) Epoch 41/300 -- Iteration 39024 - Batch 504/963 - Train loss: 0.29931302  - Train acc: 0.8862 - Val loss: 1.01195407\n",
      "(8.14 min) Epoch 41/300 -- Iteration 39033 - Batch 513/963 - Train loss: 0.29936584  - Train acc: 0.8861 - Val loss: 1.01195407\n",
      "(8.14 min) Epoch 41/300 -- Iteration 39042 - Batch 522/963 - Train loss: 0.29944581  - Train acc: 0.8861 - Val loss: 1.01195407\n",
      "(8.14 min) Epoch 41/300 -- Iteration 39051 - Batch 531/963 - Train loss: 0.30013072  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.14 min) Epoch 41/300 -- Iteration 39060 - Batch 540/963 - Train loss: 0.30038270  - Train acc: 0.8857 - Val loss: 1.01195407\n",
      "(8.14 min) Epoch 41/300 -- Iteration 39069 - Batch 549/963 - Train loss: 0.30031404  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.14 min) Epoch 41/300 -- Iteration 39078 - Batch 558/963 - Train loss: 0.29971482  - Train acc: 0.8861 - Val loss: 1.01195407\n",
      "(8.15 min) Epoch 41/300 -- Iteration 39087 - Batch 567/963 - Train loss: 0.29993753  - Train acc: 0.8860 - Val loss: 1.01195407\n",
      "(8.15 min) Epoch 41/300 -- Iteration 39096 - Batch 576/963 - Train loss: 0.29940577  - Train acc: 0.8863 - Val loss: 1.01195407\n",
      "(8.15 min) Epoch 41/300 -- Iteration 39105 - Batch 585/963 - Train loss: 0.29944143  - Train acc: 0.8863 - Val loss: 1.01195407\n",
      "(8.15 min) Epoch 41/300 -- Iteration 39114 - Batch 594/963 - Train loss: 0.29963302  - Train acc: 0.8860 - Val loss: 1.01195407\n",
      "(8.15 min) Epoch 41/300 -- Iteration 39123 - Batch 603/963 - Train loss: 0.29960505  - Train acc: 0.8861 - Val loss: 1.01195407\n",
      "(8.16 min) Epoch 41/300 -- Iteration 39132 - Batch 612/963 - Train loss: 0.29983584  - Train acc: 0.8860 - Val loss: 1.01195407\n",
      "(8.16 min) Epoch 41/300 -- Iteration 39141 - Batch 621/963 - Train loss: 0.29944855  - Train acc: 0.8862 - Val loss: 1.01195407\n",
      "(8.16 min) Epoch 41/300 -- Iteration 39150 - Batch 630/963 - Train loss: 0.29981262  - Train acc: 0.8860 - Val loss: 1.01195407\n",
      "(8.16 min) Epoch 41/300 -- Iteration 39159 - Batch 639/963 - Train loss: 0.29945949  - Train acc: 0.8860 - Val loss: 1.01195407\n",
      "(8.16 min) Epoch 41/300 -- Iteration 39168 - Batch 648/963 - Train loss: 0.29971214  - Train acc: 0.8859 - Val loss: 1.01195407\n",
      "(8.17 min) Epoch 41/300 -- Iteration 39177 - Batch 657/963 - Train loss: 0.30008639  - Train acc: 0.8856 - Val loss: 1.01195407\n",
      "(8.17 min) Epoch 41/300 -- Iteration 39186 - Batch 666/963 - Train loss: 0.29993135  - Train acc: 0.8857 - Val loss: 1.01195407\n",
      "(8.17 min) Epoch 41/300 -- Iteration 39195 - Batch 675/963 - Train loss: 0.29967674  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.17 min) Epoch 41/300 -- Iteration 39204 - Batch 684/963 - Train loss: 0.29950999  - Train acc: 0.8860 - Val loss: 1.01195407\n",
      "(8.17 min) Epoch 41/300 -- Iteration 39213 - Batch 693/963 - Train loss: 0.29967640  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.17 min) Epoch 41/300 -- Iteration 39222 - Batch 702/963 - Train loss: 0.29941031  - Train acc: 0.8859 - Val loss: 1.01195407\n",
      "(8.18 min) Epoch 41/300 -- Iteration 39231 - Batch 711/963 - Train loss: 0.29968518  - Train acc: 0.8859 - Val loss: 1.01195407\n",
      "(8.18 min) Epoch 41/300 -- Iteration 39240 - Batch 720/963 - Train loss: 0.29969931  - Train acc: 0.8859 - Val loss: 1.01195407\n",
      "(8.18 min) Epoch 41/300 -- Iteration 39249 - Batch 729/963 - Train loss: 0.29985806  - Train acc: 0.8859 - Val loss: 1.01195407\n",
      "(8.18 min) Epoch 41/300 -- Iteration 39258 - Batch 738/963 - Train loss: 0.30003275  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.18 min) Epoch 41/300 -- Iteration 39267 - Batch 747/963 - Train loss: 0.29993538  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.19 min) Epoch 41/300 -- Iteration 39276 - Batch 756/963 - Train loss: 0.30022881  - Train acc: 0.8857 - Val loss: 1.01195407\n",
      "(8.19 min) Epoch 41/300 -- Iteration 39285 - Batch 765/963 - Train loss: 0.29998205  - Train acc: 0.8857 - Val loss: 1.01195407\n",
      "(8.19 min) Epoch 41/300 -- Iteration 39294 - Batch 774/963 - Train loss: 0.29981828  - Train acc: 0.8857 - Val loss: 1.01195407\n",
      "(8.19 min) Epoch 41/300 -- Iteration 39303 - Batch 783/963 - Train loss: 0.29975552  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.19 min) Epoch 41/300 -- Iteration 39312 - Batch 792/963 - Train loss: 0.29976438  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.20 min) Epoch 41/300 -- Iteration 39321 - Batch 801/963 - Train loss: 0.29948896  - Train acc: 0.8861 - Val loss: 1.01195407\n",
      "(8.20 min) Epoch 41/300 -- Iteration 39330 - Batch 810/963 - Train loss: 0.30010889  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.20 min) Epoch 41/300 -- Iteration 39339 - Batch 819/963 - Train loss: 0.30027141  - Train acc: 0.8858 - Val loss: 1.01195407\n",
      "(8.20 min) Epoch 41/300 -- Iteration 39348 - Batch 828/963 - Train loss: 0.30003374  - Train acc: 0.8860 - Val loss: 1.01195407\n",
      "(8.20 min) Epoch 41/300 -- Iteration 39357 - Batch 837/963 - Train loss: 0.29964527  - Train acc: 0.8861 - Val loss: 1.01195407\n",
      "(8.20 min) Epoch 41/300 -- Iteration 39366 - Batch 846/963 - Train loss: 0.29952324  - Train acc: 0.8862 - Val loss: 1.01195407\n",
      "(8.21 min) Epoch 41/300 -- Iteration 39375 - Batch 855/963 - Train loss: 0.29899712  - Train acc: 0.8864 - Val loss: 1.01195407\n",
      "(8.21 min) Epoch 41/300 -- Iteration 39384 - Batch 864/963 - Train loss: 0.29889035  - Train acc: 0.8864 - Val loss: 1.01195407\n",
      "(8.21 min) Epoch 41/300 -- Iteration 39393 - Batch 873/963 - Train loss: 0.29869292  - Train acc: 0.8865 - Val loss: 1.01195407\n",
      "(8.21 min) Epoch 41/300 -- Iteration 39402 - Batch 882/963 - Train loss: 0.29887721  - Train acc: 0.8864 - Val loss: 1.01195407\n",
      "(8.21 min) Epoch 41/300 -- Iteration 39411 - Batch 891/963 - Train loss: 0.29883515  - Train acc: 0.8865 - Val loss: 1.01195407\n",
      "(8.22 min) Epoch 41/300 -- Iteration 39420 - Batch 900/963 - Train loss: 0.29893049  - Train acc: 0.8863 - Val loss: 1.01195407\n",
      "(8.22 min) Epoch 41/300 -- Iteration 39429 - Batch 909/963 - Train loss: 0.29893848  - Train acc: 0.8863 - Val loss: 1.01195407\n",
      "(8.22 min) Epoch 41/300 -- Iteration 39438 - Batch 918/963 - Train loss: 0.29915429  - Train acc: 0.8863 - Val loss: 1.01195407\n",
      "(8.22 min) Epoch 41/300 -- Iteration 39447 - Batch 927/963 - Train loss: 0.29902712  - Train acc: 0.8863 - Val loss: 1.01195407\n",
      "(8.22 min) Epoch 41/300 -- Iteration 39456 - Batch 936/963 - Train loss: 0.29891114  - Train acc: 0.8864 - Val loss: 1.01195407\n",
      "(8.22 min) Epoch 41/300 -- Iteration 39465 - Batch 945/963 - Train loss: 0.29871602  - Train acc: 0.8864 - Val loss: 1.01195407\n",
      "(8.23 min) Epoch 41/300 -- Iteration 39474 - Batch 954/963 - Train loss: 0.29844573  - Train acc: 0.8865 - Val loss: 1.01195407\n",
      "(8.23 min) Epoch 41/300 -- Iteration 39483 - Batch 962/963 - Train loss: 0.29840246  - Train acc: 0.8865 - Val loss: 1.01041305 - Val acc: 0.5950\n",
      "(8.23 min) Epoch 42/300 -- Iteration 39492 - Batch 9/963 - Train loss: 0.30410621  - Train acc: 0.8867 - Val loss: 1.01041305\n",
      "(8.23 min) Epoch 42/300 -- Iteration 39501 - Batch 18/963 - Train loss: 0.29680345  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.23 min) Epoch 42/300 -- Iteration 39510 - Batch 27/963 - Train loss: 0.28631891  - Train acc: 0.8920 - Val loss: 1.01041305\n",
      "(8.24 min) Epoch 42/300 -- Iteration 39519 - Batch 36/963 - Train loss: 0.28778383  - Train acc: 0.8910 - Val loss: 1.01041305\n",
      "(8.24 min) Epoch 42/300 -- Iteration 39528 - Batch 45/963 - Train loss: 0.29452700  - Train acc: 0.8876 - Val loss: 1.01041305\n",
      "(8.24 min) Epoch 42/300 -- Iteration 39537 - Batch 54/963 - Train loss: 0.29325674  - Train acc: 0.8888 - Val loss: 1.01041305\n",
      "(8.24 min) Epoch 42/300 -- Iteration 39546 - Batch 63/963 - Train loss: 0.29651431  - Train acc: 0.8867 - Val loss: 1.01041305\n",
      "(8.24 min) Epoch 42/300 -- Iteration 39555 - Batch 72/963 - Train loss: 0.29707916  - Train acc: 0.8870 - Val loss: 1.01041305\n",
      "(8.25 min) Epoch 42/300 -- Iteration 39564 - Batch 81/963 - Train loss: 0.29682337  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.25 min) Epoch 42/300 -- Iteration 39573 - Batch 90/963 - Train loss: 0.29462502  - Train acc: 0.8895 - Val loss: 1.01041305\n",
      "(8.25 min) Epoch 42/300 -- Iteration 39582 - Batch 99/963 - Train loss: 0.29411184  - Train acc: 0.8888 - Val loss: 1.01041305\n",
      "(8.25 min) Epoch 42/300 -- Iteration 39591 - Batch 108/963 - Train loss: 0.29200413  - Train acc: 0.8899 - Val loss: 1.01041305\n",
      "(8.25 min) Epoch 42/300 -- Iteration 39600 - Batch 117/963 - Train loss: 0.29222688  - Train acc: 0.8899 - Val loss: 1.01041305\n",
      "(8.26 min) Epoch 42/300 -- Iteration 39609 - Batch 126/963 - Train loss: 0.29397518  - Train acc: 0.8891 - Val loss: 1.01041305\n",
      "(8.26 min) Epoch 42/300 -- Iteration 39618 - Batch 135/963 - Train loss: 0.29329442  - Train acc: 0.8901 - Val loss: 1.01041305\n",
      "(8.26 min) Epoch 42/300 -- Iteration 39627 - Batch 144/963 - Train loss: 0.29149699  - Train acc: 0.8905 - Val loss: 1.01041305\n",
      "(8.26 min) Epoch 42/300 -- Iteration 39636 - Batch 153/963 - Train loss: 0.29089098  - Train acc: 0.8908 - Val loss: 1.01041305\n",
      "(8.26 min) Epoch 42/300 -- Iteration 39645 - Batch 162/963 - Train loss: 0.29343247  - Train acc: 0.8894 - Val loss: 1.01041305\n",
      "(8.27 min) Epoch 42/300 -- Iteration 39654 - Batch 171/963 - Train loss: 0.29188015  - Train acc: 0.8899 - Val loss: 1.01041305\n",
      "(8.27 min) Epoch 42/300 -- Iteration 39663 - Batch 180/963 - Train loss: 0.29263341  - Train acc: 0.8898 - Val loss: 1.01041305\n",
      "(8.27 min) Epoch 42/300 -- Iteration 39672 - Batch 189/963 - Train loss: 0.29276423  - Train acc: 0.8894 - Val loss: 1.01041305\n",
      "(8.27 min) Epoch 42/300 -- Iteration 39681 - Batch 198/963 - Train loss: 0.29276166  - Train acc: 0.8894 - Val loss: 1.01041305\n",
      "(8.27 min) Epoch 42/300 -- Iteration 39690 - Batch 207/963 - Train loss: 0.29337814  - Train acc: 0.8888 - Val loss: 1.01041305\n",
      "(8.27 min) Epoch 42/300 -- Iteration 39699 - Batch 216/963 - Train loss: 0.29437258  - Train acc: 0.8889 - Val loss: 1.01041305\n",
      "(8.28 min) Epoch 42/300 -- Iteration 39708 - Batch 225/963 - Train loss: 0.29297308  - Train acc: 0.8893 - Val loss: 1.01041305\n",
      "(8.28 min) Epoch 42/300 -- Iteration 39717 - Batch 234/963 - Train loss: 0.29326259  - Train acc: 0.8891 - Val loss: 1.01041305\n",
      "(8.28 min) Epoch 42/300 -- Iteration 39726 - Batch 243/963 - Train loss: 0.29347018  - Train acc: 0.8888 - Val loss: 1.01041305\n",
      "(8.28 min) Epoch 42/300 -- Iteration 39735 - Batch 252/963 - Train loss: 0.29369155  - Train acc: 0.8886 - Val loss: 1.01041305\n",
      "(8.28 min) Epoch 42/300 -- Iteration 39744 - Batch 261/963 - Train loss: 0.29268221  - Train acc: 0.8890 - Val loss: 1.01041305\n",
      "(8.29 min) Epoch 42/300 -- Iteration 39753 - Batch 270/963 - Train loss: 0.29287437  - Train acc: 0.8890 - Val loss: 1.01041305\n",
      "(8.29 min) Epoch 42/300 -- Iteration 39762 - Batch 279/963 - Train loss: 0.29386680  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.29 min) Epoch 42/300 -- Iteration 39771 - Batch 288/963 - Train loss: 0.29258029  - Train acc: 0.8885 - Val loss: 1.01041305\n",
      "(8.29 min) Epoch 42/300 -- Iteration 39780 - Batch 297/963 - Train loss: 0.29333832  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.29 min) Epoch 42/300 -- Iteration 39789 - Batch 306/963 - Train loss: 0.29318106  - Train acc: 0.8884 - Val loss: 1.01041305\n",
      "(8.29 min) Epoch 42/300 -- Iteration 39798 - Batch 315/963 - Train loss: 0.29291887  - Train acc: 0.8885 - Val loss: 1.01041305\n",
      "(8.30 min) Epoch 42/300 -- Iteration 39807 - Batch 324/963 - Train loss: 0.29284675  - Train acc: 0.8885 - Val loss: 1.01041305\n",
      "(8.30 min) Epoch 42/300 -- Iteration 39816 - Batch 333/963 - Train loss: 0.29315908  - Train acc: 0.8880 - Val loss: 1.01041305\n",
      "(8.30 min) Epoch 42/300 -- Iteration 39825 - Batch 342/963 - Train loss: 0.29316532  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.30 min) Epoch 42/300 -- Iteration 39834 - Batch 351/963 - Train loss: 0.29340363  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.30 min) Epoch 42/300 -- Iteration 39843 - Batch 360/963 - Train loss: 0.29340967  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.31 min) Epoch 42/300 -- Iteration 39852 - Batch 369/963 - Train loss: 0.29379380  - Train acc: 0.8880 - Val loss: 1.01041305\n",
      "(8.31 min) Epoch 42/300 -- Iteration 39861 - Batch 378/963 - Train loss: 0.29363217  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.31 min) Epoch 42/300 -- Iteration 39870 - Batch 387/963 - Train loss: 0.29407704  - Train acc: 0.8879 - Val loss: 1.01041305\n",
      "(8.31 min) Epoch 42/300 -- Iteration 39879 - Batch 396/963 - Train loss: 0.29432825  - Train acc: 0.8877 - Val loss: 1.01041305\n",
      "(8.31 min) Epoch 42/300 -- Iteration 39888 - Batch 405/963 - Train loss: 0.29476166  - Train acc: 0.8876 - Val loss: 1.01041305\n",
      "(8.32 min) Epoch 42/300 -- Iteration 39897 - Batch 414/963 - Train loss: 0.29418744  - Train acc: 0.8877 - Val loss: 1.01041305\n",
      "(8.32 min) Epoch 42/300 -- Iteration 39906 - Batch 423/963 - Train loss: 0.29431082  - Train acc: 0.8878 - Val loss: 1.01041305\n",
      "(8.32 min) Epoch 42/300 -- Iteration 39915 - Batch 432/963 - Train loss: 0.29463951  - Train acc: 0.8877 - Val loss: 1.01041305\n",
      "(8.32 min) Epoch 42/300 -- Iteration 39924 - Batch 441/963 - Train loss: 0.29486838  - Train acc: 0.8876 - Val loss: 1.01041305\n",
      "(8.32 min) Epoch 42/300 -- Iteration 39933 - Batch 450/963 - Train loss: 0.29497177  - Train acc: 0.8875 - Val loss: 1.01041305\n",
      "(8.32 min) Epoch 42/300 -- Iteration 39942 - Batch 459/963 - Train loss: 0.29463307  - Train acc: 0.8876 - Val loss: 1.01041305\n",
      "(8.33 min) Epoch 42/300 -- Iteration 39951 - Batch 468/963 - Train loss: 0.29440079  - Train acc: 0.8876 - Val loss: 1.01041305\n",
      "(8.33 min) Epoch 42/300 -- Iteration 39960 - Batch 477/963 - Train loss: 0.29405091  - Train acc: 0.8878 - Val loss: 1.01041305\n",
      "(8.33 min) Epoch 42/300 -- Iteration 39969 - Batch 486/963 - Train loss: 0.29391078  - Train acc: 0.8877 - Val loss: 1.01041305\n",
      "(8.33 min) Epoch 42/300 -- Iteration 39978 - Batch 495/963 - Train loss: 0.29307818  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.33 min) Epoch 42/300 -- Iteration 39987 - Batch 504/963 - Train loss: 0.29299975  - Train acc: 0.8884 - Val loss: 1.01041305\n",
      "(8.34 min) Epoch 42/300 -- Iteration 39996 - Batch 513/963 - Train loss: 0.29310378  - Train acc: 0.8885 - Val loss: 1.01041305\n",
      "(8.34 min) Epoch 42/300 -- Iteration 40005 - Batch 522/963 - Train loss: 0.29365522  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.34 min) Epoch 42/300 -- Iteration 40014 - Batch 531/963 - Train loss: 0.29402356  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.34 min) Epoch 42/300 -- Iteration 40023 - Batch 540/963 - Train loss: 0.29397217  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.34 min) Epoch 42/300 -- Iteration 40032 - Batch 549/963 - Train loss: 0.29351695  - Train acc: 0.8883 - Val loss: 1.01041305\n",
      "(8.35 min) Epoch 42/300 -- Iteration 40041 - Batch 558/963 - Train loss: 0.29341309  - Train acc: 0.8884 - Val loss: 1.01041305\n",
      "(8.35 min) Epoch 42/300 -- Iteration 40050 - Batch 567/963 - Train loss: 0.29342087  - Train acc: 0.8883 - Val loss: 1.01041305\n",
      "(8.35 min) Epoch 42/300 -- Iteration 40059 - Batch 576/963 - Train loss: 0.29331771  - Train acc: 0.8884 - Val loss: 1.01041305\n",
      "(8.35 min) Epoch 42/300 -- Iteration 40068 - Batch 585/963 - Train loss: 0.29342699  - Train acc: 0.8884 - Val loss: 1.01041305\n",
      "(8.35 min) Epoch 42/300 -- Iteration 40077 - Batch 594/963 - Train loss: 0.29380200  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.35 min) Epoch 42/300 -- Iteration 40086 - Batch 603/963 - Train loss: 0.29395535  - Train acc: 0.8883 - Val loss: 1.01041305\n",
      "(8.36 min) Epoch 42/300 -- Iteration 40095 - Batch 612/963 - Train loss: 0.29420157  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.36 min) Epoch 42/300 -- Iteration 40104 - Batch 621/963 - Train loss: 0.29383231  - Train acc: 0.8883 - Val loss: 1.01041305\n",
      "(8.36 min) Epoch 42/300 -- Iteration 40113 - Batch 630/963 - Train loss: 0.29396858  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.36 min) Epoch 42/300 -- Iteration 40122 - Batch 639/963 - Train loss: 0.29429988  - Train acc: 0.8880 - Val loss: 1.01041305\n",
      "(8.36 min) Epoch 42/300 -- Iteration 40131 - Batch 648/963 - Train loss: 0.29406241  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.37 min) Epoch 42/300 -- Iteration 40140 - Batch 657/963 - Train loss: 0.29444132  - Train acc: 0.8879 - Val loss: 1.01041305\n",
      "(8.37 min) Epoch 42/300 -- Iteration 40149 - Batch 666/963 - Train loss: 0.29409630  - Train acc: 0.8880 - Val loss: 1.01041305\n",
      "(8.37 min) Epoch 42/300 -- Iteration 40158 - Batch 675/963 - Train loss: 0.29376470  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.37 min) Epoch 42/300 -- Iteration 40167 - Batch 684/963 - Train loss: 0.29359131  - Train acc: 0.8884 - Val loss: 1.01041305\n",
      "(8.37 min) Epoch 42/300 -- Iteration 40176 - Batch 693/963 - Train loss: 0.29364794  - Train acc: 0.8884 - Val loss: 1.01041305\n",
      "(8.38 min) Epoch 42/300 -- Iteration 40185 - Batch 702/963 - Train loss: 0.29382389  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.38 min) Epoch 42/300 -- Iteration 40194 - Batch 711/963 - Train loss: 0.29409002  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.38 min) Epoch 42/300 -- Iteration 40203 - Batch 720/963 - Train loss: 0.29446454  - Train acc: 0.8879 - Val loss: 1.01041305\n",
      "(8.38 min) Epoch 42/300 -- Iteration 40212 - Batch 729/963 - Train loss: 0.29439224  - Train acc: 0.8879 - Val loss: 1.01041305\n",
      "(8.38 min) Epoch 42/300 -- Iteration 40221 - Batch 738/963 - Train loss: 0.29466387  - Train acc: 0.8879 - Val loss: 1.01041305\n",
      "(8.38 min) Epoch 42/300 -- Iteration 40230 - Batch 747/963 - Train loss: 0.29463751  - Train acc: 0.8879 - Val loss: 1.01041305\n",
      "(8.39 min) Epoch 42/300 -- Iteration 40239 - Batch 756/963 - Train loss: 0.29413558  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.39 min) Epoch 42/300 -- Iteration 40248 - Batch 765/963 - Train loss: 0.29415651  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.39 min) Epoch 42/300 -- Iteration 40257 - Batch 774/963 - Train loss: 0.29441207  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.39 min) Epoch 42/300 -- Iteration 40266 - Batch 783/963 - Train loss: 0.29451168  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.39 min) Epoch 42/300 -- Iteration 40275 - Batch 792/963 - Train loss: 0.29456781  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.40 min) Epoch 42/300 -- Iteration 40284 - Batch 801/963 - Train loss: 0.29460231  - Train acc: 0.8880 - Val loss: 1.01041305\n",
      "(8.40 min) Epoch 42/300 -- Iteration 40293 - Batch 810/963 - Train loss: 0.29415327  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.40 min) Epoch 42/300 -- Iteration 40302 - Batch 819/963 - Train loss: 0.29459692  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.40 min) Epoch 42/300 -- Iteration 40311 - Batch 828/963 - Train loss: 0.29465344  - Train acc: 0.8880 - Val loss: 1.01041305\n",
      "(8.40 min) Epoch 42/300 -- Iteration 40320 - Batch 837/963 - Train loss: 0.29480091  - Train acc: 0.8879 - Val loss: 1.01041305\n",
      "(8.41 min) Epoch 42/300 -- Iteration 40329 - Batch 846/963 - Train loss: 0.29446051  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.41 min) Epoch 42/300 -- Iteration 40338 - Batch 855/963 - Train loss: 0.29464854  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.41 min) Epoch 42/300 -- Iteration 40347 - Batch 864/963 - Train loss: 0.29485786  - Train acc: 0.8880 - Val loss: 1.01041305\n",
      "(8.41 min) Epoch 42/300 -- Iteration 40356 - Batch 873/963 - Train loss: 0.29449512  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.41 min) Epoch 42/300 -- Iteration 40365 - Batch 882/963 - Train loss: 0.29426662  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.41 min) Epoch 42/300 -- Iteration 40374 - Batch 891/963 - Train loss: 0.29423517  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.42 min) Epoch 42/300 -- Iteration 40383 - Batch 900/963 - Train loss: 0.29426993  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.42 min) Epoch 42/300 -- Iteration 40392 - Batch 909/963 - Train loss: 0.29419597  - Train acc: 0.8883 - Val loss: 1.01041305\n",
      "(8.42 min) Epoch 42/300 -- Iteration 40401 - Batch 918/963 - Train loss: 0.29433074  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.42 min) Epoch 42/300 -- Iteration 40410 - Batch 927/963 - Train loss: 0.29471541  - Train acc: 0.8880 - Val loss: 1.01041305\n",
      "(8.42 min) Epoch 42/300 -- Iteration 40419 - Batch 936/963 - Train loss: 0.29461534  - Train acc: 0.8880 - Val loss: 1.01041305\n",
      "(8.43 min) Epoch 42/300 -- Iteration 40428 - Batch 945/963 - Train loss: 0.29437958  - Train acc: 0.8881 - Val loss: 1.01041305\n",
      "(8.43 min) Epoch 42/300 -- Iteration 40437 - Batch 954/963 - Train loss: 0.29428256  - Train acc: 0.8882 - Val loss: 1.01041305\n",
      "(8.43 min) Epoch 42/300 -- Iteration 40446 - Batch 962/963 - Train loss: 0.29421349  - Train acc: 0.8881 - Val loss: 1.02344882 - Val acc: 0.5967\n",
      "(8.43 min) Epoch 43/300 -- Iteration 40455 - Batch 9/963 - Train loss: 0.28492072  - Train acc: 0.8867 - Val loss: 1.02344882\n",
      "(8.43 min) Epoch 43/300 -- Iteration 40464 - Batch 18/963 - Train loss: 0.28547394  - Train acc: 0.8882 - Val loss: 1.02344882\n",
      "(8.44 min) Epoch 43/300 -- Iteration 40473 - Batch 27/963 - Train loss: 0.27456553  - Train acc: 0.8970 - Val loss: 1.02344882\n",
      "(8.44 min) Epoch 43/300 -- Iteration 40482 - Batch 36/963 - Train loss: 0.27759185  - Train acc: 0.8961 - Val loss: 1.02344882\n",
      "(8.44 min) Epoch 43/300 -- Iteration 40491 - Batch 45/963 - Train loss: 0.27886428  - Train acc: 0.8956 - Val loss: 1.02344882\n",
      "(8.44 min) Epoch 43/300 -- Iteration 40500 - Batch 54/963 - Train loss: 0.28311616  - Train acc: 0.8928 - Val loss: 1.02344882\n",
      "(8.44 min) Epoch 43/300 -- Iteration 40509 - Batch 63/963 - Train loss: 0.28590500  - Train acc: 0.8905 - Val loss: 1.02344882\n",
      "(8.45 min) Epoch 43/300 -- Iteration 40518 - Batch 72/963 - Train loss: 0.28982992  - Train acc: 0.8886 - Val loss: 1.02344882\n",
      "(8.45 min) Epoch 43/300 -- Iteration 40527 - Batch 81/963 - Train loss: 0.29657432  - Train acc: 0.8849 - Val loss: 1.02344882\n",
      "(8.45 min) Epoch 43/300 -- Iteration 40536 - Batch 90/963 - Train loss: 0.29619627  - Train acc: 0.8853 - Val loss: 1.02344882\n",
      "(8.45 min) Epoch 43/300 -- Iteration 40545 - Batch 99/963 - Train loss: 0.29550038  - Train acc: 0.8852 - Val loss: 1.02344882\n",
      "(8.45 min) Epoch 43/300 -- Iteration 40554 - Batch 108/963 - Train loss: 0.29957947  - Train acc: 0.8841 - Val loss: 1.02344882\n",
      "(8.45 min) Epoch 43/300 -- Iteration 40563 - Batch 117/963 - Train loss: 0.30012173  - Train acc: 0.8837 - Val loss: 1.02344882\n",
      "(8.46 min) Epoch 43/300 -- Iteration 40572 - Batch 126/963 - Train loss: 0.29974489  - Train acc: 0.8837 - Val loss: 1.02344882\n",
      "(8.46 min) Epoch 43/300 -- Iteration 40581 - Batch 135/963 - Train loss: 0.29797843  - Train acc: 0.8847 - Val loss: 1.02344882\n",
      "(8.46 min) Epoch 43/300 -- Iteration 40590 - Batch 144/963 - Train loss: 0.29745201  - Train acc: 0.8852 - Val loss: 1.02344882\n",
      "(8.46 min) Epoch 43/300 -- Iteration 40599 - Batch 153/963 - Train loss: 0.29918082  - Train acc: 0.8840 - Val loss: 1.02344882\n",
      "(8.46 min) Epoch 43/300 -- Iteration 40608 - Batch 162/963 - Train loss: 0.29911882  - Train acc: 0.8842 - Val loss: 1.02344882\n",
      "(8.47 min) Epoch 43/300 -- Iteration 40617 - Batch 171/963 - Train loss: 0.29867109  - Train acc: 0.8849 - Val loss: 1.02344882\n",
      "(8.47 min) Epoch 43/300 -- Iteration 40626 - Batch 180/963 - Train loss: 0.29823274  - Train acc: 0.8848 - Val loss: 1.02344882\n",
      "(8.47 min) Epoch 43/300 -- Iteration 40635 - Batch 189/963 - Train loss: 0.29859228  - Train acc: 0.8846 - Val loss: 1.02344882\n",
      "(8.47 min) Epoch 43/300 -- Iteration 40644 - Batch 198/963 - Train loss: 0.29866153  - Train acc: 0.8840 - Val loss: 1.02344882\n",
      "(8.47 min) Epoch 43/300 -- Iteration 40653 - Batch 207/963 - Train loss: 0.29763060  - Train acc: 0.8845 - Val loss: 1.02344882\n",
      "(8.48 min) Epoch 43/300 -- Iteration 40662 - Batch 216/963 - Train loss: 0.29678841  - Train acc: 0.8851 - Val loss: 1.02344882\n",
      "(8.48 min) Epoch 43/300 -- Iteration 40671 - Batch 225/963 - Train loss: 0.29750936  - Train acc: 0.8851 - Val loss: 1.02344882\n",
      "(8.48 min) Epoch 43/300 -- Iteration 40680 - Batch 234/963 - Train loss: 0.29627493  - Train acc: 0.8854 - Val loss: 1.02344882\n",
      "(8.48 min) Epoch 43/300 -- Iteration 40689 - Batch 243/963 - Train loss: 0.29566418  - Train acc: 0.8857 - Val loss: 1.02344882\n",
      "(8.48 min) Epoch 43/300 -- Iteration 40698 - Batch 252/963 - Train loss: 0.29548155  - Train acc: 0.8854 - Val loss: 1.02344882\n",
      "(8.49 min) Epoch 43/300 -- Iteration 40707 - Batch 261/963 - Train loss: 0.29461159  - Train acc: 0.8861 - Val loss: 1.02344882\n",
      "(8.49 min) Epoch 43/300 -- Iteration 40716 - Batch 270/963 - Train loss: 0.29501718  - Train acc: 0.8860 - Val loss: 1.02344882\n",
      "(8.49 min) Epoch 43/300 -- Iteration 40725 - Batch 279/963 - Train loss: 0.29566673  - Train acc: 0.8855 - Val loss: 1.02344882\n",
      "(8.49 min) Epoch 43/300 -- Iteration 40734 - Batch 288/963 - Train loss: 0.29581368  - Train acc: 0.8856 - Val loss: 1.02344882\n",
      "(8.49 min) Epoch 43/300 -- Iteration 40743 - Batch 297/963 - Train loss: 0.29563074  - Train acc: 0.8857 - Val loss: 1.02344882\n",
      "(8.49 min) Epoch 43/300 -- Iteration 40752 - Batch 306/963 - Train loss: 0.29535894  - Train acc: 0.8856 - Val loss: 1.02344882\n",
      "(8.50 min) Epoch 43/300 -- Iteration 40761 - Batch 315/963 - Train loss: 0.29465544  - Train acc: 0.8859 - Val loss: 1.02344882\n",
      "(8.50 min) Epoch 43/300 -- Iteration 40770 - Batch 324/963 - Train loss: 0.29424872  - Train acc: 0.8861 - Val loss: 1.02344882\n",
      "(8.50 min) Epoch 43/300 -- Iteration 40779 - Batch 333/963 - Train loss: 0.29410758  - Train acc: 0.8863 - Val loss: 1.02344882\n",
      "(8.50 min) Epoch 43/300 -- Iteration 40788 - Batch 342/963 - Train loss: 0.29364430  - Train acc: 0.8864 - Val loss: 1.02344882\n",
      "(8.50 min) Epoch 43/300 -- Iteration 40797 - Batch 351/963 - Train loss: 0.29384142  - Train acc: 0.8861 - Val loss: 1.02344882\n",
      "(8.51 min) Epoch 43/300 -- Iteration 40806 - Batch 360/963 - Train loss: 0.29358505  - Train acc: 0.8863 - Val loss: 1.02344882\n",
      "(8.51 min) Epoch 43/300 -- Iteration 40815 - Batch 369/963 - Train loss: 0.29417222  - Train acc: 0.8861 - Val loss: 1.02344882\n",
      "(8.51 min) Epoch 43/300 -- Iteration 40824 - Batch 378/963 - Train loss: 0.29467590  - Train acc: 0.8859 - Val loss: 1.02344882\n",
      "(8.51 min) Epoch 43/300 -- Iteration 40833 - Batch 387/963 - Train loss: 0.29430018  - Train acc: 0.8861 - Val loss: 1.02344882\n",
      "(8.51 min) Epoch 43/300 -- Iteration 40842 - Batch 396/963 - Train loss: 0.29432907  - Train acc: 0.8861 - Val loss: 1.02344882\n",
      "(8.51 min) Epoch 43/300 -- Iteration 40851 - Batch 405/963 - Train loss: 0.29457467  - Train acc: 0.8859 - Val loss: 1.02344882\n",
      "(8.52 min) Epoch 43/300 -- Iteration 40860 - Batch 414/963 - Train loss: 0.29455920  - Train acc: 0.8860 - Val loss: 1.02344882\n",
      "(8.52 min) Epoch 43/300 -- Iteration 40869 - Batch 423/963 - Train loss: 0.29455096  - Train acc: 0.8861 - Val loss: 1.02344882\n",
      "(8.52 min) Epoch 43/300 -- Iteration 40878 - Batch 432/963 - Train loss: 0.29448202  - Train acc: 0.8863 - Val loss: 1.02344882\n",
      "(8.52 min) Epoch 43/300 -- Iteration 40887 - Batch 441/963 - Train loss: 0.29475666  - Train acc: 0.8863 - Val loss: 1.02344882\n",
      "(8.52 min) Epoch 43/300 -- Iteration 40896 - Batch 450/963 - Train loss: 0.29466012  - Train acc: 0.8863 - Val loss: 1.02344882\n",
      "(8.53 min) Epoch 43/300 -- Iteration 40905 - Batch 459/963 - Train loss: 0.29410521  - Train acc: 0.8866 - Val loss: 1.02344882\n",
      "(8.53 min) Epoch 43/300 -- Iteration 40914 - Batch 468/963 - Train loss: 0.29488877  - Train acc: 0.8864 - Val loss: 1.02344882\n",
      "(8.53 min) Epoch 43/300 -- Iteration 40923 - Batch 477/963 - Train loss: 0.29457311  - Train acc: 0.8864 - Val loss: 1.02344882\n",
      "(8.53 min) Epoch 43/300 -- Iteration 40932 - Batch 486/963 - Train loss: 0.29438951  - Train acc: 0.8864 - Val loss: 1.02344882\n",
      "(8.53 min) Epoch 43/300 -- Iteration 40941 - Batch 495/963 - Train loss: 0.29386092  - Train acc: 0.8868 - Val loss: 1.02344882\n",
      "(8.54 min) Epoch 43/300 -- Iteration 40950 - Batch 504/963 - Train loss: 0.29364479  - Train acc: 0.8870 - Val loss: 1.02344882\n",
      "(8.54 min) Epoch 43/300 -- Iteration 40959 - Batch 513/963 - Train loss: 0.29358095  - Train acc: 0.8869 - Val loss: 1.02344882\n",
      "(8.54 min) Epoch 43/300 -- Iteration 40968 - Batch 522/963 - Train loss: 0.29332750  - Train acc: 0.8870 - Val loss: 1.02344882\n",
      "(8.54 min) Epoch 43/300 -- Iteration 40977 - Batch 531/963 - Train loss: 0.29314582  - Train acc: 0.8870 - Val loss: 1.02344882\n",
      "(8.54 min) Epoch 43/300 -- Iteration 40986 - Batch 540/963 - Train loss: 0.29325495  - Train acc: 0.8868 - Val loss: 1.02344882\n",
      "(8.55 min) Epoch 43/300 -- Iteration 40995 - Batch 549/963 - Train loss: 0.29313121  - Train acc: 0.8870 - Val loss: 1.02344882\n",
      "(8.55 min) Epoch 43/300 -- Iteration 41004 - Batch 558/963 - Train loss: 0.29265869  - Train acc: 0.8871 - Val loss: 1.02344882\n",
      "(8.55 min) Epoch 43/300 -- Iteration 41013 - Batch 567/963 - Train loss: 0.29260313  - Train acc: 0.8871 - Val loss: 1.02344882\n",
      "(8.55 min) Epoch 43/300 -- Iteration 41022 - Batch 576/963 - Train loss: 0.29218993  - Train acc: 0.8873 - Val loss: 1.02344882\n",
      "(8.55 min) Epoch 43/300 -- Iteration 41031 - Batch 585/963 - Train loss: 0.29216994  - Train acc: 0.8873 - Val loss: 1.02344882\n",
      "(8.55 min) Epoch 43/300 -- Iteration 41040 - Batch 594/963 - Train loss: 0.29223840  - Train acc: 0.8872 - Val loss: 1.02344882\n",
      "(8.56 min) Epoch 43/300 -- Iteration 41049 - Batch 603/963 - Train loss: 0.29255631  - Train acc: 0.8871 - Val loss: 1.02344882\n",
      "(8.56 min) Epoch 43/300 -- Iteration 41058 - Batch 612/963 - Train loss: 0.29238619  - Train acc: 0.8871 - Val loss: 1.02344882\n",
      "(8.56 min) Epoch 43/300 -- Iteration 41067 - Batch 621/963 - Train loss: 0.29266905  - Train acc: 0.8871 - Val loss: 1.02344882\n",
      "(8.56 min) Epoch 43/300 -- Iteration 41076 - Batch 630/963 - Train loss: 0.29240184  - Train acc: 0.8872 - Val loss: 1.02344882\n",
      "(8.56 min) Epoch 43/300 -- Iteration 41085 - Batch 639/963 - Train loss: 0.29214022  - Train acc: 0.8874 - Val loss: 1.02344882\n",
      "(8.57 min) Epoch 43/300 -- Iteration 41094 - Batch 648/963 - Train loss: 0.29240345  - Train acc: 0.8871 - Val loss: 1.02344882\n",
      "(8.57 min) Epoch 43/300 -- Iteration 41103 - Batch 657/963 - Train loss: 0.29260247  - Train acc: 0.8871 - Val loss: 1.02344882\n",
      "(8.57 min) Epoch 43/300 -- Iteration 41112 - Batch 666/963 - Train loss: 0.29252656  - Train acc: 0.8871 - Val loss: 1.02344882\n",
      "(8.57 min) Epoch 43/300 -- Iteration 41121 - Batch 675/963 - Train loss: 0.29269017  - Train acc: 0.8871 - Val loss: 1.02344882\n",
      "(8.57 min) Epoch 43/300 -- Iteration 41130 - Batch 684/963 - Train loss: 0.29240244  - Train acc: 0.8872 - Val loss: 1.02344882\n",
      "(8.58 min) Epoch 43/300 -- Iteration 41139 - Batch 693/963 - Train loss: 0.29243891  - Train acc: 0.8872 - Val loss: 1.02344882\n",
      "(8.58 min) Epoch 43/300 -- Iteration 41148 - Batch 702/963 - Train loss: 0.29259464  - Train acc: 0.8872 - Val loss: 1.02344882\n",
      "(8.58 min) Epoch 43/300 -- Iteration 41157 - Batch 711/963 - Train loss: 0.29238531  - Train acc: 0.8873 - Val loss: 1.02344882\n",
      "(8.58 min) Epoch 43/300 -- Iteration 41166 - Batch 720/963 - Train loss: 0.29199528  - Train acc: 0.8876 - Val loss: 1.02344882\n",
      "(8.58 min) Epoch 43/300 -- Iteration 41175 - Batch 729/963 - Train loss: 0.29219461  - Train acc: 0.8876 - Val loss: 1.02344882\n",
      "(8.59 min) Epoch 43/300 -- Iteration 41184 - Batch 738/963 - Train loss: 0.29203183  - Train acc: 0.8876 - Val loss: 1.02344882\n",
      "(8.59 min) Epoch 43/300 -- Iteration 41193 - Batch 747/963 - Train loss: 0.29185870  - Train acc: 0.8879 - Val loss: 1.02344882\n",
      "(8.59 min) Epoch 43/300 -- Iteration 41202 - Batch 756/963 - Train loss: 0.29181223  - Train acc: 0.8880 - Val loss: 1.02344882\n",
      "(8.59 min) Epoch 43/300 -- Iteration 41211 - Batch 765/963 - Train loss: 0.29190205  - Train acc: 0.8880 - Val loss: 1.02344882\n",
      "(8.59 min) Epoch 43/300 -- Iteration 41220 - Batch 774/963 - Train loss: 0.29203530  - Train acc: 0.8880 - Val loss: 1.02344882\n",
      "(8.59 min) Epoch 43/300 -- Iteration 41229 - Batch 783/963 - Train loss: 0.29198034  - Train acc: 0.8879 - Val loss: 1.02344882\n",
      "(8.60 min) Epoch 43/300 -- Iteration 41238 - Batch 792/963 - Train loss: 0.29202585  - Train acc: 0.8879 - Val loss: 1.02344882\n",
      "(8.60 min) Epoch 43/300 -- Iteration 41247 - Batch 801/963 - Train loss: 0.29230112  - Train acc: 0.8878 - Val loss: 1.02344882\n",
      "(8.60 min) Epoch 43/300 -- Iteration 41256 - Batch 810/963 - Train loss: 0.29236242  - Train acc: 0.8879 - Val loss: 1.02344882\n",
      "(8.60 min) Epoch 43/300 -- Iteration 41265 - Batch 819/963 - Train loss: 0.29264574  - Train acc: 0.8877 - Val loss: 1.02344882\n",
      "(8.60 min) Epoch 43/300 -- Iteration 41274 - Batch 828/963 - Train loss: 0.29246618  - Train acc: 0.8878 - Val loss: 1.02344882\n",
      "(8.61 min) Epoch 43/300 -- Iteration 41283 - Batch 837/963 - Train loss: 0.29260010  - Train acc: 0.8878 - Val loss: 1.02344882\n",
      "(8.61 min) Epoch 43/300 -- Iteration 41292 - Batch 846/963 - Train loss: 0.29285898  - Train acc: 0.8876 - Val loss: 1.02344882\n",
      "(8.61 min) Epoch 43/300 -- Iteration 41301 - Batch 855/963 - Train loss: 0.29281171  - Train acc: 0.8877 - Val loss: 1.02344882\n",
      "(8.61 min) Epoch 43/300 -- Iteration 41310 - Batch 864/963 - Train loss: 0.29294538  - Train acc: 0.8877 - Val loss: 1.02344882\n",
      "(8.61 min) Epoch 43/300 -- Iteration 41319 - Batch 873/963 - Train loss: 0.29277907  - Train acc: 0.8877 - Val loss: 1.02344882\n",
      "(8.62 min) Epoch 43/300 -- Iteration 41328 - Batch 882/963 - Train loss: 0.29282551  - Train acc: 0.8878 - Val loss: 1.02344882\n",
      "(8.62 min) Epoch 43/300 -- Iteration 41337 - Batch 891/963 - Train loss: 0.29281167  - Train acc: 0.8878 - Val loss: 1.02344882\n",
      "(8.62 min) Epoch 43/300 -- Iteration 41346 - Batch 900/963 - Train loss: 0.29279047  - Train acc: 0.8878 - Val loss: 1.02344882\n",
      "(8.62 min) Epoch 43/300 -- Iteration 41355 - Batch 909/963 - Train loss: 0.29255999  - Train acc: 0.8879 - Val loss: 1.02344882\n",
      "(8.62 min) Epoch 43/300 -- Iteration 41364 - Batch 918/963 - Train loss: 0.29264497  - Train acc: 0.8879 - Val loss: 1.02344882\n",
      "(8.62 min) Epoch 43/300 -- Iteration 41373 - Batch 927/963 - Train loss: 0.29239818  - Train acc: 0.8881 - Val loss: 1.02344882\n",
      "(8.63 min) Epoch 43/300 -- Iteration 41382 - Batch 936/963 - Train loss: 0.29249665  - Train acc: 0.8881 - Val loss: 1.02344882\n",
      "(8.63 min) Epoch 43/300 -- Iteration 41391 - Batch 945/963 - Train loss: 0.29245415  - Train acc: 0.8881 - Val loss: 1.02344882\n",
      "(8.63 min) Epoch 43/300 -- Iteration 41400 - Batch 954/963 - Train loss: 0.29243923  - Train acc: 0.8880 - Val loss: 1.02344882\n",
      "(8.63 min) Epoch 43/300 -- Iteration 41409 - Batch 962/963 - Train loss: 0.29221650  - Train acc: 0.8881 - Val loss: 1.01599789 - Val acc: 0.5967\n",
      "(8.63 min) Epoch 44/300 -- Iteration 41418 - Batch 9/963 - Train loss: 0.28661658  - Train acc: 0.8797 - Val loss: 1.01599789\n",
      "(8.64 min) Epoch 44/300 -- Iteration 41427 - Batch 18/963 - Train loss: 0.30170731  - Train acc: 0.8820 - Val loss: 1.01599789\n",
      "(8.64 min) Epoch 44/300 -- Iteration 41436 - Batch 27/963 - Train loss: 0.28737837  - Train acc: 0.8878 - Val loss: 1.01599789\n",
      "(8.64 min) Epoch 44/300 -- Iteration 41445 - Batch 36/963 - Train loss: 0.28905924  - Train acc: 0.8866 - Val loss: 1.01599789\n",
      "(8.64 min) Epoch 44/300 -- Iteration 41454 - Batch 45/963 - Train loss: 0.29216217  - Train acc: 0.8862 - Val loss: 1.01599789\n",
      "(8.64 min) Epoch 44/300 -- Iteration 41463 - Batch 54/963 - Train loss: 0.28688764  - Train acc: 0.8886 - Val loss: 1.01599789\n",
      "(8.65 min) Epoch 44/300 -- Iteration 41472 - Batch 63/963 - Train loss: 0.28634333  - Train acc: 0.8905 - Val loss: 1.01599789\n",
      "(8.65 min) Epoch 44/300 -- Iteration 41481 - Batch 72/963 - Train loss: 0.28696102  - Train acc: 0.8901 - Val loss: 1.01599789\n",
      "(8.65 min) Epoch 44/300 -- Iteration 41490 - Batch 81/963 - Train loss: 0.28604958  - Train acc: 0.8910 - Val loss: 1.01599789\n",
      "(8.65 min) Epoch 44/300 -- Iteration 41499 - Batch 90/963 - Train loss: 0.28663210  - Train acc: 0.8914 - Val loss: 1.01599789\n",
      "(8.65 min) Epoch 44/300 -- Iteration 41508 - Batch 99/963 - Train loss: 0.29292996  - Train acc: 0.8888 - Val loss: 1.01599789\n",
      "(8.66 min) Epoch 44/300 -- Iteration 41517 - Batch 108/963 - Train loss: 0.28927776  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.66 min) Epoch 44/300 -- Iteration 41526 - Batch 117/963 - Train loss: 0.28794146  - Train acc: 0.8905 - Val loss: 1.01599789\n",
      "(8.66 min) Epoch 44/300 -- Iteration 41535 - Batch 126/963 - Train loss: 0.28788573  - Train acc: 0.8904 - Val loss: 1.01599789\n",
      "(8.66 min) Epoch 44/300 -- Iteration 41544 - Batch 135/963 - Train loss: 0.28785838  - Train acc: 0.8903 - Val loss: 1.01599789\n",
      "(8.66 min) Epoch 44/300 -- Iteration 41553 - Batch 144/963 - Train loss: 0.28957307  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.66 min) Epoch 44/300 -- Iteration 41562 - Batch 153/963 - Train loss: 0.28904077  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.67 min) Epoch 44/300 -- Iteration 41571 - Batch 162/963 - Train loss: 0.28885410  - Train acc: 0.8898 - Val loss: 1.01599789\n",
      "(8.67 min) Epoch 44/300 -- Iteration 41580 - Batch 171/963 - Train loss: 0.28871227  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.67 min) Epoch 44/300 -- Iteration 41589 - Batch 180/963 - Train loss: 0.28943910  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.67 min) Epoch 44/300 -- Iteration 41598 - Batch 189/963 - Train loss: 0.29026499  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.67 min) Epoch 44/300 -- Iteration 41607 - Batch 198/963 - Train loss: 0.29019059  - Train acc: 0.8896 - Val loss: 1.01599789\n",
      "(8.68 min) Epoch 44/300 -- Iteration 41616 - Batch 207/963 - Train loss: 0.28982578  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.68 min) Epoch 44/300 -- Iteration 41625 - Batch 216/963 - Train loss: 0.28944660  - Train acc: 0.8902 - Val loss: 1.01599789\n",
      "(8.68 min) Epoch 44/300 -- Iteration 41634 - Batch 225/963 - Train loss: 0.28971399  - Train acc: 0.8902 - Val loss: 1.01599789\n",
      "(8.68 min) Epoch 44/300 -- Iteration 41643 - Batch 234/963 - Train loss: 0.28901205  - Train acc: 0.8904 - Val loss: 1.01599789\n",
      "(8.68 min) Epoch 44/300 -- Iteration 41652 - Batch 243/963 - Train loss: 0.28767915  - Train acc: 0.8909 - Val loss: 1.01599789\n",
      "(8.69 min) Epoch 44/300 -- Iteration 41661 - Batch 252/963 - Train loss: 0.28959695  - Train acc: 0.8901 - Val loss: 1.01599789\n",
      "(8.69 min) Epoch 44/300 -- Iteration 41670 - Batch 261/963 - Train loss: 0.28894528  - Train acc: 0.8903 - Val loss: 1.01599789\n",
      "(8.69 min) Epoch 44/300 -- Iteration 41679 - Batch 270/963 - Train loss: 0.28983913  - Train acc: 0.8901 - Val loss: 1.01599789\n",
      "(8.69 min) Epoch 44/300 -- Iteration 41688 - Batch 279/963 - Train loss: 0.28933378  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.69 min) Epoch 44/300 -- Iteration 41697 - Batch 288/963 - Train loss: 0.28982642  - Train acc: 0.8898 - Val loss: 1.01599789\n",
      "(8.69 min) Epoch 44/300 -- Iteration 41706 - Batch 297/963 - Train loss: 0.29052623  - Train acc: 0.8894 - Val loss: 1.01599789\n",
      "(8.70 min) Epoch 44/300 -- Iteration 41715 - Batch 306/963 - Train loss: 0.29050059  - Train acc: 0.8896 - Val loss: 1.01599789\n",
      "(8.70 min) Epoch 44/300 -- Iteration 41724 - Batch 315/963 - Train loss: 0.29097413  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.70 min) Epoch 44/300 -- Iteration 41733 - Batch 324/963 - Train loss: 0.29128697  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.70 min) Epoch 44/300 -- Iteration 41742 - Batch 333/963 - Train loss: 0.29218590  - Train acc: 0.8892 - Val loss: 1.01599789\n",
      "(8.70 min) Epoch 44/300 -- Iteration 41751 - Batch 342/963 - Train loss: 0.29275069  - Train acc: 0.8891 - Val loss: 1.01599789\n",
      "(8.71 min) Epoch 44/300 -- Iteration 41760 - Batch 351/963 - Train loss: 0.29293509  - Train acc: 0.8892 - Val loss: 1.01599789\n",
      "(8.71 min) Epoch 44/300 -- Iteration 41769 - Batch 360/963 - Train loss: 0.29339347  - Train acc: 0.8892 - Val loss: 1.01599789\n",
      "(8.71 min) Epoch 44/300 -- Iteration 41778 - Batch 369/963 - Train loss: 0.29372744  - Train acc: 0.8892 - Val loss: 1.01599789\n",
      "(8.71 min) Epoch 44/300 -- Iteration 41787 - Batch 378/963 - Train loss: 0.29431694  - Train acc: 0.8890 - Val loss: 1.01599789\n",
      "(8.71 min) Epoch 44/300 -- Iteration 41796 - Batch 387/963 - Train loss: 0.29435936  - Train acc: 0.8889 - Val loss: 1.01599789\n",
      "(8.72 min) Epoch 44/300 -- Iteration 41805 - Batch 396/963 - Train loss: 0.29435332  - Train acc: 0.8889 - Val loss: 1.01599789\n",
      "(8.72 min) Epoch 44/300 -- Iteration 41814 - Batch 405/963 - Train loss: 0.29364257  - Train acc: 0.8893 - Val loss: 1.01599789\n",
      "(8.72 min) Epoch 44/300 -- Iteration 41823 - Batch 414/963 - Train loss: 0.29398500  - Train acc: 0.8893 - Val loss: 1.01599789\n",
      "(8.72 min) Epoch 44/300 -- Iteration 41832 - Batch 423/963 - Train loss: 0.29305207  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.72 min) Epoch 44/300 -- Iteration 41841 - Batch 432/963 - Train loss: 0.29290977  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.72 min) Epoch 44/300 -- Iteration 41850 - Batch 441/963 - Train loss: 0.29383708  - Train acc: 0.8894 - Val loss: 1.01599789\n",
      "(8.73 min) Epoch 44/300 -- Iteration 41859 - Batch 450/963 - Train loss: 0.29384932  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.73 min) Epoch 44/300 -- Iteration 41868 - Batch 459/963 - Train loss: 0.29380063  - Train acc: 0.8896 - Val loss: 1.01599789\n",
      "(8.73 min) Epoch 44/300 -- Iteration 41877 - Batch 468/963 - Train loss: 0.29370103  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.73 min) Epoch 44/300 -- Iteration 41886 - Batch 477/963 - Train loss: 0.29326576  - Train acc: 0.8898 - Val loss: 1.01599789\n",
      "(8.73 min) Epoch 44/300 -- Iteration 41895 - Batch 486/963 - Train loss: 0.29294724  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.74 min) Epoch 44/300 -- Iteration 41904 - Batch 495/963 - Train loss: 0.29284241  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.74 min) Epoch 44/300 -- Iteration 41913 - Batch 504/963 - Train loss: 0.29265571  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.74 min) Epoch 44/300 -- Iteration 41922 - Batch 513/963 - Train loss: 0.29282140  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.74 min) Epoch 44/300 -- Iteration 41931 - Batch 522/963 - Train loss: 0.29338197  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.74 min) Epoch 44/300 -- Iteration 41940 - Batch 531/963 - Train loss: 0.29333660  - Train acc: 0.8894 - Val loss: 1.01599789\n",
      "(8.75 min) Epoch 44/300 -- Iteration 41949 - Batch 540/963 - Train loss: 0.29317857  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.75 min) Epoch 44/300 -- Iteration 41958 - Batch 549/963 - Train loss: 0.29341203  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.75 min) Epoch 44/300 -- Iteration 41967 - Batch 558/963 - Train loss: 0.29364734  - Train acc: 0.8896 - Val loss: 1.01599789\n",
      "(8.75 min) Epoch 44/300 -- Iteration 41976 - Batch 567/963 - Train loss: 0.29401816  - Train acc: 0.8896 - Val loss: 1.01599789\n",
      "(8.75 min) Epoch 44/300 -- Iteration 41985 - Batch 576/963 - Train loss: 0.29392833  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.75 min) Epoch 44/300 -- Iteration 41994 - Batch 585/963 - Train loss: 0.29391990  - Train acc: 0.8898 - Val loss: 1.01599789\n",
      "(8.76 min) Epoch 44/300 -- Iteration 42003 - Batch 594/963 - Train loss: 0.29359860  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.76 min) Epoch 44/300 -- Iteration 42012 - Batch 603/963 - Train loss: 0.29365509  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.76 min) Epoch 44/300 -- Iteration 42021 - Batch 612/963 - Train loss: 0.29359505  - Train acc: 0.8898 - Val loss: 1.01599789\n",
      "(8.76 min) Epoch 44/300 -- Iteration 42030 - Batch 621/963 - Train loss: 0.29335831  - Train acc: 0.8898 - Val loss: 1.01599789\n",
      "(8.76 min) Epoch 44/300 -- Iteration 42039 - Batch 630/963 - Train loss: 0.29359089  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.77 min) Epoch 44/300 -- Iteration 42048 - Batch 639/963 - Train loss: 0.29326839  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.77 min) Epoch 44/300 -- Iteration 42057 - Batch 648/963 - Train loss: 0.29366273  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.77 min) Epoch 44/300 -- Iteration 42066 - Batch 657/963 - Train loss: 0.29350629  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.77 min) Epoch 44/300 -- Iteration 42075 - Batch 666/963 - Train loss: 0.29327736  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.77 min) Epoch 44/300 -- Iteration 42084 - Batch 675/963 - Train loss: 0.29347225  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.78 min) Epoch 44/300 -- Iteration 42093 - Batch 684/963 - Train loss: 0.29356213  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.78 min) Epoch 44/300 -- Iteration 42102 - Batch 693/963 - Train loss: 0.29340107  - Train acc: 0.8898 - Val loss: 1.01599789\n",
      "(8.78 min) Epoch 44/300 -- Iteration 42111 - Batch 702/963 - Train loss: 0.29334207  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.78 min) Epoch 44/300 -- Iteration 42120 - Batch 711/963 - Train loss: 0.29272122  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.78 min) Epoch 44/300 -- Iteration 42129 - Batch 720/963 - Train loss: 0.29257612  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.78 min) Epoch 44/300 -- Iteration 42138 - Batch 729/963 - Train loss: 0.29255133  - Train acc: 0.8901 - Val loss: 1.01599789\n",
      "(8.79 min) Epoch 44/300 -- Iteration 42147 - Batch 738/963 - Train loss: 0.29250414  - Train acc: 0.8901 - Val loss: 1.01599789\n",
      "(8.79 min) Epoch 44/300 -- Iteration 42156 - Batch 747/963 - Train loss: 0.29215645  - Train acc: 0.8902 - Val loss: 1.01599789\n",
      "(8.79 min) Epoch 44/300 -- Iteration 42165 - Batch 756/963 - Train loss: 0.29193970  - Train acc: 0.8902 - Val loss: 1.01599789\n",
      "(8.79 min) Epoch 44/300 -- Iteration 42174 - Batch 765/963 - Train loss: 0.29231601  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.79 min) Epoch 44/300 -- Iteration 42183 - Batch 774/963 - Train loss: 0.29242754  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.80 min) Epoch 44/300 -- Iteration 42192 - Batch 783/963 - Train loss: 0.29315929  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.80 min) Epoch 44/300 -- Iteration 42201 - Batch 792/963 - Train loss: 0.29325779  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.80 min) Epoch 44/300 -- Iteration 42210 - Batch 801/963 - Train loss: 0.29336866  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.80 min) Epoch 44/300 -- Iteration 42219 - Batch 810/963 - Train loss: 0.29318066  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.80 min) Epoch 44/300 -- Iteration 42228 - Batch 819/963 - Train loss: 0.29313765  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.81 min) Epoch 44/300 -- Iteration 42237 - Batch 828/963 - Train loss: 0.29296141  - Train acc: 0.8898 - Val loss: 1.01599789\n",
      "(8.81 min) Epoch 44/300 -- Iteration 42246 - Batch 837/963 - Train loss: 0.29278403  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.81 min) Epoch 44/300 -- Iteration 42255 - Batch 846/963 - Train loss: 0.29271318  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.81 min) Epoch 44/300 -- Iteration 42264 - Batch 855/963 - Train loss: 0.29276226  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.81 min) Epoch 44/300 -- Iteration 42273 - Batch 864/963 - Train loss: 0.29270897  - Train acc: 0.8900 - Val loss: 1.01599789\n",
      "(8.81 min) Epoch 44/300 -- Iteration 42282 - Batch 873/963 - Train loss: 0.29296265  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.82 min) Epoch 44/300 -- Iteration 42291 - Batch 882/963 - Train loss: 0.29308213  - Train acc: 0.8899 - Val loss: 1.01599789\n",
      "(8.82 min) Epoch 44/300 -- Iteration 42300 - Batch 891/963 - Train loss: 0.29305255  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.82 min) Epoch 44/300 -- Iteration 42309 - Batch 900/963 - Train loss: 0.29292211  - Train acc: 0.8897 - Val loss: 1.01599789\n",
      "(8.82 min) Epoch 44/300 -- Iteration 42318 - Batch 909/963 - Train loss: 0.29309838  - Train acc: 0.8896 - Val loss: 1.01599789\n",
      "(8.82 min) Epoch 44/300 -- Iteration 42327 - Batch 918/963 - Train loss: 0.29351857  - Train acc: 0.8894 - Val loss: 1.01599789\n",
      "(8.83 min) Epoch 44/300 -- Iteration 42336 - Batch 927/963 - Train loss: 0.29359851  - Train acc: 0.8894 - Val loss: 1.01599789\n",
      "(8.83 min) Epoch 44/300 -- Iteration 42345 - Batch 936/963 - Train loss: 0.29350703  - Train acc: 0.8894 - Val loss: 1.01599789\n",
      "(8.83 min) Epoch 44/300 -- Iteration 42354 - Batch 945/963 - Train loss: 0.29352524  - Train acc: 0.8895 - Val loss: 1.01599789\n",
      "(8.83 min) Epoch 44/300 -- Iteration 42363 - Batch 954/963 - Train loss: 0.29346822  - Train acc: 0.8894 - Val loss: 1.01599789\n",
      "(8.83 min) Epoch 44/300 -- Iteration 42372 - Batch 962/963 - Train loss: 0.29345756  - Train acc: 0.8895 - Val loss: 1.00099492 - Val acc: 0.5983\n",
      "(8.84 min) Epoch 45/300 -- Iteration 42381 - Batch 9/963 - Train loss: 0.26923359  - Train acc: 0.8914 - Val loss: 1.00099492\n",
      "(8.84 min) Epoch 45/300 -- Iteration 42390 - Batch 18/963 - Train loss: 0.27745023  - Train acc: 0.8923 - Val loss: 1.00099492\n",
      "(8.84 min) Epoch 45/300 -- Iteration 42399 - Batch 27/963 - Train loss: 0.27270079  - Train acc: 0.8965 - Val loss: 1.00099492\n",
      "(8.84 min) Epoch 45/300 -- Iteration 42408 - Batch 36/963 - Train loss: 0.27814636  - Train acc: 0.8948 - Val loss: 1.00099492\n",
      "(8.84 min) Epoch 45/300 -- Iteration 42417 - Batch 45/963 - Train loss: 0.28250988  - Train acc: 0.8923 - Val loss: 1.00099492\n",
      "(8.84 min) Epoch 45/300 -- Iteration 42426 - Batch 54/963 - Train loss: 0.29108221  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(8.85 min) Epoch 45/300 -- Iteration 42435 - Batch 63/963 - Train loss: 0.29457977  - Train acc: 0.8881 - Val loss: 1.00099492\n",
      "(8.85 min) Epoch 45/300 -- Iteration 42444 - Batch 72/963 - Train loss: 0.29419380  - Train acc: 0.8889 - Val loss: 1.00099492\n",
      "(8.85 min) Epoch 45/300 -- Iteration 42453 - Batch 81/963 - Train loss: 0.29500807  - Train acc: 0.8885 - Val loss: 1.00099492\n",
      "(8.85 min) Epoch 45/300 -- Iteration 42462 - Batch 90/963 - Train loss: 0.29308560  - Train acc: 0.8894 - Val loss: 1.00099492\n",
      "(8.85 min) Epoch 45/300 -- Iteration 42471 - Batch 99/963 - Train loss: 0.29062503  - Train acc: 0.8901 - Val loss: 1.00099492\n",
      "(8.86 min) Epoch 45/300 -- Iteration 42480 - Batch 108/963 - Train loss: 0.29045087  - Train acc: 0.8904 - Val loss: 1.00099492\n",
      "(8.86 min) Epoch 45/300 -- Iteration 42489 - Batch 117/963 - Train loss: 0.29100192  - Train acc: 0.8901 - Val loss: 1.00099492\n",
      "(8.86 min) Epoch 45/300 -- Iteration 42498 - Batch 126/963 - Train loss: 0.29097899  - Train acc: 0.8899 - Val loss: 1.00099492\n",
      "(8.86 min) Epoch 45/300 -- Iteration 42507 - Batch 135/963 - Train loss: 0.29068515  - Train acc: 0.8902 - Val loss: 1.00099492\n",
      "(8.86 min) Epoch 45/300 -- Iteration 42516 - Batch 144/963 - Train loss: 0.29170985  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(8.87 min) Epoch 45/300 -- Iteration 42525 - Batch 153/963 - Train loss: 0.29220373  - Train acc: 0.8893 - Val loss: 1.00099492\n",
      "(8.87 min) Epoch 45/300 -- Iteration 42534 - Batch 162/963 - Train loss: 0.29207589  - Train acc: 0.8893 - Val loss: 1.00099492\n",
      "(8.87 min) Epoch 45/300 -- Iteration 42543 - Batch 171/963 - Train loss: 0.29165495  - Train acc: 0.8894 - Val loss: 1.00099492\n",
      "(8.87 min) Epoch 45/300 -- Iteration 42552 - Batch 180/963 - Train loss: 0.29251466  - Train acc: 0.8887 - Val loss: 1.00099492\n",
      "(8.87 min) Epoch 45/300 -- Iteration 42561 - Batch 189/963 - Train loss: 0.29172102  - Train acc: 0.8890 - Val loss: 1.00099492\n",
      "(8.88 min) Epoch 45/300 -- Iteration 42570 - Batch 198/963 - Train loss: 0.29310512  - Train acc: 0.8880 - Val loss: 1.00099492\n",
      "(8.88 min) Epoch 45/300 -- Iteration 42579 - Batch 207/963 - Train loss: 0.29261418  - Train acc: 0.8878 - Val loss: 1.00099492\n",
      "(8.88 min) Epoch 45/300 -- Iteration 42588 - Batch 216/963 - Train loss: 0.29309601  - Train acc: 0.8876 - Val loss: 1.00099492\n",
      "(8.88 min) Epoch 45/300 -- Iteration 42597 - Batch 225/963 - Train loss: 0.29191295  - Train acc: 0.8883 - Val loss: 1.00099492\n",
      "(8.88 min) Epoch 45/300 -- Iteration 42606 - Batch 234/963 - Train loss: 0.29112701  - Train acc: 0.8888 - Val loss: 1.00099492\n",
      "(8.88 min) Epoch 45/300 -- Iteration 42615 - Batch 243/963 - Train loss: 0.29073138  - Train acc: 0.8887 - Val loss: 1.00099492\n",
      "(8.89 min) Epoch 45/300 -- Iteration 42624 - Batch 252/963 - Train loss: 0.28983870  - Train acc: 0.8893 - Val loss: 1.00099492\n",
      "(8.89 min) Epoch 45/300 -- Iteration 42633 - Batch 261/963 - Train loss: 0.28982374  - Train acc: 0.8893 - Val loss: 1.00099492\n",
      "(8.89 min) Epoch 45/300 -- Iteration 42642 - Batch 270/963 - Train loss: 0.29065069  - Train acc: 0.8890 - Val loss: 1.00099492\n",
      "(8.89 min) Epoch 45/300 -- Iteration 42651 - Batch 279/963 - Train loss: 0.29086674  - Train acc: 0.8889 - Val loss: 1.00099492\n",
      "(8.89 min) Epoch 45/300 -- Iteration 42660 - Batch 288/963 - Train loss: 0.29120317  - Train acc: 0.8887 - Val loss: 1.00099492\n",
      "(8.90 min) Epoch 45/300 -- Iteration 42669 - Batch 297/963 - Train loss: 0.29100925  - Train acc: 0.8890 - Val loss: 1.00099492\n",
      "(8.90 min) Epoch 45/300 -- Iteration 42678 - Batch 306/963 - Train loss: 0.29020936  - Train acc: 0.8892 - Val loss: 1.00099492\n",
      "(8.90 min) Epoch 45/300 -- Iteration 42687 - Batch 315/963 - Train loss: 0.29017604  - Train acc: 0.8889 - Val loss: 1.00099492\n",
      "(8.90 min) Epoch 45/300 -- Iteration 42696 - Batch 324/963 - Train loss: 0.29073581  - Train acc: 0.8890 - Val loss: 1.00099492\n",
      "(8.90 min) Epoch 45/300 -- Iteration 42705 - Batch 333/963 - Train loss: 0.29071071  - Train acc: 0.8891 - Val loss: 1.00099492\n",
      "(8.91 min) Epoch 45/300 -- Iteration 42714 - Batch 342/963 - Train loss: 0.29111973  - Train acc: 0.8890 - Val loss: 1.00099492\n",
      "(8.91 min) Epoch 45/300 -- Iteration 42723 - Batch 351/963 - Train loss: 0.29071504  - Train acc: 0.8892 - Val loss: 1.00099492\n",
      "(8.91 min) Epoch 45/300 -- Iteration 42732 - Batch 360/963 - Train loss: 0.29089493  - Train acc: 0.8890 - Val loss: 1.00099492\n",
      "(8.91 min) Epoch 45/300 -- Iteration 42741 - Batch 369/963 - Train loss: 0.29057752  - Train acc: 0.8891 - Val loss: 1.00099492\n",
      "(8.91 min) Epoch 45/300 -- Iteration 42750 - Batch 378/963 - Train loss: 0.29016256  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(8.92 min) Epoch 45/300 -- Iteration 42759 - Batch 387/963 - Train loss: 0.29071135  - Train acc: 0.8894 - Val loss: 1.00099492\n",
      "(8.92 min) Epoch 45/300 -- Iteration 42768 - Batch 396/963 - Train loss: 0.29112791  - Train acc: 0.8890 - Val loss: 1.00099492\n",
      "(8.92 min) Epoch 45/300 -- Iteration 42777 - Batch 405/963 - Train loss: 0.29120644  - Train acc: 0.8889 - Val loss: 1.00099492\n",
      "(8.92 min) Epoch 45/300 -- Iteration 42786 - Batch 414/963 - Train loss: 0.29163953  - Train acc: 0.8888 - Val loss: 1.00099492\n",
      "(8.92 min) Epoch 45/300 -- Iteration 42795 - Batch 423/963 - Train loss: 0.29139012  - Train acc: 0.8889 - Val loss: 1.00099492\n",
      "(8.92 min) Epoch 45/300 -- Iteration 42804 - Batch 432/963 - Train loss: 0.29091758  - Train acc: 0.8891 - Val loss: 1.00099492\n",
      "(8.93 min) Epoch 45/300 -- Iteration 42813 - Batch 441/963 - Train loss: 0.29060287  - Train acc: 0.8893 - Val loss: 1.00099492\n",
      "(8.93 min) Epoch 45/300 -- Iteration 42822 - Batch 450/963 - Train loss: 0.29047375  - Train acc: 0.8893 - Val loss: 1.00099492\n",
      "(8.93 min) Epoch 45/300 -- Iteration 42831 - Batch 459/963 - Train loss: 0.29049360  - Train acc: 0.8892 - Val loss: 1.00099492\n",
      "(8.93 min) Epoch 45/300 -- Iteration 42840 - Batch 468/963 - Train loss: 0.29008338  - Train acc: 0.8893 - Val loss: 1.00099492\n",
      "(8.93 min) Epoch 45/300 -- Iteration 42849 - Batch 477/963 - Train loss: 0.28963030  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(8.94 min) Epoch 45/300 -- Iteration 42858 - Batch 486/963 - Train loss: 0.28953381  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(8.94 min) Epoch 45/300 -- Iteration 42867 - Batch 495/963 - Train loss: 0.28941931  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(8.94 min) Epoch 45/300 -- Iteration 42876 - Batch 504/963 - Train loss: 0.28966771  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(8.94 min) Epoch 45/300 -- Iteration 42885 - Batch 513/963 - Train loss: 0.29008650  - Train acc: 0.8893 - Val loss: 1.00099492\n",
      "(8.94 min) Epoch 45/300 -- Iteration 42894 - Batch 522/963 - Train loss: 0.29021838  - Train acc: 0.8894 - Val loss: 1.00099492\n",
      "(8.95 min) Epoch 45/300 -- Iteration 42903 - Batch 531/963 - Train loss: 0.28997227  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(8.95 min) Epoch 45/300 -- Iteration 42912 - Batch 540/963 - Train loss: 0.28995466  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(8.95 min) Epoch 45/300 -- Iteration 42921 - Batch 549/963 - Train loss: 0.28981959  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(8.95 min) Epoch 45/300 -- Iteration 42930 - Batch 558/963 - Train loss: 0.29012909  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(8.95 min) Epoch 45/300 -- Iteration 42939 - Batch 567/963 - Train loss: 0.28996708  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(8.95 min) Epoch 45/300 -- Iteration 42948 - Batch 576/963 - Train loss: 0.28966879  - Train acc: 0.8899 - Val loss: 1.00099492\n",
      "(8.96 min) Epoch 45/300 -- Iteration 42957 - Batch 585/963 - Train loss: 0.28958967  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(8.96 min) Epoch 45/300 -- Iteration 42966 - Batch 594/963 - Train loss: 0.28965911  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(8.96 min) Epoch 45/300 -- Iteration 42975 - Batch 603/963 - Train loss: 0.28966908  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(8.96 min) Epoch 45/300 -- Iteration 42984 - Batch 612/963 - Train loss: 0.28951473  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(8.96 min) Epoch 45/300 -- Iteration 42993 - Batch 621/963 - Train loss: 0.28958666  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(8.97 min) Epoch 45/300 -- Iteration 43002 - Batch 630/963 - Train loss: 0.28989076  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(8.97 min) Epoch 45/300 -- Iteration 43011 - Batch 639/963 - Train loss: 0.29014601  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(8.97 min) Epoch 45/300 -- Iteration 43020 - Batch 648/963 - Train loss: 0.28991828  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(8.97 min) Epoch 45/300 -- Iteration 43029 - Batch 657/963 - Train loss: 0.28992701  - Train acc: 0.8899 - Val loss: 1.00099492\n",
      "(8.97 min) Epoch 45/300 -- Iteration 43038 - Batch 666/963 - Train loss: 0.28992802  - Train acc: 0.8899 - Val loss: 1.00099492\n",
      "(8.98 min) Epoch 45/300 -- Iteration 43047 - Batch 675/963 - Train loss: 0.29032293  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(8.98 min) Epoch 45/300 -- Iteration 43056 - Batch 684/963 - Train loss: 0.29043199  - Train acc: 0.8899 - Val loss: 1.00099492\n",
      "(8.98 min) Epoch 45/300 -- Iteration 43065 - Batch 693/963 - Train loss: 0.29043031  - Train acc: 0.8899 - Val loss: 1.00099492\n",
      "(8.98 min) Epoch 45/300 -- Iteration 43074 - Batch 702/963 - Train loss: 0.29040855  - Train acc: 0.8899 - Val loss: 1.00099492\n",
      "(8.98 min) Epoch 45/300 -- Iteration 43083 - Batch 711/963 - Train loss: 0.29052710  - Train acc: 0.8899 - Val loss: 1.00099492\n",
      "(8.98 min) Epoch 45/300 -- Iteration 43092 - Batch 720/963 - Train loss: 0.29091451  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(8.99 min) Epoch 45/300 -- Iteration 43101 - Batch 729/963 - Train loss: 0.29114928  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(8.99 min) Epoch 45/300 -- Iteration 43110 - Batch 738/963 - Train loss: 0.29116593  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(8.99 min) Epoch 45/300 -- Iteration 43119 - Batch 747/963 - Train loss: 0.29136050  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(8.99 min) Epoch 45/300 -- Iteration 43128 - Batch 756/963 - Train loss: 0.29118634  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(8.99 min) Epoch 45/300 -- Iteration 43137 - Batch 765/963 - Train loss: 0.29098632  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(9.00 min) Epoch 45/300 -- Iteration 43146 - Batch 774/963 - Train loss: 0.29084185  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(9.00 min) Epoch 45/300 -- Iteration 43155 - Batch 783/963 - Train loss: 0.29071468  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(9.00 min) Epoch 45/300 -- Iteration 43164 - Batch 792/963 - Train loss: 0.29070021  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(9.00 min) Epoch 45/300 -- Iteration 43173 - Batch 801/963 - Train loss: 0.29061637  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(9.00 min) Epoch 45/300 -- Iteration 43182 - Batch 810/963 - Train loss: 0.29044084  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(9.01 min) Epoch 45/300 -- Iteration 43191 - Batch 819/963 - Train loss: 0.29054010  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(9.01 min) Epoch 45/300 -- Iteration 43200 - Batch 828/963 - Train loss: 0.29078345  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(9.01 min) Epoch 45/300 -- Iteration 43209 - Batch 837/963 - Train loss: 0.29087559  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(9.01 min) Epoch 45/300 -- Iteration 43218 - Batch 846/963 - Train loss: 0.29059634  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(9.01 min) Epoch 45/300 -- Iteration 43227 - Batch 855/963 - Train loss: 0.29078794  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(9.01 min) Epoch 45/300 -- Iteration 43236 - Batch 864/963 - Train loss: 0.29066412  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(9.02 min) Epoch 45/300 -- Iteration 43245 - Batch 873/963 - Train loss: 0.29091693  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(9.02 min) Epoch 45/300 -- Iteration 43254 - Batch 882/963 - Train loss: 0.29098426  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(9.02 min) Epoch 45/300 -- Iteration 43263 - Batch 891/963 - Train loss: 0.29112869  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(9.02 min) Epoch 45/300 -- Iteration 43272 - Batch 900/963 - Train loss: 0.29097350  - Train acc: 0.8894 - Val loss: 1.00099492\n",
      "(9.02 min) Epoch 45/300 -- Iteration 43281 - Batch 909/963 - Train loss: 0.29077789  - Train acc: 0.8895 - Val loss: 1.00099492\n",
      "(9.03 min) Epoch 45/300 -- Iteration 43290 - Batch 918/963 - Train loss: 0.29064196  - Train acc: 0.8896 - Val loss: 1.00099492\n",
      "(9.03 min) Epoch 45/300 -- Iteration 43299 - Batch 927/963 - Train loss: 0.29066514  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(9.03 min) Epoch 45/300 -- Iteration 43308 - Batch 936/963 - Train loss: 0.29056050  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(9.03 min) Epoch 45/300 -- Iteration 43317 - Batch 945/963 - Train loss: 0.29032734  - Train acc: 0.8897 - Val loss: 1.00099492\n",
      "(9.03 min) Epoch 45/300 -- Iteration 43326 - Batch 954/963 - Train loss: 0.29018558  - Train acc: 0.8898 - Val loss: 1.00099492\n",
      "(9.04 min) Epoch 45/300 -- Iteration 43335 - Batch 962/963 - Train loss: 0.29011565  - Train acc: 0.8898 - Val loss: 0.99690580 - Val acc: 0.5983\n",
      "(9.04 min) Epoch 46/300 -- Iteration 43344 - Batch 9/963 - Train loss: 0.26452484  - Train acc: 0.9016 - Val loss: 0.99690580\n",
      "(9.04 min) Epoch 46/300 -- Iteration 43353 - Batch 18/963 - Train loss: 0.29535929  - Train acc: 0.8914 - Val loss: 0.99690580\n",
      "(9.04 min) Epoch 46/300 -- Iteration 43362 - Batch 27/963 - Train loss: 0.29173462  - Train acc: 0.8906 - Val loss: 0.99690580\n",
      "(9.04 min) Epoch 46/300 -- Iteration 43371 - Batch 36/963 - Train loss: 0.29276025  - Train acc: 0.8894 - Val loss: 0.99690580\n",
      "(9.05 min) Epoch 46/300 -- Iteration 43380 - Batch 45/963 - Train loss: 0.29241718  - Train acc: 0.8884 - Val loss: 0.99690580\n",
      "(9.05 min) Epoch 46/300 -- Iteration 43389 - Batch 54/963 - Train loss: 0.29257700  - Train acc: 0.8885 - Val loss: 0.99690580\n",
      "(9.05 min) Epoch 46/300 -- Iteration 43398 - Batch 63/963 - Train loss: 0.29344539  - Train acc: 0.8884 - Val loss: 0.99690580\n",
      "(9.05 min) Epoch 46/300 -- Iteration 43407 - Batch 72/963 - Train loss: 0.29637618  - Train acc: 0.8871 - Val loss: 0.99690580\n",
      "(9.05 min) Epoch 46/300 -- Iteration 43416 - Batch 81/963 - Train loss: 0.29643120  - Train acc: 0.8876 - Val loss: 0.99690580\n",
      "(9.05 min) Epoch 46/300 -- Iteration 43425 - Batch 90/963 - Train loss: 0.29606138  - Train acc: 0.8880 - Val loss: 0.99690580\n",
      "(9.06 min) Epoch 46/300 -- Iteration 43434 - Batch 99/963 - Train loss: 0.29570416  - Train acc: 0.8880 - Val loss: 0.99690580\n",
      "(9.06 min) Epoch 46/300 -- Iteration 43443 - Batch 108/963 - Train loss: 0.29683485  - Train acc: 0.8870 - Val loss: 0.99690580\n",
      "(9.06 min) Epoch 46/300 -- Iteration 43452 - Batch 117/963 - Train loss: 0.29450804  - Train acc: 0.8877 - Val loss: 0.99690580\n",
      "(9.06 min) Epoch 46/300 -- Iteration 43461 - Batch 126/963 - Train loss: 0.29226769  - Train acc: 0.8886 - Val loss: 0.99690580\n",
      "(9.06 min) Epoch 46/300 -- Iteration 43470 - Batch 135/963 - Train loss: 0.29391557  - Train acc: 0.8880 - Val loss: 0.99690580\n",
      "(9.07 min) Epoch 46/300 -- Iteration 43479 - Batch 144/963 - Train loss: 0.29216129  - Train acc: 0.8885 - Val loss: 0.99690580\n",
      "(9.07 min) Epoch 46/300 -- Iteration 43488 - Batch 153/963 - Train loss: 0.29166947  - Train acc: 0.8886 - Val loss: 0.99690580\n",
      "(9.07 min) Epoch 46/300 -- Iteration 43497 - Batch 162/963 - Train loss: 0.29242845  - Train acc: 0.8883 - Val loss: 0.99690580\n",
      "(9.07 min) Epoch 46/300 -- Iteration 43506 - Batch 171/963 - Train loss: 0.29224369  - Train acc: 0.8884 - Val loss: 0.99690580\n",
      "(9.07 min) Epoch 46/300 -- Iteration 43515 - Batch 180/963 - Train loss: 0.29357005  - Train acc: 0.8876 - Val loss: 0.99690580\n",
      "(9.08 min) Epoch 46/300 -- Iteration 43524 - Batch 189/963 - Train loss: 0.29250831  - Train acc: 0.8881 - Val loss: 0.99690580\n",
      "(9.08 min) Epoch 46/300 -- Iteration 43533 - Batch 198/963 - Train loss: 0.29109595  - Train acc: 0.8887 - Val loss: 0.99690580\n",
      "(9.08 min) Epoch 46/300 -- Iteration 43542 - Batch 207/963 - Train loss: 0.29105358  - Train acc: 0.8887 - Val loss: 0.99690580\n",
      "(9.08 min) Epoch 46/300 -- Iteration 43551 - Batch 216/963 - Train loss: 0.29291342  - Train acc: 0.8885 - Val loss: 0.99690580\n",
      "(9.08 min) Epoch 46/300 -- Iteration 43560 - Batch 225/963 - Train loss: 0.29284408  - Train acc: 0.8881 - Val loss: 0.99690580\n",
      "(9.08 min) Epoch 46/300 -- Iteration 43569 - Batch 234/963 - Train loss: 0.29241124  - Train acc: 0.8886 - Val loss: 0.99690580\n",
      "(9.09 min) Epoch 46/300 -- Iteration 43578 - Batch 243/963 - Train loss: 0.29313668  - Train acc: 0.8883 - Val loss: 0.99690580\n",
      "(9.09 min) Epoch 46/300 -- Iteration 43587 - Batch 252/963 - Train loss: 0.29415180  - Train acc: 0.8880 - Val loss: 0.99690580\n",
      "(9.09 min) Epoch 46/300 -- Iteration 43596 - Batch 261/963 - Train loss: 0.29399888  - Train acc: 0.8877 - Val loss: 0.99690580\n",
      "(9.09 min) Epoch 46/300 -- Iteration 43605 - Batch 270/963 - Train loss: 0.29406065  - Train acc: 0.8877 - Val loss: 0.99690580\n",
      "(9.09 min) Epoch 46/300 -- Iteration 43614 - Batch 279/963 - Train loss: 0.29249981  - Train acc: 0.8884 - Val loss: 0.99690580\n",
      "(9.10 min) Epoch 46/300 -- Iteration 43623 - Batch 288/963 - Train loss: 0.29222168  - Train acc: 0.8886 - Val loss: 0.99690580\n",
      "(9.10 min) Epoch 46/300 -- Iteration 43632 - Batch 297/963 - Train loss: 0.29216580  - Train acc: 0.8882 - Val loss: 0.99690580\n",
      "(9.10 min) Epoch 46/300 -- Iteration 43641 - Batch 306/963 - Train loss: 0.29217515  - Train acc: 0.8884 - Val loss: 0.99690580\n",
      "(9.10 min) Epoch 46/300 -- Iteration 43650 - Batch 315/963 - Train loss: 0.29190012  - Train acc: 0.8885 - Val loss: 0.99690580\n",
      "(9.10 min) Epoch 46/300 -- Iteration 43659 - Batch 324/963 - Train loss: 0.29212305  - Train acc: 0.8885 - Val loss: 0.99690580\n",
      "(9.11 min) Epoch 46/300 -- Iteration 43668 - Batch 333/963 - Train loss: 0.29209059  - Train acc: 0.8884 - Val loss: 0.99690580\n",
      "(9.11 min) Epoch 46/300 -- Iteration 43677 - Batch 342/963 - Train loss: 0.29287808  - Train acc: 0.8881 - Val loss: 0.99690580\n",
      "(9.11 min) Epoch 46/300 -- Iteration 43686 - Batch 351/963 - Train loss: 0.29376482  - Train acc: 0.8877 - Val loss: 0.99690580\n",
      "(9.11 min) Epoch 46/300 -- Iteration 43695 - Batch 360/963 - Train loss: 0.29406662  - Train acc: 0.8879 - Val loss: 0.99690580\n",
      "(9.11 min) Epoch 46/300 -- Iteration 43704 - Batch 369/963 - Train loss: 0.29466047  - Train acc: 0.8878 - Val loss: 0.99690580\n",
      "(9.11 min) Epoch 46/300 -- Iteration 43713 - Batch 378/963 - Train loss: 0.29432929  - Train acc: 0.8878 - Val loss: 0.99690580\n",
      "(9.12 min) Epoch 46/300 -- Iteration 43722 - Batch 387/963 - Train loss: 0.29471116  - Train acc: 0.8877 - Val loss: 0.99690580\n",
      "(9.12 min) Epoch 46/300 -- Iteration 43731 - Batch 396/963 - Train loss: 0.29397498  - Train acc: 0.8879 - Val loss: 0.99690580\n",
      "(9.12 min) Epoch 46/300 -- Iteration 43740 - Batch 405/963 - Train loss: 0.29457672  - Train acc: 0.8876 - Val loss: 0.99690580\n",
      "(9.12 min) Epoch 46/300 -- Iteration 43749 - Batch 414/963 - Train loss: 0.29462985  - Train acc: 0.8877 - Val loss: 0.99690580\n",
      "(9.12 min) Epoch 46/300 -- Iteration 43758 - Batch 423/963 - Train loss: 0.29351878  - Train acc: 0.8880 - Val loss: 0.99690580\n",
      "(9.13 min) Epoch 46/300 -- Iteration 43767 - Batch 432/963 - Train loss: 0.29387550  - Train acc: 0.8879 - Val loss: 0.99690580\n",
      "(9.13 min) Epoch 46/300 -- Iteration 43776 - Batch 441/963 - Train loss: 0.29359486  - Train acc: 0.8882 - Val loss: 0.99690580\n",
      "(9.13 min) Epoch 46/300 -- Iteration 43785 - Batch 450/963 - Train loss: 0.29365345  - Train acc: 0.8880 - Val loss: 0.99690580\n",
      "(9.13 min) Epoch 46/300 -- Iteration 43794 - Batch 459/963 - Train loss: 0.29375605  - Train acc: 0.8881 - Val loss: 0.99690580\n",
      "(9.13 min) Epoch 46/300 -- Iteration 43803 - Batch 468/963 - Train loss: 0.29438908  - Train acc: 0.8879 - Val loss: 0.99690580\n",
      "(9.14 min) Epoch 46/300 -- Iteration 43812 - Batch 477/963 - Train loss: 0.29476381  - Train acc: 0.8877 - Val loss: 0.99690580\n",
      "(9.14 min) Epoch 46/300 -- Iteration 43821 - Batch 486/963 - Train loss: 0.29486425  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.14 min) Epoch 46/300 -- Iteration 43830 - Batch 495/963 - Train loss: 0.29485069  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.14 min) Epoch 46/300 -- Iteration 43839 - Batch 504/963 - Train loss: 0.29500680  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.14 min) Epoch 46/300 -- Iteration 43848 - Batch 513/963 - Train loss: 0.29554931  - Train acc: 0.8871 - Val loss: 0.99690580\n",
      "(9.14 min) Epoch 46/300 -- Iteration 43857 - Batch 522/963 - Train loss: 0.29586716  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.15 min) Epoch 46/300 -- Iteration 43866 - Batch 531/963 - Train loss: 0.29578464  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.15 min) Epoch 46/300 -- Iteration 43875 - Batch 540/963 - Train loss: 0.29561562  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.15 min) Epoch 46/300 -- Iteration 43884 - Batch 549/963 - Train loss: 0.29582240  - Train acc: 0.8871 - Val loss: 0.99690580\n",
      "(9.15 min) Epoch 46/300 -- Iteration 43893 - Batch 558/963 - Train loss: 0.29566930  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.15 min) Epoch 46/300 -- Iteration 43902 - Batch 567/963 - Train loss: 0.29557793  - Train acc: 0.8871 - Val loss: 0.99690580\n",
      "(9.16 min) Epoch 46/300 -- Iteration 43911 - Batch 576/963 - Train loss: 0.29548769  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.16 min) Epoch 46/300 -- Iteration 43920 - Batch 585/963 - Train loss: 0.29544634  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.16 min) Epoch 46/300 -- Iteration 43929 - Batch 594/963 - Train loss: 0.29518804  - Train acc: 0.8874 - Val loss: 0.99690580\n",
      "(9.16 min) Epoch 46/300 -- Iteration 43938 - Batch 603/963 - Train loss: 0.29485087  - Train acc: 0.8874 - Val loss: 0.99690580\n",
      "(9.16 min) Epoch 46/300 -- Iteration 43947 - Batch 612/963 - Train loss: 0.29475820  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.16 min) Epoch 46/300 -- Iteration 43956 - Batch 621/963 - Train loss: 0.29507538  - Train acc: 0.8873 - Val loss: 0.99690580\n",
      "(9.17 min) Epoch 46/300 -- Iteration 43965 - Batch 630/963 - Train loss: 0.29495345  - Train acc: 0.8874 - Val loss: 0.99690580\n",
      "(9.17 min) Epoch 46/300 -- Iteration 43974 - Batch 639/963 - Train loss: 0.29511031  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.17 min) Epoch 46/300 -- Iteration 43983 - Batch 648/963 - Train loss: 0.29520639  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.17 min) Epoch 46/300 -- Iteration 43992 - Batch 657/963 - Train loss: 0.29563366  - Train acc: 0.8874 - Val loss: 0.99690580\n",
      "(9.17 min) Epoch 46/300 -- Iteration 44001 - Batch 666/963 - Train loss: 0.29543734  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.18 min) Epoch 46/300 -- Iteration 44010 - Batch 675/963 - Train loss: 0.29549808  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.18 min) Epoch 46/300 -- Iteration 44019 - Batch 684/963 - Train loss: 0.29508022  - Train acc: 0.8876 - Val loss: 0.99690580\n",
      "(9.18 min) Epoch 46/300 -- Iteration 44028 - Batch 693/963 - Train loss: 0.29498583  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.18 min) Epoch 46/300 -- Iteration 44037 - Batch 702/963 - Train loss: 0.29495826  - Train acc: 0.8875 - Val loss: 0.99690580\n",
      "(9.18 min) Epoch 46/300 -- Iteration 44046 - Batch 711/963 - Train loss: 0.29507634  - Train acc: 0.8874 - Val loss: 0.99690580\n",
      "(9.19 min) Epoch 46/300 -- Iteration 44055 - Batch 720/963 - Train loss: 0.29515932  - Train acc: 0.8873 - Val loss: 0.99690580\n",
      "(9.19 min) Epoch 46/300 -- Iteration 44064 - Batch 729/963 - Train loss: 0.29544353  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.19 min) Epoch 46/300 -- Iteration 44073 - Batch 738/963 - Train loss: 0.29543987  - Train acc: 0.8873 - Val loss: 0.99690580\n",
      "(9.19 min) Epoch 46/300 -- Iteration 44082 - Batch 747/963 - Train loss: 0.29552911  - Train acc: 0.8873 - Val loss: 0.99690580\n",
      "(9.19 min) Epoch 46/300 -- Iteration 44091 - Batch 756/963 - Train loss: 0.29556704  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.19 min) Epoch 46/300 -- Iteration 44100 - Batch 765/963 - Train loss: 0.29555707  - Train acc: 0.8873 - Val loss: 0.99690580\n",
      "(9.20 min) Epoch 46/300 -- Iteration 44109 - Batch 774/963 - Train loss: 0.29559582  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.20 min) Epoch 46/300 -- Iteration 44118 - Batch 783/963 - Train loss: 0.29576875  - Train acc: 0.8871 - Val loss: 0.99690580\n",
      "(9.20 min) Epoch 46/300 -- Iteration 44127 - Batch 792/963 - Train loss: 0.29596618  - Train acc: 0.8870 - Val loss: 0.99690580\n",
      "(9.20 min) Epoch 46/300 -- Iteration 44136 - Batch 801/963 - Train loss: 0.29595211  - Train acc: 0.8870 - Val loss: 0.99690580\n",
      "(9.20 min) Epoch 46/300 -- Iteration 44145 - Batch 810/963 - Train loss: 0.29573357  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.21 min) Epoch 46/300 -- Iteration 44154 - Batch 819/963 - Train loss: 0.29588281  - Train acc: 0.8870 - Val loss: 0.99690580\n",
      "(9.21 min) Epoch 46/300 -- Iteration 44163 - Batch 828/963 - Train loss: 0.29580865  - Train acc: 0.8870 - Val loss: 0.99690580\n",
      "(9.21 min) Epoch 46/300 -- Iteration 44172 - Batch 837/963 - Train loss: 0.29579538  - Train acc: 0.8870 - Val loss: 0.99690580\n",
      "(9.21 min) Epoch 46/300 -- Iteration 44181 - Batch 846/963 - Train loss: 0.29569637  - Train acc: 0.8871 - Val loss: 0.99690580\n",
      "(9.21 min) Epoch 46/300 -- Iteration 44190 - Batch 855/963 - Train loss: 0.29586601  - Train acc: 0.8870 - Val loss: 0.99690580\n",
      "(9.22 min) Epoch 46/300 -- Iteration 44199 - Batch 864/963 - Train loss: 0.29591330  - Train acc: 0.8870 - Val loss: 0.99690580\n",
      "(9.22 min) Epoch 46/300 -- Iteration 44208 - Batch 873/963 - Train loss: 0.29565092  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.22 min) Epoch 46/300 -- Iteration 44217 - Batch 882/963 - Train loss: 0.29541161  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.22 min) Epoch 46/300 -- Iteration 44226 - Batch 891/963 - Train loss: 0.29542608  - Train acc: 0.8873 - Val loss: 0.99690580\n",
      "(9.22 min) Epoch 46/300 -- Iteration 44235 - Batch 900/963 - Train loss: 0.29525789  - Train acc: 0.8874 - Val loss: 0.99690580\n",
      "(9.22 min) Epoch 46/300 -- Iteration 44244 - Batch 909/963 - Train loss: 0.29566628  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.23 min) Epoch 46/300 -- Iteration 44253 - Batch 918/963 - Train loss: 0.29559597  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.23 min) Epoch 46/300 -- Iteration 44262 - Batch 927/963 - Train loss: 0.29541847  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.23 min) Epoch 46/300 -- Iteration 44271 - Batch 936/963 - Train loss: 0.29528770  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.23 min) Epoch 46/300 -- Iteration 44280 - Batch 945/963 - Train loss: 0.29545526  - Train acc: 0.8871 - Val loss: 0.99690580\n",
      "(9.23 min) Epoch 46/300 -- Iteration 44289 - Batch 954/963 - Train loss: 0.29544152  - Train acc: 0.8872 - Val loss: 0.99690580\n",
      "(9.24 min) Epoch 46/300 -- Iteration 44298 - Batch 962/963 - Train loss: 0.29551332  - Train acc: 0.8872 - Val loss: 0.98880166 - Val acc: 0.5967\n",
      "(9.24 min) Epoch 47/300 -- Iteration 44307 - Batch 9/963 - Train loss: 0.31161758  - Train acc: 0.8789 - Val loss: 0.98880166\n",
      "(9.24 min) Epoch 47/300 -- Iteration 44316 - Batch 18/963 - Train loss: 0.29300933  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.24 min) Epoch 47/300 -- Iteration 44325 - Batch 27/963 - Train loss: 0.29953311  - Train acc: 0.8836 - Val loss: 0.98880166\n",
      "(9.24 min) Epoch 47/300 -- Iteration 44334 - Batch 36/963 - Train loss: 0.30082400  - Train acc: 0.8849 - Val loss: 0.98880166\n",
      "(9.25 min) Epoch 47/300 -- Iteration 44343 - Batch 45/963 - Train loss: 0.30081970  - Train acc: 0.8852 - Val loss: 0.98880166\n",
      "(9.25 min) Epoch 47/300 -- Iteration 44352 - Batch 54/963 - Train loss: 0.29978474  - Train acc: 0.8837 - Val loss: 0.98880166\n",
      "(9.25 min) Epoch 47/300 -- Iteration 44361 - Batch 63/963 - Train loss: 0.29752105  - Train acc: 0.8848 - Val loss: 0.98880166\n",
      "(9.25 min) Epoch 47/300 -- Iteration 44370 - Batch 72/963 - Train loss: 0.29554699  - Train acc: 0.8860 - Val loss: 0.98880166\n",
      "(9.25 min) Epoch 47/300 -- Iteration 44379 - Batch 81/963 - Train loss: 0.29462353  - Train acc: 0.8865 - Val loss: 0.98880166\n",
      "(9.26 min) Epoch 47/300 -- Iteration 44388 - Batch 90/963 - Train loss: 0.29371820  - Train acc: 0.8875 - Val loss: 0.98880166\n",
      "(9.26 min) Epoch 47/300 -- Iteration 44397 - Batch 99/963 - Train loss: 0.29132213  - Train acc: 0.8881 - Val loss: 0.98880166\n",
      "(9.26 min) Epoch 47/300 -- Iteration 44406 - Batch 108/963 - Train loss: 0.29194529  - Train acc: 0.8880 - Val loss: 0.98880166\n",
      "(9.26 min) Epoch 47/300 -- Iteration 44415 - Batch 117/963 - Train loss: 0.29172346  - Train acc: 0.8882 - Val loss: 0.98880166\n",
      "(9.26 min) Epoch 47/300 -- Iteration 44424 - Batch 126/963 - Train loss: 0.29183509  - Train acc: 0.8882 - Val loss: 0.98880166\n",
      "(9.26 min) Epoch 47/300 -- Iteration 44433 - Batch 135/963 - Train loss: 0.29086284  - Train acc: 0.8881 - Val loss: 0.98880166\n",
      "(9.27 min) Epoch 47/300 -- Iteration 44442 - Batch 144/963 - Train loss: 0.29115627  - Train acc: 0.8873 - Val loss: 0.98880166\n",
      "(9.27 min) Epoch 47/300 -- Iteration 44451 - Batch 153/963 - Train loss: 0.29099916  - Train acc: 0.8873 - Val loss: 0.98880166\n",
      "(9.27 min) Epoch 47/300 -- Iteration 44460 - Batch 162/963 - Train loss: 0.28996261  - Train acc: 0.8875 - Val loss: 0.98880166\n",
      "(9.27 min) Epoch 47/300 -- Iteration 44469 - Batch 171/963 - Train loss: 0.28911076  - Train acc: 0.8879 - Val loss: 0.98880166\n",
      "(9.27 min) Epoch 47/300 -- Iteration 44478 - Batch 180/963 - Train loss: 0.28841655  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.28 min) Epoch 47/300 -- Iteration 44487 - Batch 189/963 - Train loss: 0.28910600  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.28 min) Epoch 47/300 -- Iteration 44496 - Batch 198/963 - Train loss: 0.29020546  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.28 min) Epoch 47/300 -- Iteration 44505 - Batch 207/963 - Train loss: 0.29003694  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.28 min) Epoch 47/300 -- Iteration 44514 - Batch 216/963 - Train loss: 0.29020191  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.28 min) Epoch 47/300 -- Iteration 44523 - Batch 225/963 - Train loss: 0.29114180  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.28 min) Epoch 47/300 -- Iteration 44532 - Batch 234/963 - Train loss: 0.29103336  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.29 min) Epoch 47/300 -- Iteration 44541 - Batch 243/963 - Train loss: 0.29196216  - Train acc: 0.8883 - Val loss: 0.98880166\n",
      "(9.29 min) Epoch 47/300 -- Iteration 44550 - Batch 252/963 - Train loss: 0.29222642  - Train acc: 0.8885 - Val loss: 0.98880166\n",
      "(9.29 min) Epoch 47/300 -- Iteration 44559 - Batch 261/963 - Train loss: 0.29258787  - Train acc: 0.8884 - Val loss: 0.98880166\n",
      "(9.29 min) Epoch 47/300 -- Iteration 44568 - Batch 270/963 - Train loss: 0.29235792  - Train acc: 0.8882 - Val loss: 0.98880166\n",
      "(9.29 min) Epoch 47/300 -- Iteration 44577 - Batch 279/963 - Train loss: 0.29390635  - Train acc: 0.8876 - Val loss: 0.98880166\n",
      "(9.30 min) Epoch 47/300 -- Iteration 44586 - Batch 288/963 - Train loss: 0.29347061  - Train acc: 0.8877 - Val loss: 0.98880166\n",
      "(9.30 min) Epoch 47/300 -- Iteration 44595 - Batch 297/963 - Train loss: 0.29437500  - Train acc: 0.8872 - Val loss: 0.98880166\n",
      "(9.30 min) Epoch 47/300 -- Iteration 44604 - Batch 306/963 - Train loss: 0.29420428  - Train acc: 0.8873 - Val loss: 0.98880166\n",
      "(9.30 min) Epoch 47/300 -- Iteration 44613 - Batch 315/963 - Train loss: 0.29438776  - Train acc: 0.8873 - Val loss: 0.98880166\n",
      "(9.30 min) Epoch 47/300 -- Iteration 44622 - Batch 324/963 - Train loss: 0.29460914  - Train acc: 0.8874 - Val loss: 0.98880166\n",
      "(9.31 min) Epoch 47/300 -- Iteration 44631 - Batch 333/963 - Train loss: 0.29442754  - Train acc: 0.8876 - Val loss: 0.98880166\n",
      "(9.31 min) Epoch 47/300 -- Iteration 44640 - Batch 342/963 - Train loss: 0.29429427  - Train acc: 0.8876 - Val loss: 0.98880166\n",
      "(9.31 min) Epoch 47/300 -- Iteration 44649 - Batch 351/963 - Train loss: 0.29354793  - Train acc: 0.8879 - Val loss: 0.98880166\n",
      "(9.31 min) Epoch 47/300 -- Iteration 44658 - Batch 360/963 - Train loss: 0.29328719  - Train acc: 0.8880 - Val loss: 0.98880166\n",
      "(9.31 min) Epoch 47/300 -- Iteration 44667 - Batch 369/963 - Train loss: 0.29350083  - Train acc: 0.8878 - Val loss: 0.98880166\n",
      "(9.31 min) Epoch 47/300 -- Iteration 44676 - Batch 378/963 - Train loss: 0.29407631  - Train acc: 0.8875 - Val loss: 0.98880166\n",
      "(9.32 min) Epoch 47/300 -- Iteration 44685 - Batch 387/963 - Train loss: 0.29417653  - Train acc: 0.8875 - Val loss: 0.98880166\n",
      "(9.32 min) Epoch 47/300 -- Iteration 44694 - Batch 396/963 - Train loss: 0.29408096  - Train acc: 0.8874 - Val loss: 0.98880166\n",
      "(9.32 min) Epoch 47/300 -- Iteration 44703 - Batch 405/963 - Train loss: 0.29381767  - Train acc: 0.8875 - Val loss: 0.98880166\n",
      "(9.32 min) Epoch 47/300 -- Iteration 44712 - Batch 414/963 - Train loss: 0.29412503  - Train acc: 0.8873 - Val loss: 0.98880166\n",
      "(9.32 min) Epoch 47/300 -- Iteration 44721 - Batch 423/963 - Train loss: 0.29422768  - Train acc: 0.8872 - Val loss: 0.98880166\n",
      "(9.33 min) Epoch 47/300 -- Iteration 44730 - Batch 432/963 - Train loss: 0.29359170  - Train acc: 0.8876 - Val loss: 0.98880166\n",
      "(9.33 min) Epoch 47/300 -- Iteration 44739 - Batch 441/963 - Train loss: 0.29336635  - Train acc: 0.8880 - Val loss: 0.98880166\n",
      "(9.33 min) Epoch 47/300 -- Iteration 44748 - Batch 450/963 - Train loss: 0.29305264  - Train acc: 0.8880 - Val loss: 0.98880166\n",
      "(9.33 min) Epoch 47/300 -- Iteration 44757 - Batch 459/963 - Train loss: 0.29251527  - Train acc: 0.8884 - Val loss: 0.98880166\n",
      "(9.33 min) Epoch 47/300 -- Iteration 44766 - Batch 468/963 - Train loss: 0.29220715  - Train acc: 0.8885 - Val loss: 0.98880166\n",
      "(9.34 min) Epoch 47/300 -- Iteration 44775 - Batch 477/963 - Train loss: 0.29210308  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.34 min) Epoch 47/300 -- Iteration 44784 - Batch 486/963 - Train loss: 0.29166737  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.34 min) Epoch 47/300 -- Iteration 44793 - Batch 495/963 - Train loss: 0.29196352  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.34 min) Epoch 47/300 -- Iteration 44802 - Batch 504/963 - Train loss: 0.29164793  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.34 min) Epoch 47/300 -- Iteration 44811 - Batch 513/963 - Train loss: 0.29170684  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.35 min) Epoch 47/300 -- Iteration 44820 - Batch 522/963 - Train loss: 0.29170764  - Train acc: 0.8889 - Val loss: 0.98880166\n",
      "(9.35 min) Epoch 47/300 -- Iteration 44829 - Batch 531/963 - Train loss: 0.29142873  - Train acc: 0.8890 - Val loss: 0.98880166\n",
      "(9.35 min) Epoch 47/300 -- Iteration 44838 - Batch 540/963 - Train loss: 0.29111164  - Train acc: 0.8893 - Val loss: 0.98880166\n",
      "(9.35 min) Epoch 47/300 -- Iteration 44847 - Batch 549/963 - Train loss: 0.29109949  - Train acc: 0.8892 - Val loss: 0.98880166\n",
      "(9.35 min) Epoch 47/300 -- Iteration 44856 - Batch 558/963 - Train loss: 0.29094777  - Train acc: 0.8892 - Val loss: 0.98880166\n",
      "(9.35 min) Epoch 47/300 -- Iteration 44865 - Batch 567/963 - Train loss: 0.29136385  - Train acc: 0.8890 - Val loss: 0.98880166\n",
      "(9.36 min) Epoch 47/300 -- Iteration 44874 - Batch 576/963 - Train loss: 0.29150653  - Train acc: 0.8889 - Val loss: 0.98880166\n",
      "(9.36 min) Epoch 47/300 -- Iteration 44883 - Batch 585/963 - Train loss: 0.29189990  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.36 min) Epoch 47/300 -- Iteration 44892 - Batch 594/963 - Train loss: 0.29234538  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.36 min) Epoch 47/300 -- Iteration 44901 - Batch 603/963 - Train loss: 0.29254497  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.36 min) Epoch 47/300 -- Iteration 44910 - Batch 612/963 - Train loss: 0.29302288  - Train acc: 0.8884 - Val loss: 0.98880166\n",
      "(9.37 min) Epoch 47/300 -- Iteration 44919 - Batch 621/963 - Train loss: 0.29308746  - Train acc: 0.8883 - Val loss: 0.98880166\n",
      "(9.37 min) Epoch 47/300 -- Iteration 44928 - Batch 630/963 - Train loss: 0.29345242  - Train acc: 0.8881 - Val loss: 0.98880166\n",
      "(9.37 min) Epoch 47/300 -- Iteration 44937 - Batch 639/963 - Train loss: 0.29331217  - Train acc: 0.8882 - Val loss: 0.98880166\n",
      "(9.37 min) Epoch 47/300 -- Iteration 44946 - Batch 648/963 - Train loss: 0.29325695  - Train acc: 0.8883 - Val loss: 0.98880166\n",
      "(9.37 min) Epoch 47/300 -- Iteration 44955 - Batch 657/963 - Train loss: 0.29377586  - Train acc: 0.8879 - Val loss: 0.98880166\n",
      "(9.38 min) Epoch 47/300 -- Iteration 44964 - Batch 666/963 - Train loss: 0.29369184  - Train acc: 0.8880 - Val loss: 0.98880166\n",
      "(9.38 min) Epoch 47/300 -- Iteration 44973 - Batch 675/963 - Train loss: 0.29362448  - Train acc: 0.8882 - Val loss: 0.98880166\n",
      "(9.38 min) Epoch 47/300 -- Iteration 44982 - Batch 684/963 - Train loss: 0.29355234  - Train acc: 0.8883 - Val loss: 0.98880166\n",
      "(9.38 min) Epoch 47/300 -- Iteration 44991 - Batch 693/963 - Train loss: 0.29342301  - Train acc: 0.8883 - Val loss: 0.98880166\n",
      "(9.38 min) Epoch 47/300 -- Iteration 45000 - Batch 702/963 - Train loss: 0.29357004  - Train acc: 0.8883 - Val loss: 0.98880166\n",
      "(9.38 min) Epoch 47/300 -- Iteration 45009 - Batch 711/963 - Train loss: 0.29326540  - Train acc: 0.8884 - Val loss: 0.98880166\n",
      "(9.39 min) Epoch 47/300 -- Iteration 45018 - Batch 720/963 - Train loss: 0.29285597  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.39 min) Epoch 47/300 -- Iteration 45027 - Batch 729/963 - Train loss: 0.29291962  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.39 min) Epoch 47/300 -- Iteration 45036 - Batch 738/963 - Train loss: 0.29270397  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.39 min) Epoch 47/300 -- Iteration 45045 - Batch 747/963 - Train loss: 0.29284172  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.39 min) Epoch 47/300 -- Iteration 45054 - Batch 756/963 - Train loss: 0.29288353  - Train acc: 0.8885 - Val loss: 0.98880166\n",
      "(9.40 min) Epoch 47/300 -- Iteration 45063 - Batch 765/963 - Train loss: 0.29258908  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.40 min) Epoch 47/300 -- Iteration 45072 - Batch 774/963 - Train loss: 0.29247122  - Train acc: 0.8889 - Val loss: 0.98880166\n",
      "(9.40 min) Epoch 47/300 -- Iteration 45081 - Batch 783/963 - Train loss: 0.29232400  - Train acc: 0.8889 - Val loss: 0.98880166\n",
      "(9.40 min) Epoch 47/300 -- Iteration 45090 - Batch 792/963 - Train loss: 0.29246234  - Train acc: 0.8889 - Val loss: 0.98880166\n",
      "(9.40 min) Epoch 47/300 -- Iteration 45099 - Batch 801/963 - Train loss: 0.29238626  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.41 min) Epoch 47/300 -- Iteration 45108 - Batch 810/963 - Train loss: 0.29206867  - Train acc: 0.8890 - Val loss: 0.98880166\n",
      "(9.41 min) Epoch 47/300 -- Iteration 45117 - Batch 819/963 - Train loss: 0.29214326  - Train acc: 0.8890 - Val loss: 0.98880166\n",
      "(9.41 min) Epoch 47/300 -- Iteration 45126 - Batch 828/963 - Train loss: 0.29226607  - Train acc: 0.8889 - Val loss: 0.98880166\n",
      "(9.41 min) Epoch 47/300 -- Iteration 45135 - Batch 837/963 - Train loss: 0.29233394  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.41 min) Epoch 47/300 -- Iteration 45144 - Batch 846/963 - Train loss: 0.29265266  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.41 min) Epoch 47/300 -- Iteration 45153 - Batch 855/963 - Train loss: 0.29281732  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.42 min) Epoch 47/300 -- Iteration 45162 - Batch 864/963 - Train loss: 0.29257154  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.42 min) Epoch 47/300 -- Iteration 45171 - Batch 873/963 - Train loss: 0.29256867  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.42 min) Epoch 47/300 -- Iteration 45180 - Batch 882/963 - Train loss: 0.29274365  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.42 min) Epoch 47/300 -- Iteration 45189 - Batch 891/963 - Train loss: 0.29259038  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.42 min) Epoch 47/300 -- Iteration 45198 - Batch 900/963 - Train loss: 0.29243274  - Train acc: 0.8888 - Val loss: 0.98880166\n",
      "(9.43 min) Epoch 47/300 -- Iteration 45207 - Batch 909/963 - Train loss: 0.29270316  - Train acc: 0.8887 - Val loss: 0.98880166\n",
      "(9.43 min) Epoch 47/300 -- Iteration 45216 - Batch 918/963 - Train loss: 0.29310660  - Train acc: 0.8886 - Val loss: 0.98880166\n",
      "(9.43 min) Epoch 47/300 -- Iteration 45225 - Batch 927/963 - Train loss: 0.29330652  - Train acc: 0.8885 - Val loss: 0.98880166\n",
      "(9.43 min) Epoch 47/300 -- Iteration 45234 - Batch 936/963 - Train loss: 0.29346771  - Train acc: 0.8884 - Val loss: 0.98880166\n",
      "(9.43 min) Epoch 47/300 -- Iteration 45243 - Batch 945/963 - Train loss: 0.29348437  - Train acc: 0.8884 - Val loss: 0.98880166\n",
      "(9.44 min) Epoch 47/300 -- Iteration 45252 - Batch 954/963 - Train loss: 0.29333971  - Train acc: 0.8885 - Val loss: 0.98880166\n",
      "(9.44 min) Epoch 47/300 -- Iteration 45261 - Batch 962/963 - Train loss: 0.29329173  - Train acc: 0.8886 - Val loss: 0.95567828 - Val acc: 0.5983\n",
      "(9.44 min) Epoch 48/300 -- Iteration 45270 - Batch 9/963 - Train loss: 0.26895493  - Train acc: 0.9031 - Val loss: 0.95567828\n",
      "(9.44 min) Epoch 48/300 -- Iteration 45279 - Batch 18/963 - Train loss: 0.29242944  - Train acc: 0.8927 - Val loss: 0.95567828\n",
      "(9.44 min) Epoch 48/300 -- Iteration 45288 - Batch 27/963 - Train loss: 0.29732785  - Train acc: 0.8884 - Val loss: 0.95567828\n",
      "(9.45 min) Epoch 48/300 -- Iteration 45297 - Batch 36/963 - Train loss: 0.30046580  - Train acc: 0.8847 - Val loss: 0.95567828\n",
      "(9.45 min) Epoch 48/300 -- Iteration 45306 - Batch 45/963 - Train loss: 0.29630974  - Train acc: 0.8855 - Val loss: 0.95567828\n",
      "(9.45 min) Epoch 48/300 -- Iteration 45315 - Batch 54/963 - Train loss: 0.29854453  - Train acc: 0.8849 - Val loss: 0.95567828\n",
      "(9.45 min) Epoch 48/300 -- Iteration 45324 - Batch 63/963 - Train loss: 0.30018234  - Train acc: 0.8840 - Val loss: 0.95567828\n",
      "(9.45 min) Epoch 48/300 -- Iteration 45333 - Batch 72/963 - Train loss: 0.30062923  - Train acc: 0.8848 - Val loss: 0.95567828\n",
      "(9.45 min) Epoch 48/300 -- Iteration 45342 - Batch 81/963 - Train loss: 0.29534805  - Train acc: 0.8873 - Val loss: 0.95567828\n",
      "(9.46 min) Epoch 48/300 -- Iteration 45351 - Batch 90/963 - Train loss: 0.29330595  - Train acc: 0.8872 - Val loss: 0.95567828\n",
      "(9.46 min) Epoch 48/300 -- Iteration 45360 - Batch 99/963 - Train loss: 0.29429364  - Train acc: 0.8879 - Val loss: 0.95567828\n",
      "(9.46 min) Epoch 48/300 -- Iteration 45369 - Batch 108/963 - Train loss: 0.29391682  - Train acc: 0.8880 - Val loss: 0.95567828\n",
      "(9.46 min) Epoch 48/300 -- Iteration 45378 - Batch 117/963 - Train loss: 0.29341107  - Train acc: 0.8883 - Val loss: 0.95567828\n",
      "(9.46 min) Epoch 48/300 -- Iteration 45387 - Batch 126/963 - Train loss: 0.29351464  - Train acc: 0.8886 - Val loss: 0.95567828\n",
      "(9.47 min) Epoch 48/300 -- Iteration 45396 - Batch 135/963 - Train loss: 0.29309924  - Train acc: 0.8886 - Val loss: 0.95567828\n",
      "(9.47 min) Epoch 48/300 -- Iteration 45405 - Batch 144/963 - Train loss: 0.29448241  - Train acc: 0.8883 - Val loss: 0.95567828\n",
      "(9.47 min) Epoch 48/300 -- Iteration 45414 - Batch 153/963 - Train loss: 0.29520001  - Train acc: 0.8879 - Val loss: 0.95567828\n",
      "(9.47 min) Epoch 48/300 -- Iteration 45423 - Batch 162/963 - Train loss: 0.29313526  - Train acc: 0.8888 - Val loss: 0.95567828\n",
      "(9.47 min) Epoch 48/300 -- Iteration 45432 - Batch 171/963 - Train loss: 0.29354196  - Train acc: 0.8887 - Val loss: 0.95567828\n",
      "(9.48 min) Epoch 48/300 -- Iteration 45441 - Batch 180/963 - Train loss: 0.29342481  - Train acc: 0.8890 - Val loss: 0.95567828\n",
      "(9.48 min) Epoch 48/300 -- Iteration 45450 - Batch 189/963 - Train loss: 0.29326445  - Train acc: 0.8891 - Val loss: 0.95567828\n",
      "(9.48 min) Epoch 48/300 -- Iteration 45459 - Batch 198/963 - Train loss: 0.29362327  - Train acc: 0.8889 - Val loss: 0.95567828\n",
      "(9.48 min) Epoch 48/300 -- Iteration 45468 - Batch 207/963 - Train loss: 0.29334559  - Train acc: 0.8892 - Val loss: 0.95567828\n",
      "(9.48 min) Epoch 48/300 -- Iteration 45477 - Batch 216/963 - Train loss: 0.29306620  - Train acc: 0.8893 - Val loss: 0.95567828\n",
      "(9.49 min) Epoch 48/300 -- Iteration 45486 - Batch 225/963 - Train loss: 0.29252585  - Train acc: 0.8898 - Val loss: 0.95567828\n",
      "(9.49 min) Epoch 48/300 -- Iteration 45495 - Batch 234/963 - Train loss: 0.29301006  - Train acc: 0.8896 - Val loss: 0.95567828\n",
      "(9.49 min) Epoch 48/300 -- Iteration 45504 - Batch 243/963 - Train loss: 0.29327547  - Train acc: 0.8895 - Val loss: 0.95567828\n",
      "(9.49 min) Epoch 48/300 -- Iteration 45513 - Batch 252/963 - Train loss: 0.29373148  - Train acc: 0.8892 - Val loss: 0.95567828\n",
      "(9.49 min) Epoch 48/300 -- Iteration 45522 - Batch 261/963 - Train loss: 0.29419515  - Train acc: 0.8890 - Val loss: 0.95567828\n",
      "(9.49 min) Epoch 48/300 -- Iteration 45531 - Batch 270/963 - Train loss: 0.29509551  - Train acc: 0.8886 - Val loss: 0.95567828\n",
      "(9.50 min) Epoch 48/300 -- Iteration 45540 - Batch 279/963 - Train loss: 0.29464129  - Train acc: 0.8889 - Val loss: 0.95567828\n",
      "(9.50 min) Epoch 48/300 -- Iteration 45549 - Batch 288/963 - Train loss: 0.29534869  - Train acc: 0.8882 - Val loss: 0.95567828\n",
      "(9.50 min) Epoch 48/300 -- Iteration 45558 - Batch 297/963 - Train loss: 0.29530529  - Train acc: 0.8883 - Val loss: 0.95567828\n",
      "(9.50 min) Epoch 48/300 -- Iteration 45567 - Batch 306/963 - Train loss: 0.29483461  - Train acc: 0.8883 - Val loss: 0.95567828\n",
      "(9.50 min) Epoch 48/300 -- Iteration 45576 - Batch 315/963 - Train loss: 0.29460348  - Train acc: 0.8885 - Val loss: 0.95567828\n",
      "(9.51 min) Epoch 48/300 -- Iteration 45585 - Batch 324/963 - Train loss: 0.29517952  - Train acc: 0.8882 - Val loss: 0.95567828\n",
      "(9.51 min) Epoch 48/300 -- Iteration 45594 - Batch 333/963 - Train loss: 0.29549541  - Train acc: 0.8879 - Val loss: 0.95567828\n",
      "(9.51 min) Epoch 48/300 -- Iteration 45603 - Batch 342/963 - Train loss: 0.29571027  - Train acc: 0.8880 - Val loss: 0.95567828\n",
      "(9.51 min) Epoch 48/300 -- Iteration 45612 - Batch 351/963 - Train loss: 0.29629285  - Train acc: 0.8877 - Val loss: 0.95567828\n",
      "(9.51 min) Epoch 48/300 -- Iteration 45621 - Batch 360/963 - Train loss: 0.29638836  - Train acc: 0.8876 - Val loss: 0.95567828\n",
      "(9.51 min) Epoch 48/300 -- Iteration 45630 - Batch 369/963 - Train loss: 0.29629169  - Train acc: 0.8878 - Val loss: 0.95567828\n",
      "(9.52 min) Epoch 48/300 -- Iteration 45639 - Batch 378/963 - Train loss: 0.29574933  - Train acc: 0.8879 - Val loss: 0.95567828\n",
      "(9.52 min) Epoch 48/300 -- Iteration 45648 - Batch 387/963 - Train loss: 0.29540777  - Train acc: 0.8881 - Val loss: 0.95567828\n",
      "(9.52 min) Epoch 48/300 -- Iteration 45657 - Batch 396/963 - Train loss: 0.29601858  - Train acc: 0.8882 - Val loss: 0.95567828\n",
      "(9.52 min) Epoch 48/300 -- Iteration 45666 - Batch 405/963 - Train loss: 0.29597651  - Train acc: 0.8881 - Val loss: 0.95567828\n",
      "(9.52 min) Epoch 48/300 -- Iteration 45675 - Batch 414/963 - Train loss: 0.29617925  - Train acc: 0.8879 - Val loss: 0.95567828\n",
      "(9.53 min) Epoch 48/300 -- Iteration 45684 - Batch 423/963 - Train loss: 0.29576873  - Train acc: 0.8882 - Val loss: 0.95567828\n",
      "(9.53 min) Epoch 48/300 -- Iteration 45693 - Batch 432/963 - Train loss: 0.29643284  - Train acc: 0.8878 - Val loss: 0.95567828\n",
      "(9.53 min) Epoch 48/300 -- Iteration 45702 - Batch 441/963 - Train loss: 0.29626351  - Train acc: 0.8878 - Val loss: 0.95567828\n",
      "(9.53 min) Epoch 48/300 -- Iteration 45711 - Batch 450/963 - Train loss: 0.29668940  - Train acc: 0.8876 - Val loss: 0.95567828\n",
      "(9.53 min) Epoch 48/300 -- Iteration 45720 - Batch 459/963 - Train loss: 0.29619817  - Train acc: 0.8878 - Val loss: 0.95567828\n",
      "(9.54 min) Epoch 48/300 -- Iteration 45729 - Batch 468/963 - Train loss: 0.29632466  - Train acc: 0.8877 - Val loss: 0.95567828\n",
      "(9.54 min) Epoch 48/300 -- Iteration 45738 - Batch 477/963 - Train loss: 0.29653475  - Train acc: 0.8876 - Val loss: 0.95567828\n",
      "(9.54 min) Epoch 48/300 -- Iteration 45747 - Batch 486/963 - Train loss: 0.29685918  - Train acc: 0.8874 - Val loss: 0.95567828\n",
      "(9.54 min) Epoch 48/300 -- Iteration 45756 - Batch 495/963 - Train loss: 0.29701155  - Train acc: 0.8874 - Val loss: 0.95567828\n",
      "(9.54 min) Epoch 48/300 -- Iteration 45765 - Batch 504/963 - Train loss: 0.29701688  - Train acc: 0.8874 - Val loss: 0.95567828\n",
      "(9.54 min) Epoch 48/300 -- Iteration 45774 - Batch 513/963 - Train loss: 0.29604570  - Train acc: 0.8879 - Val loss: 0.95567828\n",
      "(9.55 min) Epoch 48/300 -- Iteration 45783 - Batch 522/963 - Train loss: 0.29582255  - Train acc: 0.8879 - Val loss: 0.95567828\n",
      "(9.55 min) Epoch 48/300 -- Iteration 45792 - Batch 531/963 - Train loss: 0.29530231  - Train acc: 0.8881 - Val loss: 0.95567828\n",
      "(9.55 min) Epoch 48/300 -- Iteration 45801 - Batch 540/963 - Train loss: 0.29549938  - Train acc: 0.8880 - Val loss: 0.95567828\n",
      "(9.55 min) Epoch 48/300 -- Iteration 45810 - Batch 549/963 - Train loss: 0.29536090  - Train acc: 0.8880 - Val loss: 0.95567828\n",
      "(9.55 min) Epoch 48/300 -- Iteration 45819 - Batch 558/963 - Train loss: 0.29474687  - Train acc: 0.8882 - Val loss: 0.95567828\n",
      "(9.56 min) Epoch 48/300 -- Iteration 45828 - Batch 567/963 - Train loss: 0.29455149  - Train acc: 0.8882 - Val loss: 0.95567828\n",
      "(9.56 min) Epoch 48/300 -- Iteration 45837 - Batch 576/963 - Train loss: 0.29449812  - Train acc: 0.8882 - Val loss: 0.95567828\n",
      "(9.56 min) Epoch 48/300 -- Iteration 45846 - Batch 585/963 - Train loss: 0.29394509  - Train acc: 0.8885 - Val loss: 0.95567828\n",
      "(9.56 min) Epoch 48/300 -- Iteration 45855 - Batch 594/963 - Train loss: 0.29370550  - Train acc: 0.8887 - Val loss: 0.95567828\n",
      "(9.56 min) Epoch 48/300 -- Iteration 45864 - Batch 603/963 - Train loss: 0.29393412  - Train acc: 0.8886 - Val loss: 0.95567828\n",
      "(9.57 min) Epoch 48/300 -- Iteration 45873 - Batch 612/963 - Train loss: 0.29353319  - Train acc: 0.8888 - Val loss: 0.95567828\n",
      "(9.57 min) Epoch 48/300 -- Iteration 45882 - Batch 621/963 - Train loss: 0.29359353  - Train acc: 0.8887 - Val loss: 0.95567828\n",
      "(9.57 min) Epoch 48/300 -- Iteration 45891 - Batch 630/963 - Train loss: 0.29371537  - Train acc: 0.8886 - Val loss: 0.95567828\n",
      "(9.57 min) Epoch 48/300 -- Iteration 45900 - Batch 639/963 - Train loss: 0.29348238  - Train acc: 0.8887 - Val loss: 0.95567828\n",
      "(9.57 min) Epoch 48/300 -- Iteration 45909 - Batch 648/963 - Train loss: 0.29302656  - Train acc: 0.8889 - Val loss: 0.95567828\n",
      "(9.57 min) Epoch 48/300 -- Iteration 45918 - Batch 657/963 - Train loss: 0.29321387  - Train acc: 0.8887 - Val loss: 0.95567828\n",
      "(9.58 min) Epoch 48/300 -- Iteration 45927 - Batch 666/963 - Train loss: 0.29291896  - Train acc: 0.8888 - Val loss: 0.95567828\n",
      "(9.58 min) Epoch 48/300 -- Iteration 45936 - Batch 675/963 - Train loss: 0.29260082  - Train acc: 0.8890 - Val loss: 0.95567828\n",
      "(9.58 min) Epoch 48/300 -- Iteration 45945 - Batch 684/963 - Train loss: 0.29248834  - Train acc: 0.8890 - Val loss: 0.95567828\n",
      "(9.58 min) Epoch 48/300 -- Iteration 45954 - Batch 693/963 - Train loss: 0.29241564  - Train acc: 0.8890 - Val loss: 0.95567828\n",
      "(9.58 min) Epoch 48/300 -- Iteration 45963 - Batch 702/963 - Train loss: 0.29205685  - Train acc: 0.8891 - Val loss: 0.95567828\n",
      "(9.59 min) Epoch 48/300 -- Iteration 45972 - Batch 711/963 - Train loss: 0.29203461  - Train acc: 0.8891 - Val loss: 0.95567828\n",
      "(9.59 min) Epoch 48/300 -- Iteration 45981 - Batch 720/963 - Train loss: 0.29226963  - Train acc: 0.8891 - Val loss: 0.95567828\n",
      "(9.59 min) Epoch 48/300 -- Iteration 45990 - Batch 729/963 - Train loss: 0.29270580  - Train acc: 0.8888 - Val loss: 0.95567828\n",
      "(9.59 min) Epoch 48/300 -- Iteration 45999 - Batch 738/963 - Train loss: 0.29247640  - Train acc: 0.8888 - Val loss: 0.95567828\n",
      "(9.59 min) Epoch 48/300 -- Iteration 46008 - Batch 747/963 - Train loss: 0.29236216  - Train acc: 0.8889 - Val loss: 0.95567828\n",
      "(9.59 min) Epoch 48/300 -- Iteration 46017 - Batch 756/963 - Train loss: 0.29235705  - Train acc: 0.8889 - Val loss: 0.95567828\n",
      "(9.60 min) Epoch 48/300 -- Iteration 46026 - Batch 765/963 - Train loss: 0.29229642  - Train acc: 0.8890 - Val loss: 0.95567828\n",
      "(9.60 min) Epoch 48/300 -- Iteration 46035 - Batch 774/963 - Train loss: 0.29218750  - Train acc: 0.8889 - Val loss: 0.95567828\n",
      "(9.60 min) Epoch 48/300 -- Iteration 46044 - Batch 783/963 - Train loss: 0.29209029  - Train acc: 0.8890 - Val loss: 0.95567828\n",
      "(9.60 min) Epoch 48/300 -- Iteration 46053 - Batch 792/963 - Train loss: 0.29182684  - Train acc: 0.8892 - Val loss: 0.95567828\n",
      "(9.60 min) Epoch 48/300 -- Iteration 46062 - Batch 801/963 - Train loss: 0.29184318  - Train acc: 0.8892 - Val loss: 0.95567828\n",
      "(9.61 min) Epoch 48/300 -- Iteration 46071 - Batch 810/963 - Train loss: 0.29149881  - Train acc: 0.8893 - Val loss: 0.95567828\n",
      "(9.61 min) Epoch 48/300 -- Iteration 46080 - Batch 819/963 - Train loss: 0.29122292  - Train acc: 0.8894 - Val loss: 0.95567828\n",
      "(9.61 min) Epoch 48/300 -- Iteration 46089 - Batch 828/963 - Train loss: 0.29144356  - Train acc: 0.8894 - Val loss: 0.95567828\n",
      "(9.61 min) Epoch 48/300 -- Iteration 46098 - Batch 837/963 - Train loss: 0.29136659  - Train acc: 0.8895 - Val loss: 0.95567828\n",
      "(9.61 min) Epoch 48/300 -- Iteration 46107 - Batch 846/963 - Train loss: 0.29136822  - Train acc: 0.8894 - Val loss: 0.95567828\n",
      "(9.62 min) Epoch 48/300 -- Iteration 46116 - Batch 855/963 - Train loss: 0.29151006  - Train acc: 0.8893 - Val loss: 0.95567828\n",
      "(9.62 min) Epoch 48/300 -- Iteration 46125 - Batch 864/963 - Train loss: 0.29161820  - Train acc: 0.8893 - Val loss: 0.95567828\n",
      "(9.62 min) Epoch 48/300 -- Iteration 46134 - Batch 873/963 - Train loss: 0.29131435  - Train acc: 0.8894 - Val loss: 0.95567828\n",
      "(9.62 min) Epoch 48/300 -- Iteration 46143 - Batch 882/963 - Train loss: 0.29136950  - Train acc: 0.8894 - Val loss: 0.95567828\n",
      "(9.62 min) Epoch 48/300 -- Iteration 46152 - Batch 891/963 - Train loss: 0.29145151  - Train acc: 0.8894 - Val loss: 0.95567828\n",
      "(9.62 min) Epoch 48/300 -- Iteration 46161 - Batch 900/963 - Train loss: 0.29158808  - Train acc: 0.8893 - Val loss: 0.95567828\n",
      "(9.63 min) Epoch 48/300 -- Iteration 46170 - Batch 909/963 - Train loss: 0.29145782  - Train acc: 0.8894 - Val loss: 0.95567828\n",
      "(9.63 min) Epoch 48/300 -- Iteration 46179 - Batch 918/963 - Train loss: 0.29148439  - Train acc: 0.8893 - Val loss: 0.95567828\n",
      "(9.63 min) Epoch 48/300 -- Iteration 46188 - Batch 927/963 - Train loss: 0.29149015  - Train acc: 0.8893 - Val loss: 0.95567828\n",
      "(9.63 min) Epoch 48/300 -- Iteration 46197 - Batch 936/963 - Train loss: 0.29127952  - Train acc: 0.8894 - Val loss: 0.95567828\n",
      "(9.63 min) Epoch 48/300 -- Iteration 46206 - Batch 945/963 - Train loss: 0.29107197  - Train acc: 0.8894 - Val loss: 0.95567828\n",
      "(9.64 min) Epoch 48/300 -- Iteration 46215 - Batch 954/963 - Train loss: 0.29129036  - Train acc: 0.8893 - Val loss: 0.95567828\n",
      "(9.64 min) Epoch 48/300 -- Iteration 46224 - Batch 962/963 - Train loss: 0.29134200  - Train acc: 0.8893 - Val loss: 1.00161433 - Val acc: 0.5933\n",
      "(9.64 min) Epoch 49/300 -- Iteration 46233 - Batch 9/963 - Train loss: 0.28593651  - Train acc: 0.8859 - Val loss: 1.00161433\n",
      "(9.64 min) Epoch 49/300 -- Iteration 46242 - Batch 18/963 - Train loss: 0.28405181  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.64 min) Epoch 49/300 -- Iteration 46251 - Batch 27/963 - Train loss: 0.29464087  - Train acc: 0.8873 - Val loss: 1.00161433\n",
      "(9.65 min) Epoch 49/300 -- Iteration 46260 - Batch 36/963 - Train loss: 0.29733373  - Train acc: 0.8856 - Val loss: 1.00161433\n",
      "(9.65 min) Epoch 49/300 -- Iteration 46269 - Batch 45/963 - Train loss: 0.29352931  - Train acc: 0.8864 - Val loss: 1.00161433\n",
      "(9.65 min) Epoch 49/300 -- Iteration 46278 - Batch 54/963 - Train loss: 0.29227978  - Train acc: 0.8876 - Val loss: 1.00161433\n",
      "(9.65 min) Epoch 49/300 -- Iteration 46287 - Batch 63/963 - Train loss: 0.29085124  - Train acc: 0.8878 - Val loss: 1.00161433\n",
      "(9.65 min) Epoch 49/300 -- Iteration 46296 - Batch 72/963 - Train loss: 0.29202897  - Train acc: 0.8871 - Val loss: 1.00161433\n",
      "(9.66 min) Epoch 49/300 -- Iteration 46305 - Batch 81/963 - Train loss: 0.29238690  - Train acc: 0.8877 - Val loss: 1.00161433\n",
      "(9.66 min) Epoch 49/300 -- Iteration 46314 - Batch 90/963 - Train loss: 0.29288995  - Train acc: 0.8880 - Val loss: 1.00161433\n",
      "(9.66 min) Epoch 49/300 -- Iteration 46323 - Batch 99/963 - Train loss: 0.29055088  - Train acc: 0.8884 - Val loss: 1.00161433\n",
      "(9.66 min) Epoch 49/300 -- Iteration 46332 - Batch 108/963 - Train loss: 0.29319260  - Train acc: 0.8867 - Val loss: 1.00161433\n",
      "(9.66 min) Epoch 49/300 -- Iteration 46341 - Batch 117/963 - Train loss: 0.29356469  - Train acc: 0.8867 - Val loss: 1.00161433\n",
      "(9.67 min) Epoch 49/300 -- Iteration 46350 - Batch 126/963 - Train loss: 0.29353632  - Train acc: 0.8865 - Val loss: 1.00161433\n",
      "(9.67 min) Epoch 49/300 -- Iteration 46359 - Batch 135/963 - Train loss: 0.29364481  - Train acc: 0.8865 - Val loss: 1.00161433\n",
      "(9.67 min) Epoch 49/300 -- Iteration 46368 - Batch 144/963 - Train loss: 0.29407760  - Train acc: 0.8866 - Val loss: 1.00161433\n",
      "(9.67 min) Epoch 49/300 -- Iteration 46377 - Batch 153/963 - Train loss: 0.29310627  - Train acc: 0.8870 - Val loss: 1.00161433\n",
      "(9.67 min) Epoch 49/300 -- Iteration 46386 - Batch 162/963 - Train loss: 0.29292213  - Train acc: 0.8871 - Val loss: 1.00161433\n",
      "(9.68 min) Epoch 49/300 -- Iteration 46395 - Batch 171/963 - Train loss: 0.29151568  - Train acc: 0.8876 - Val loss: 1.00161433\n",
      "(9.68 min) Epoch 49/300 -- Iteration 46404 - Batch 180/963 - Train loss: 0.29127133  - Train acc: 0.8879 - Val loss: 1.00161433\n",
      "(9.68 min) Epoch 49/300 -- Iteration 46413 - Batch 189/963 - Train loss: 0.29081619  - Train acc: 0.8885 - Val loss: 1.00161433\n",
      "(9.68 min) Epoch 49/300 -- Iteration 46422 - Batch 198/963 - Train loss: 0.29073456  - Train acc: 0.8882 - Val loss: 1.00161433\n",
      "(9.68 min) Epoch 49/300 -- Iteration 46431 - Batch 207/963 - Train loss: 0.29003647  - Train acc: 0.8887 - Val loss: 1.00161433\n",
      "(9.68 min) Epoch 49/300 -- Iteration 46440 - Batch 216/963 - Train loss: 0.29011143  - Train acc: 0.8889 - Val loss: 1.00161433\n",
      "(9.69 min) Epoch 49/300 -- Iteration 46449 - Batch 225/963 - Train loss: 0.29105517  - Train acc: 0.8889 - Val loss: 1.00161433\n",
      "(9.69 min) Epoch 49/300 -- Iteration 46458 - Batch 234/963 - Train loss: 0.29089120  - Train acc: 0.8886 - Val loss: 1.00161433\n",
      "(9.69 min) Epoch 49/300 -- Iteration 46467 - Batch 243/963 - Train loss: 0.29103247  - Train acc: 0.8887 - Val loss: 1.00161433\n",
      "(9.69 min) Epoch 49/300 -- Iteration 46476 - Batch 252/963 - Train loss: 0.29100766  - Train acc: 0.8887 - Val loss: 1.00161433\n",
      "(9.69 min) Epoch 49/300 -- Iteration 46485 - Batch 261/963 - Train loss: 0.29121103  - Train acc: 0.8887 - Val loss: 1.00161433\n",
      "(9.70 min) Epoch 49/300 -- Iteration 46494 - Batch 270/963 - Train loss: 0.29126578  - Train acc: 0.8886 - Val loss: 1.00161433\n",
      "(9.70 min) Epoch 49/300 -- Iteration 46503 - Batch 279/963 - Train loss: 0.29082035  - Train acc: 0.8886 - Val loss: 1.00161433\n",
      "(9.70 min) Epoch 49/300 -- Iteration 46512 - Batch 288/963 - Train loss: 0.29138050  - Train acc: 0.8883 - Val loss: 1.00161433\n",
      "(9.70 min) Epoch 49/300 -- Iteration 46521 - Batch 297/963 - Train loss: 0.29194980  - Train acc: 0.8881 - Val loss: 1.00161433\n",
      "(9.70 min) Epoch 49/300 -- Iteration 46530 - Batch 306/963 - Train loss: 0.29193507  - Train acc: 0.8882 - Val loss: 1.00161433\n",
      "(9.71 min) Epoch 49/300 -- Iteration 46539 - Batch 315/963 - Train loss: 0.29134302  - Train acc: 0.8886 - Val loss: 1.00161433\n",
      "(9.71 min) Epoch 49/300 -- Iteration 46548 - Batch 324/963 - Train loss: 0.29154073  - Train acc: 0.8883 - Val loss: 1.00161433\n",
      "(9.71 min) Epoch 49/300 -- Iteration 46557 - Batch 333/963 - Train loss: 0.29192242  - Train acc: 0.8883 - Val loss: 1.00161433\n",
      "(9.71 min) Epoch 49/300 -- Iteration 46566 - Batch 342/963 - Train loss: 0.29134325  - Train acc: 0.8887 - Val loss: 1.00161433\n",
      "(9.71 min) Epoch 49/300 -- Iteration 46575 - Batch 351/963 - Train loss: 0.29097899  - Train acc: 0.8890 - Val loss: 1.00161433\n",
      "(9.71 min) Epoch 49/300 -- Iteration 46584 - Batch 360/963 - Train loss: 0.29173564  - Train acc: 0.8890 - Val loss: 1.00161433\n",
      "(9.72 min) Epoch 49/300 -- Iteration 46593 - Batch 369/963 - Train loss: 0.29171582  - Train acc: 0.8889 - Val loss: 1.00161433\n",
      "(9.72 min) Epoch 49/300 -- Iteration 46602 - Batch 378/963 - Train loss: 0.29099795  - Train acc: 0.8891 - Val loss: 1.00161433\n",
      "(9.72 min) Epoch 49/300 -- Iteration 46611 - Batch 387/963 - Train loss: 0.29105716  - Train acc: 0.8893 - Val loss: 1.00161433\n",
      "(9.72 min) Epoch 49/300 -- Iteration 46620 - Batch 396/963 - Train loss: 0.29076450  - Train acc: 0.8892 - Val loss: 1.00161433\n",
      "(9.72 min) Epoch 49/300 -- Iteration 46629 - Batch 405/963 - Train loss: 0.29081131  - Train acc: 0.8890 - Val loss: 1.00161433\n",
      "(9.73 min) Epoch 49/300 -- Iteration 46638 - Batch 414/963 - Train loss: 0.29090824  - Train acc: 0.8890 - Val loss: 1.00161433\n",
      "(9.73 min) Epoch 49/300 -- Iteration 46647 - Batch 423/963 - Train loss: 0.29156155  - Train acc: 0.8888 - Val loss: 1.00161433\n",
      "(9.73 min) Epoch 49/300 -- Iteration 46656 - Batch 432/963 - Train loss: 0.29163528  - Train acc: 0.8890 - Val loss: 1.00161433\n",
      "(9.73 min) Epoch 49/300 -- Iteration 46665 - Batch 441/963 - Train loss: 0.29128944  - Train acc: 0.8893 - Val loss: 1.00161433\n",
      "(9.73 min) Epoch 49/300 -- Iteration 46674 - Batch 450/963 - Train loss: 0.29123307  - Train acc: 0.8893 - Val loss: 1.00161433\n",
      "(9.74 min) Epoch 49/300 -- Iteration 46683 - Batch 459/963 - Train loss: 0.29120215  - Train acc: 0.8894 - Val loss: 1.00161433\n",
      "(9.74 min) Epoch 49/300 -- Iteration 46692 - Batch 468/963 - Train loss: 0.29100538  - Train acc: 0.8895 - Val loss: 1.00161433\n",
      "(9.74 min) Epoch 49/300 -- Iteration 46701 - Batch 477/963 - Train loss: 0.29143420  - Train acc: 0.8893 - Val loss: 1.00161433\n",
      "(9.74 min) Epoch 49/300 -- Iteration 46710 - Batch 486/963 - Train loss: 0.29167250  - Train acc: 0.8893 - Val loss: 1.00161433\n",
      "(9.74 min) Epoch 49/300 -- Iteration 46719 - Batch 495/963 - Train loss: 0.29161335  - Train acc: 0.8893 - Val loss: 1.00161433\n",
      "(9.74 min) Epoch 49/300 -- Iteration 46728 - Batch 504/963 - Train loss: 0.29099913  - Train acc: 0.8895 - Val loss: 1.00161433\n",
      "(9.75 min) Epoch 49/300 -- Iteration 46737 - Batch 513/963 - Train loss: 0.29097210  - Train acc: 0.8894 - Val loss: 1.00161433\n",
      "(9.75 min) Epoch 49/300 -- Iteration 46746 - Batch 522/963 - Train loss: 0.29151653  - Train acc: 0.8893 - Val loss: 1.00161433\n",
      "(9.75 min) Epoch 49/300 -- Iteration 46755 - Batch 531/963 - Train loss: 0.29090781  - Train acc: 0.8895 - Val loss: 1.00161433\n",
      "(9.75 min) Epoch 49/300 -- Iteration 46764 - Batch 540/963 - Train loss: 0.29039453  - Train acc: 0.8897 - Val loss: 1.00161433\n",
      "(9.75 min) Epoch 49/300 -- Iteration 46773 - Batch 549/963 - Train loss: 0.29062439  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.76 min) Epoch 49/300 -- Iteration 46782 - Batch 558/963 - Train loss: 0.29095146  - Train acc: 0.8896 - Val loss: 1.00161433\n",
      "(9.76 min) Epoch 49/300 -- Iteration 46791 - Batch 567/963 - Train loss: 0.29068100  - Train acc: 0.8895 - Val loss: 1.00161433\n",
      "(9.76 min) Epoch 49/300 -- Iteration 46800 - Batch 576/963 - Train loss: 0.29062441  - Train acc: 0.8896 - Val loss: 1.00161433\n",
      "(9.76 min) Epoch 49/300 -- Iteration 46809 - Batch 585/963 - Train loss: 0.29078885  - Train acc: 0.8896 - Val loss: 1.00161433\n",
      "(9.76 min) Epoch 49/300 -- Iteration 46818 - Batch 594/963 - Train loss: 0.29053638  - Train acc: 0.8896 - Val loss: 1.00161433\n",
      "(9.77 min) Epoch 49/300 -- Iteration 46827 - Batch 603/963 - Train loss: 0.29048733  - Train acc: 0.8897 - Val loss: 1.00161433\n",
      "(9.77 min) Epoch 49/300 -- Iteration 46836 - Batch 612/963 - Train loss: 0.28996267  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.77 min) Epoch 49/300 -- Iteration 46845 - Batch 621/963 - Train loss: 0.29012886  - Train acc: 0.8900 - Val loss: 1.00161433\n",
      "(9.77 min) Epoch 49/300 -- Iteration 46854 - Batch 630/963 - Train loss: 0.29048859  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.77 min) Epoch 49/300 -- Iteration 46863 - Batch 639/963 - Train loss: 0.29088693  - Train acc: 0.8897 - Val loss: 1.00161433\n",
      "(9.77 min) Epoch 49/300 -- Iteration 46872 - Batch 648/963 - Train loss: 0.29089035  - Train acc: 0.8897 - Val loss: 1.00161433\n",
      "(9.78 min) Epoch 49/300 -- Iteration 46881 - Batch 657/963 - Train loss: 0.29064519  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.78 min) Epoch 49/300 -- Iteration 46890 - Batch 666/963 - Train loss: 0.29054051  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.78 min) Epoch 49/300 -- Iteration 46899 - Batch 675/963 - Train loss: 0.29063144  - Train acc: 0.8897 - Val loss: 1.00161433\n",
      "(9.78 min) Epoch 49/300 -- Iteration 46908 - Batch 684/963 - Train loss: 0.29057822  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.78 min) Epoch 49/300 -- Iteration 46917 - Batch 693/963 - Train loss: 0.29061943  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.79 min) Epoch 49/300 -- Iteration 46926 - Batch 702/963 - Train loss: 0.29103681  - Train acc: 0.8896 - Val loss: 1.00161433\n",
      "(9.79 min) Epoch 49/300 -- Iteration 46935 - Batch 711/963 - Train loss: 0.29076810  - Train acc: 0.8897 - Val loss: 1.00161433\n",
      "(9.79 min) Epoch 49/300 -- Iteration 46944 - Batch 720/963 - Train loss: 0.29062427  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.79 min) Epoch 49/300 -- Iteration 46953 - Batch 729/963 - Train loss: 0.29052876  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.79 min) Epoch 49/300 -- Iteration 46962 - Batch 738/963 - Train loss: 0.29058659  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.80 min) Epoch 49/300 -- Iteration 46971 - Batch 747/963 - Train loss: 0.29072891  - Train acc: 0.8897 - Val loss: 1.00161433\n",
      "(9.80 min) Epoch 49/300 -- Iteration 46980 - Batch 756/963 - Train loss: 0.29045795  - Train acc: 0.8900 - Val loss: 1.00161433\n",
      "(9.80 min) Epoch 49/300 -- Iteration 46989 - Batch 765/963 - Train loss: 0.29005408  - Train acc: 0.8900 - Val loss: 1.00161433\n",
      "(9.80 min) Epoch 49/300 -- Iteration 46998 - Batch 774/963 - Train loss: 0.29007458  - Train acc: 0.8900 - Val loss: 1.00161433\n",
      "(9.80 min) Epoch 49/300 -- Iteration 47007 - Batch 783/963 - Train loss: 0.29034395  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.80 min) Epoch 49/300 -- Iteration 47016 - Batch 792/963 - Train loss: 0.29018506  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.81 min) Epoch 49/300 -- Iteration 47025 - Batch 801/963 - Train loss: 0.28990681  - Train acc: 0.8900 - Val loss: 1.00161433\n",
      "(9.81 min) Epoch 49/300 -- Iteration 47034 - Batch 810/963 - Train loss: 0.29002246  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.81 min) Epoch 49/300 -- Iteration 47043 - Batch 819/963 - Train loss: 0.29014839  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.81 min) Epoch 49/300 -- Iteration 47052 - Batch 828/963 - Train loss: 0.29024040  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.81 min) Epoch 49/300 -- Iteration 47061 - Batch 837/963 - Train loss: 0.29007445  - Train acc: 0.8900 - Val loss: 1.00161433\n",
      "(9.82 min) Epoch 49/300 -- Iteration 47070 - Batch 846/963 - Train loss: 0.29005097  - Train acc: 0.8900 - Val loss: 1.00161433\n",
      "(9.82 min) Epoch 49/300 -- Iteration 47079 - Batch 855/963 - Train loss: 0.29026334  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.82 min) Epoch 49/300 -- Iteration 47088 - Batch 864/963 - Train loss: 0.29020736  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.82 min) Epoch 49/300 -- Iteration 47097 - Batch 873/963 - Train loss: 0.29006262  - Train acc: 0.8901 - Val loss: 1.00161433\n",
      "(9.82 min) Epoch 49/300 -- Iteration 47106 - Batch 882/963 - Train loss: 0.29012643  - Train acc: 0.8900 - Val loss: 1.00161433\n",
      "(9.83 min) Epoch 49/300 -- Iteration 47115 - Batch 891/963 - Train loss: 0.29017506  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.83 min) Epoch 49/300 -- Iteration 47124 - Batch 900/963 - Train loss: 0.29003434  - Train acc: 0.8900 - Val loss: 1.00161433\n",
      "(9.83 min) Epoch 49/300 -- Iteration 47133 - Batch 909/963 - Train loss: 0.29027326  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.83 min) Epoch 49/300 -- Iteration 47142 - Batch 918/963 - Train loss: 0.28985674  - Train acc: 0.8901 - Val loss: 1.00161433\n",
      "(9.83 min) Epoch 49/300 -- Iteration 47151 - Batch 927/963 - Train loss: 0.29014524  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.83 min) Epoch 49/300 -- Iteration 47160 - Batch 936/963 - Train loss: 0.29001705  - Train acc: 0.8899 - Val loss: 1.00161433\n",
      "(9.84 min) Epoch 49/300 -- Iteration 47169 - Batch 945/963 - Train loss: 0.29022104  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.84 min) Epoch 49/300 -- Iteration 47178 - Batch 954/963 - Train loss: 0.29029369  - Train acc: 0.8898 - Val loss: 1.00161433\n",
      "(9.84 min) Epoch 49/300 -- Iteration 47187 - Batch 962/963 - Train loss: 0.29046356  - Train acc: 0.8897 - Val loss: 0.98125130 - Val acc: 0.5983\n",
      "(9.84 min) Epoch 50/300 -- Iteration 47196 - Batch 9/963 - Train loss: 0.26323517  - Train acc: 0.8953 - Val loss: 0.98125130\n",
      "(9.85 min) Epoch 50/300 -- Iteration 47205 - Batch 18/963 - Train loss: 0.26001238  - Train acc: 0.9013 - Val loss: 0.98125130\n",
      "(9.85 min) Epoch 50/300 -- Iteration 47214 - Batch 27/963 - Train loss: 0.26791568  - Train acc: 0.8993 - Val loss: 0.98125130\n",
      "(9.85 min) Epoch 50/300 -- Iteration 47223 - Batch 36/963 - Train loss: 0.27233104  - Train acc: 0.8986 - Val loss: 0.98125130\n",
      "(9.85 min) Epoch 50/300 -- Iteration 47232 - Batch 45/963 - Train loss: 0.27339702  - Train acc: 0.8979 - Val loss: 0.98125130\n",
      "(9.85 min) Epoch 50/300 -- Iteration 47241 - Batch 54/963 - Train loss: 0.27817883  - Train acc: 0.8953 - Val loss: 0.98125130\n",
      "(9.85 min) Epoch 50/300 -- Iteration 47250 - Batch 63/963 - Train loss: 0.27541289  - Train acc: 0.8958 - Val loss: 0.98125130\n",
      "(9.86 min) Epoch 50/300 -- Iteration 47259 - Batch 72/963 - Train loss: 0.27637401  - Train acc: 0.8948 - Val loss: 0.98125130\n",
      "(9.86 min) Epoch 50/300 -- Iteration 47268 - Batch 81/963 - Train loss: 0.27757723  - Train acc: 0.8936 - Val loss: 0.98125130\n",
      "(9.86 min) Epoch 50/300 -- Iteration 47277 - Batch 90/963 - Train loss: 0.28059868  - Train acc: 0.8927 - Val loss: 0.98125130\n",
      "(9.86 min) Epoch 50/300 -- Iteration 47286 - Batch 99/963 - Train loss: 0.28341823  - Train acc: 0.8923 - Val loss: 0.98125130\n",
      "(9.86 min) Epoch 50/300 -- Iteration 47295 - Batch 108/963 - Train loss: 0.28319746  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.87 min) Epoch 50/300 -- Iteration 47304 - Batch 117/963 - Train loss: 0.28228579  - Train acc: 0.8928 - Val loss: 0.98125130\n",
      "(9.87 min) Epoch 50/300 -- Iteration 47313 - Batch 126/963 - Train loss: 0.28249998  - Train acc: 0.8925 - Val loss: 0.98125130\n",
      "(9.87 min) Epoch 50/300 -- Iteration 47322 - Batch 135/963 - Train loss: 0.28342651  - Train acc: 0.8925 - Val loss: 0.98125130\n",
      "(9.87 min) Epoch 50/300 -- Iteration 47331 - Batch 144/963 - Train loss: 0.28624556  - Train acc: 0.8914 - Val loss: 0.98125130\n",
      "(9.87 min) Epoch 50/300 -- Iteration 47340 - Batch 153/963 - Train loss: 0.28497717  - Train acc: 0.8920 - Val loss: 0.98125130\n",
      "(9.88 min) Epoch 50/300 -- Iteration 47349 - Batch 162/963 - Train loss: 0.28674638  - Train acc: 0.8912 - Val loss: 0.98125130\n",
      "(9.88 min) Epoch 50/300 -- Iteration 47358 - Batch 171/963 - Train loss: 0.28578801  - Train acc: 0.8917 - Val loss: 0.98125130\n",
      "(9.88 min) Epoch 50/300 -- Iteration 47367 - Batch 180/963 - Train loss: 0.28505105  - Train acc: 0.8919 - Val loss: 0.98125130\n",
      "(9.88 min) Epoch 50/300 -- Iteration 47376 - Batch 189/963 - Train loss: 0.28479902  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.88 min) Epoch 50/300 -- Iteration 47385 - Batch 198/963 - Train loss: 0.28505671  - Train acc: 0.8920 - Val loss: 0.98125130\n",
      "(9.88 min) Epoch 50/300 -- Iteration 47394 - Batch 207/963 - Train loss: 0.28585357  - Train acc: 0.8917 - Val loss: 0.98125130\n",
      "(9.89 min) Epoch 50/300 -- Iteration 47403 - Batch 216/963 - Train loss: 0.28616138  - Train acc: 0.8915 - Val loss: 0.98125130\n",
      "(9.89 min) Epoch 50/300 -- Iteration 47412 - Batch 225/963 - Train loss: 0.28525941  - Train acc: 0.8919 - Val loss: 0.98125130\n",
      "(9.89 min) Epoch 50/300 -- Iteration 47421 - Batch 234/963 - Train loss: 0.28517639  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.89 min) Epoch 50/300 -- Iteration 47430 - Batch 243/963 - Train loss: 0.28459271  - Train acc: 0.8925 - Val loss: 0.98125130\n",
      "(9.89 min) Epoch 50/300 -- Iteration 47439 - Batch 252/963 - Train loss: 0.28426115  - Train acc: 0.8928 - Val loss: 0.98125130\n",
      "(9.90 min) Epoch 50/300 -- Iteration 47448 - Batch 261/963 - Train loss: 0.28411118  - Train acc: 0.8928 - Val loss: 0.98125130\n",
      "(9.90 min) Epoch 50/300 -- Iteration 47457 - Batch 270/963 - Train loss: 0.28467696  - Train acc: 0.8925 - Val loss: 0.98125130\n",
      "(9.90 min) Epoch 50/300 -- Iteration 47466 - Batch 279/963 - Train loss: 0.28476483  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.90 min) Epoch 50/300 -- Iteration 47475 - Batch 288/963 - Train loss: 0.28552764  - Train acc: 0.8919 - Val loss: 0.98125130\n",
      "(9.90 min) Epoch 50/300 -- Iteration 47484 - Batch 297/963 - Train loss: 0.28527163  - Train acc: 0.8916 - Val loss: 0.98125130\n",
      "(9.91 min) Epoch 50/300 -- Iteration 47493 - Batch 306/963 - Train loss: 0.28554470  - Train acc: 0.8914 - Val loss: 0.98125130\n",
      "(9.91 min) Epoch 50/300 -- Iteration 47502 - Batch 315/963 - Train loss: 0.28613292  - Train acc: 0.8912 - Val loss: 0.98125130\n",
      "(9.91 min) Epoch 50/300 -- Iteration 47511 - Batch 324/963 - Train loss: 0.28554344  - Train acc: 0.8916 - Val loss: 0.98125130\n",
      "(9.91 min) Epoch 50/300 -- Iteration 47520 - Batch 333/963 - Train loss: 0.28584484  - Train acc: 0.8914 - Val loss: 0.98125130\n",
      "(9.91 min) Epoch 50/300 -- Iteration 47529 - Batch 342/963 - Train loss: 0.28599208  - Train acc: 0.8914 - Val loss: 0.98125130\n",
      "(9.91 min) Epoch 50/300 -- Iteration 47538 - Batch 351/963 - Train loss: 0.28574066  - Train acc: 0.8914 - Val loss: 0.98125130\n",
      "(9.92 min) Epoch 50/300 -- Iteration 47547 - Batch 360/963 - Train loss: 0.28589394  - Train acc: 0.8914 - Val loss: 0.98125130\n",
      "(9.92 min) Epoch 50/300 -- Iteration 47556 - Batch 369/963 - Train loss: 0.28592737  - Train acc: 0.8915 - Val loss: 0.98125130\n",
      "(9.92 min) Epoch 50/300 -- Iteration 47565 - Batch 378/963 - Train loss: 0.28536958  - Train acc: 0.8917 - Val loss: 0.98125130\n",
      "(9.92 min) Epoch 50/300 -- Iteration 47574 - Batch 387/963 - Train loss: 0.28498142  - Train acc: 0.8919 - Val loss: 0.98125130\n",
      "(9.92 min) Epoch 50/300 -- Iteration 47583 - Batch 396/963 - Train loss: 0.28501800  - Train acc: 0.8917 - Val loss: 0.98125130\n",
      "(9.93 min) Epoch 50/300 -- Iteration 47592 - Batch 405/963 - Train loss: 0.28456220  - Train acc: 0.8920 - Val loss: 0.98125130\n",
      "(9.93 min) Epoch 50/300 -- Iteration 47601 - Batch 414/963 - Train loss: 0.28418576  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.93 min) Epoch 50/300 -- Iteration 47610 - Batch 423/963 - Train loss: 0.28452015  - Train acc: 0.8919 - Val loss: 0.98125130\n",
      "(9.93 min) Epoch 50/300 -- Iteration 47619 - Batch 432/963 - Train loss: 0.28536412  - Train acc: 0.8917 - Val loss: 0.98125130\n",
      "(9.93 min) Epoch 50/300 -- Iteration 47628 - Batch 441/963 - Train loss: 0.28581068  - Train acc: 0.8915 - Val loss: 0.98125130\n",
      "(9.94 min) Epoch 50/300 -- Iteration 47637 - Batch 450/963 - Train loss: 0.28573621  - Train acc: 0.8915 - Val loss: 0.98125130\n",
      "(9.94 min) Epoch 50/300 -- Iteration 47646 - Batch 459/963 - Train loss: 0.28572180  - Train acc: 0.8916 - Val loss: 0.98125130\n",
      "(9.94 min) Epoch 50/300 -- Iteration 47655 - Batch 468/963 - Train loss: 0.28493085  - Train acc: 0.8919 - Val loss: 0.98125130\n",
      "(9.94 min) Epoch 50/300 -- Iteration 47664 - Batch 477/963 - Train loss: 0.28446021  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.94 min) Epoch 50/300 -- Iteration 47673 - Batch 486/963 - Train loss: 0.28489911  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.95 min) Epoch 50/300 -- Iteration 47682 - Batch 495/963 - Train loss: 0.28476189  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.95 min) Epoch 50/300 -- Iteration 47691 - Batch 504/963 - Train loss: 0.28459382  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.95 min) Epoch 50/300 -- Iteration 47700 - Batch 513/963 - Train loss: 0.28454219  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.95 min) Epoch 50/300 -- Iteration 47709 - Batch 522/963 - Train loss: 0.28430088  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.95 min) Epoch 50/300 -- Iteration 47718 - Batch 531/963 - Train loss: 0.28435327  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.95 min) Epoch 50/300 -- Iteration 47727 - Batch 540/963 - Train loss: 0.28413320  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.96 min) Epoch 50/300 -- Iteration 47736 - Batch 549/963 - Train loss: 0.28393700  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.96 min) Epoch 50/300 -- Iteration 47745 - Batch 558/963 - Train loss: 0.28414559  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.96 min) Epoch 50/300 -- Iteration 47754 - Batch 567/963 - Train loss: 0.28436226  - Train acc: 0.8920 - Val loss: 0.98125130\n",
      "(9.96 min) Epoch 50/300 -- Iteration 47763 - Batch 576/963 - Train loss: 0.28430873  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.96 min) Epoch 50/300 -- Iteration 47772 - Batch 585/963 - Train loss: 0.28417532  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.97 min) Epoch 50/300 -- Iteration 47781 - Batch 594/963 - Train loss: 0.28376674  - Train acc: 0.8924 - Val loss: 0.98125130\n",
      "(9.97 min) Epoch 50/300 -- Iteration 47790 - Batch 603/963 - Train loss: 0.28388847  - Train acc: 0.8924 - Val loss: 0.98125130\n",
      "(9.97 min) Epoch 50/300 -- Iteration 47799 - Batch 612/963 - Train loss: 0.28426894  - Train acc: 0.8923 - Val loss: 0.98125130\n",
      "(9.97 min) Epoch 50/300 -- Iteration 47808 - Batch 621/963 - Train loss: 0.28465256  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.97 min) Epoch 50/300 -- Iteration 47817 - Batch 630/963 - Train loss: 0.28466502  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(9.98 min) Epoch 50/300 -- Iteration 47826 - Batch 639/963 - Train loss: 0.28505293  - Train acc: 0.8919 - Val loss: 0.98125130\n",
      "(9.98 min) Epoch 50/300 -- Iteration 47835 - Batch 648/963 - Train loss: 0.28443846  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.98 min) Epoch 50/300 -- Iteration 47844 - Batch 657/963 - Train loss: 0.28440591  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.98 min) Epoch 50/300 -- Iteration 47853 - Batch 666/963 - Train loss: 0.28436675  - Train acc: 0.8923 - Val loss: 0.98125130\n",
      "(9.98 min) Epoch 50/300 -- Iteration 47862 - Batch 675/963 - Train loss: 0.28476205  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.98 min) Epoch 50/300 -- Iteration 47871 - Batch 684/963 - Train loss: 0.28458209  - Train acc: 0.8923 - Val loss: 0.98125130\n",
      "(9.99 min) Epoch 50/300 -- Iteration 47880 - Batch 693/963 - Train loss: 0.28458181  - Train acc: 0.8923 - Val loss: 0.98125130\n",
      "(9.99 min) Epoch 50/300 -- Iteration 47889 - Batch 702/963 - Train loss: 0.28476665  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.99 min) Epoch 50/300 -- Iteration 47898 - Batch 711/963 - Train loss: 0.28461724  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(9.99 min) Epoch 50/300 -- Iteration 47907 - Batch 720/963 - Train loss: 0.28462772  - Train acc: 0.8923 - Val loss: 0.98125130\n",
      "(9.99 min) Epoch 50/300 -- Iteration 47916 - Batch 729/963 - Train loss: 0.28460989  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(10.00 min) Epoch 50/300 -- Iteration 47925 - Batch 738/963 - Train loss: 0.28455517  - Train acc: 0.8923 - Val loss: 0.98125130\n",
      "(10.00 min) Epoch 50/300 -- Iteration 47934 - Batch 747/963 - Train loss: 0.28463514  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(10.00 min) Epoch 50/300 -- Iteration 47943 - Batch 756/963 - Train loss: 0.28464396  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(10.00 min) Epoch 50/300 -- Iteration 47952 - Batch 765/963 - Train loss: 0.28476894  - Train acc: 0.8919 - Val loss: 0.98125130\n",
      "(10.00 min) Epoch 50/300 -- Iteration 47961 - Batch 774/963 - Train loss: 0.28464567  - Train acc: 0.8920 - Val loss: 0.98125130\n",
      "(10.01 min) Epoch 50/300 -- Iteration 47970 - Batch 783/963 - Train loss: 0.28447699  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(10.01 min) Epoch 50/300 -- Iteration 47979 - Batch 792/963 - Train loss: 0.28440720  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(10.01 min) Epoch 50/300 -- Iteration 47988 - Batch 801/963 - Train loss: 0.28407658  - Train acc: 0.8923 - Val loss: 0.98125130\n",
      "(10.01 min) Epoch 50/300 -- Iteration 47997 - Batch 810/963 - Train loss: 0.28435980  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(10.01 min) Epoch 50/300 -- Iteration 48006 - Batch 819/963 - Train loss: 0.28442280  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(10.02 min) Epoch 50/300 -- Iteration 48015 - Batch 828/963 - Train loss: 0.28430776  - Train acc: 0.8922 - Val loss: 0.98125130\n",
      "(10.02 min) Epoch 50/300 -- Iteration 48024 - Batch 837/963 - Train loss: 0.28455716  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(10.02 min) Epoch 50/300 -- Iteration 48033 - Batch 846/963 - Train loss: 0.28471150  - Train acc: 0.8921 - Val loss: 0.98125130\n",
      "(10.02 min) Epoch 50/300 -- Iteration 48042 - Batch 855/963 - Train loss: 0.28499261  - Train acc: 0.8919 - Val loss: 0.98125130\n",
      "(10.02 min) Epoch 50/300 -- Iteration 48051 - Batch 864/963 - Train loss: 0.28547209  - Train acc: 0.8917 - Val loss: 0.98125130\n",
      "(10.02 min) Epoch 50/300 -- Iteration 48060 - Batch 873/963 - Train loss: 0.28568536  - Train acc: 0.8917 - Val loss: 0.98125130\n",
      "(10.03 min) Epoch 50/300 -- Iteration 48069 - Batch 882/963 - Train loss: 0.28557472  - Train acc: 0.8917 - Val loss: 0.98125130\n",
      "(10.03 min) Epoch 50/300 -- Iteration 48078 - Batch 891/963 - Train loss: 0.28546550  - Train acc: 0.8917 - Val loss: 0.98125130\n",
      "(10.03 min) Epoch 50/300 -- Iteration 48087 - Batch 900/963 - Train loss: 0.28565816  - Train acc: 0.8916 - Val loss: 0.98125130\n",
      "(10.03 min) Epoch 50/300 -- Iteration 48096 - Batch 909/963 - Train loss: 0.28574470  - Train acc: 0.8914 - Val loss: 0.98125130\n",
      "(10.03 min) Epoch 50/300 -- Iteration 48105 - Batch 918/963 - Train loss: 0.28605254  - Train acc: 0.8913 - Val loss: 0.98125130\n",
      "(10.04 min) Epoch 50/300 -- Iteration 48114 - Batch 927/963 - Train loss: 0.28639953  - Train acc: 0.8911 - Val loss: 0.98125130\n",
      "(10.04 min) Epoch 50/300 -- Iteration 48123 - Batch 936/963 - Train loss: 0.28650450  - Train acc: 0.8911 - Val loss: 0.98125130\n",
      "(10.04 min) Epoch 50/300 -- Iteration 48132 - Batch 945/963 - Train loss: 0.28610171  - Train acc: 0.8913 - Val loss: 0.98125130\n",
      "(10.04 min) Epoch 50/300 -- Iteration 48141 - Batch 954/963 - Train loss: 0.28649305  - Train acc: 0.8911 - Val loss: 0.98125130\n",
      "(10.04 min) Epoch 50/300 -- Iteration 48150 - Batch 962/963 - Train loss: 0.28619974  - Train acc: 0.8913 - Val loss: 0.99701852 - Val acc: 0.5967\n",
      "(10.05 min) Epoch 51/300 -- Iteration 48159 - Batch 9/963 - Train loss: 0.29084923  - Train acc: 0.8906 - Val loss: 0.99701852\n",
      "(10.05 min) Epoch 51/300 -- Iteration 48168 - Batch 18/963 - Train loss: 0.28902702  - Train acc: 0.8898 - Val loss: 0.99701852\n",
      "(10.05 min) Epoch 51/300 -- Iteration 48177 - Batch 27/963 - Train loss: 0.28464812  - Train acc: 0.8948 - Val loss: 0.99701852\n",
      "(10.05 min) Epoch 51/300 -- Iteration 48186 - Batch 36/963 - Train loss: 0.28343707  - Train acc: 0.8940 - Val loss: 0.99701852\n",
      "(10.05 min) Epoch 51/300 -- Iteration 48195 - Batch 45/963 - Train loss: 0.28162392  - Train acc: 0.8949 - Val loss: 0.99701852\n",
      "(10.06 min) Epoch 51/300 -- Iteration 48204 - Batch 54/963 - Train loss: 0.28229820  - Train acc: 0.8932 - Val loss: 0.99701852\n",
      "(10.06 min) Epoch 51/300 -- Iteration 48213 - Batch 63/963 - Train loss: 0.28348766  - Train acc: 0.8926 - Val loss: 0.99701852\n",
      "(10.06 min) Epoch 51/300 -- Iteration 48222 - Batch 72/963 - Train loss: 0.28409545  - Train acc: 0.8929 - Val loss: 0.99701852\n",
      "(10.06 min) Epoch 51/300 -- Iteration 48231 - Batch 81/963 - Train loss: 0.28296089  - Train acc: 0.8923 - Val loss: 0.99701852\n",
      "(10.06 min) Epoch 51/300 -- Iteration 48240 - Batch 90/963 - Train loss: 0.28291669  - Train acc: 0.8920 - Val loss: 0.99701852\n",
      "(10.06 min) Epoch 51/300 -- Iteration 48249 - Batch 99/963 - Train loss: 0.28670560  - Train acc: 0.8904 - Val loss: 0.99701852\n",
      "(10.07 min) Epoch 51/300 -- Iteration 48258 - Batch 108/963 - Train loss: 0.28763451  - Train acc: 0.8901 - Val loss: 0.99701852\n",
      "(10.07 min) Epoch 51/300 -- Iteration 48267 - Batch 117/963 - Train loss: 0.28778105  - Train acc: 0.8895 - Val loss: 0.99701852\n",
      "(10.07 min) Epoch 51/300 -- Iteration 48276 - Batch 126/963 - Train loss: 0.28799054  - Train acc: 0.8895 - Val loss: 0.99701852\n",
      "(10.07 min) Epoch 51/300 -- Iteration 48285 - Batch 135/963 - Train loss: 0.28855110  - Train acc: 0.8895 - Val loss: 0.99701852\n",
      "(10.07 min) Epoch 51/300 -- Iteration 48294 - Batch 144/963 - Train loss: 0.28816107  - Train acc: 0.8901 - Val loss: 0.99701852\n",
      "(10.08 min) Epoch 51/300 -- Iteration 48303 - Batch 153/963 - Train loss: 0.28836863  - Train acc: 0.8899 - Val loss: 0.99701852\n",
      "(10.08 min) Epoch 51/300 -- Iteration 48312 - Batch 162/963 - Train loss: 0.28655822  - Train acc: 0.8914 - Val loss: 0.99701852\n",
      "(10.08 min) Epoch 51/300 -- Iteration 48321 - Batch 171/963 - Train loss: 0.28659419  - Train acc: 0.8915 - Val loss: 0.99701852\n",
      "(10.08 min) Epoch 51/300 -- Iteration 48330 - Batch 180/963 - Train loss: 0.28651036  - Train acc: 0.8916 - Val loss: 0.99701852\n",
      "(10.08 min) Epoch 51/300 -- Iteration 48339 - Batch 189/963 - Train loss: 0.28757626  - Train acc: 0.8909 - Val loss: 0.99701852\n",
      "(10.09 min) Epoch 51/300 -- Iteration 48348 - Batch 198/963 - Train loss: 0.28773493  - Train acc: 0.8911 - Val loss: 0.99701852\n",
      "(10.09 min) Epoch 51/300 -- Iteration 48357 - Batch 207/963 - Train loss: 0.28760393  - Train acc: 0.8910 - Val loss: 0.99701852\n",
      "(10.09 min) Epoch 51/300 -- Iteration 48366 - Batch 216/963 - Train loss: 0.28803728  - Train acc: 0.8906 - Val loss: 0.99701852\n",
      "(10.09 min) Epoch 51/300 -- Iteration 48375 - Batch 225/963 - Train loss: 0.28675370  - Train acc: 0.8910 - Val loss: 0.99701852\n",
      "(10.09 min) Epoch 51/300 -- Iteration 48384 - Batch 234/963 - Train loss: 0.28624731  - Train acc: 0.8913 - Val loss: 0.99701852\n",
      "(10.10 min) Epoch 51/300 -- Iteration 48393 - Batch 243/963 - Train loss: 0.28634341  - Train acc: 0.8912 - Val loss: 0.99701852\n",
      "(10.10 min) Epoch 51/300 -- Iteration 48402 - Batch 252/963 - Train loss: 0.28672676  - Train acc: 0.8910 - Val loss: 0.99701852\n",
      "(10.10 min) Epoch 51/300 -- Iteration 48411 - Batch 261/963 - Train loss: 0.28753239  - Train acc: 0.8907 - Val loss: 0.99701852\n",
      "(10.10 min) Epoch 51/300 -- Iteration 48420 - Batch 270/963 - Train loss: 0.28814268  - Train acc: 0.8905 - Val loss: 0.99701852\n",
      "(10.10 min) Epoch 51/300 -- Iteration 48429 - Batch 279/963 - Train loss: 0.28857956  - Train acc: 0.8905 - Val loss: 0.99701852\n",
      "(10.10 min) Epoch 51/300 -- Iteration 48438 - Batch 288/963 - Train loss: 0.28897975  - Train acc: 0.8902 - Val loss: 0.99701852\n",
      "(10.11 min) Epoch 51/300 -- Iteration 48447 - Batch 297/963 - Train loss: 0.28922184  - Train acc: 0.8900 - Val loss: 0.99701852\n",
      "(10.11 min) Epoch 51/300 -- Iteration 48456 - Batch 306/963 - Train loss: 0.29002631  - Train acc: 0.8898 - Val loss: 0.99701852\n",
      "(10.11 min) Epoch 51/300 -- Iteration 48465 - Batch 315/963 - Train loss: 0.28988339  - Train acc: 0.8896 - Val loss: 0.99701852\n",
      "(10.11 min) Epoch 51/300 -- Iteration 48474 - Batch 324/963 - Train loss: 0.28981000  - Train acc: 0.8897 - Val loss: 0.99701852\n",
      "(10.11 min) Epoch 51/300 -- Iteration 48483 - Batch 333/963 - Train loss: 0.28983359  - Train acc: 0.8898 - Val loss: 0.99701852\n",
      "(10.12 min) Epoch 51/300 -- Iteration 48492 - Batch 342/963 - Train loss: 0.28992656  - Train acc: 0.8899 - Val loss: 0.99701852\n",
      "(10.12 min) Epoch 51/300 -- Iteration 48501 - Batch 351/963 - Train loss: 0.29007731  - Train acc: 0.8899 - Val loss: 0.99701852\n",
      "(10.12 min) Epoch 51/300 -- Iteration 48510 - Batch 360/963 - Train loss: 0.29021474  - Train acc: 0.8898 - Val loss: 0.99701852\n",
      "(10.12 min) Epoch 51/300 -- Iteration 48519 - Batch 369/963 - Train loss: 0.29027615  - Train acc: 0.8897 - Val loss: 0.99701852\n",
      "(10.12 min) Epoch 51/300 -- Iteration 48528 - Batch 378/963 - Train loss: 0.29105582  - Train acc: 0.8897 - Val loss: 0.99701852\n",
      "(10.13 min) Epoch 51/300 -- Iteration 48537 - Batch 387/963 - Train loss: 0.29116584  - Train acc: 0.8895 - Val loss: 0.99701852\n",
      "(10.13 min) Epoch 51/300 -- Iteration 48546 - Batch 396/963 - Train loss: 0.29086091  - Train acc: 0.8896 - Val loss: 0.99701852\n",
      "(10.13 min) Epoch 51/300 -- Iteration 48555 - Batch 405/963 - Train loss: 0.29134490  - Train acc: 0.8894 - Val loss: 0.99701852\n",
      "(10.13 min) Epoch 51/300 -- Iteration 48564 - Batch 414/963 - Train loss: 0.29091267  - Train acc: 0.8895 - Val loss: 0.99701852\n",
      "(10.13 min) Epoch 51/300 -- Iteration 48573 - Batch 423/963 - Train loss: 0.29088995  - Train acc: 0.8896 - Val loss: 0.99701852\n",
      "(10.13 min) Epoch 51/300 -- Iteration 48582 - Batch 432/963 - Train loss: 0.29111568  - Train acc: 0.8893 - Val loss: 0.99701852\n",
      "(10.14 min) Epoch 51/300 -- Iteration 48591 - Batch 441/963 - Train loss: 0.29088522  - Train acc: 0.8894 - Val loss: 0.99701852\n",
      "(10.14 min) Epoch 51/300 -- Iteration 48600 - Batch 450/963 - Train loss: 0.29119979  - Train acc: 0.8894 - Val loss: 0.99701852\n",
      "(10.14 min) Epoch 51/300 -- Iteration 48609 - Batch 459/963 - Train loss: 0.29144175  - Train acc: 0.8895 - Val loss: 0.99701852\n",
      "(10.14 min) Epoch 51/300 -- Iteration 48618 - Batch 468/963 - Train loss: 0.29119663  - Train acc: 0.8895 - Val loss: 0.99701852\n",
      "(10.14 min) Epoch 51/300 -- Iteration 48627 - Batch 477/963 - Train loss: 0.29169169  - Train acc: 0.8892 - Val loss: 0.99701852\n",
      "(10.15 min) Epoch 51/300 -- Iteration 48636 - Batch 486/963 - Train loss: 0.29145886  - Train acc: 0.8893 - Val loss: 0.99701852\n",
      "(10.15 min) Epoch 51/300 -- Iteration 48645 - Batch 495/963 - Train loss: 0.29144143  - Train acc: 0.8893 - Val loss: 0.99701852\n",
      "(10.15 min) Epoch 51/300 -- Iteration 48654 - Batch 504/963 - Train loss: 0.29133735  - Train acc: 0.8892 - Val loss: 0.99701852\n",
      "(10.15 min) Epoch 51/300 -- Iteration 48663 - Batch 513/963 - Train loss: 0.29130640  - Train acc: 0.8890 - Val loss: 0.99701852\n",
      "(10.15 min) Epoch 51/300 -- Iteration 48672 - Batch 522/963 - Train loss: 0.29144560  - Train acc: 0.8891 - Val loss: 0.99701852\n",
      "(10.16 min) Epoch 51/300 -- Iteration 48681 - Batch 531/963 - Train loss: 0.29167355  - Train acc: 0.8891 - Val loss: 0.99701852\n",
      "(10.16 min) Epoch 51/300 -- Iteration 48690 - Batch 540/963 - Train loss: 0.29144010  - Train acc: 0.8891 - Val loss: 0.99701852\n",
      "(10.16 min) Epoch 51/300 -- Iteration 48699 - Batch 549/963 - Train loss: 0.29161526  - Train acc: 0.8891 - Val loss: 0.99701852\n",
      "(10.16 min) Epoch 51/300 -- Iteration 48708 - Batch 558/963 - Train loss: 0.29124225  - Train acc: 0.8892 - Val loss: 0.99701852\n",
      "(10.16 min) Epoch 51/300 -- Iteration 48717 - Batch 567/963 - Train loss: 0.29138809  - Train acc: 0.8891 - Val loss: 0.99701852\n",
      "(10.16 min) Epoch 51/300 -- Iteration 48726 - Batch 576/963 - Train loss: 0.29108009  - Train acc: 0.8891 - Val loss: 0.99701852\n",
      "(10.17 min) Epoch 51/300 -- Iteration 48735 - Batch 585/963 - Train loss: 0.29055647  - Train acc: 0.8894 - Val loss: 0.99701852\n",
      "(10.17 min) Epoch 51/300 -- Iteration 48744 - Batch 594/963 - Train loss: 0.29016615  - Train acc: 0.8895 - Val loss: 0.99701852\n",
      "(10.17 min) Epoch 51/300 -- Iteration 48753 - Batch 603/963 - Train loss: 0.28976817  - Train acc: 0.8897 - Val loss: 0.99701852\n",
      "(10.17 min) Epoch 51/300 -- Iteration 48762 - Batch 612/963 - Train loss: 0.28968425  - Train acc: 0.8897 - Val loss: 0.99701852\n",
      "(10.17 min) Epoch 51/300 -- Iteration 48771 - Batch 621/963 - Train loss: 0.28937132  - Train acc: 0.8898 - Val loss: 0.99701852\n",
      "(10.18 min) Epoch 51/300 -- Iteration 48780 - Batch 630/963 - Train loss: 0.28911850  - Train acc: 0.8900 - Val loss: 0.99701852\n",
      "(10.18 min) Epoch 51/300 -- Iteration 48789 - Batch 639/963 - Train loss: 0.28920466  - Train acc: 0.8899 - Val loss: 0.99701852\n",
      "(10.18 min) Epoch 51/300 -- Iteration 48798 - Batch 648/963 - Train loss: 0.28955253  - Train acc: 0.8898 - Val loss: 0.99701852\n",
      "(10.18 min) Epoch 51/300 -- Iteration 48807 - Batch 657/963 - Train loss: 0.28914206  - Train acc: 0.8900 - Val loss: 0.99701852\n",
      "(10.18 min) Epoch 51/300 -- Iteration 48816 - Batch 666/963 - Train loss: 0.28900329  - Train acc: 0.8901 - Val loss: 0.99701852\n",
      "(10.19 min) Epoch 51/300 -- Iteration 48825 - Batch 675/963 - Train loss: 0.28931521  - Train acc: 0.8899 - Val loss: 0.99701852\n",
      "(10.19 min) Epoch 51/300 -- Iteration 48834 - Batch 684/963 - Train loss: 0.28901621  - Train acc: 0.8900 - Val loss: 0.99701852\n",
      "(10.19 min) Epoch 51/300 -- Iteration 48843 - Batch 693/963 - Train loss: 0.28878520  - Train acc: 0.8902 - Val loss: 0.99701852\n",
      "(10.19 min) Epoch 51/300 -- Iteration 48852 - Batch 702/963 - Train loss: 0.28817886  - Train acc: 0.8905 - Val loss: 0.99701852\n",
      "(10.19 min) Epoch 51/300 -- Iteration 48861 - Batch 711/963 - Train loss: 0.28797790  - Train acc: 0.8905 - Val loss: 0.99701852\n",
      "(10.19 min) Epoch 51/300 -- Iteration 48870 - Batch 720/963 - Train loss: 0.28823220  - Train acc: 0.8904 - Val loss: 0.99701852\n",
      "(10.20 min) Epoch 51/300 -- Iteration 48879 - Batch 729/963 - Train loss: 0.28819273  - Train acc: 0.8905 - Val loss: 0.99701852\n",
      "(10.20 min) Epoch 51/300 -- Iteration 48888 - Batch 738/963 - Train loss: 0.28783633  - Train acc: 0.8906 - Val loss: 0.99701852\n",
      "(10.20 min) Epoch 51/300 -- Iteration 48897 - Batch 747/963 - Train loss: 0.28761373  - Train acc: 0.8908 - Val loss: 0.99701852\n",
      "(10.20 min) Epoch 51/300 -- Iteration 48906 - Batch 756/963 - Train loss: 0.28757005  - Train acc: 0.8908 - Val loss: 0.99701852\n",
      "(10.20 min) Epoch 51/300 -- Iteration 48915 - Batch 765/963 - Train loss: 0.28757972  - Train acc: 0.8909 - Val loss: 0.99701852\n",
      "(10.21 min) Epoch 51/300 -- Iteration 48924 - Batch 774/963 - Train loss: 0.28747808  - Train acc: 0.8909 - Val loss: 0.99701852\n",
      "(10.21 min) Epoch 51/300 -- Iteration 48933 - Batch 783/963 - Train loss: 0.28739915  - Train acc: 0.8909 - Val loss: 0.99701852\n",
      "(10.21 min) Epoch 51/300 -- Iteration 48942 - Batch 792/963 - Train loss: 0.28750509  - Train acc: 0.8908 - Val loss: 0.99701852\n",
      "(10.21 min) Epoch 51/300 -- Iteration 48951 - Batch 801/963 - Train loss: 0.28740829  - Train acc: 0.8908 - Val loss: 0.99701852\n",
      "(10.21 min) Epoch 51/300 -- Iteration 48960 - Batch 810/963 - Train loss: 0.28747409  - Train acc: 0.8908 - Val loss: 0.99701852\n",
      "(10.22 min) Epoch 51/300 -- Iteration 48969 - Batch 819/963 - Train loss: 0.28759550  - Train acc: 0.8908 - Val loss: 0.99701852\n",
      "(10.22 min) Epoch 51/300 -- Iteration 48978 - Batch 828/963 - Train loss: 0.28760916  - Train acc: 0.8907 - Val loss: 0.99701852\n",
      "(10.22 min) Epoch 51/300 -- Iteration 48987 - Batch 837/963 - Train loss: 0.28782637  - Train acc: 0.8906 - Val loss: 0.99701852\n",
      "(10.22 min) Epoch 51/300 -- Iteration 48996 - Batch 846/963 - Train loss: 0.28755814  - Train acc: 0.8907 - Val loss: 0.99701852\n",
      "(10.22 min) Epoch 51/300 -- Iteration 49005 - Batch 855/963 - Train loss: 0.28775381  - Train acc: 0.8906 - Val loss: 0.99701852\n",
      "(10.22 min) Epoch 51/300 -- Iteration 49014 - Batch 864/963 - Train loss: 0.28786294  - Train acc: 0.8906 - Val loss: 0.99701852\n",
      "(10.23 min) Epoch 51/300 -- Iteration 49023 - Batch 873/963 - Train loss: 0.28778391  - Train acc: 0.8906 - Val loss: 0.99701852\n",
      "(10.23 min) Epoch 51/300 -- Iteration 49032 - Batch 882/963 - Train loss: 0.28763327  - Train acc: 0.8907 - Val loss: 0.99701852\n",
      "(10.23 min) Epoch 51/300 -- Iteration 49041 - Batch 891/963 - Train loss: 0.28817348  - Train acc: 0.8904 - Val loss: 0.99701852\n",
      "(10.23 min) Epoch 51/300 -- Iteration 49050 - Batch 900/963 - Train loss: 0.28841274  - Train acc: 0.8902 - Val loss: 0.99701852\n",
      "(10.23 min) Epoch 51/300 -- Iteration 49059 - Batch 909/963 - Train loss: 0.28813577  - Train acc: 0.8902 - Val loss: 0.99701852\n",
      "(10.24 min) Epoch 51/300 -- Iteration 49068 - Batch 918/963 - Train loss: 0.28803549  - Train acc: 0.8902 - Val loss: 0.99701852\n",
      "(10.24 min) Epoch 51/300 -- Iteration 49077 - Batch 927/963 - Train loss: 0.28782300  - Train acc: 0.8902 - Val loss: 0.99701852\n",
      "(10.24 min) Epoch 51/300 -- Iteration 49086 - Batch 936/963 - Train loss: 0.28739188  - Train acc: 0.8903 - Val loss: 0.99701852\n",
      "(10.24 min) Epoch 51/300 -- Iteration 49095 - Batch 945/963 - Train loss: 0.28758034  - Train acc: 0.8902 - Val loss: 0.99701852\n",
      "(10.24 min) Epoch 51/300 -- Iteration 49104 - Batch 954/963 - Train loss: 0.28746648  - Train acc: 0.8902 - Val loss: 0.99701852\n",
      "(10.25 min) Epoch 51/300 -- Iteration 49113 - Batch 962/963 - Train loss: 0.28738154  - Train acc: 0.8902 - Val loss: 0.98091686 - Val acc: 0.5950\n",
      "(10.25 min) Epoch 52/300 -- Iteration 49122 - Batch 9/963 - Train loss: 0.28827391  - Train acc: 0.8930 - Val loss: 0.98091686\n",
      "(10.25 min) Epoch 52/300 -- Iteration 49131 - Batch 18/963 - Train loss: 0.29416479  - Train acc: 0.8898 - Val loss: 0.98091686\n",
      "(10.25 min) Epoch 52/300 -- Iteration 49140 - Batch 27/963 - Train loss: 0.28822816  - Train acc: 0.8931 - Val loss: 0.98091686\n",
      "(10.25 min) Epoch 52/300 -- Iteration 49149 - Batch 36/963 - Train loss: 0.28685808  - Train acc: 0.8929 - Val loss: 0.98091686\n",
      "(10.26 min) Epoch 52/300 -- Iteration 49158 - Batch 45/963 - Train loss: 0.28657161  - Train acc: 0.8925 - Val loss: 0.98091686\n",
      "(10.26 min) Epoch 52/300 -- Iteration 49167 - Batch 54/963 - Train loss: 0.29094089  - Train acc: 0.8908 - Val loss: 0.98091686\n",
      "(10.26 min) Epoch 52/300 -- Iteration 49176 - Batch 63/963 - Train loss: 0.28870409  - Train acc: 0.8906 - Val loss: 0.98091686\n",
      "(10.26 min) Epoch 52/300 -- Iteration 49185 - Batch 72/963 - Train loss: 0.29118498  - Train acc: 0.8890 - Val loss: 0.98091686\n",
      "(10.26 min) Epoch 52/300 -- Iteration 49194 - Batch 81/963 - Train loss: 0.29063537  - Train acc: 0.8898 - Val loss: 0.98091686\n",
      "(10.27 min) Epoch 52/300 -- Iteration 49203 - Batch 90/963 - Train loss: 0.29439970  - Train acc: 0.8883 - Val loss: 0.98091686\n",
      "(10.27 min) Epoch 52/300 -- Iteration 49212 - Batch 99/963 - Train loss: 0.29274785  - Train acc: 0.8890 - Val loss: 0.98091686\n",
      "(10.27 min) Epoch 52/300 -- Iteration 49221 - Batch 108/963 - Train loss: 0.29384672  - Train acc: 0.8889 - Val loss: 0.98091686\n",
      "(10.27 min) Epoch 52/300 -- Iteration 49230 - Batch 117/963 - Train loss: 0.29494662  - Train acc: 0.8884 - Val loss: 0.98091686\n",
      "(10.27 min) Epoch 52/300 -- Iteration 49239 - Batch 126/963 - Train loss: 0.29431539  - Train acc: 0.8880 - Val loss: 0.98091686\n",
      "(10.27 min) Epoch 52/300 -- Iteration 49248 - Batch 135/963 - Train loss: 0.29509048  - Train acc: 0.8872 - Val loss: 0.98091686\n",
      "(10.28 min) Epoch 52/300 -- Iteration 49257 - Batch 144/963 - Train loss: 0.29564156  - Train acc: 0.8873 - Val loss: 0.98091686\n",
      "(10.28 min) Epoch 52/300 -- Iteration 49266 - Batch 153/963 - Train loss: 0.29459398  - Train acc: 0.8880 - Val loss: 0.98091686\n",
      "(10.28 min) Epoch 52/300 -- Iteration 49275 - Batch 162/963 - Train loss: 0.29307170  - Train acc: 0.8888 - Val loss: 0.98091686\n",
      "(10.28 min) Epoch 52/300 -- Iteration 49284 - Batch 171/963 - Train loss: 0.29234061  - Train acc: 0.8889 - Val loss: 0.98091686\n",
      "(10.28 min) Epoch 52/300 -- Iteration 49293 - Batch 180/963 - Train loss: 0.29071700  - Train acc: 0.8900 - Val loss: 0.98091686\n",
      "(10.29 min) Epoch 52/300 -- Iteration 49302 - Batch 189/963 - Train loss: 0.28990909  - Train acc: 0.8907 - Val loss: 0.98091686\n",
      "(10.29 min) Epoch 52/300 -- Iteration 49311 - Batch 198/963 - Train loss: 0.28995929  - Train acc: 0.8905 - Val loss: 0.98091686\n",
      "(10.29 min) Epoch 52/300 -- Iteration 49320 - Batch 207/963 - Train loss: 0.29031942  - Train acc: 0.8903 - Val loss: 0.98091686\n",
      "(10.29 min) Epoch 52/300 -- Iteration 49329 - Batch 216/963 - Train loss: 0.28930512  - Train acc: 0.8908 - Val loss: 0.98091686\n",
      "(10.29 min) Epoch 52/300 -- Iteration 49338 - Batch 225/963 - Train loss: 0.28817368  - Train acc: 0.8910 - Val loss: 0.98091686\n",
      "(10.29 min) Epoch 52/300 -- Iteration 49347 - Batch 234/963 - Train loss: 0.28919265  - Train acc: 0.8905 - Val loss: 0.98091686\n",
      "(10.30 min) Epoch 52/300 -- Iteration 49356 - Batch 243/963 - Train loss: 0.28925837  - Train acc: 0.8901 - Val loss: 0.98091686\n",
      "(10.30 min) Epoch 52/300 -- Iteration 49365 - Batch 252/963 - Train loss: 0.28902549  - Train acc: 0.8903 - Val loss: 0.98091686\n",
      "(10.30 min) Epoch 52/300 -- Iteration 49374 - Batch 261/963 - Train loss: 0.28931935  - Train acc: 0.8899 - Val loss: 0.98091686\n",
      "(10.30 min) Epoch 52/300 -- Iteration 49383 - Batch 270/963 - Train loss: 0.28908853  - Train acc: 0.8903 - Val loss: 0.98091686\n",
      "(10.30 min) Epoch 52/300 -- Iteration 49392 - Batch 279/963 - Train loss: 0.29010710  - Train acc: 0.8900 - Val loss: 0.98091686\n",
      "(10.31 min) Epoch 52/300 -- Iteration 49401 - Batch 288/963 - Train loss: 0.29027309  - Train acc: 0.8898 - Val loss: 0.98091686\n",
      "(10.31 min) Epoch 52/300 -- Iteration 49410 - Batch 297/963 - Train loss: 0.28989087  - Train acc: 0.8897 - Val loss: 0.98091686\n",
      "(10.31 min) Epoch 52/300 -- Iteration 49419 - Batch 306/963 - Train loss: 0.28920923  - Train acc: 0.8899 - Val loss: 0.98091686\n",
      "(10.31 min) Epoch 52/300 -- Iteration 49428 - Batch 315/963 - Train loss: 0.28840327  - Train acc: 0.8904 - Val loss: 0.98091686\n",
      "(10.31 min) Epoch 52/300 -- Iteration 49437 - Batch 324/963 - Train loss: 0.28825065  - Train acc: 0.8904 - Val loss: 0.98091686\n",
      "(10.32 min) Epoch 52/300 -- Iteration 49446 - Batch 333/963 - Train loss: 0.28767458  - Train acc: 0.8905 - Val loss: 0.98091686\n",
      "(10.32 min) Epoch 52/300 -- Iteration 49455 - Batch 342/963 - Train loss: 0.28683873  - Train acc: 0.8908 - Val loss: 0.98091686\n",
      "(10.32 min) Epoch 52/300 -- Iteration 49464 - Batch 351/963 - Train loss: 0.28731411  - Train acc: 0.8907 - Val loss: 0.98091686\n",
      "(10.32 min) Epoch 52/300 -- Iteration 49473 - Batch 360/963 - Train loss: 0.28706716  - Train acc: 0.8908 - Val loss: 0.98091686\n",
      "(10.32 min) Epoch 52/300 -- Iteration 49482 - Batch 369/963 - Train loss: 0.28690038  - Train acc: 0.8909 - Val loss: 0.98091686\n",
      "(10.33 min) Epoch 52/300 -- Iteration 49491 - Batch 378/963 - Train loss: 0.28644341  - Train acc: 0.8910 - Val loss: 0.98091686\n",
      "(10.33 min) Epoch 52/300 -- Iteration 49500 - Batch 387/963 - Train loss: 0.28700429  - Train acc: 0.8909 - Val loss: 0.98091686\n",
      "(10.33 min) Epoch 52/300 -- Iteration 49509 - Batch 396/963 - Train loss: 0.28641316  - Train acc: 0.8914 - Val loss: 0.98091686\n",
      "(10.33 min) Epoch 52/300 -- Iteration 49518 - Batch 405/963 - Train loss: 0.28651342  - Train acc: 0.8912 - Val loss: 0.98091686\n",
      "(10.33 min) Epoch 52/300 -- Iteration 49527 - Batch 414/963 - Train loss: 0.28674164  - Train acc: 0.8911 - Val loss: 0.98091686\n",
      "(10.33 min) Epoch 52/300 -- Iteration 49536 - Batch 423/963 - Train loss: 0.28636572  - Train acc: 0.8914 - Val loss: 0.98091686\n",
      "(10.34 min) Epoch 52/300 -- Iteration 49545 - Batch 432/963 - Train loss: 0.28632717  - Train acc: 0.8914 - Val loss: 0.98091686\n",
      "(10.34 min) Epoch 52/300 -- Iteration 49554 - Batch 441/963 - Train loss: 0.28597855  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.34 min) Epoch 52/300 -- Iteration 49563 - Batch 450/963 - Train loss: 0.28593824  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.34 min) Epoch 52/300 -- Iteration 49572 - Batch 459/963 - Train loss: 0.28625467  - Train acc: 0.8915 - Val loss: 0.98091686\n",
      "(10.34 min) Epoch 52/300 -- Iteration 49581 - Batch 468/963 - Train loss: 0.28603102  - Train acc: 0.8916 - Val loss: 0.98091686\n",
      "(10.35 min) Epoch 52/300 -- Iteration 49590 - Batch 477/963 - Train loss: 0.28608926  - Train acc: 0.8915 - Val loss: 0.98091686\n",
      "(10.35 min) Epoch 52/300 -- Iteration 49599 - Batch 486/963 - Train loss: 0.28607060  - Train acc: 0.8914 - Val loss: 0.98091686\n",
      "(10.35 min) Epoch 52/300 -- Iteration 49608 - Batch 495/963 - Train loss: 0.28597894  - Train acc: 0.8914 - Val loss: 0.98091686\n",
      "(10.35 min) Epoch 52/300 -- Iteration 49617 - Batch 504/963 - Train loss: 0.28580888  - Train acc: 0.8914 - Val loss: 0.98091686\n",
      "(10.35 min) Epoch 52/300 -- Iteration 49626 - Batch 513/963 - Train loss: 0.28534400  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.36 min) Epoch 52/300 -- Iteration 49635 - Batch 522/963 - Train loss: 0.28561721  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.36 min) Epoch 52/300 -- Iteration 49644 - Batch 531/963 - Train loss: 0.28572965  - Train acc: 0.8916 - Val loss: 0.98091686\n",
      "(10.36 min) Epoch 52/300 -- Iteration 49653 - Batch 540/963 - Train loss: 0.28653281  - Train acc: 0.8912 - Val loss: 0.98091686\n",
      "(10.36 min) Epoch 52/300 -- Iteration 49662 - Batch 549/963 - Train loss: 0.28658969  - Train acc: 0.8912 - Val loss: 0.98091686\n",
      "(10.36 min) Epoch 52/300 -- Iteration 49671 - Batch 558/963 - Train loss: 0.28641024  - Train acc: 0.8913 - Val loss: 0.98091686\n",
      "(10.36 min) Epoch 52/300 -- Iteration 49680 - Batch 567/963 - Train loss: 0.28631313  - Train acc: 0.8915 - Val loss: 0.98091686\n",
      "(10.37 min) Epoch 52/300 -- Iteration 49689 - Batch 576/963 - Train loss: 0.28611524  - Train acc: 0.8916 - Val loss: 0.98091686\n",
      "(10.37 min) Epoch 52/300 -- Iteration 49698 - Batch 585/963 - Train loss: 0.28561848  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.37 min) Epoch 52/300 -- Iteration 49707 - Batch 594/963 - Train loss: 0.28556629  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.37 min) Epoch 52/300 -- Iteration 49716 - Batch 603/963 - Train loss: 0.28564334  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.37 min) Epoch 52/300 -- Iteration 49725 - Batch 612/963 - Train loss: 0.28577172  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.38 min) Epoch 52/300 -- Iteration 49734 - Batch 621/963 - Train loss: 0.28597031  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.38 min) Epoch 52/300 -- Iteration 49743 - Batch 630/963 - Train loss: 0.28575397  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.38 min) Epoch 52/300 -- Iteration 49752 - Batch 639/963 - Train loss: 0.28560259  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.38 min) Epoch 52/300 -- Iteration 49761 - Batch 648/963 - Train loss: 0.28595561  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.38 min) Epoch 52/300 -- Iteration 49770 - Batch 657/963 - Train loss: 0.28602882  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.39 min) Epoch 52/300 -- Iteration 49779 - Batch 666/963 - Train loss: 0.28572359  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.39 min) Epoch 52/300 -- Iteration 49788 - Batch 675/963 - Train loss: 0.28553273  - Train acc: 0.8920 - Val loss: 0.98091686\n",
      "(10.39 min) Epoch 52/300 -- Iteration 49797 - Batch 684/963 - Train loss: 0.28579741  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.39 min) Epoch 52/300 -- Iteration 49806 - Batch 693/963 - Train loss: 0.28620264  - Train acc: 0.8916 - Val loss: 0.98091686\n",
      "(10.39 min) Epoch 52/300 -- Iteration 49815 - Batch 702/963 - Train loss: 0.28598104  - Train acc: 0.8916 - Val loss: 0.98091686\n",
      "(10.39 min) Epoch 52/300 -- Iteration 49824 - Batch 711/963 - Train loss: 0.28590688  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.40 min) Epoch 52/300 -- Iteration 49833 - Batch 720/963 - Train loss: 0.28593163  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.40 min) Epoch 52/300 -- Iteration 49842 - Batch 729/963 - Train loss: 0.28609216  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.40 min) Epoch 52/300 -- Iteration 49851 - Batch 738/963 - Train loss: 0.28632980  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.40 min) Epoch 52/300 -- Iteration 49860 - Batch 747/963 - Train loss: 0.28609193  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.40 min) Epoch 52/300 -- Iteration 49869 - Batch 756/963 - Train loss: 0.28619036  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.41 min) Epoch 52/300 -- Iteration 49878 - Batch 765/963 - Train loss: 0.28595281  - Train acc: 0.8920 - Val loss: 0.98091686\n",
      "(10.41 min) Epoch 52/300 -- Iteration 49887 - Batch 774/963 - Train loss: 0.28632699  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.41 min) Epoch 52/300 -- Iteration 49896 - Batch 783/963 - Train loss: 0.28655598  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.41 min) Epoch 52/300 -- Iteration 49905 - Batch 792/963 - Train loss: 0.28651553  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.41 min) Epoch 52/300 -- Iteration 49914 - Batch 801/963 - Train loss: 0.28640904  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.42 min) Epoch 52/300 -- Iteration 49923 - Batch 810/963 - Train loss: 0.28661304  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.42 min) Epoch 52/300 -- Iteration 49932 - Batch 819/963 - Train loss: 0.28707355  - Train acc: 0.8915 - Val loss: 0.98091686\n",
      "(10.42 min) Epoch 52/300 -- Iteration 49941 - Batch 828/963 - Train loss: 0.28695646  - Train acc: 0.8915 - Val loss: 0.98091686\n",
      "(10.42 min) Epoch 52/300 -- Iteration 49950 - Batch 837/963 - Train loss: 0.28698525  - Train acc: 0.8915 - Val loss: 0.98091686\n",
      "(10.42 min) Epoch 52/300 -- Iteration 49959 - Batch 846/963 - Train loss: 0.28666606  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.42 min) Epoch 52/300 -- Iteration 49968 - Batch 855/963 - Train loss: 0.28653063  - Train acc: 0.8917 - Val loss: 0.98091686\n",
      "(10.43 min) Epoch 52/300 -- Iteration 49977 - Batch 864/963 - Train loss: 0.28644802  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.43 min) Epoch 52/300 -- Iteration 49986 - Batch 873/963 - Train loss: 0.28620132  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.43 min) Epoch 52/300 -- Iteration 49995 - Batch 882/963 - Train loss: 0.28616758  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.43 min) Epoch 52/300 -- Iteration 50004 - Batch 891/963 - Train loss: 0.28611744  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.44 min) Epoch 52/300 -- Iteration 50013 - Batch 900/963 - Train loss: 0.28590848  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.44 min) Epoch 52/300 -- Iteration 50022 - Batch 909/963 - Train loss: 0.28598250  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.44 min) Epoch 52/300 -- Iteration 50031 - Batch 918/963 - Train loss: 0.28608900  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.44 min) Epoch 52/300 -- Iteration 50040 - Batch 927/963 - Train loss: 0.28609459  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.44 min) Epoch 52/300 -- Iteration 50049 - Batch 936/963 - Train loss: 0.28597310  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.45 min) Epoch 52/300 -- Iteration 50058 - Batch 945/963 - Train loss: 0.28609969  - Train acc: 0.8918 - Val loss: 0.98091686\n",
      "(10.45 min) Epoch 52/300 -- Iteration 50067 - Batch 954/963 - Train loss: 0.28580846  - Train acc: 0.8919 - Val loss: 0.98091686\n",
      "(10.45 min) Epoch 52/300 -- Iteration 50076 - Batch 962/963 - Train loss: 0.28603993  - Train acc: 0.8918 - Val loss: 0.97176075 - Val acc: 0.5983\n",
      "(10.45 min) Epoch 53/300 -- Iteration 50085 - Batch 9/963 - Train loss: 0.29635225  - Train acc: 0.8859 - Val loss: 0.97176075\n",
      "(10.45 min) Epoch 53/300 -- Iteration 50094 - Batch 18/963 - Train loss: 0.28856112  - Train acc: 0.8902 - Val loss: 0.97176075\n",
      "(10.46 min) Epoch 53/300 -- Iteration 50103 - Batch 27/963 - Train loss: 0.28321429  - Train acc: 0.8934 - Val loss: 0.97176075\n",
      "(10.46 min) Epoch 53/300 -- Iteration 50112 - Batch 36/963 - Train loss: 0.28013712  - Train acc: 0.8946 - Val loss: 0.97176075\n",
      "(10.46 min) Epoch 53/300 -- Iteration 50121 - Batch 45/963 - Train loss: 0.28440490  - Train acc: 0.8933 - Val loss: 0.97176075\n",
      "(10.46 min) Epoch 53/300 -- Iteration 50130 - Batch 54/963 - Train loss: 0.28410488  - Train acc: 0.8940 - Val loss: 0.97176075\n",
      "(10.46 min) Epoch 53/300 -- Iteration 50139 - Batch 63/963 - Train loss: 0.28755387  - Train acc: 0.8929 - Val loss: 0.97176075\n",
      "(10.47 min) Epoch 53/300 -- Iteration 50148 - Batch 72/963 - Train loss: 0.28407893  - Train acc: 0.8946 - Val loss: 0.97176075\n",
      "(10.47 min) Epoch 53/300 -- Iteration 50157 - Batch 81/963 - Train loss: 0.28034833  - Train acc: 0.8959 - Val loss: 0.97176075\n",
      "(10.47 min) Epoch 53/300 -- Iteration 50166 - Batch 90/963 - Train loss: 0.27893149  - Train acc: 0.8957 - Val loss: 0.97176075\n",
      "(10.47 min) Epoch 53/300 -- Iteration 50175 - Batch 99/963 - Train loss: 0.27906012  - Train acc: 0.8949 - Val loss: 0.97176075\n",
      "(10.47 min) Epoch 53/300 -- Iteration 50184 - Batch 108/963 - Train loss: 0.27918064  - Train acc: 0.8954 - Val loss: 0.97176075\n",
      "(10.48 min) Epoch 53/300 -- Iteration 50193 - Batch 117/963 - Train loss: 0.28099410  - Train acc: 0.8951 - Val loss: 0.97176075\n",
      "(10.48 min) Epoch 53/300 -- Iteration 50202 - Batch 126/963 - Train loss: 0.28102453  - Train acc: 0.8951 - Val loss: 0.97176075\n",
      "(10.48 min) Epoch 53/300 -- Iteration 50211 - Batch 135/963 - Train loss: 0.28236928  - Train acc: 0.8951 - Val loss: 0.97176075\n",
      "(10.48 min) Epoch 53/300 -- Iteration 50220 - Batch 144/963 - Train loss: 0.28287509  - Train acc: 0.8948 - Val loss: 0.97176075\n",
      "(10.48 min) Epoch 53/300 -- Iteration 50229 - Batch 153/963 - Train loss: 0.28309130  - Train acc: 0.8950 - Val loss: 0.97176075\n",
      "(10.49 min) Epoch 53/300 -- Iteration 50238 - Batch 162/963 - Train loss: 0.28310605  - Train acc: 0.8948 - Val loss: 0.97176075\n",
      "(10.49 min) Epoch 53/300 -- Iteration 50247 - Batch 171/963 - Train loss: 0.28321153  - Train acc: 0.8948 - Val loss: 0.97176075\n",
      "(10.49 min) Epoch 53/300 -- Iteration 50256 - Batch 180/963 - Train loss: 0.28428419  - Train acc: 0.8940 - Val loss: 0.97176075\n",
      "(10.49 min) Epoch 53/300 -- Iteration 50265 - Batch 189/963 - Train loss: 0.28434229  - Train acc: 0.8937 - Val loss: 0.97176075\n",
      "(10.49 min) Epoch 53/300 -- Iteration 50274 - Batch 198/963 - Train loss: 0.28468256  - Train acc: 0.8932 - Val loss: 0.97176075\n",
      "(10.50 min) Epoch 53/300 -- Iteration 50283 - Batch 207/963 - Train loss: 0.28525085  - Train acc: 0.8925 - Val loss: 0.97176075\n",
      "(10.50 min) Epoch 53/300 -- Iteration 50292 - Batch 216/963 - Train loss: 0.28336826  - Train acc: 0.8932 - Val loss: 0.97176075\n",
      "(10.50 min) Epoch 53/300 -- Iteration 50301 - Batch 225/963 - Train loss: 0.28402414  - Train acc: 0.8931 - Val loss: 0.97176075\n",
      "(10.50 min) Epoch 53/300 -- Iteration 50310 - Batch 234/963 - Train loss: 0.28416996  - Train acc: 0.8934 - Val loss: 0.97176075\n",
      "(10.50 min) Epoch 53/300 -- Iteration 50319 - Batch 243/963 - Train loss: 0.28391971  - Train acc: 0.8936 - Val loss: 0.97176075\n",
      "(10.51 min) Epoch 53/300 -- Iteration 50328 - Batch 252/963 - Train loss: 0.28389029  - Train acc: 0.8937 - Val loss: 0.97176075\n",
      "(10.51 min) Epoch 53/300 -- Iteration 50337 - Batch 261/963 - Train loss: 0.28383080  - Train acc: 0.8936 - Val loss: 0.97176075\n",
      "(10.51 min) Epoch 53/300 -- Iteration 50346 - Batch 270/963 - Train loss: 0.28396513  - Train acc: 0.8937 - Val loss: 0.97176075\n",
      "(10.51 min) Epoch 53/300 -- Iteration 50355 - Batch 279/963 - Train loss: 0.28522110  - Train acc: 0.8930 - Val loss: 0.97176075\n",
      "(10.51 min) Epoch 53/300 -- Iteration 50364 - Batch 288/963 - Train loss: 0.28531898  - Train acc: 0.8930 - Val loss: 0.97176075\n",
      "(10.52 min) Epoch 53/300 -- Iteration 50373 - Batch 297/963 - Train loss: 0.28599608  - Train acc: 0.8924 - Val loss: 0.97176075\n",
      "(10.52 min) Epoch 53/300 -- Iteration 50382 - Batch 306/963 - Train loss: 0.28473868  - Train acc: 0.8926 - Val loss: 0.97176075\n",
      "(10.52 min) Epoch 53/300 -- Iteration 50391 - Batch 315/963 - Train loss: 0.28460331  - Train acc: 0.8926 - Val loss: 0.97176075\n",
      "(10.52 min) Epoch 53/300 -- Iteration 50400 - Batch 324/963 - Train loss: 0.28385073  - Train acc: 0.8929 - Val loss: 0.97176075\n",
      "(10.53 min) Epoch 53/300 -- Iteration 50409 - Batch 333/963 - Train loss: 0.28450468  - Train acc: 0.8928 - Val loss: 0.97176075\n",
      "(10.53 min) Epoch 53/300 -- Iteration 50418 - Batch 342/963 - Train loss: 0.28491130  - Train acc: 0.8926 - Val loss: 0.97176075\n",
      "(10.53 min) Epoch 53/300 -- Iteration 50427 - Batch 351/963 - Train loss: 0.28514665  - Train acc: 0.8923 - Val loss: 0.97176075\n",
      "(10.53 min) Epoch 53/300 -- Iteration 50436 - Batch 360/963 - Train loss: 0.28585899  - Train acc: 0.8921 - Val loss: 0.97176075\n",
      "(10.53 min) Epoch 53/300 -- Iteration 50445 - Batch 369/963 - Train loss: 0.28584276  - Train acc: 0.8919 - Val loss: 0.97176075\n",
      "(10.54 min) Epoch 53/300 -- Iteration 50454 - Batch 378/963 - Train loss: 0.28524662  - Train acc: 0.8922 - Val loss: 0.97176075\n",
      "(10.54 min) Epoch 53/300 -- Iteration 50463 - Batch 387/963 - Train loss: 0.28467593  - Train acc: 0.8924 - Val loss: 0.97176075\n",
      "(10.54 min) Epoch 53/300 -- Iteration 50472 - Batch 396/963 - Train loss: 0.28529747  - Train acc: 0.8920 - Val loss: 0.97176075\n",
      "(10.54 min) Epoch 53/300 -- Iteration 50481 - Batch 405/963 - Train loss: 0.28590803  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.54 min) Epoch 53/300 -- Iteration 50490 - Batch 414/963 - Train loss: 0.28542906  - Train acc: 0.8920 - Val loss: 0.97176075\n",
      "(10.55 min) Epoch 53/300 -- Iteration 50499 - Batch 423/963 - Train loss: 0.28551113  - Train acc: 0.8919 - Val loss: 0.97176075\n",
      "(10.55 min) Epoch 53/300 -- Iteration 50508 - Batch 432/963 - Train loss: 0.28589177  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.55 min) Epoch 53/300 -- Iteration 50517 - Batch 441/963 - Train loss: 0.28616463  - Train acc: 0.8917 - Val loss: 0.97176075\n",
      "(10.55 min) Epoch 53/300 -- Iteration 50526 - Batch 450/963 - Train loss: 0.28647775  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.55 min) Epoch 53/300 -- Iteration 50535 - Batch 459/963 - Train loss: 0.28690173  - Train acc: 0.8916 - Val loss: 0.97176075\n",
      "(10.56 min) Epoch 53/300 -- Iteration 50544 - Batch 468/963 - Train loss: 0.28667775  - Train acc: 0.8916 - Val loss: 0.97176075\n",
      "(10.56 min) Epoch 53/300 -- Iteration 50553 - Batch 477/963 - Train loss: 0.28662073  - Train acc: 0.8915 - Val loss: 0.97176075\n",
      "(10.56 min) Epoch 53/300 -- Iteration 50562 - Batch 486/963 - Train loss: 0.28633940  - Train acc: 0.8916 - Val loss: 0.97176075\n",
      "(10.56 min) Epoch 53/300 -- Iteration 50571 - Batch 495/963 - Train loss: 0.28673975  - Train acc: 0.8916 - Val loss: 0.97176075\n",
      "(10.56 min) Epoch 53/300 -- Iteration 50580 - Batch 504/963 - Train loss: 0.28683455  - Train acc: 0.8917 - Val loss: 0.97176075\n",
      "(10.57 min) Epoch 53/300 -- Iteration 50589 - Batch 513/963 - Train loss: 0.28684738  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.57 min) Epoch 53/300 -- Iteration 50598 - Batch 522/963 - Train loss: 0.28681058  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.57 min) Epoch 53/300 -- Iteration 50607 - Batch 531/963 - Train loss: 0.28715133  - Train acc: 0.8916 - Val loss: 0.97176075\n",
      "(10.57 min) Epoch 53/300 -- Iteration 50616 - Batch 540/963 - Train loss: 0.28670416  - Train acc: 0.8917 - Val loss: 0.97176075\n",
      "(10.58 min) Epoch 53/300 -- Iteration 50625 - Batch 549/963 - Train loss: 0.28679527  - Train acc: 0.8917 - Val loss: 0.97176075\n",
      "(10.58 min) Epoch 53/300 -- Iteration 50634 - Batch 558/963 - Train loss: 0.28675381  - Train acc: 0.8917 - Val loss: 0.97176075\n",
      "(10.58 min) Epoch 53/300 -- Iteration 50643 - Batch 567/963 - Train loss: 0.28728301  - Train acc: 0.8915 - Val loss: 0.97176075\n",
      "(10.58 min) Epoch 53/300 -- Iteration 50652 - Batch 576/963 - Train loss: 0.28740844  - Train acc: 0.8914 - Val loss: 0.97176075\n",
      "(10.58 min) Epoch 53/300 -- Iteration 50661 - Batch 585/963 - Train loss: 0.28692359  - Train acc: 0.8915 - Val loss: 0.97176075\n",
      "(10.59 min) Epoch 53/300 -- Iteration 50670 - Batch 594/963 - Train loss: 0.28721669  - Train acc: 0.8915 - Val loss: 0.97176075\n",
      "(10.59 min) Epoch 53/300 -- Iteration 50679 - Batch 603/963 - Train loss: 0.28692096  - Train acc: 0.8915 - Val loss: 0.97176075\n",
      "(10.59 min) Epoch 53/300 -- Iteration 50688 - Batch 612/963 - Train loss: 0.28672042  - Train acc: 0.8916 - Val loss: 0.97176075\n",
      "(10.59 min) Epoch 53/300 -- Iteration 50697 - Batch 621/963 - Train loss: 0.28707866  - Train acc: 0.8913 - Val loss: 0.97176075\n",
      "(10.59 min) Epoch 53/300 -- Iteration 50706 - Batch 630/963 - Train loss: 0.28671742  - Train acc: 0.8915 - Val loss: 0.97176075\n",
      "(10.60 min) Epoch 53/300 -- Iteration 50715 - Batch 639/963 - Train loss: 0.28648148  - Train acc: 0.8915 - Val loss: 0.97176075\n",
      "(10.60 min) Epoch 53/300 -- Iteration 50724 - Batch 648/963 - Train loss: 0.28633591  - Train acc: 0.8916 - Val loss: 0.97176075\n",
      "(10.60 min) Epoch 53/300 -- Iteration 50733 - Batch 657/963 - Train loss: 0.28656229  - Train acc: 0.8915 - Val loss: 0.97176075\n",
      "(10.60 min) Epoch 53/300 -- Iteration 50742 - Batch 666/963 - Train loss: 0.28619358  - Train acc: 0.8917 - Val loss: 0.97176075\n",
      "(10.60 min) Epoch 53/300 -- Iteration 50751 - Batch 675/963 - Train loss: 0.28619452  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.61 min) Epoch 53/300 -- Iteration 50760 - Batch 684/963 - Train loss: 0.28598615  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.61 min) Epoch 53/300 -- Iteration 50769 - Batch 693/963 - Train loss: 0.28600132  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.61 min) Epoch 53/300 -- Iteration 50778 - Batch 702/963 - Train loss: 0.28607163  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.61 min) Epoch 53/300 -- Iteration 50787 - Batch 711/963 - Train loss: 0.28607896  - Train acc: 0.8919 - Val loss: 0.97176075\n",
      "(10.61 min) Epoch 53/300 -- Iteration 50796 - Batch 720/963 - Train loss: 0.28610107  - Train acc: 0.8919 - Val loss: 0.97176075\n",
      "(10.62 min) Epoch 53/300 -- Iteration 50805 - Batch 729/963 - Train loss: 0.28598862  - Train acc: 0.8920 - Val loss: 0.97176075\n",
      "(10.62 min) Epoch 53/300 -- Iteration 50814 - Batch 738/963 - Train loss: 0.28565215  - Train acc: 0.8921 - Val loss: 0.97176075\n",
      "(10.62 min) Epoch 53/300 -- Iteration 50823 - Batch 747/963 - Train loss: 0.28544131  - Train acc: 0.8921 - Val loss: 0.97176075\n",
      "(10.62 min) Epoch 53/300 -- Iteration 50832 - Batch 756/963 - Train loss: 0.28570349  - Train acc: 0.8920 - Val loss: 0.97176075\n",
      "(10.62 min) Epoch 53/300 -- Iteration 50841 - Batch 765/963 - Train loss: 0.28535414  - Train acc: 0.8921 - Val loss: 0.97176075\n",
      "(10.63 min) Epoch 53/300 -- Iteration 50850 - Batch 774/963 - Train loss: 0.28562817  - Train acc: 0.8920 - Val loss: 0.97176075\n",
      "(10.63 min) Epoch 53/300 -- Iteration 50859 - Batch 783/963 - Train loss: 0.28561553  - Train acc: 0.8920 - Val loss: 0.97176075\n",
      "(10.63 min) Epoch 53/300 -- Iteration 50868 - Batch 792/963 - Train loss: 0.28574787  - Train acc: 0.8919 - Val loss: 0.97176075\n",
      "(10.63 min) Epoch 53/300 -- Iteration 50877 - Batch 801/963 - Train loss: 0.28553477  - Train acc: 0.8920 - Val loss: 0.97176075\n",
      "(10.64 min) Epoch 53/300 -- Iteration 50886 - Batch 810/963 - Train loss: 0.28554906  - Train acc: 0.8920 - Val loss: 0.97176075\n",
      "(10.64 min) Epoch 53/300 -- Iteration 50895 - Batch 819/963 - Train loss: 0.28550724  - Train acc: 0.8919 - Val loss: 0.97176075\n",
      "(10.64 min) Epoch 53/300 -- Iteration 50904 - Batch 828/963 - Train loss: 0.28578112  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.64 min) Epoch 53/300 -- Iteration 50913 - Batch 837/963 - Train loss: 0.28567540  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.64 min) Epoch 53/300 -- Iteration 50922 - Batch 846/963 - Train loss: 0.28540573  - Train acc: 0.8919 - Val loss: 0.97176075\n",
      "(10.65 min) Epoch 53/300 -- Iteration 50931 - Batch 855/963 - Train loss: 0.28540190  - Train acc: 0.8919 - Val loss: 0.97176075\n",
      "(10.65 min) Epoch 53/300 -- Iteration 50940 - Batch 864/963 - Train loss: 0.28549722  - Train acc: 0.8918 - Val loss: 0.97176075\n",
      "(10.65 min) Epoch 53/300 -- Iteration 50949 - Batch 873/963 - Train loss: 0.28577456  - Train acc: 0.8916 - Val loss: 0.97176075\n",
      "(10.65 min) Epoch 53/300 -- Iteration 50958 - Batch 882/963 - Train loss: 0.28621993  - Train acc: 0.8914 - Val loss: 0.97176075\n",
      "(10.66 min) Epoch 53/300 -- Iteration 50967 - Batch 891/963 - Train loss: 0.28603611  - Train acc: 0.8914 - Val loss: 0.97176075\n",
      "(10.66 min) Epoch 53/300 -- Iteration 50976 - Batch 900/963 - Train loss: 0.28599788  - Train acc: 0.8913 - Val loss: 0.97176075\n",
      "(10.66 min) Epoch 53/300 -- Iteration 50985 - Batch 909/963 - Train loss: 0.28590494  - Train acc: 0.8914 - Val loss: 0.97176075\n",
      "(10.66 min) Epoch 53/300 -- Iteration 50994 - Batch 918/963 - Train loss: 0.28615335  - Train acc: 0.8912 - Val loss: 0.97176075\n",
      "(10.66 min) Epoch 53/300 -- Iteration 51003 - Batch 927/963 - Train loss: 0.28646207  - Train acc: 0.8911 - Val loss: 0.97176075\n",
      "(10.67 min) Epoch 53/300 -- Iteration 51012 - Batch 936/963 - Train loss: 0.28623675  - Train acc: 0.8912 - Val loss: 0.97176075\n",
      "(10.67 min) Epoch 53/300 -- Iteration 51021 - Batch 945/963 - Train loss: 0.28602588  - Train acc: 0.8912 - Val loss: 0.97176075\n",
      "(10.67 min) Epoch 53/300 -- Iteration 51030 - Batch 954/963 - Train loss: 0.28605666  - Train acc: 0.8911 - Val loss: 0.97176075\n",
      "(10.67 min) Epoch 53/300 -- Iteration 51039 - Batch 962/963 - Train loss: 0.28583606  - Train acc: 0.8912 - Val loss: 0.97975636 - Val acc: 0.5950\n",
      "(10.68 min) Epoch 54/300 -- Iteration 51048 - Batch 9/963 - Train loss: 0.28129781  - Train acc: 0.8945 - Val loss: 0.97975636\n",
      "(10.68 min) Epoch 54/300 -- Iteration 51057 - Batch 18/963 - Train loss: 0.29079794  - Train acc: 0.8935 - Val loss: 0.97975636\n",
      "(10.68 min) Epoch 54/300 -- Iteration 51066 - Batch 27/963 - Train loss: 0.29856961  - Train acc: 0.8887 - Val loss: 0.97975636\n",
      "(10.68 min) Epoch 54/300 -- Iteration 51075 - Batch 36/963 - Train loss: 0.28998407  - Train acc: 0.8902 - Val loss: 0.97975636\n",
      "(10.68 min) Epoch 54/300 -- Iteration 51084 - Batch 45/963 - Train loss: 0.28831471  - Train acc: 0.8916 - Val loss: 0.97975636\n",
      "(10.68 min) Epoch 54/300 -- Iteration 51093 - Batch 54/963 - Train loss: 0.28769695  - Train acc: 0.8916 - Val loss: 0.97975636\n",
      "(10.69 min) Epoch 54/300 -- Iteration 51102 - Batch 63/963 - Train loss: 0.29010310  - Train acc: 0.8898 - Val loss: 0.97975636\n",
      "(10.69 min) Epoch 54/300 -- Iteration 51111 - Batch 72/963 - Train loss: 0.28999649  - Train acc: 0.8890 - Val loss: 0.97975636\n",
      "(10.69 min) Epoch 54/300 -- Iteration 51120 - Batch 81/963 - Train loss: 0.29066372  - Train acc: 0.8886 - Val loss: 0.97975636\n",
      "(10.69 min) Epoch 54/300 -- Iteration 51129 - Batch 90/963 - Train loss: 0.28845902  - Train acc: 0.8894 - Val loss: 0.97975636\n",
      "(10.70 min) Epoch 54/300 -- Iteration 51138 - Batch 99/963 - Train loss: 0.28732872  - Train acc: 0.8904 - Val loss: 0.97975636\n",
      "(10.70 min) Epoch 54/300 -- Iteration 51147 - Batch 108/963 - Train loss: 0.28595739  - Train acc: 0.8911 - Val loss: 0.97975636\n",
      "(10.70 min) Epoch 54/300 -- Iteration 51156 - Batch 117/963 - Train loss: 0.28392716  - Train acc: 0.8921 - Val loss: 0.97975636\n",
      "(10.70 min) Epoch 54/300 -- Iteration 51165 - Batch 126/963 - Train loss: 0.28461432  - Train acc: 0.8919 - Val loss: 0.97975636\n",
      "(10.70 min) Epoch 54/300 -- Iteration 51174 - Batch 135/963 - Train loss: 0.28558185  - Train acc: 0.8914 - Val loss: 0.97975636\n",
      "(10.71 min) Epoch 54/300 -- Iteration 51183 - Batch 144/963 - Train loss: 0.28675902  - Train acc: 0.8912 - Val loss: 0.97975636\n",
      "(10.71 min) Epoch 54/300 -- Iteration 51192 - Batch 153/963 - Train loss: 0.28643610  - Train acc: 0.8913 - Val loss: 0.97975636\n",
      "(10.71 min) Epoch 54/300 -- Iteration 51201 - Batch 162/963 - Train loss: 0.28688061  - Train acc: 0.8909 - Val loss: 0.97975636\n",
      "(10.71 min) Epoch 54/300 -- Iteration 51210 - Batch 171/963 - Train loss: 0.28701459  - Train acc: 0.8914 - Val loss: 0.97975636\n",
      "(10.71 min) Epoch 54/300 -- Iteration 51219 - Batch 180/963 - Train loss: 0.28712412  - Train acc: 0.8911 - Val loss: 0.97975636\n",
      "(10.72 min) Epoch 54/300 -- Iteration 51228 - Batch 189/963 - Train loss: 0.28784821  - Train acc: 0.8909 - Val loss: 0.97975636\n",
      "(10.72 min) Epoch 54/300 -- Iteration 51237 - Batch 198/963 - Train loss: 0.28832697  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.72 min) Epoch 54/300 -- Iteration 51246 - Batch 207/963 - Train loss: 0.28715700  - Train acc: 0.8912 - Val loss: 0.97975636\n",
      "(10.72 min) Epoch 54/300 -- Iteration 51255 - Batch 216/963 - Train loss: 0.28726833  - Train acc: 0.8911 - Val loss: 0.97975636\n",
      "(10.73 min) Epoch 54/300 -- Iteration 51264 - Batch 225/963 - Train loss: 0.28699860  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.73 min) Epoch 54/300 -- Iteration 51273 - Batch 234/963 - Train loss: 0.28672423  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.73 min) Epoch 54/300 -- Iteration 51282 - Batch 243/963 - Train loss: 0.28740233  - Train acc: 0.8904 - Val loss: 0.97975636\n",
      "(10.73 min) Epoch 54/300 -- Iteration 51291 - Batch 252/963 - Train loss: 0.28798110  - Train acc: 0.8899 - Val loss: 0.97975636\n",
      "(10.73 min) Epoch 54/300 -- Iteration 51300 - Batch 261/963 - Train loss: 0.28730466  - Train acc: 0.8900 - Val loss: 0.97975636\n",
      "(10.74 min) Epoch 54/300 -- Iteration 51309 - Batch 270/963 - Train loss: 0.28662384  - Train acc: 0.8903 - Val loss: 0.97975636\n",
      "(10.74 min) Epoch 54/300 -- Iteration 51318 - Batch 279/963 - Train loss: 0.28555202  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.74 min) Epoch 54/300 -- Iteration 51327 - Batch 288/963 - Train loss: 0.28505797  - Train acc: 0.8912 - Val loss: 0.97975636\n",
      "(10.74 min) Epoch 54/300 -- Iteration 51336 - Batch 297/963 - Train loss: 0.28507537  - Train acc: 0.8910 - Val loss: 0.97975636\n",
      "(10.74 min) Epoch 54/300 -- Iteration 51345 - Batch 306/963 - Train loss: 0.28575462  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.75 min) Epoch 54/300 -- Iteration 51354 - Batch 315/963 - Train loss: 0.28473932  - Train acc: 0.8910 - Val loss: 0.97975636\n",
      "(10.75 min) Epoch 54/300 -- Iteration 51363 - Batch 324/963 - Train loss: 0.28521220  - Train acc: 0.8909 - Val loss: 0.97975636\n",
      "(10.75 min) Epoch 54/300 -- Iteration 51372 - Batch 333/963 - Train loss: 0.28628826  - Train acc: 0.8903 - Val loss: 0.97975636\n",
      "(10.75 min) Epoch 54/300 -- Iteration 51381 - Batch 342/963 - Train loss: 0.28590300  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.75 min) Epoch 54/300 -- Iteration 51390 - Batch 351/963 - Train loss: 0.28586957  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.75 min) Epoch 54/300 -- Iteration 51399 - Batch 360/963 - Train loss: 0.28600989  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.76 min) Epoch 54/300 -- Iteration 51408 - Batch 369/963 - Train loss: 0.28656588  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.76 min) Epoch 54/300 -- Iteration 51417 - Batch 378/963 - Train loss: 0.28647537  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.76 min) Epoch 54/300 -- Iteration 51426 - Batch 387/963 - Train loss: 0.28667675  - Train acc: 0.8904 - Val loss: 0.97975636\n",
      "(10.76 min) Epoch 54/300 -- Iteration 51435 - Batch 396/963 - Train loss: 0.28741945  - Train acc: 0.8900 - Val loss: 0.97975636\n",
      "(10.76 min) Epoch 54/300 -- Iteration 51444 - Batch 405/963 - Train loss: 0.28781409  - Train acc: 0.8899 - Val loss: 0.97975636\n",
      "(10.77 min) Epoch 54/300 -- Iteration 51453 - Batch 414/963 - Train loss: 0.28800367  - Train acc: 0.8898 - Val loss: 0.97975636\n",
      "(10.77 min) Epoch 54/300 -- Iteration 51462 - Batch 423/963 - Train loss: 0.28816714  - Train acc: 0.8897 - Val loss: 0.97975636\n",
      "(10.77 min) Epoch 54/300 -- Iteration 51471 - Batch 432/963 - Train loss: 0.28788234  - Train acc: 0.8900 - Val loss: 0.97975636\n",
      "(10.77 min) Epoch 54/300 -- Iteration 51480 - Batch 441/963 - Train loss: 0.28785861  - Train acc: 0.8900 - Val loss: 0.97975636\n",
      "(10.78 min) Epoch 54/300 -- Iteration 51489 - Batch 450/963 - Train loss: 0.28786837  - Train acc: 0.8901 - Val loss: 0.97975636\n",
      "(10.78 min) Epoch 54/300 -- Iteration 51498 - Batch 459/963 - Train loss: 0.28769312  - Train acc: 0.8902 - Val loss: 0.97975636\n",
      "(10.78 min) Epoch 54/300 -- Iteration 51507 - Batch 468/963 - Train loss: 0.28744714  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.78 min) Epoch 54/300 -- Iteration 51516 - Batch 477/963 - Train loss: 0.28745125  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.78 min) Epoch 54/300 -- Iteration 51525 - Batch 486/963 - Train loss: 0.28776410  - Train acc: 0.8904 - Val loss: 0.97975636\n",
      "(10.79 min) Epoch 54/300 -- Iteration 51534 - Batch 495/963 - Train loss: 0.28742722  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.79 min) Epoch 54/300 -- Iteration 51543 - Batch 504/963 - Train loss: 0.28726255  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.79 min) Epoch 54/300 -- Iteration 51552 - Batch 513/963 - Train loss: 0.28780424  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.79 min) Epoch 54/300 -- Iteration 51561 - Batch 522/963 - Train loss: 0.28757986  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.79 min) Epoch 54/300 -- Iteration 51570 - Batch 531/963 - Train loss: 0.28789771  - Train acc: 0.8904 - Val loss: 0.97975636\n",
      "(10.80 min) Epoch 54/300 -- Iteration 51579 - Batch 540/963 - Train loss: 0.28805451  - Train acc: 0.8903 - Val loss: 0.97975636\n",
      "(10.80 min) Epoch 54/300 -- Iteration 51588 - Batch 549/963 - Train loss: 0.28759739  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.80 min) Epoch 54/300 -- Iteration 51597 - Batch 558/963 - Train loss: 0.28742982  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.80 min) Epoch 54/300 -- Iteration 51606 - Batch 567/963 - Train loss: 0.28706999  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.81 min) Epoch 54/300 -- Iteration 51615 - Batch 576/963 - Train loss: 0.28695147  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.81 min) Epoch 54/300 -- Iteration 51624 - Batch 585/963 - Train loss: 0.28701271  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.81 min) Epoch 54/300 -- Iteration 51633 - Batch 594/963 - Train loss: 0.28735583  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.81 min) Epoch 54/300 -- Iteration 51642 - Batch 603/963 - Train loss: 0.28745659  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.81 min) Epoch 54/300 -- Iteration 51651 - Batch 612/963 - Train loss: 0.28707942  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.82 min) Epoch 54/300 -- Iteration 51660 - Batch 621/963 - Train loss: 0.28693531  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.82 min) Epoch 54/300 -- Iteration 51669 - Batch 630/963 - Train loss: 0.28677919  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.82 min) Epoch 54/300 -- Iteration 51678 - Batch 639/963 - Train loss: 0.28642281  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.82 min) Epoch 54/300 -- Iteration 51687 - Batch 648/963 - Train loss: 0.28628816  - Train acc: 0.8909 - Val loss: 0.97975636\n",
      "(10.82 min) Epoch 54/300 -- Iteration 51696 - Batch 657/963 - Train loss: 0.28642937  - Train acc: 0.8909 - Val loss: 0.97975636\n",
      "(10.83 min) Epoch 54/300 -- Iteration 51705 - Batch 666/963 - Train loss: 0.28652503  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.83 min) Epoch 54/300 -- Iteration 51714 - Batch 675/963 - Train loss: 0.28636170  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.83 min) Epoch 54/300 -- Iteration 51723 - Batch 684/963 - Train loss: 0.28652747  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.83 min) Epoch 54/300 -- Iteration 51732 - Batch 693/963 - Train loss: 0.28670463  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.83 min) Epoch 54/300 -- Iteration 51741 - Batch 702/963 - Train loss: 0.28651407  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.83 min) Epoch 54/300 -- Iteration 51750 - Batch 711/963 - Train loss: 0.28653950  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.84 min) Epoch 54/300 -- Iteration 51759 - Batch 720/963 - Train loss: 0.28669448  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.84 min) Epoch 54/300 -- Iteration 51768 - Batch 729/963 - Train loss: 0.28690721  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.84 min) Epoch 54/300 -- Iteration 51777 - Batch 738/963 - Train loss: 0.28691405  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.84 min) Epoch 54/300 -- Iteration 51786 - Batch 747/963 - Train loss: 0.28721621  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.84 min) Epoch 54/300 -- Iteration 51795 - Batch 756/963 - Train loss: 0.28743469  - Train acc: 0.8904 - Val loss: 0.97975636\n",
      "(10.85 min) Epoch 54/300 -- Iteration 51804 - Batch 765/963 - Train loss: 0.28738618  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.85 min) Epoch 54/300 -- Iteration 51813 - Batch 774/963 - Train loss: 0.28776139  - Train acc: 0.8903 - Val loss: 0.97975636\n",
      "(10.85 min) Epoch 54/300 -- Iteration 51822 - Batch 783/963 - Train loss: 0.28787960  - Train acc: 0.8902 - Val loss: 0.97975636\n",
      "(10.85 min) Epoch 54/300 -- Iteration 51831 - Batch 792/963 - Train loss: 0.28799119  - Train acc: 0.8901 - Val loss: 0.97975636\n",
      "(10.85 min) Epoch 54/300 -- Iteration 51840 - Batch 801/963 - Train loss: 0.28765999  - Train acc: 0.8903 - Val loss: 0.97975636\n",
      "(10.85 min) Epoch 54/300 -- Iteration 51849 - Batch 810/963 - Train loss: 0.28732441  - Train acc: 0.8905 - Val loss: 0.97975636\n",
      "(10.86 min) Epoch 54/300 -- Iteration 51858 - Batch 819/963 - Train loss: 0.28703483  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.86 min) Epoch 54/300 -- Iteration 51867 - Batch 828/963 - Train loss: 0.28699507  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.86 min) Epoch 54/300 -- Iteration 51876 - Batch 837/963 - Train loss: 0.28660206  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.86 min) Epoch 54/300 -- Iteration 51885 - Batch 846/963 - Train loss: 0.28653058  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.86 min) Epoch 54/300 -- Iteration 51894 - Batch 855/963 - Train loss: 0.28658030  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.87 min) Epoch 54/300 -- Iteration 51903 - Batch 864/963 - Train loss: 0.28687483  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.87 min) Epoch 54/300 -- Iteration 51912 - Batch 873/963 - Train loss: 0.28689429  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.87 min) Epoch 54/300 -- Iteration 51921 - Batch 882/963 - Train loss: 0.28668903  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.87 min) Epoch 54/300 -- Iteration 51930 - Batch 891/963 - Train loss: 0.28710904  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.87 min) Epoch 54/300 -- Iteration 51939 - Batch 900/963 - Train loss: 0.28696379  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.88 min) Epoch 54/300 -- Iteration 51948 - Batch 909/963 - Train loss: 0.28687802  - Train acc: 0.8906 - Val loss: 0.97975636\n",
      "(10.88 min) Epoch 54/300 -- Iteration 51957 - Batch 918/963 - Train loss: 0.28655053  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.88 min) Epoch 54/300 -- Iteration 51966 - Batch 927/963 - Train loss: 0.28651130  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.88 min) Epoch 54/300 -- Iteration 51975 - Batch 936/963 - Train loss: 0.28646158  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.88 min) Epoch 54/300 -- Iteration 51984 - Batch 945/963 - Train loss: 0.28643416  - Train acc: 0.8907 - Val loss: 0.97975636\n",
      "(10.89 min) Epoch 54/300 -- Iteration 51993 - Batch 954/963 - Train loss: 0.28631836  - Train acc: 0.8908 - Val loss: 0.97975636\n",
      "(10.89 min) Epoch 54/300 -- Iteration 52002 - Batch 962/963 - Train loss: 0.28626304  - Train acc: 0.8908 - Val loss: 0.99115407 - Val acc: 0.5950\n",
      "(10.89 min) Epoch 55/300 -- Iteration 52011 - Batch 9/963 - Train loss: 0.26549680  - Train acc: 0.8938 - Val loss: 0.99115407\n",
      "(10.89 min) Epoch 55/300 -- Iteration 52020 - Batch 18/963 - Train loss: 0.27111296  - Train acc: 0.8935 - Val loss: 0.99115407\n",
      "(10.89 min) Epoch 55/300 -- Iteration 52029 - Batch 27/963 - Train loss: 0.28827388  - Train acc: 0.8870 - Val loss: 0.99115407\n",
      "(10.90 min) Epoch 55/300 -- Iteration 52038 - Batch 36/963 - Train loss: 0.28698697  - Train acc: 0.8879 - Val loss: 0.99115407\n",
      "(10.90 min) Epoch 55/300 -- Iteration 52047 - Batch 45/963 - Train loss: 0.28058123  - Train acc: 0.8922 - Val loss: 0.99115407\n",
      "(10.90 min) Epoch 55/300 -- Iteration 52056 - Batch 54/963 - Train loss: 0.29037066  - Train acc: 0.8895 - Val loss: 0.99115407\n",
      "(10.90 min) Epoch 55/300 -- Iteration 52065 - Batch 63/963 - Train loss: 0.28526075  - Train acc: 0.8917 - Val loss: 0.99115407\n",
      "(10.90 min) Epoch 55/300 -- Iteration 52074 - Batch 72/963 - Train loss: 0.28298049  - Train acc: 0.8923 - Val loss: 0.99115407\n",
      "(10.91 min) Epoch 55/300 -- Iteration 52083 - Batch 81/963 - Train loss: 0.28028845  - Train acc: 0.8931 - Val loss: 0.99115407\n",
      "(10.91 min) Epoch 55/300 -- Iteration 52092 - Batch 90/963 - Train loss: 0.27839841  - Train acc: 0.8932 - Val loss: 0.99115407\n",
      "(10.91 min) Epoch 55/300 -- Iteration 52101 - Batch 99/963 - Train loss: 0.28165874  - Train acc: 0.8915 - Val loss: 0.99115407\n",
      "(10.91 min) Epoch 55/300 -- Iteration 52110 - Batch 108/963 - Train loss: 0.28074160  - Train acc: 0.8914 - Val loss: 0.99115407\n",
      "(10.91 min) Epoch 55/300 -- Iteration 52119 - Batch 117/963 - Train loss: 0.28074769  - Train acc: 0.8915 - Val loss: 0.99115407\n",
      "(10.91 min) Epoch 55/300 -- Iteration 52128 - Batch 126/963 - Train loss: 0.28144621  - Train acc: 0.8910 - Val loss: 0.99115407\n",
      "(10.92 min) Epoch 55/300 -- Iteration 52137 - Batch 135/963 - Train loss: 0.28269291  - Train acc: 0.8909 - Val loss: 0.99115407\n",
      "(10.92 min) Epoch 55/300 -- Iteration 52146 - Batch 144/963 - Train loss: 0.28525474  - Train acc: 0.8901 - Val loss: 0.99115407\n",
      "(10.92 min) Epoch 55/300 -- Iteration 52155 - Batch 153/963 - Train loss: 0.28567305  - Train acc: 0.8901 - Val loss: 0.99115407\n",
      "(10.92 min) Epoch 55/300 -- Iteration 52164 - Batch 162/963 - Train loss: 0.28615209  - Train acc: 0.8900 - Val loss: 0.99115407\n",
      "(10.92 min) Epoch 55/300 -- Iteration 52173 - Batch 171/963 - Train loss: 0.28604934  - Train acc: 0.8899 - Val loss: 0.99115407\n",
      "(10.93 min) Epoch 55/300 -- Iteration 52182 - Batch 180/963 - Train loss: 0.28674566  - Train acc: 0.8898 - Val loss: 0.99115407\n",
      "(10.93 min) Epoch 55/300 -- Iteration 52191 - Batch 189/963 - Train loss: 0.28637794  - Train acc: 0.8903 - Val loss: 0.99115407\n",
      "(10.93 min) Epoch 55/300 -- Iteration 52200 - Batch 198/963 - Train loss: 0.28684987  - Train acc: 0.8902 - Val loss: 0.99115407\n",
      "(10.93 min) Epoch 55/300 -- Iteration 52209 - Batch 207/963 - Train loss: 0.28742973  - Train acc: 0.8899 - Val loss: 0.99115407\n",
      "(10.93 min) Epoch 55/300 -- Iteration 52218 - Batch 216/963 - Train loss: 0.28628899  - Train acc: 0.8904 - Val loss: 0.99115407\n",
      "(10.94 min) Epoch 55/300 -- Iteration 52227 - Batch 225/963 - Train loss: 0.28635371  - Train acc: 0.8904 - Val loss: 0.99115407\n",
      "(10.94 min) Epoch 55/300 -- Iteration 52236 - Batch 234/963 - Train loss: 0.28879568  - Train acc: 0.8892 - Val loss: 0.99115407\n",
      "(10.94 min) Epoch 55/300 -- Iteration 52245 - Batch 243/963 - Train loss: 0.28866564  - Train acc: 0.8894 - Val loss: 0.99115407\n",
      "(10.94 min) Epoch 55/300 -- Iteration 52254 - Batch 252/963 - Train loss: 0.28866556  - Train acc: 0.8895 - Val loss: 0.99115407\n",
      "(10.94 min) Epoch 55/300 -- Iteration 52263 - Batch 261/963 - Train loss: 0.28710615  - Train acc: 0.8901 - Val loss: 0.99115407\n",
      "(10.95 min) Epoch 55/300 -- Iteration 52272 - Batch 270/963 - Train loss: 0.28740012  - Train acc: 0.8900 - Val loss: 0.99115407\n",
      "(10.95 min) Epoch 55/300 -- Iteration 52281 - Batch 279/963 - Train loss: 0.28776184  - Train acc: 0.8897 - Val loss: 0.99115407\n",
      "(10.95 min) Epoch 55/300 -- Iteration 52290 - Batch 288/963 - Train loss: 0.28785901  - Train acc: 0.8896 - Val loss: 0.99115407\n",
      "(10.95 min) Epoch 55/300 -- Iteration 52299 - Batch 297/963 - Train loss: 0.28732091  - Train acc: 0.8900 - Val loss: 0.99115407\n",
      "(10.95 min) Epoch 55/300 -- Iteration 52308 - Batch 306/963 - Train loss: 0.28823939  - Train acc: 0.8895 - Val loss: 0.99115407\n",
      "(10.95 min) Epoch 55/300 -- Iteration 52317 - Batch 315/963 - Train loss: 0.28788508  - Train acc: 0.8897 - Val loss: 0.99115407\n",
      "(10.96 min) Epoch 55/300 -- Iteration 52326 - Batch 324/963 - Train loss: 0.28805864  - Train acc: 0.8899 - Val loss: 0.99115407\n",
      "(10.96 min) Epoch 55/300 -- Iteration 52335 - Batch 333/963 - Train loss: 0.28780517  - Train acc: 0.8899 - Val loss: 0.99115407\n",
      "(10.96 min) Epoch 55/300 -- Iteration 52344 - Batch 342/963 - Train loss: 0.28727794  - Train acc: 0.8899 - Val loss: 0.99115407\n",
      "(10.96 min) Epoch 55/300 -- Iteration 52353 - Batch 351/963 - Train loss: 0.28751410  - Train acc: 0.8898 - Val loss: 0.99115407\n",
      "(10.96 min) Epoch 55/300 -- Iteration 52362 - Batch 360/963 - Train loss: 0.28685964  - Train acc: 0.8902 - Val loss: 0.99115407\n",
      "(10.97 min) Epoch 55/300 -- Iteration 52371 - Batch 369/963 - Train loss: 0.28675746  - Train acc: 0.8903 - Val loss: 0.99115407\n",
      "(10.97 min) Epoch 55/300 -- Iteration 52380 - Batch 378/963 - Train loss: 0.28729271  - Train acc: 0.8899 - Val loss: 0.99115407\n",
      "(10.97 min) Epoch 55/300 -- Iteration 52389 - Batch 387/963 - Train loss: 0.28658845  - Train acc: 0.8902 - Val loss: 0.99115407\n",
      "(10.97 min) Epoch 55/300 -- Iteration 52398 - Batch 396/963 - Train loss: 0.28710943  - Train acc: 0.8900 - Val loss: 0.99115407\n",
      "(10.97 min) Epoch 55/300 -- Iteration 52407 - Batch 405/963 - Train loss: 0.28738737  - Train acc: 0.8900 - Val loss: 0.99115407\n",
      "(10.98 min) Epoch 55/300 -- Iteration 52416 - Batch 414/963 - Train loss: 0.28746626  - Train acc: 0.8900 - Val loss: 0.99115407\n",
      "(10.98 min) Epoch 55/300 -- Iteration 52425 - Batch 423/963 - Train loss: 0.28685051  - Train acc: 0.8904 - Val loss: 0.99115407\n",
      "(10.98 min) Epoch 55/300 -- Iteration 52434 - Batch 432/963 - Train loss: 0.28668227  - Train acc: 0.8903 - Val loss: 0.99115407\n",
      "(10.98 min) Epoch 55/300 -- Iteration 52443 - Batch 441/963 - Train loss: 0.28648504  - Train acc: 0.8904 - Val loss: 0.99115407\n",
      "(10.99 min) Epoch 55/300 -- Iteration 52452 - Batch 450/963 - Train loss: 0.28579685  - Train acc: 0.8907 - Val loss: 0.99115407\n",
      "(10.99 min) Epoch 55/300 -- Iteration 52461 - Batch 459/963 - Train loss: 0.28556150  - Train acc: 0.8909 - Val loss: 0.99115407\n",
      "(10.99 min) Epoch 55/300 -- Iteration 52470 - Batch 468/963 - Train loss: 0.28528044  - Train acc: 0.8913 - Val loss: 0.99115407\n",
      "(10.99 min) Epoch 55/300 -- Iteration 52479 - Batch 477/963 - Train loss: 0.28529723  - Train acc: 0.8913 - Val loss: 0.99115407\n",
      "(10.99 min) Epoch 55/300 -- Iteration 52488 - Batch 486/963 - Train loss: 0.28562434  - Train acc: 0.8912 - Val loss: 0.99115407\n",
      "(10.99 min) Epoch 55/300 -- Iteration 52497 - Batch 495/963 - Train loss: 0.28579076  - Train acc: 0.8911 - Val loss: 0.99115407\n",
      "(11.00 min) Epoch 55/300 -- Iteration 52506 - Batch 504/963 - Train loss: 0.28546790  - Train acc: 0.8913 - Val loss: 0.99115407\n",
      "(11.00 min) Epoch 55/300 -- Iteration 52515 - Batch 513/963 - Train loss: 0.28547919  - Train acc: 0.8914 - Val loss: 0.99115407\n",
      "(11.00 min) Epoch 55/300 -- Iteration 52524 - Batch 522/963 - Train loss: 0.28570356  - Train acc: 0.8913 - Val loss: 0.99115407\n",
      "(11.00 min) Epoch 55/300 -- Iteration 52533 - Batch 531/963 - Train loss: 0.28537726  - Train acc: 0.8914 - Val loss: 0.99115407\n",
      "(11.00 min) Epoch 55/300 -- Iteration 52542 - Batch 540/963 - Train loss: 0.28534993  - Train acc: 0.8915 - Val loss: 0.99115407\n",
      "(11.01 min) Epoch 55/300 -- Iteration 52551 - Batch 549/963 - Train loss: 0.28551349  - Train acc: 0.8912 - Val loss: 0.99115407\n",
      "(11.01 min) Epoch 55/300 -- Iteration 52560 - Batch 558/963 - Train loss: 0.28491110  - Train acc: 0.8916 - Val loss: 0.99115407\n",
      "(11.01 min) Epoch 55/300 -- Iteration 52569 - Batch 567/963 - Train loss: 0.28465475  - Train acc: 0.8918 - Val loss: 0.99115407\n",
      "(11.01 min) Epoch 55/300 -- Iteration 52578 - Batch 576/963 - Train loss: 0.28433263  - Train acc: 0.8919 - Val loss: 0.99115407\n",
      "(11.01 min) Epoch 55/300 -- Iteration 52587 - Batch 585/963 - Train loss: 0.28394412  - Train acc: 0.8920 - Val loss: 0.99115407\n",
      "(11.02 min) Epoch 55/300 -- Iteration 52596 - Batch 594/963 - Train loss: 0.28395209  - Train acc: 0.8920 - Val loss: 0.99115407\n",
      "(11.02 min) Epoch 55/300 -- Iteration 52605 - Batch 603/963 - Train loss: 0.28361128  - Train acc: 0.8921 - Val loss: 0.99115407\n",
      "(11.02 min) Epoch 55/300 -- Iteration 52614 - Batch 612/963 - Train loss: 0.28361892  - Train acc: 0.8921 - Val loss: 0.99115407\n",
      "(11.02 min) Epoch 55/300 -- Iteration 52623 - Batch 621/963 - Train loss: 0.28397436  - Train acc: 0.8918 - Val loss: 0.99115407\n",
      "(11.02 min) Epoch 55/300 -- Iteration 52632 - Batch 630/963 - Train loss: 0.28421552  - Train acc: 0.8918 - Val loss: 0.99115407\n",
      "(11.03 min) Epoch 55/300 -- Iteration 52641 - Batch 639/963 - Train loss: 0.28404535  - Train acc: 0.8919 - Val loss: 0.99115407\n",
      "(11.03 min) Epoch 55/300 -- Iteration 52650 - Batch 648/963 - Train loss: 0.28369546  - Train acc: 0.8922 - Val loss: 0.99115407\n",
      "(11.03 min) Epoch 55/300 -- Iteration 52659 - Batch 657/963 - Train loss: 0.28349003  - Train acc: 0.8922 - Val loss: 0.99115407\n",
      "(11.03 min) Epoch 55/300 -- Iteration 52668 - Batch 666/963 - Train loss: 0.28333791  - Train acc: 0.8923 - Val loss: 0.99115407\n",
      "(11.03 min) Epoch 55/300 -- Iteration 52677 - Batch 675/963 - Train loss: 0.28381986  - Train acc: 0.8921 - Val loss: 0.99115407\n",
      "(11.04 min) Epoch 55/300 -- Iteration 52686 - Batch 684/963 - Train loss: 0.28347374  - Train acc: 0.8923 - Val loss: 0.99115407\n",
      "(11.04 min) Epoch 55/300 -- Iteration 52695 - Batch 693/963 - Train loss: 0.28308504  - Train acc: 0.8924 - Val loss: 0.99115407\n",
      "(11.04 min) Epoch 55/300 -- Iteration 52704 - Batch 702/963 - Train loss: 0.28306877  - Train acc: 0.8924 - Val loss: 0.99115407\n",
      "(11.04 min) Epoch 55/300 -- Iteration 52713 - Batch 711/963 - Train loss: 0.28292661  - Train acc: 0.8924 - Val loss: 0.99115407\n",
      "(11.04 min) Epoch 55/300 -- Iteration 52722 - Batch 720/963 - Train loss: 0.28292147  - Train acc: 0.8925 - Val loss: 0.99115407\n",
      "(11.04 min) Epoch 55/300 -- Iteration 52731 - Batch 729/963 - Train loss: 0.28317637  - Train acc: 0.8925 - Val loss: 0.99115407\n",
      "(11.05 min) Epoch 55/300 -- Iteration 52740 - Batch 738/963 - Train loss: 0.28321497  - Train acc: 0.8925 - Val loss: 0.99115407\n",
      "(11.05 min) Epoch 55/300 -- Iteration 52749 - Batch 747/963 - Train loss: 0.28321162  - Train acc: 0.8925 - Val loss: 0.99115407\n",
      "(11.05 min) Epoch 55/300 -- Iteration 52758 - Batch 756/963 - Train loss: 0.28348571  - Train acc: 0.8923 - Val loss: 0.99115407\n",
      "(11.05 min) Epoch 55/300 -- Iteration 52767 - Batch 765/963 - Train loss: 0.28333529  - Train acc: 0.8924 - Val loss: 0.99115407\n",
      "(11.05 min) Epoch 55/300 -- Iteration 52776 - Batch 774/963 - Train loss: 0.28358896  - Train acc: 0.8924 - Val loss: 0.99115407\n",
      "(11.06 min) Epoch 55/300 -- Iteration 52785 - Batch 783/963 - Train loss: 0.28376775  - Train acc: 0.8923 - Val loss: 0.99115407\n",
      "(11.06 min) Epoch 55/300 -- Iteration 52794 - Batch 792/963 - Train loss: 0.28370498  - Train acc: 0.8924 - Val loss: 0.99115407\n",
      "(11.06 min) Epoch 55/300 -- Iteration 52803 - Batch 801/963 - Train loss: 0.28398156  - Train acc: 0.8923 - Val loss: 0.99115407\n",
      "(11.06 min) Epoch 55/300 -- Iteration 52812 - Batch 810/963 - Train loss: 0.28368890  - Train acc: 0.8925 - Val loss: 0.99115407\n",
      "(11.06 min) Epoch 55/300 -- Iteration 52821 - Batch 819/963 - Train loss: 0.28384088  - Train acc: 0.8925 - Val loss: 0.99115407\n",
      "(11.06 min) Epoch 55/300 -- Iteration 52830 - Batch 828/963 - Train loss: 0.28358665  - Train acc: 0.8926 - Val loss: 0.99115407\n",
      "(11.07 min) Epoch 55/300 -- Iteration 52839 - Batch 837/963 - Train loss: 0.28377381  - Train acc: 0.8925 - Val loss: 0.99115407\n",
      "(11.07 min) Epoch 55/300 -- Iteration 52848 - Batch 846/963 - Train loss: 0.28385011  - Train acc: 0.8926 - Val loss: 0.99115407\n",
      "(11.07 min) Epoch 55/300 -- Iteration 52857 - Batch 855/963 - Train loss: 0.28353725  - Train acc: 0.8928 - Val loss: 0.99115407\n",
      "(11.07 min) Epoch 55/300 -- Iteration 52866 - Batch 864/963 - Train loss: 0.28327160  - Train acc: 0.8928 - Val loss: 0.99115407\n",
      "(11.07 min) Epoch 55/300 -- Iteration 52875 - Batch 873/963 - Train loss: 0.28323599  - Train acc: 0.8929 - Val loss: 0.99115407\n",
      "(11.08 min) Epoch 55/300 -- Iteration 52884 - Batch 882/963 - Train loss: 0.28349138  - Train acc: 0.8927 - Val loss: 0.99115407\n",
      "(11.08 min) Epoch 55/300 -- Iteration 52893 - Batch 891/963 - Train loss: 0.28340161  - Train acc: 0.8928 - Val loss: 0.99115407\n",
      "(11.08 min) Epoch 55/300 -- Iteration 52902 - Batch 900/963 - Train loss: 0.28325498  - Train acc: 0.8929 - Val loss: 0.99115407\n",
      "(11.08 min) Epoch 55/300 -- Iteration 52911 - Batch 909/963 - Train loss: 0.28335166  - Train acc: 0.8929 - Val loss: 0.99115407\n",
      "(11.08 min) Epoch 55/300 -- Iteration 52920 - Batch 918/963 - Train loss: 0.28330817  - Train acc: 0.8929 - Val loss: 0.99115407\n",
      "(11.09 min) Epoch 55/300 -- Iteration 52929 - Batch 927/963 - Train loss: 0.28340977  - Train acc: 0.8928 - Val loss: 0.99115407\n",
      "(11.09 min) Epoch 55/300 -- Iteration 52938 - Batch 936/963 - Train loss: 0.28357894  - Train acc: 0.8926 - Val loss: 0.99115407\n",
      "(11.09 min) Epoch 55/300 -- Iteration 52947 - Batch 945/963 - Train loss: 0.28335436  - Train acc: 0.8928 - Val loss: 0.99115407\n",
      "(11.09 min) Epoch 55/300 -- Iteration 52956 - Batch 954/963 - Train loss: 0.28313642  - Train acc: 0.8929 - Val loss: 0.99115407\n",
      "(11.09 min) Epoch 55/300 -- Iteration 52965 - Batch 962/963 - Train loss: 0.28319428  - Train acc: 0.8928 - Val loss: 0.99934030 - Val acc: 0.5917\n",
      "(11.10 min) Epoch 56/300 -- Iteration 52974 - Batch 9/963 - Train loss: 0.27423597  - Train acc: 0.8938 - Val loss: 0.99934030\n",
      "(11.10 min) Epoch 56/300 -- Iteration 52983 - Batch 18/963 - Train loss: 0.27703113  - Train acc: 0.8980 - Val loss: 0.99934030\n",
      "(11.10 min) Epoch 56/300 -- Iteration 52992 - Batch 27/963 - Train loss: 0.28179430  - Train acc: 0.8973 - Val loss: 0.99934030\n",
      "(11.10 min) Epoch 56/300 -- Iteration 53001 - Batch 36/963 - Train loss: 0.27447425  - Train acc: 0.8995 - Val loss: 0.99934030\n",
      "(11.10 min) Epoch 56/300 -- Iteration 53010 - Batch 45/963 - Train loss: 0.27127310  - Train acc: 0.9003 - Val loss: 0.99934030\n",
      "(11.10 min) Epoch 56/300 -- Iteration 53019 - Batch 54/963 - Train loss: 0.27292288  - Train acc: 0.8977 - Val loss: 0.99934030\n",
      "(11.11 min) Epoch 56/300 -- Iteration 53028 - Batch 63/963 - Train loss: 0.27557363  - Train acc: 0.8970 - Val loss: 0.99934030\n",
      "(11.11 min) Epoch 56/300 -- Iteration 53037 - Batch 72/963 - Train loss: 0.27134864  - Train acc: 0.8989 - Val loss: 0.99934030\n",
      "(11.11 min) Epoch 56/300 -- Iteration 53046 - Batch 81/963 - Train loss: 0.27333599  - Train acc: 0.8983 - Val loss: 0.99934030\n",
      "(11.11 min) Epoch 56/300 -- Iteration 53055 - Batch 90/963 - Train loss: 0.27513189  - Train acc: 0.8971 - Val loss: 0.99934030\n",
      "(11.11 min) Epoch 56/300 -- Iteration 53064 - Batch 99/963 - Train loss: 0.27601953  - Train acc: 0.8955 - Val loss: 0.99934030\n",
      "(11.12 min) Epoch 56/300 -- Iteration 53073 - Batch 108/963 - Train loss: 0.27552967  - Train acc: 0.8956 - Val loss: 0.99934030\n",
      "(11.12 min) Epoch 56/300 -- Iteration 53082 - Batch 117/963 - Train loss: 0.27674305  - Train acc: 0.8950 - Val loss: 0.99934030\n",
      "(11.12 min) Epoch 56/300 -- Iteration 53091 - Batch 126/963 - Train loss: 0.27983116  - Train acc: 0.8947 - Val loss: 0.99934030\n",
      "(11.12 min) Epoch 56/300 -- Iteration 53100 - Batch 135/963 - Train loss: 0.27861270  - Train acc: 0.8952 - Val loss: 0.99934030\n",
      "(11.12 min) Epoch 56/300 -- Iteration 53109 - Batch 144/963 - Train loss: 0.27992153  - Train acc: 0.8948 - Val loss: 0.99934030\n",
      "(11.13 min) Epoch 56/300 -- Iteration 53118 - Batch 153/963 - Train loss: 0.28003239  - Train acc: 0.8944 - Val loss: 0.99934030\n",
      "(11.13 min) Epoch 56/300 -- Iteration 53127 - Batch 162/963 - Train loss: 0.28063946  - Train acc: 0.8940 - Val loss: 0.99934030\n",
      "(11.13 min) Epoch 56/300 -- Iteration 53136 - Batch 171/963 - Train loss: 0.28102458  - Train acc: 0.8939 - Val loss: 0.99934030\n",
      "(11.13 min) Epoch 56/300 -- Iteration 53145 - Batch 180/963 - Train loss: 0.28116832  - Train acc: 0.8940 - Val loss: 0.99934030\n",
      "(11.13 min) Epoch 56/300 -- Iteration 53154 - Batch 189/963 - Train loss: 0.28091086  - Train acc: 0.8939 - Val loss: 0.99934030\n",
      "(11.13 min) Epoch 56/300 -- Iteration 53163 - Batch 198/963 - Train loss: 0.28126995  - Train acc: 0.8935 - Val loss: 0.99934030\n",
      "(11.14 min) Epoch 56/300 -- Iteration 53172 - Batch 207/963 - Train loss: 0.28131262  - Train acc: 0.8938 - Val loss: 0.99934030\n",
      "(11.14 min) Epoch 56/300 -- Iteration 53181 - Batch 216/963 - Train loss: 0.28260237  - Train acc: 0.8932 - Val loss: 0.99934030\n",
      "(11.14 min) Epoch 56/300 -- Iteration 53190 - Batch 225/963 - Train loss: 0.28262757  - Train acc: 0.8929 - Val loss: 0.99934030\n",
      "(11.14 min) Epoch 56/300 -- Iteration 53199 - Batch 234/963 - Train loss: 0.28340559  - Train acc: 0.8927 - Val loss: 0.99934030\n",
      "(11.14 min) Epoch 56/300 -- Iteration 53208 - Batch 243/963 - Train loss: 0.28343284  - Train acc: 0.8925 - Val loss: 0.99934030\n",
      "(11.15 min) Epoch 56/300 -- Iteration 53217 - Batch 252/963 - Train loss: 0.28336095  - Train acc: 0.8923 - Val loss: 0.99934030\n",
      "(11.15 min) Epoch 56/300 -- Iteration 53226 - Batch 261/963 - Train loss: 0.28335943  - Train acc: 0.8923 - Val loss: 0.99934030\n",
      "(11.15 min) Epoch 56/300 -- Iteration 53235 - Batch 270/963 - Train loss: 0.28265246  - Train acc: 0.8925 - Val loss: 0.99934030\n",
      "(11.15 min) Epoch 56/300 -- Iteration 53244 - Batch 279/963 - Train loss: 0.28254132  - Train acc: 0.8923 - Val loss: 0.99934030\n",
      "(11.15 min) Epoch 56/300 -- Iteration 53253 - Batch 288/963 - Train loss: 0.28295070  - Train acc: 0.8921 - Val loss: 0.99934030\n",
      "(11.15 min) Epoch 56/300 -- Iteration 53262 - Batch 297/963 - Train loss: 0.28264951  - Train acc: 0.8922 - Val loss: 0.99934030\n",
      "(11.16 min) Epoch 56/300 -- Iteration 53271 - Batch 306/963 - Train loss: 0.28192586  - Train acc: 0.8923 - Val loss: 0.99934030\n",
      "(11.16 min) Epoch 56/300 -- Iteration 53280 - Batch 315/963 - Train loss: 0.28155416  - Train acc: 0.8925 - Val loss: 0.99934030\n",
      "(11.16 min) Epoch 56/300 -- Iteration 53289 - Batch 324/963 - Train loss: 0.28117469  - Train acc: 0.8927 - Val loss: 0.99934030\n",
      "(11.16 min) Epoch 56/300 -- Iteration 53298 - Batch 333/963 - Train loss: 0.28165301  - Train acc: 0.8926 - Val loss: 0.99934030\n",
      "(11.16 min) Epoch 56/300 -- Iteration 53307 - Batch 342/963 - Train loss: 0.28149768  - Train acc: 0.8926 - Val loss: 0.99934030\n",
      "(11.17 min) Epoch 56/300 -- Iteration 53316 - Batch 351/963 - Train loss: 0.28158116  - Train acc: 0.8926 - Val loss: 0.99934030\n",
      "(11.17 min) Epoch 56/300 -- Iteration 53325 - Batch 360/963 - Train loss: 0.28199705  - Train acc: 0.8926 - Val loss: 0.99934030\n",
      "(11.17 min) Epoch 56/300 -- Iteration 53334 - Batch 369/963 - Train loss: 0.28241145  - Train acc: 0.8924 - Val loss: 0.99934030\n",
      "(11.17 min) Epoch 56/300 -- Iteration 53343 - Batch 378/963 - Train loss: 0.28205133  - Train acc: 0.8926 - Val loss: 0.99934030\n",
      "(11.17 min) Epoch 56/300 -- Iteration 53352 - Batch 387/963 - Train loss: 0.28159714  - Train acc: 0.8929 - Val loss: 0.99934030\n",
      "(11.18 min) Epoch 56/300 -- Iteration 53361 - Batch 396/963 - Train loss: 0.28142606  - Train acc: 0.8928 - Val loss: 0.99934030\n",
      "(11.18 min) Epoch 56/300 -- Iteration 53370 - Batch 405/963 - Train loss: 0.28253405  - Train acc: 0.8924 - Val loss: 0.99934030\n",
      "(11.18 min) Epoch 56/300 -- Iteration 53379 - Batch 414/963 - Train loss: 0.28259555  - Train acc: 0.8924 - Val loss: 0.99934030\n",
      "(11.18 min) Epoch 56/300 -- Iteration 53388 - Batch 423/963 - Train loss: 0.28284501  - Train acc: 0.8923 - Val loss: 0.99934030\n",
      "(11.18 min) Epoch 56/300 -- Iteration 53397 - Batch 432/963 - Train loss: 0.28279806  - Train acc: 0.8924 - Val loss: 0.99934030\n",
      "(11.18 min) Epoch 56/300 -- Iteration 53406 - Batch 441/963 - Train loss: 0.28279366  - Train acc: 0.8926 - Val loss: 0.99934030\n",
      "(11.19 min) Epoch 56/300 -- Iteration 53415 - Batch 450/963 - Train loss: 0.28283505  - Train acc: 0.8926 - Val loss: 0.99934030\n",
      "(11.19 min) Epoch 56/300 -- Iteration 53424 - Batch 459/963 - Train loss: 0.28256062  - Train acc: 0.8929 - Val loss: 0.99934030\n",
      "(11.19 min) Epoch 56/300 -- Iteration 53433 - Batch 468/963 - Train loss: 0.28233691  - Train acc: 0.8929 - Val loss: 0.99934030\n",
      "(11.19 min) Epoch 56/300 -- Iteration 53442 - Batch 477/963 - Train loss: 0.28231175  - Train acc: 0.8927 - Val loss: 0.99934030\n",
      "(11.19 min) Epoch 56/300 -- Iteration 53451 - Batch 486/963 - Train loss: 0.28231889  - Train acc: 0.8930 - Val loss: 0.99934030\n",
      "(11.20 min) Epoch 56/300 -- Iteration 53460 - Batch 495/963 - Train loss: 0.28225659  - Train acc: 0.8931 - Val loss: 0.99934030\n",
      "(11.20 min) Epoch 56/300 -- Iteration 53469 - Batch 504/963 - Train loss: 0.28229912  - Train acc: 0.8932 - Val loss: 0.99934030\n",
      "(11.20 min) Epoch 56/300 -- Iteration 53478 - Batch 513/963 - Train loss: 0.28240484  - Train acc: 0.8932 - Val loss: 0.99934030\n",
      "(11.20 min) Epoch 56/300 -- Iteration 53487 - Batch 522/963 - Train loss: 0.28221745  - Train acc: 0.8931 - Val loss: 0.99934030\n",
      "(11.20 min) Epoch 56/300 -- Iteration 53496 - Batch 531/963 - Train loss: 0.28184528  - Train acc: 0.8934 - Val loss: 0.99934030\n",
      "(11.21 min) Epoch 56/300 -- Iteration 53505 - Batch 540/963 - Train loss: 0.28186264  - Train acc: 0.8934 - Val loss: 0.99934030\n",
      "(11.21 min) Epoch 56/300 -- Iteration 53514 - Batch 549/963 - Train loss: 0.28219689  - Train acc: 0.8933 - Val loss: 0.99934030\n",
      "(11.21 min) Epoch 56/300 -- Iteration 53523 - Batch 558/963 - Train loss: 0.28241596  - Train acc: 0.8931 - Val loss: 0.99934030\n",
      "(11.21 min) Epoch 56/300 -- Iteration 53532 - Batch 567/963 - Train loss: 0.28230761  - Train acc: 0.8933 - Val loss: 0.99934030\n",
      "(11.21 min) Epoch 56/300 -- Iteration 53541 - Batch 576/963 - Train loss: 0.28207934  - Train acc: 0.8934 - Val loss: 0.99934030\n",
      "(11.21 min) Epoch 56/300 -- Iteration 53550 - Batch 585/963 - Train loss: 0.28187977  - Train acc: 0.8935 - Val loss: 0.99934030\n",
      "(11.22 min) Epoch 56/300 -- Iteration 53559 - Batch 594/963 - Train loss: 0.28187085  - Train acc: 0.8934 - Val loss: 0.99934030\n",
      "(11.22 min) Epoch 56/300 -- Iteration 53568 - Batch 603/963 - Train loss: 0.28148367  - Train acc: 0.8935 - Val loss: 0.99934030\n",
      "(11.22 min) Epoch 56/300 -- Iteration 53577 - Batch 612/963 - Train loss: 0.28126729  - Train acc: 0.8936 - Val loss: 0.99934030\n",
      "(11.22 min) Epoch 56/300 -- Iteration 53586 - Batch 621/963 - Train loss: 0.28159396  - Train acc: 0.8935 - Val loss: 0.99934030\n",
      "(11.22 min) Epoch 56/300 -- Iteration 53595 - Batch 630/963 - Train loss: 0.28172324  - Train acc: 0.8935 - Val loss: 0.99934030\n",
      "(11.23 min) Epoch 56/300 -- Iteration 53604 - Batch 639/963 - Train loss: 0.28150472  - Train acc: 0.8936 - Val loss: 0.99934030\n",
      "(11.23 min) Epoch 56/300 -- Iteration 53613 - Batch 648/963 - Train loss: 0.28162135  - Train acc: 0.8935 - Val loss: 0.99934030\n",
      "(11.23 min) Epoch 56/300 -- Iteration 53622 - Batch 657/963 - Train loss: 0.28174057  - Train acc: 0.8935 - Val loss: 0.99934030\n",
      "(11.23 min) Epoch 56/300 -- Iteration 53631 - Batch 666/963 - Train loss: 0.28163040  - Train acc: 0.8937 - Val loss: 0.99934030\n",
      "(11.23 min) Epoch 56/300 -- Iteration 53640 - Batch 675/963 - Train loss: 0.28137227  - Train acc: 0.8937 - Val loss: 0.99934030\n",
      "(11.24 min) Epoch 56/300 -- Iteration 53649 - Batch 684/963 - Train loss: 0.28096481  - Train acc: 0.8940 - Val loss: 0.99934030\n",
      "(11.24 min) Epoch 56/300 -- Iteration 53658 - Batch 693/963 - Train loss: 0.28042836  - Train acc: 0.8942 - Val loss: 0.99934030\n",
      "(11.24 min) Epoch 56/300 -- Iteration 53667 - Batch 702/963 - Train loss: 0.28035737  - Train acc: 0.8941 - Val loss: 0.99934030\n",
      "(11.24 min) Epoch 56/300 -- Iteration 53676 - Batch 711/963 - Train loss: 0.28026392  - Train acc: 0.8942 - Val loss: 0.99934030\n",
      "(11.24 min) Epoch 56/300 -- Iteration 53685 - Batch 720/963 - Train loss: 0.27998145  - Train acc: 0.8943 - Val loss: 0.99934030\n",
      "(11.25 min) Epoch 56/300 -- Iteration 53694 - Batch 729/963 - Train loss: 0.27996657  - Train acc: 0.8943 - Val loss: 0.99934030\n",
      "(11.25 min) Epoch 56/300 -- Iteration 53703 - Batch 738/963 - Train loss: 0.27995914  - Train acc: 0.8943 - Val loss: 0.99934030\n",
      "(11.25 min) Epoch 56/300 -- Iteration 53712 - Batch 747/963 - Train loss: 0.28015327  - Train acc: 0.8943 - Val loss: 0.99934030\n",
      "(11.25 min) Epoch 56/300 -- Iteration 53721 - Batch 756/963 - Train loss: 0.27987768  - Train acc: 0.8944 - Val loss: 0.99934030\n",
      "(11.25 min) Epoch 56/300 -- Iteration 53730 - Batch 765/963 - Train loss: 0.27963751  - Train acc: 0.8946 - Val loss: 0.99934030\n",
      "(11.26 min) Epoch 56/300 -- Iteration 53739 - Batch 774/963 - Train loss: 0.27921337  - Train acc: 0.8946 - Val loss: 0.99934030\n",
      "(11.26 min) Epoch 56/300 -- Iteration 53748 - Batch 783/963 - Train loss: 0.27927016  - Train acc: 0.8947 - Val loss: 0.99934030\n",
      "(11.26 min) Epoch 56/300 -- Iteration 53757 - Batch 792/963 - Train loss: 0.27933521  - Train acc: 0.8946 - Val loss: 0.99934030\n",
      "(11.26 min) Epoch 56/300 -- Iteration 53766 - Batch 801/963 - Train loss: 0.27924435  - Train acc: 0.8946 - Val loss: 0.99934030\n",
      "(11.27 min) Epoch 56/300 -- Iteration 53775 - Batch 810/963 - Train loss: 0.27892871  - Train acc: 0.8946 - Val loss: 0.99934030\n",
      "(11.27 min) Epoch 56/300 -- Iteration 53784 - Batch 819/963 - Train loss: 0.27904211  - Train acc: 0.8946 - Val loss: 0.99934030\n",
      "(11.27 min) Epoch 56/300 -- Iteration 53793 - Batch 828/963 - Train loss: 0.27902911  - Train acc: 0.8946 - Val loss: 0.99934030\n",
      "(11.27 min) Epoch 56/300 -- Iteration 53802 - Batch 837/963 - Train loss: 0.27920548  - Train acc: 0.8945 - Val loss: 0.99934030\n",
      "(11.27 min) Epoch 56/300 -- Iteration 53811 - Batch 846/963 - Train loss: 0.27951690  - Train acc: 0.8943 - Val loss: 0.99934030\n",
      "(11.28 min) Epoch 56/300 -- Iteration 53820 - Batch 855/963 - Train loss: 0.27961947  - Train acc: 0.8942 - Val loss: 0.99934030\n",
      "(11.28 min) Epoch 56/300 -- Iteration 53829 - Batch 864/963 - Train loss: 0.27967475  - Train acc: 0.8942 - Val loss: 0.99934030\n",
      "(11.28 min) Epoch 56/300 -- Iteration 53838 - Batch 873/963 - Train loss: 0.27975784  - Train acc: 0.8941 - Val loss: 0.99934030\n",
      "(11.28 min) Epoch 56/300 -- Iteration 53847 - Batch 882/963 - Train loss: 0.27987992  - Train acc: 0.8940 - Val loss: 0.99934030\n",
      "(11.28 min) Epoch 56/300 -- Iteration 53856 - Batch 891/963 - Train loss: 0.28013991  - Train acc: 0.8937 - Val loss: 0.99934030\n",
      "(11.28 min) Epoch 56/300 -- Iteration 53865 - Batch 900/963 - Train loss: 0.28029353  - Train acc: 0.8937 - Val loss: 0.99934030\n",
      "(11.29 min) Epoch 56/300 -- Iteration 53874 - Batch 909/963 - Train loss: 0.28034092  - Train acc: 0.8937 - Val loss: 0.99934030\n",
      "(11.29 min) Epoch 56/300 -- Iteration 53883 - Batch 918/963 - Train loss: 0.28054239  - Train acc: 0.8936 - Val loss: 0.99934030\n",
      "(11.29 min) Epoch 56/300 -- Iteration 53892 - Batch 927/963 - Train loss: 0.28043695  - Train acc: 0.8937 - Val loss: 0.99934030\n",
      "(11.29 min) Epoch 56/300 -- Iteration 53901 - Batch 936/963 - Train loss: 0.28043491  - Train acc: 0.8937 - Val loss: 0.99934030\n",
      "(11.29 min) Epoch 56/300 -- Iteration 53910 - Batch 945/963 - Train loss: 0.28068616  - Train acc: 0.8935 - Val loss: 0.99934030\n",
      "(11.30 min) Epoch 56/300 -- Iteration 53919 - Batch 954/963 - Train loss: 0.28086124  - Train acc: 0.8935 - Val loss: 0.99934030\n",
      "(11.30 min) Epoch 56/300 -- Iteration 53928 - Batch 962/963 - Train loss: 0.28061240  - Train acc: 0.8936 - Val loss: 0.97266299 - Val acc: 0.5967\n",
      "(11.30 min) Epoch 57/300 -- Iteration 53937 - Batch 9/963 - Train loss: 0.29748004  - Train acc: 0.8875 - Val loss: 0.97266299\n",
      "(11.30 min) Epoch 57/300 -- Iteration 53946 - Batch 18/963 - Train loss: 0.28236251  - Train acc: 0.8947 - Val loss: 0.97266299\n",
      "(11.30 min) Epoch 57/300 -- Iteration 53955 - Batch 27/963 - Train loss: 0.28150027  - Train acc: 0.8945 - Val loss: 0.97266299\n",
      "(11.31 min) Epoch 57/300 -- Iteration 53964 - Batch 36/963 - Train loss: 0.28435487  - Train acc: 0.8906 - Val loss: 0.97266299\n",
      "(11.31 min) Epoch 57/300 -- Iteration 53973 - Batch 45/963 - Train loss: 0.28387416  - Train acc: 0.8906 - Val loss: 0.97266299\n",
      "(11.31 min) Epoch 57/300 -- Iteration 53982 - Batch 54/963 - Train loss: 0.28508929  - Train acc: 0.8909 - Val loss: 0.97266299\n",
      "(11.31 min) Epoch 57/300 -- Iteration 53991 - Batch 63/963 - Train loss: 0.28853549  - Train acc: 0.8893 - Val loss: 0.97266299\n",
      "(11.31 min) Epoch 57/300 -- Iteration 54000 - Batch 72/963 - Train loss: 0.28844260  - Train acc: 0.8897 - Val loss: 0.97266299\n",
      "(11.32 min) Epoch 57/300 -- Iteration 54009 - Batch 81/963 - Train loss: 0.28495203  - Train acc: 0.8917 - Val loss: 0.97266299\n",
      "(11.32 min) Epoch 57/300 -- Iteration 54018 - Batch 90/963 - Train loss: 0.28582846  - Train acc: 0.8915 - Val loss: 0.97266299\n",
      "(11.32 min) Epoch 57/300 -- Iteration 54027 - Batch 99/963 - Train loss: 0.28521360  - Train acc: 0.8919 - Val loss: 0.97266299\n",
      "(11.32 min) Epoch 57/300 -- Iteration 54036 - Batch 108/963 - Train loss: 0.28568260  - Train acc: 0.8922 - Val loss: 0.97266299\n",
      "(11.32 min) Epoch 57/300 -- Iteration 54045 - Batch 117/963 - Train loss: 0.28545794  - Train acc: 0.8925 - Val loss: 0.97266299\n",
      "(11.33 min) Epoch 57/300 -- Iteration 54054 - Batch 126/963 - Train loss: 0.28309111  - Train acc: 0.8937 - Val loss: 0.97266299\n",
      "(11.33 min) Epoch 57/300 -- Iteration 54063 - Batch 135/963 - Train loss: 0.28257140  - Train acc: 0.8938 - Val loss: 0.97266299\n",
      "(11.33 min) Epoch 57/300 -- Iteration 54072 - Batch 144/963 - Train loss: 0.28434457  - Train acc: 0.8939 - Val loss: 0.97266299\n",
      "(11.33 min) Epoch 57/300 -- Iteration 54081 - Batch 153/963 - Train loss: 0.28250358  - Train acc: 0.8944 - Val loss: 0.97266299\n",
      "(11.33 min) Epoch 57/300 -- Iteration 54090 - Batch 162/963 - Train loss: 0.28355286  - Train acc: 0.8940 - Val loss: 0.97266299\n",
      "(11.33 min) Epoch 57/300 -- Iteration 54099 - Batch 171/963 - Train loss: 0.28271648  - Train acc: 0.8944 - Val loss: 0.97266299\n",
      "(11.34 min) Epoch 57/300 -- Iteration 54108 - Batch 180/963 - Train loss: 0.28074357  - Train acc: 0.8953 - Val loss: 0.97266299\n",
      "(11.34 min) Epoch 57/300 -- Iteration 54117 - Batch 189/963 - Train loss: 0.27953287  - Train acc: 0.8961 - Val loss: 0.97266299\n",
      "(11.34 min) Epoch 57/300 -- Iteration 54126 - Batch 198/963 - Train loss: 0.27932728  - Train acc: 0.8961 - Val loss: 0.97266299\n",
      "(11.34 min) Epoch 57/300 -- Iteration 54135 - Batch 207/963 - Train loss: 0.27978051  - Train acc: 0.8957 - Val loss: 0.97266299\n",
      "(11.34 min) Epoch 57/300 -- Iteration 54144 - Batch 216/963 - Train loss: 0.28029503  - Train acc: 0.8954 - Val loss: 0.97266299\n",
      "(11.35 min) Epoch 57/300 -- Iteration 54153 - Batch 225/963 - Train loss: 0.28158343  - Train acc: 0.8949 - Val loss: 0.97266299\n",
      "(11.35 min) Epoch 57/300 -- Iteration 54162 - Batch 234/963 - Train loss: 0.28142531  - Train acc: 0.8949 - Val loss: 0.97266299\n",
      "(11.35 min) Epoch 57/300 -- Iteration 54171 - Batch 243/963 - Train loss: 0.28214898  - Train acc: 0.8946 - Val loss: 0.97266299\n",
      "(11.35 min) Epoch 57/300 -- Iteration 54180 - Batch 252/963 - Train loss: 0.28258113  - Train acc: 0.8945 - Val loss: 0.97266299\n",
      "(11.35 min) Epoch 57/300 -- Iteration 54189 - Batch 261/963 - Train loss: 0.28318047  - Train acc: 0.8939 - Val loss: 0.97266299\n",
      "(11.36 min) Epoch 57/300 -- Iteration 54198 - Batch 270/963 - Train loss: 0.28287293  - Train acc: 0.8944 - Val loss: 0.97266299\n",
      "(11.36 min) Epoch 57/300 -- Iteration 54207 - Batch 279/963 - Train loss: 0.28307694  - Train acc: 0.8942 - Val loss: 0.97266299\n",
      "(11.36 min) Epoch 57/300 -- Iteration 54216 - Batch 288/963 - Train loss: 0.28369362  - Train acc: 0.8938 - Val loss: 0.97266299\n",
      "(11.36 min) Epoch 57/300 -- Iteration 54225 - Batch 297/963 - Train loss: 0.28462618  - Train acc: 0.8935 - Val loss: 0.97266299\n",
      "(11.36 min) Epoch 57/300 -- Iteration 54234 - Batch 306/963 - Train loss: 0.28447349  - Train acc: 0.8935 - Val loss: 0.97266299\n",
      "(11.36 min) Epoch 57/300 -- Iteration 54243 - Batch 315/963 - Train loss: 0.28460070  - Train acc: 0.8935 - Val loss: 0.97266299\n",
      "(11.37 min) Epoch 57/300 -- Iteration 54252 - Batch 324/963 - Train loss: 0.28378375  - Train acc: 0.8938 - Val loss: 0.97266299\n",
      "(11.37 min) Epoch 57/300 -- Iteration 54261 - Batch 333/963 - Train loss: 0.28326993  - Train acc: 0.8939 - Val loss: 0.97266299\n",
      "(11.37 min) Epoch 57/300 -- Iteration 54270 - Batch 342/963 - Train loss: 0.28298458  - Train acc: 0.8941 - Val loss: 0.97266299\n",
      "(11.37 min) Epoch 57/300 -- Iteration 54279 - Batch 351/963 - Train loss: 0.28237210  - Train acc: 0.8943 - Val loss: 0.97266299\n",
      "(11.37 min) Epoch 57/300 -- Iteration 54288 - Batch 360/963 - Train loss: 0.28268552  - Train acc: 0.8938 - Val loss: 0.97266299\n",
      "(11.38 min) Epoch 57/300 -- Iteration 54297 - Batch 369/963 - Train loss: 0.28314287  - Train acc: 0.8937 - Val loss: 0.97266299\n",
      "(11.38 min) Epoch 57/300 -- Iteration 54306 - Batch 378/963 - Train loss: 0.28332798  - Train acc: 0.8934 - Val loss: 0.97266299\n",
      "(11.38 min) Epoch 57/300 -- Iteration 54315 - Batch 387/963 - Train loss: 0.28315646  - Train acc: 0.8935 - Val loss: 0.97266299\n",
      "(11.38 min) Epoch 57/300 -- Iteration 54324 - Batch 396/963 - Train loss: 0.28272779  - Train acc: 0.8938 - Val loss: 0.97266299\n",
      "(11.39 min) Epoch 57/300 -- Iteration 54333 - Batch 405/963 - Train loss: 0.28240125  - Train acc: 0.8939 - Val loss: 0.97266299\n",
      "(11.39 min) Epoch 57/300 -- Iteration 54342 - Batch 414/963 - Train loss: 0.28284085  - Train acc: 0.8938 - Val loss: 0.97266299\n",
      "(11.39 min) Epoch 57/300 -- Iteration 54351 - Batch 423/963 - Train loss: 0.28260660  - Train acc: 0.8937 - Val loss: 0.97266299\n",
      "(11.39 min) Epoch 57/300 -- Iteration 54360 - Batch 432/963 - Train loss: 0.28237075  - Train acc: 0.8938 - Val loss: 0.97266299\n",
      "(11.39 min) Epoch 57/300 -- Iteration 54369 - Batch 441/963 - Train loss: 0.28262439  - Train acc: 0.8936 - Val loss: 0.97266299\n",
      "(11.39 min) Epoch 57/300 -- Iteration 54378 - Batch 450/963 - Train loss: 0.28280900  - Train acc: 0.8934 - Val loss: 0.97266299\n",
      "(11.40 min) Epoch 57/300 -- Iteration 54387 - Batch 459/963 - Train loss: 0.28312649  - Train acc: 0.8933 - Val loss: 0.97266299\n",
      "(11.40 min) Epoch 57/300 -- Iteration 54396 - Batch 468/963 - Train loss: 0.28268693  - Train acc: 0.8934 - Val loss: 0.97266299\n",
      "(11.40 min) Epoch 57/300 -- Iteration 54405 - Batch 477/963 - Train loss: 0.28274903  - Train acc: 0.8936 - Val loss: 0.97266299\n",
      "(11.40 min) Epoch 57/300 -- Iteration 54414 - Batch 486/963 - Train loss: 0.28260187  - Train acc: 0.8935 - Val loss: 0.97266299\n",
      "(11.40 min) Epoch 57/300 -- Iteration 54423 - Batch 495/963 - Train loss: 0.28265373  - Train acc: 0.8936 - Val loss: 0.97266299\n",
      "(11.41 min) Epoch 57/300 -- Iteration 54432 - Batch 504/963 - Train loss: 0.28289749  - Train acc: 0.8934 - Val loss: 0.97266299\n",
      "(11.41 min) Epoch 57/300 -- Iteration 54441 - Batch 513/963 - Train loss: 0.28294458  - Train acc: 0.8933 - Val loss: 0.97266299\n",
      "(11.41 min) Epoch 57/300 -- Iteration 54450 - Batch 522/963 - Train loss: 0.28261494  - Train acc: 0.8933 - Val loss: 0.97266299\n",
      "(11.41 min) Epoch 57/300 -- Iteration 54459 - Batch 531/963 - Train loss: 0.28240797  - Train acc: 0.8934 - Val loss: 0.97266299\n",
      "(11.41 min) Epoch 57/300 -- Iteration 54468 - Batch 540/963 - Train loss: 0.28208790  - Train acc: 0.8936 - Val loss: 0.97266299\n",
      "(11.42 min) Epoch 57/300 -- Iteration 54477 - Batch 549/963 - Train loss: 0.28221423  - Train acc: 0.8935 - Val loss: 0.97266299\n",
      "(11.42 min) Epoch 57/300 -- Iteration 54486 - Batch 558/963 - Train loss: 0.28208551  - Train acc: 0.8935 - Val loss: 0.97266299\n",
      "(11.42 min) Epoch 57/300 -- Iteration 54495 - Batch 567/963 - Train loss: 0.28233522  - Train acc: 0.8935 - Val loss: 0.97266299\n",
      "(11.42 min) Epoch 57/300 -- Iteration 54504 - Batch 576/963 - Train loss: 0.28233602  - Train acc: 0.8936 - Val loss: 0.97266299\n",
      "(11.42 min) Epoch 57/300 -- Iteration 54513 - Batch 585/963 - Train loss: 0.28270904  - Train acc: 0.8935 - Val loss: 0.97266299\n",
      "(11.43 min) Epoch 57/300 -- Iteration 54522 - Batch 594/963 - Train loss: 0.28253183  - Train acc: 0.8934 - Val loss: 0.97266299\n",
      "(11.43 min) Epoch 57/300 -- Iteration 54531 - Batch 603/963 - Train loss: 0.28265330  - Train acc: 0.8932 - Val loss: 0.97266299\n",
      "(11.43 min) Epoch 57/300 -- Iteration 54540 - Batch 612/963 - Train loss: 0.28239399  - Train acc: 0.8933 - Val loss: 0.97266299\n",
      "(11.43 min) Epoch 57/300 -- Iteration 54549 - Batch 621/963 - Train loss: 0.28235317  - Train acc: 0.8934 - Val loss: 0.97266299\n",
      "(11.43 min) Epoch 57/300 -- Iteration 54558 - Batch 630/963 - Train loss: 0.28280876  - Train acc: 0.8932 - Val loss: 0.97266299\n",
      "(11.43 min) Epoch 57/300 -- Iteration 54567 - Batch 639/963 - Train loss: 0.28301123  - Train acc: 0.8931 - Val loss: 0.97266299\n",
      "(11.44 min) Epoch 57/300 -- Iteration 54576 - Batch 648/963 - Train loss: 0.28260913  - Train acc: 0.8934 - Val loss: 0.97266299\n",
      "(11.44 min) Epoch 57/300 -- Iteration 54585 - Batch 657/963 - Train loss: 0.28273650  - Train acc: 0.8933 - Val loss: 0.97266299\n",
      "(11.44 min) Epoch 57/300 -- Iteration 54594 - Batch 666/963 - Train loss: 0.28271877  - Train acc: 0.8933 - Val loss: 0.97266299\n",
      "(11.44 min) Epoch 57/300 -- Iteration 54603 - Batch 675/963 - Train loss: 0.28294086  - Train acc: 0.8932 - Val loss: 0.97266299\n",
      "(11.44 min) Epoch 57/300 -- Iteration 54612 - Batch 684/963 - Train loss: 0.28263130  - Train acc: 0.8933 - Val loss: 0.97266299\n",
      "(11.45 min) Epoch 57/300 -- Iteration 54621 - Batch 693/963 - Train loss: 0.28327928  - Train acc: 0.8930 - Val loss: 0.97266299\n",
      "(11.45 min) Epoch 57/300 -- Iteration 54630 - Batch 702/963 - Train loss: 0.28320356  - Train acc: 0.8930 - Val loss: 0.97266299\n",
      "(11.45 min) Epoch 57/300 -- Iteration 54639 - Batch 711/963 - Train loss: 0.28366866  - Train acc: 0.8928 - Val loss: 0.97266299\n",
      "(11.45 min) Epoch 57/300 -- Iteration 54648 - Batch 720/963 - Train loss: 0.28336123  - Train acc: 0.8929 - Val loss: 0.97266299\n",
      "(11.45 min) Epoch 57/300 -- Iteration 54657 - Batch 729/963 - Train loss: 0.28325957  - Train acc: 0.8929 - Val loss: 0.97266299\n",
      "(11.46 min) Epoch 57/300 -- Iteration 54666 - Batch 738/963 - Train loss: 0.28362824  - Train acc: 0.8927 - Val loss: 0.97266299\n",
      "(11.46 min) Epoch 57/300 -- Iteration 54675 - Batch 747/963 - Train loss: 0.28382735  - Train acc: 0.8926 - Val loss: 0.97266299\n",
      "(11.46 min) Epoch 57/300 -- Iteration 54684 - Batch 756/963 - Train loss: 0.28394419  - Train acc: 0.8926 - Val loss: 0.97266299\n",
      "(11.46 min) Epoch 57/300 -- Iteration 54693 - Batch 765/963 - Train loss: 0.28386633  - Train acc: 0.8925 - Val loss: 0.97266299\n",
      "(11.46 min) Epoch 57/300 -- Iteration 54702 - Batch 774/963 - Train loss: 0.28381877  - Train acc: 0.8926 - Val loss: 0.97266299\n",
      "(11.47 min) Epoch 57/300 -- Iteration 54711 - Batch 783/963 - Train loss: 0.28359797  - Train acc: 0.8927 - Val loss: 0.97266299\n",
      "(11.47 min) Epoch 57/300 -- Iteration 54720 - Batch 792/963 - Train loss: 0.28370490  - Train acc: 0.8925 - Val loss: 0.97266299\n",
      "(11.47 min) Epoch 57/300 -- Iteration 54729 - Batch 801/963 - Train loss: 0.28368592  - Train acc: 0.8926 - Val loss: 0.97266299\n",
      "(11.47 min) Epoch 57/300 -- Iteration 54738 - Batch 810/963 - Train loss: 0.28390421  - Train acc: 0.8924 - Val loss: 0.97266299\n",
      "(11.47 min) Epoch 57/300 -- Iteration 54747 - Batch 819/963 - Train loss: 0.28387985  - Train acc: 0.8924 - Val loss: 0.97266299\n",
      "(11.48 min) Epoch 57/300 -- Iteration 54756 - Batch 828/963 - Train loss: 0.28364348  - Train acc: 0.8924 - Val loss: 0.97266299\n",
      "(11.48 min) Epoch 57/300 -- Iteration 54765 - Batch 837/963 - Train loss: 0.28378243  - Train acc: 0.8924 - Val loss: 0.97266299\n",
      "(11.48 min) Epoch 57/300 -- Iteration 54774 - Batch 846/963 - Train loss: 0.28392560  - Train acc: 0.8923 - Val loss: 0.97266299\n",
      "(11.48 min) Epoch 57/300 -- Iteration 54783 - Batch 855/963 - Train loss: 0.28425131  - Train acc: 0.8922 - Val loss: 0.97266299\n",
      "(11.48 min) Epoch 57/300 -- Iteration 54792 - Batch 864/963 - Train loss: 0.28438202  - Train acc: 0.8922 - Val loss: 0.97266299\n",
      "(11.49 min) Epoch 57/300 -- Iteration 54801 - Batch 873/963 - Train loss: 0.28478270  - Train acc: 0.8919 - Val loss: 0.97266299\n",
      "(11.49 min) Epoch 57/300 -- Iteration 54810 - Batch 882/963 - Train loss: 0.28490925  - Train acc: 0.8919 - Val loss: 0.97266299\n",
      "(11.49 min) Epoch 57/300 -- Iteration 54819 - Batch 891/963 - Train loss: 0.28483166  - Train acc: 0.8920 - Val loss: 0.97266299\n",
      "(11.49 min) Epoch 57/300 -- Iteration 54828 - Batch 900/963 - Train loss: 0.28493594  - Train acc: 0.8918 - Val loss: 0.97266299\n",
      "(11.49 min) Epoch 57/300 -- Iteration 54837 - Batch 909/963 - Train loss: 0.28478986  - Train acc: 0.8917 - Val loss: 0.97266299\n",
      "(11.50 min) Epoch 57/300 -- Iteration 54846 - Batch 918/963 - Train loss: 0.28479999  - Train acc: 0.8917 - Val loss: 0.97266299\n",
      "(11.50 min) Epoch 57/300 -- Iteration 54855 - Batch 927/963 - Train loss: 0.28495332  - Train acc: 0.8916 - Val loss: 0.97266299\n",
      "(11.50 min) Epoch 57/300 -- Iteration 54864 - Batch 936/963 - Train loss: 0.28511968  - Train acc: 0.8915 - Val loss: 0.97266299\n",
      "(11.50 min) Epoch 57/300 -- Iteration 54873 - Batch 945/963 - Train loss: 0.28504334  - Train acc: 0.8915 - Val loss: 0.97266299\n",
      "(11.50 min) Epoch 57/300 -- Iteration 54882 - Batch 954/963 - Train loss: 0.28489025  - Train acc: 0.8916 - Val loss: 0.97266299\n",
      "(11.50 min) Epoch 57/300 -- Iteration 54891 - Batch 962/963 - Train loss: 0.28490748  - Train acc: 0.8915 - Val loss: 0.95656127 - Val acc: 0.5967\n",
      "(11.51 min) Epoch 58/300 -- Iteration 54900 - Batch 9/963 - Train loss: 0.29064954  - Train acc: 0.8891 - Val loss: 0.95656127\n",
      "(11.51 min) Epoch 58/300 -- Iteration 54909 - Batch 18/963 - Train loss: 0.27885458  - Train acc: 0.8964 - Val loss: 0.95656127\n",
      "(11.51 min) Epoch 58/300 -- Iteration 54918 - Batch 27/963 - Train loss: 0.27634352  - Train acc: 0.8959 - Val loss: 0.95656127\n",
      "(11.51 min) Epoch 58/300 -- Iteration 54927 - Batch 36/963 - Train loss: 0.27697812  - Train acc: 0.8948 - Val loss: 0.95656127\n",
      "(11.51 min) Epoch 58/300 -- Iteration 54936 - Batch 45/963 - Train loss: 0.28091013  - Train acc: 0.8930 - Val loss: 0.95656127\n",
      "(11.52 min) Epoch 58/300 -- Iteration 54945 - Batch 54/963 - Train loss: 0.27794586  - Train acc: 0.8950 - Val loss: 0.95656127\n",
      "(11.52 min) Epoch 58/300 -- Iteration 54954 - Batch 63/963 - Train loss: 0.28094260  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.52 min) Epoch 58/300 -- Iteration 54963 - Batch 72/963 - Train loss: 0.27894618  - Train acc: 0.8965 - Val loss: 0.95656127\n",
      "(11.52 min) Epoch 58/300 -- Iteration 54972 - Batch 81/963 - Train loss: 0.28324429  - Train acc: 0.8938 - Val loss: 0.95656127\n",
      "(11.52 min) Epoch 58/300 -- Iteration 54981 - Batch 90/963 - Train loss: 0.28007837  - Train acc: 0.8947 - Val loss: 0.95656127\n",
      "(11.53 min) Epoch 58/300 -- Iteration 54990 - Batch 99/963 - Train loss: 0.28239934  - Train acc: 0.8931 - Val loss: 0.95656127\n",
      "(11.53 min) Epoch 58/300 -- Iteration 54999 - Batch 108/963 - Train loss: 0.28369674  - Train acc: 0.8933 - Val loss: 0.95656127\n",
      "(11.53 min) Epoch 58/300 -- Iteration 55008 - Batch 117/963 - Train loss: 0.28327106  - Train acc: 0.8937 - Val loss: 0.95656127\n",
      "(11.53 min) Epoch 58/300 -- Iteration 55017 - Batch 126/963 - Train loss: 0.28240595  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.53 min) Epoch 58/300 -- Iteration 55026 - Batch 135/963 - Train loss: 0.28118802  - Train acc: 0.8953 - Val loss: 0.95656127\n",
      "(11.54 min) Epoch 58/300 -- Iteration 55035 - Batch 144/963 - Train loss: 0.28097673  - Train acc: 0.8954 - Val loss: 0.95656127\n",
      "(11.54 min) Epoch 58/300 -- Iteration 55044 - Batch 153/963 - Train loss: 0.28060545  - Train acc: 0.8950 - Val loss: 0.95656127\n",
      "(11.54 min) Epoch 58/300 -- Iteration 55053 - Batch 162/963 - Train loss: 0.28126675  - Train acc: 0.8947 - Val loss: 0.95656127\n",
      "(11.54 min) Epoch 58/300 -- Iteration 55062 - Batch 171/963 - Train loss: 0.28119457  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.54 min) Epoch 58/300 -- Iteration 55071 - Batch 180/963 - Train loss: 0.28279976  - Train acc: 0.8939 - Val loss: 0.95656127\n",
      "(11.55 min) Epoch 58/300 -- Iteration 55080 - Batch 189/963 - Train loss: 0.28179901  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.55 min) Epoch 58/300 -- Iteration 55089 - Batch 198/963 - Train loss: 0.28269921  - Train acc: 0.8938 - Val loss: 0.95656127\n",
      "(11.55 min) Epoch 58/300 -- Iteration 55098 - Batch 207/963 - Train loss: 0.28270680  - Train acc: 0.8937 - Val loss: 0.95656127\n",
      "(11.55 min) Epoch 58/300 -- Iteration 55107 - Batch 216/963 - Train loss: 0.28227076  - Train acc: 0.8939 - Val loss: 0.95656127\n",
      "(11.55 min) Epoch 58/300 -- Iteration 55116 - Batch 225/963 - Train loss: 0.28216985  - Train acc: 0.8940 - Val loss: 0.95656127\n",
      "(11.56 min) Epoch 58/300 -- Iteration 55125 - Batch 234/963 - Train loss: 0.28180840  - Train acc: 0.8941 - Val loss: 0.95656127\n",
      "(11.56 min) Epoch 58/300 -- Iteration 55134 - Batch 243/963 - Train loss: 0.28283393  - Train acc: 0.8939 - Val loss: 0.95656127\n",
      "(11.56 min) Epoch 58/300 -- Iteration 55143 - Batch 252/963 - Train loss: 0.28353167  - Train acc: 0.8932 - Val loss: 0.95656127\n",
      "(11.56 min) Epoch 58/300 -- Iteration 55152 - Batch 261/963 - Train loss: 0.28387637  - Train acc: 0.8930 - Val loss: 0.95656127\n",
      "(11.56 min) Epoch 58/300 -- Iteration 55161 - Batch 270/963 - Train loss: 0.28405855  - Train acc: 0.8931 - Val loss: 0.95656127\n",
      "(11.57 min) Epoch 58/300 -- Iteration 55170 - Batch 279/963 - Train loss: 0.28378948  - Train acc: 0.8932 - Val loss: 0.95656127\n",
      "(11.57 min) Epoch 58/300 -- Iteration 55179 - Batch 288/963 - Train loss: 0.28380956  - Train acc: 0.8930 - Val loss: 0.95656127\n",
      "(11.57 min) Epoch 58/300 -- Iteration 55188 - Batch 297/963 - Train loss: 0.28346317  - Train acc: 0.8932 - Val loss: 0.95656127\n",
      "(11.57 min) Epoch 58/300 -- Iteration 55197 - Batch 306/963 - Train loss: 0.28353980  - Train acc: 0.8932 - Val loss: 0.95656127\n",
      "(11.58 min) Epoch 58/300 -- Iteration 55206 - Batch 315/963 - Train loss: 0.28327906  - Train acc: 0.8934 - Val loss: 0.95656127\n",
      "(11.58 min) Epoch 58/300 -- Iteration 55215 - Batch 324/963 - Train loss: 0.28378030  - Train acc: 0.8934 - Val loss: 0.95656127\n",
      "(11.58 min) Epoch 58/300 -- Iteration 55224 - Batch 333/963 - Train loss: 0.28328424  - Train acc: 0.8937 - Val loss: 0.95656127\n",
      "(11.58 min) Epoch 58/300 -- Iteration 55233 - Batch 342/963 - Train loss: 0.28389844  - Train acc: 0.8936 - Val loss: 0.95656127\n",
      "(11.58 min) Epoch 58/300 -- Iteration 55242 - Batch 351/963 - Train loss: 0.28387888  - Train acc: 0.8935 - Val loss: 0.95656127\n",
      "(11.59 min) Epoch 58/300 -- Iteration 55251 - Batch 360/963 - Train loss: 0.28361658  - Train acc: 0.8935 - Val loss: 0.95656127\n",
      "(11.59 min) Epoch 58/300 -- Iteration 55260 - Batch 369/963 - Train loss: 0.28306003  - Train acc: 0.8940 - Val loss: 0.95656127\n",
      "(11.59 min) Epoch 58/300 -- Iteration 55269 - Batch 378/963 - Train loss: 0.28341443  - Train acc: 0.8939 - Val loss: 0.95656127\n",
      "(11.59 min) Epoch 58/300 -- Iteration 55278 - Batch 387/963 - Train loss: 0.28324633  - Train acc: 0.8940 - Val loss: 0.95656127\n",
      "(11.59 min) Epoch 58/300 -- Iteration 55287 - Batch 396/963 - Train loss: 0.28400684  - Train acc: 0.8936 - Val loss: 0.95656127\n",
      "(11.59 min) Epoch 58/300 -- Iteration 55296 - Batch 405/963 - Train loss: 0.28405302  - Train acc: 0.8935 - Val loss: 0.95656127\n",
      "(11.60 min) Epoch 58/300 -- Iteration 55305 - Batch 414/963 - Train loss: 0.28390809  - Train acc: 0.8936 - Val loss: 0.95656127\n",
      "(11.60 min) Epoch 58/300 -- Iteration 55314 - Batch 423/963 - Train loss: 0.28355934  - Train acc: 0.8937 - Val loss: 0.95656127\n",
      "(11.60 min) Epoch 58/300 -- Iteration 55323 - Batch 432/963 - Train loss: 0.28309912  - Train acc: 0.8939 - Val loss: 0.95656127\n",
      "(11.60 min) Epoch 58/300 -- Iteration 55332 - Batch 441/963 - Train loss: 0.28266571  - Train acc: 0.8941 - Val loss: 0.95656127\n",
      "(11.60 min) Epoch 58/300 -- Iteration 55341 - Batch 450/963 - Train loss: 0.28257561  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.61 min) Epoch 58/300 -- Iteration 55350 - Batch 459/963 - Train loss: 0.28233073  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.61 min) Epoch 58/300 -- Iteration 55359 - Batch 468/963 - Train loss: 0.28297449  - Train acc: 0.8942 - Val loss: 0.95656127\n",
      "(11.61 min) Epoch 58/300 -- Iteration 55368 - Batch 477/963 - Train loss: 0.28260715  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.61 min) Epoch 58/300 -- Iteration 55377 - Batch 486/963 - Train loss: 0.28219739  - Train acc: 0.8945 - Val loss: 0.95656127\n",
      "(11.61 min) Epoch 58/300 -- Iteration 55386 - Batch 495/963 - Train loss: 0.28188727  - Train acc: 0.8945 - Val loss: 0.95656127\n",
      "(11.62 min) Epoch 58/300 -- Iteration 55395 - Batch 504/963 - Train loss: 0.28172891  - Train acc: 0.8946 - Val loss: 0.95656127\n",
      "(11.62 min) Epoch 58/300 -- Iteration 55404 - Batch 513/963 - Train loss: 0.28178801  - Train acc: 0.8945 - Val loss: 0.95656127\n",
      "(11.62 min) Epoch 58/300 -- Iteration 55413 - Batch 522/963 - Train loss: 0.28142231  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.62 min) Epoch 58/300 -- Iteration 55422 - Batch 531/963 - Train loss: 0.28128609  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.62 min) Epoch 58/300 -- Iteration 55431 - Batch 540/963 - Train loss: 0.28074334  - Train acc: 0.8946 - Val loss: 0.95656127\n",
      "(11.62 min) Epoch 58/300 -- Iteration 55440 - Batch 549/963 - Train loss: 0.28065326  - Train acc: 0.8948 - Val loss: 0.95656127\n",
      "(11.63 min) Epoch 58/300 -- Iteration 55449 - Batch 558/963 - Train loss: 0.28119187  - Train acc: 0.8946 - Val loss: 0.95656127\n",
      "(11.63 min) Epoch 58/300 -- Iteration 55458 - Batch 567/963 - Train loss: 0.28161675  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.63 min) Epoch 58/300 -- Iteration 55467 - Batch 576/963 - Train loss: 0.28164390  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.63 min) Epoch 58/300 -- Iteration 55476 - Batch 585/963 - Train loss: 0.28170669  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.63 min) Epoch 58/300 -- Iteration 55485 - Batch 594/963 - Train loss: 0.28158608  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.64 min) Epoch 58/300 -- Iteration 55494 - Batch 603/963 - Train loss: 0.28158924  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.64 min) Epoch 58/300 -- Iteration 55503 - Batch 612/963 - Train loss: 0.28132804  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.64 min) Epoch 58/300 -- Iteration 55512 - Batch 621/963 - Train loss: 0.28150235  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.64 min) Epoch 58/300 -- Iteration 55521 - Batch 630/963 - Train loss: 0.28157005  - Train acc: 0.8942 - Val loss: 0.95656127\n",
      "(11.64 min) Epoch 58/300 -- Iteration 55530 - Batch 639/963 - Train loss: 0.28144204  - Train acc: 0.8942 - Val loss: 0.95656127\n",
      "(11.65 min) Epoch 58/300 -- Iteration 55539 - Batch 648/963 - Train loss: 0.28166273  - Train acc: 0.8940 - Val loss: 0.95656127\n",
      "(11.65 min) Epoch 58/300 -- Iteration 55548 - Batch 657/963 - Train loss: 0.28159755  - Train acc: 0.8940 - Val loss: 0.95656127\n",
      "(11.65 min) Epoch 58/300 -- Iteration 55557 - Batch 666/963 - Train loss: 0.28186344  - Train acc: 0.8939 - Val loss: 0.95656127\n",
      "(11.65 min) Epoch 58/300 -- Iteration 55566 - Batch 675/963 - Train loss: 0.28213097  - Train acc: 0.8938 - Val loss: 0.95656127\n",
      "(11.65 min) Epoch 58/300 -- Iteration 55575 - Batch 684/963 - Train loss: 0.28222011  - Train acc: 0.8937 - Val loss: 0.95656127\n",
      "(11.66 min) Epoch 58/300 -- Iteration 55584 - Batch 693/963 - Train loss: 0.28191962  - Train acc: 0.8937 - Val loss: 0.95656127\n",
      "(11.66 min) Epoch 58/300 -- Iteration 55593 - Batch 702/963 - Train loss: 0.28160987  - Train acc: 0.8938 - Val loss: 0.95656127\n",
      "(11.66 min) Epoch 58/300 -- Iteration 55602 - Batch 711/963 - Train loss: 0.28135392  - Train acc: 0.8940 - Val loss: 0.95656127\n",
      "(11.66 min) Epoch 58/300 -- Iteration 55611 - Batch 720/963 - Train loss: 0.28098214  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.66 min) Epoch 58/300 -- Iteration 55620 - Batch 729/963 - Train loss: 0.28077361  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.66 min) Epoch 58/300 -- Iteration 55629 - Batch 738/963 - Train loss: 0.28076979  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.67 min) Epoch 58/300 -- Iteration 55638 - Batch 747/963 - Train loss: 0.28057161  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.67 min) Epoch 58/300 -- Iteration 55647 - Batch 756/963 - Train loss: 0.28048995  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.67 min) Epoch 58/300 -- Iteration 55656 - Batch 765/963 - Train loss: 0.28023675  - Train acc: 0.8945 - Val loss: 0.95656127\n",
      "(11.67 min) Epoch 58/300 -- Iteration 55665 - Batch 774/963 - Train loss: 0.28023249  - Train acc: 0.8945 - Val loss: 0.95656127\n",
      "(11.68 min) Epoch 58/300 -- Iteration 55674 - Batch 783/963 - Train loss: 0.28020079  - Train acc: 0.8944 - Val loss: 0.95656127\n",
      "(11.68 min) Epoch 58/300 -- Iteration 55683 - Batch 792/963 - Train loss: 0.28033900  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.68 min) Epoch 58/300 -- Iteration 55692 - Batch 801/963 - Train loss: 0.28035893  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.68 min) Epoch 58/300 -- Iteration 55701 - Batch 810/963 - Train loss: 0.28055123  - Train acc: 0.8942 - Val loss: 0.95656127\n",
      "(11.69 min) Epoch 58/300 -- Iteration 55710 - Batch 819/963 - Train loss: 0.28021413  - Train acc: 0.8943 - Val loss: 0.95656127\n",
      "(11.69 min) Epoch 58/300 -- Iteration 55719 - Batch 828/963 - Train loss: 0.28053899  - Train acc: 0.8941 - Val loss: 0.95656127\n",
      "(11.69 min) Epoch 58/300 -- Iteration 55728 - Batch 837/963 - Train loss: 0.28065764  - Train acc: 0.8941 - Val loss: 0.95656127\n",
      "(11.69 min) Epoch 58/300 -- Iteration 55737 - Batch 846/963 - Train loss: 0.28059258  - Train acc: 0.8941 - Val loss: 0.95656127\n",
      "(11.69 min) Epoch 58/300 -- Iteration 55746 - Batch 855/963 - Train loss: 0.28082777  - Train acc: 0.8940 - Val loss: 0.95656127\n",
      "(11.70 min) Epoch 58/300 -- Iteration 55755 - Batch 864/963 - Train loss: 0.28110523  - Train acc: 0.8939 - Val loss: 0.95656127\n",
      "(11.70 min) Epoch 58/300 -- Iteration 55764 - Batch 873/963 - Train loss: 0.28139663  - Train acc: 0.8938 - Val loss: 0.95656127\n",
      "(11.70 min) Epoch 58/300 -- Iteration 55773 - Batch 882/963 - Train loss: 0.28195639  - Train acc: 0.8935 - Val loss: 0.95656127\n",
      "(11.70 min) Epoch 58/300 -- Iteration 55782 - Batch 891/963 - Train loss: 0.28194912  - Train acc: 0.8935 - Val loss: 0.95656127\n",
      "(11.70 min) Epoch 58/300 -- Iteration 55791 - Batch 900/963 - Train loss: 0.28200619  - Train acc: 0.8935 - Val loss: 0.95656127\n",
      "(11.71 min) Epoch 58/300 -- Iteration 55800 - Batch 909/963 - Train loss: 0.28217112  - Train acc: 0.8934 - Val loss: 0.95656127\n",
      "(11.71 min) Epoch 58/300 -- Iteration 55809 - Batch 918/963 - Train loss: 0.28196793  - Train acc: 0.8935 - Val loss: 0.95656127\n",
      "(11.71 min) Epoch 58/300 -- Iteration 55818 - Batch 927/963 - Train loss: 0.28184746  - Train acc: 0.8934 - Val loss: 0.95656127\n",
      "(11.71 min) Epoch 58/300 -- Iteration 55827 - Batch 936/963 - Train loss: 0.28168177  - Train acc: 0.8935 - Val loss: 0.95656127\n",
      "(11.71 min) Epoch 58/300 -- Iteration 55836 - Batch 945/963 - Train loss: 0.28157723  - Train acc: 0.8935 - Val loss: 0.95656127\n",
      "(11.72 min) Epoch 58/300 -- Iteration 55845 - Batch 954/963 - Train loss: 0.28139893  - Train acc: 0.8936 - Val loss: 0.95656127\n",
      "(11.72 min) Epoch 58/300 -- Iteration 55854 - Batch 962/963 - Train loss: 0.28148382  - Train acc: 0.8936 - Val loss: 0.97344983 - Val acc: 0.5983\n",
      "(11.72 min) Epoch 59/300 -- Iteration 55863 - Batch 9/963 - Train loss: 0.26663304  - Train acc: 0.8984 - Val loss: 0.97344983\n",
      "(11.72 min) Epoch 59/300 -- Iteration 55872 - Batch 18/963 - Train loss: 0.27499953  - Train acc: 0.9005 - Val loss: 0.97344983\n",
      "(11.72 min) Epoch 59/300 -- Iteration 55881 - Batch 27/963 - Train loss: 0.27338466  - Train acc: 0.9012 - Val loss: 0.97344983\n",
      "(11.72 min) Epoch 59/300 -- Iteration 55890 - Batch 36/963 - Train loss: 0.27027298  - Train acc: 0.9022 - Val loss: 0.97344983\n",
      "(11.73 min) Epoch 59/300 -- Iteration 55899 - Batch 45/963 - Train loss: 0.27031583  - Train acc: 0.8986 - Val loss: 0.97344983\n",
      "(11.73 min) Epoch 59/300 -- Iteration 55908 - Batch 54/963 - Train loss: 0.26843940  - Train acc: 0.8980 - Val loss: 0.97344983\n",
      "(11.73 min) Epoch 59/300 -- Iteration 55917 - Batch 63/963 - Train loss: 0.26683171  - Train acc: 0.8990 - Val loss: 0.97344983\n",
      "(11.73 min) Epoch 59/300 -- Iteration 55926 - Batch 72/963 - Train loss: 0.26604675  - Train acc: 0.9001 - Val loss: 0.97344983\n",
      "(11.73 min) Epoch 59/300 -- Iteration 55935 - Batch 81/963 - Train loss: 0.26628026  - Train acc: 0.9002 - Val loss: 0.97344983\n",
      "(11.74 min) Epoch 59/300 -- Iteration 55944 - Batch 90/963 - Train loss: 0.26924513  - Train acc: 0.8999 - Val loss: 0.97344983\n",
      "(11.74 min) Epoch 59/300 -- Iteration 55953 - Batch 99/963 - Train loss: 0.27058901  - Train acc: 0.8984 - Val loss: 0.97344983\n",
      "(11.74 min) Epoch 59/300 -- Iteration 55962 - Batch 108/963 - Train loss: 0.27079790  - Train acc: 0.8982 - Val loss: 0.97344983\n",
      "(11.74 min) Epoch 59/300 -- Iteration 55971 - Batch 117/963 - Train loss: 0.26907921  - Train acc: 0.8992 - Val loss: 0.97344983\n",
      "(11.74 min) Epoch 59/300 -- Iteration 55980 - Batch 126/963 - Train loss: 0.27152849  - Train acc: 0.8983 - Val loss: 0.97344983\n",
      "(11.75 min) Epoch 59/300 -- Iteration 55989 - Batch 135/963 - Train loss: 0.27363957  - Train acc: 0.8975 - Val loss: 0.97344983\n",
      "(11.75 min) Epoch 59/300 -- Iteration 55998 - Batch 144/963 - Train loss: 0.27370481  - Train acc: 0.8980 - Val loss: 0.97344983\n",
      "(11.75 min) Epoch 59/300 -- Iteration 56007 - Batch 153/963 - Train loss: 0.27255980  - Train acc: 0.8983 - Val loss: 0.97344983\n",
      "(11.75 min) Epoch 59/300 -- Iteration 56016 - Batch 162/963 - Train loss: 0.27298372  - Train acc: 0.8980 - Val loss: 0.97344983\n",
      "(11.75 min) Epoch 59/300 -- Iteration 56025 - Batch 171/963 - Train loss: 0.27419842  - Train acc: 0.8973 - Val loss: 0.97344983\n",
      "(11.76 min) Epoch 59/300 -- Iteration 56034 - Batch 180/963 - Train loss: 0.27584136  - Train acc: 0.8971 - Val loss: 0.97344983\n",
      "(11.76 min) Epoch 59/300 -- Iteration 56043 - Batch 189/963 - Train loss: 0.27602599  - Train acc: 0.8970 - Val loss: 0.97344983\n",
      "(11.76 min) Epoch 59/300 -- Iteration 56052 - Batch 198/963 - Train loss: 0.27646323  - Train acc: 0.8971 - Val loss: 0.97344983\n",
      "(11.76 min) Epoch 59/300 -- Iteration 56061 - Batch 207/963 - Train loss: 0.27625610  - Train acc: 0.8968 - Val loss: 0.97344983\n",
      "(11.76 min) Epoch 59/300 -- Iteration 56070 - Batch 216/963 - Train loss: 0.27579953  - Train acc: 0.8970 - Val loss: 0.97344983\n",
      "(11.77 min) Epoch 59/300 -- Iteration 56079 - Batch 225/963 - Train loss: 0.27626357  - Train acc: 0.8969 - Val loss: 0.97344983\n",
      "(11.77 min) Epoch 59/300 -- Iteration 56088 - Batch 234/963 - Train loss: 0.27672439  - Train acc: 0.8965 - Val loss: 0.97344983\n",
      "(11.77 min) Epoch 59/300 -- Iteration 56097 - Batch 243/963 - Train loss: 0.27725267  - Train acc: 0.8962 - Val loss: 0.97344983\n",
      "(11.77 min) Epoch 59/300 -- Iteration 56106 - Batch 252/963 - Train loss: 0.27716393  - Train acc: 0.8961 - Val loss: 0.97344983\n",
      "(11.77 min) Epoch 59/300 -- Iteration 56115 - Batch 261/963 - Train loss: 0.27735826  - Train acc: 0.8959 - Val loss: 0.97344983\n",
      "(11.77 min) Epoch 59/300 -- Iteration 56124 - Batch 270/963 - Train loss: 0.27685519  - Train acc: 0.8962 - Val loss: 0.97344983\n",
      "(11.78 min) Epoch 59/300 -- Iteration 56133 - Batch 279/963 - Train loss: 0.27716940  - Train acc: 0.8961 - Val loss: 0.97344983\n",
      "(11.78 min) Epoch 59/300 -- Iteration 56142 - Batch 288/963 - Train loss: 0.27707636  - Train acc: 0.8962 - Val loss: 0.97344983\n",
      "(11.78 min) Epoch 59/300 -- Iteration 56151 - Batch 297/963 - Train loss: 0.27760609  - Train acc: 0.8957 - Val loss: 0.97344983\n",
      "(11.78 min) Epoch 59/300 -- Iteration 56160 - Batch 306/963 - Train loss: 0.27799549  - Train acc: 0.8955 - Val loss: 0.97344983\n",
      "(11.79 min) Epoch 59/300 -- Iteration 56169 - Batch 315/963 - Train loss: 0.27819621  - Train acc: 0.8956 - Val loss: 0.97344983\n",
      "(11.79 min) Epoch 59/300 -- Iteration 56178 - Batch 324/963 - Train loss: 0.27900204  - Train acc: 0.8954 - Val loss: 0.97344983\n",
      "(11.79 min) Epoch 59/300 -- Iteration 56187 - Batch 333/963 - Train loss: 0.27851957  - Train acc: 0.8955 - Val loss: 0.97344983\n",
      "(11.79 min) Epoch 59/300 -- Iteration 56196 - Batch 342/963 - Train loss: 0.27878723  - Train acc: 0.8954 - Val loss: 0.97344983\n",
      "(11.79 min) Epoch 59/300 -- Iteration 56205 - Batch 351/963 - Train loss: 0.27879953  - Train acc: 0.8953 - Val loss: 0.97344983\n",
      "(11.80 min) Epoch 59/300 -- Iteration 56214 - Batch 360/963 - Train loss: 0.27906136  - Train acc: 0.8950 - Val loss: 0.97344983\n",
      "(11.80 min) Epoch 59/300 -- Iteration 56223 - Batch 369/963 - Train loss: 0.27950102  - Train acc: 0.8946 - Val loss: 0.97344983\n",
      "(11.80 min) Epoch 59/300 -- Iteration 56232 - Batch 378/963 - Train loss: 0.27999676  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.80 min) Epoch 59/300 -- Iteration 56241 - Batch 387/963 - Train loss: 0.28074427  - Train acc: 0.8937 - Val loss: 0.97344983\n",
      "(11.80 min) Epoch 59/300 -- Iteration 56250 - Batch 396/963 - Train loss: 0.28085945  - Train acc: 0.8937 - Val loss: 0.97344983\n",
      "(11.81 min) Epoch 59/300 -- Iteration 56259 - Batch 405/963 - Train loss: 0.28037247  - Train acc: 0.8938 - Val loss: 0.97344983\n",
      "(11.81 min) Epoch 59/300 -- Iteration 56268 - Batch 414/963 - Train loss: 0.28085899  - Train acc: 0.8935 - Val loss: 0.97344983\n",
      "(11.81 min) Epoch 59/300 -- Iteration 56277 - Batch 423/963 - Train loss: 0.28003116  - Train acc: 0.8938 - Val loss: 0.97344983\n",
      "(11.81 min) Epoch 59/300 -- Iteration 56286 - Batch 432/963 - Train loss: 0.27995226  - Train acc: 0.8938 - Val loss: 0.97344983\n",
      "(11.81 min) Epoch 59/300 -- Iteration 56295 - Batch 441/963 - Train loss: 0.28000371  - Train acc: 0.8939 - Val loss: 0.97344983\n",
      "(11.81 min) Epoch 59/300 -- Iteration 56304 - Batch 450/963 - Train loss: 0.28010228  - Train acc: 0.8937 - Val loss: 0.97344983\n",
      "(11.82 min) Epoch 59/300 -- Iteration 56313 - Batch 459/963 - Train loss: 0.28026494  - Train acc: 0.8936 - Val loss: 0.97344983\n",
      "(11.82 min) Epoch 59/300 -- Iteration 56322 - Batch 468/963 - Train loss: 0.28001260  - Train acc: 0.8938 - Val loss: 0.97344983\n",
      "(11.82 min) Epoch 59/300 -- Iteration 56331 - Batch 477/963 - Train loss: 0.28036145  - Train acc: 0.8937 - Val loss: 0.97344983\n",
      "(11.82 min) Epoch 59/300 -- Iteration 56340 - Batch 486/963 - Train loss: 0.28067099  - Train acc: 0.8935 - Val loss: 0.97344983\n",
      "(11.82 min) Epoch 59/300 -- Iteration 56349 - Batch 495/963 - Train loss: 0.28074098  - Train acc: 0.8934 - Val loss: 0.97344983\n",
      "(11.83 min) Epoch 59/300 -- Iteration 56358 - Batch 504/963 - Train loss: 0.28062905  - Train acc: 0.8934 - Val loss: 0.97344983\n",
      "(11.83 min) Epoch 59/300 -- Iteration 56367 - Batch 513/963 - Train loss: 0.28057536  - Train acc: 0.8934 - Val loss: 0.97344983\n",
      "(11.83 min) Epoch 59/300 -- Iteration 56376 - Batch 522/963 - Train loss: 0.28086316  - Train acc: 0.8933 - Val loss: 0.97344983\n",
      "(11.83 min) Epoch 59/300 -- Iteration 56385 - Batch 531/963 - Train loss: 0.28079862  - Train acc: 0.8933 - Val loss: 0.97344983\n",
      "(11.83 min) Epoch 59/300 -- Iteration 56394 - Batch 540/963 - Train loss: 0.28098295  - Train acc: 0.8933 - Val loss: 0.97344983\n",
      "(11.84 min) Epoch 59/300 -- Iteration 56403 - Batch 549/963 - Train loss: 0.28099514  - Train acc: 0.8934 - Val loss: 0.97344983\n",
      "(11.84 min) Epoch 59/300 -- Iteration 56412 - Batch 558/963 - Train loss: 0.28092458  - Train acc: 0.8934 - Val loss: 0.97344983\n",
      "(11.84 min) Epoch 59/300 -- Iteration 56421 - Batch 567/963 - Train loss: 0.28049286  - Train acc: 0.8936 - Val loss: 0.97344983\n",
      "(11.84 min) Epoch 59/300 -- Iteration 56430 - Batch 576/963 - Train loss: 0.28030592  - Train acc: 0.8937 - Val loss: 0.97344983\n",
      "(11.84 min) Epoch 59/300 -- Iteration 56439 - Batch 585/963 - Train loss: 0.28013079  - Train acc: 0.8938 - Val loss: 0.97344983\n",
      "(11.84 min) Epoch 59/300 -- Iteration 56448 - Batch 594/963 - Train loss: 0.28020658  - Train acc: 0.8938 - Val loss: 0.97344983\n",
      "(11.85 min) Epoch 59/300 -- Iteration 56457 - Batch 603/963 - Train loss: 0.28079737  - Train acc: 0.8936 - Val loss: 0.97344983\n",
      "(11.85 min) Epoch 59/300 -- Iteration 56466 - Batch 612/963 - Train loss: 0.28104818  - Train acc: 0.8934 - Val loss: 0.97344983\n",
      "(11.85 min) Epoch 59/300 -- Iteration 56475 - Batch 621/963 - Train loss: 0.28112089  - Train acc: 0.8934 - Val loss: 0.97344983\n",
      "(11.85 min) Epoch 59/300 -- Iteration 56484 - Batch 630/963 - Train loss: 0.28075424  - Train acc: 0.8935 - Val loss: 0.97344983\n",
      "(11.85 min) Epoch 59/300 -- Iteration 56493 - Batch 639/963 - Train loss: 0.28055926  - Train acc: 0.8936 - Val loss: 0.97344983\n",
      "(11.86 min) Epoch 59/300 -- Iteration 56502 - Batch 648/963 - Train loss: 0.28047701  - Train acc: 0.8937 - Val loss: 0.97344983\n",
      "(11.86 min) Epoch 59/300 -- Iteration 56511 - Batch 657/963 - Train loss: 0.28075312  - Train acc: 0.8936 - Val loss: 0.97344983\n",
      "(11.86 min) Epoch 59/300 -- Iteration 56520 - Batch 666/963 - Train loss: 0.28043894  - Train acc: 0.8938 - Val loss: 0.97344983\n",
      "(11.86 min) Epoch 59/300 -- Iteration 56529 - Batch 675/963 - Train loss: 0.28069094  - Train acc: 0.8937 - Val loss: 0.97344983\n",
      "(11.86 min) Epoch 59/300 -- Iteration 56538 - Batch 684/963 - Train loss: 0.28069493  - Train acc: 0.8937 - Val loss: 0.97344983\n",
      "(11.87 min) Epoch 59/300 -- Iteration 56547 - Batch 693/963 - Train loss: 0.28034349  - Train acc: 0.8940 - Val loss: 0.97344983\n",
      "(11.87 min) Epoch 59/300 -- Iteration 56556 - Batch 702/963 - Train loss: 0.27998877  - Train acc: 0.8941 - Val loss: 0.97344983\n",
      "(11.87 min) Epoch 59/300 -- Iteration 56565 - Batch 711/963 - Train loss: 0.27981223  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.87 min) Epoch 59/300 -- Iteration 56574 - Batch 720/963 - Train loss: 0.27939221  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.87 min) Epoch 59/300 -- Iteration 56583 - Batch 729/963 - Train loss: 0.27917926  - Train acc: 0.8945 - Val loss: 0.97344983\n",
      "(11.87 min) Epoch 59/300 -- Iteration 56592 - Batch 738/963 - Train loss: 0.27893145  - Train acc: 0.8946 - Val loss: 0.97344983\n",
      "(11.88 min) Epoch 59/300 -- Iteration 56601 - Batch 747/963 - Train loss: 0.27919303  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.88 min) Epoch 59/300 -- Iteration 56610 - Batch 756/963 - Train loss: 0.27935405  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.88 min) Epoch 59/300 -- Iteration 56619 - Batch 765/963 - Train loss: 0.27925766  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.88 min) Epoch 59/300 -- Iteration 56628 - Batch 774/963 - Train loss: 0.27938690  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.88 min) Epoch 59/300 -- Iteration 56637 - Batch 783/963 - Train loss: 0.27958832  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.89 min) Epoch 59/300 -- Iteration 56646 - Batch 792/963 - Train loss: 0.27967921  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.89 min) Epoch 59/300 -- Iteration 56655 - Batch 801/963 - Train loss: 0.27986363  - Train acc: 0.8942 - Val loss: 0.97344983\n",
      "(11.89 min) Epoch 59/300 -- Iteration 56664 - Batch 810/963 - Train loss: 0.27998788  - Train acc: 0.8941 - Val loss: 0.97344983\n",
      "(11.89 min) Epoch 59/300 -- Iteration 56673 - Batch 819/963 - Train loss: 0.27994214  - Train acc: 0.8942 - Val loss: 0.97344983\n",
      "(11.89 min) Epoch 59/300 -- Iteration 56682 - Batch 828/963 - Train loss: 0.27957435  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.90 min) Epoch 59/300 -- Iteration 56691 - Batch 837/963 - Train loss: 0.27936165  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.90 min) Epoch 59/300 -- Iteration 56700 - Batch 846/963 - Train loss: 0.27971565  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.90 min) Epoch 59/300 -- Iteration 56709 - Batch 855/963 - Train loss: 0.27959984  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.90 min) Epoch 59/300 -- Iteration 56718 - Batch 864/963 - Train loss: 0.27943829  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.90 min) Epoch 59/300 -- Iteration 56727 - Batch 873/963 - Train loss: 0.27937593  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.90 min) Epoch 59/300 -- Iteration 56736 - Batch 882/963 - Train loss: 0.27917120  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.91 min) Epoch 59/300 -- Iteration 56745 - Batch 891/963 - Train loss: 0.27917615  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.91 min) Epoch 59/300 -- Iteration 56754 - Batch 900/963 - Train loss: 0.27923368  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.91 min) Epoch 59/300 -- Iteration 56763 - Batch 909/963 - Train loss: 0.27928091  - Train acc: 0.8944 - Val loss: 0.97344983\n",
      "(11.91 min) Epoch 59/300 -- Iteration 56772 - Batch 918/963 - Train loss: 0.27930197  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.91 min) Epoch 59/300 -- Iteration 56781 - Batch 927/963 - Train loss: 0.27915590  - Train acc: 0.8943 - Val loss: 0.97344983\n",
      "(11.92 min) Epoch 59/300 -- Iteration 56790 - Batch 936/963 - Train loss: 0.27941054  - Train acc: 0.8942 - Val loss: 0.97344983\n",
      "(11.92 min) Epoch 59/300 -- Iteration 56799 - Batch 945/963 - Train loss: 0.27949300  - Train acc: 0.8941 - Val loss: 0.97344983\n",
      "(11.92 min) Epoch 59/300 -- Iteration 56808 - Batch 954/963 - Train loss: 0.27946145  - Train acc: 0.8941 - Val loss: 0.97344983\n",
      "(11.92 min) Epoch 59/300 -- Iteration 56817 - Batch 962/963 - Train loss: 0.27960888  - Train acc: 0.8941 - Val loss: 0.97527385 - Val acc: 0.5983\n",
      "(11.92 min) Epoch 60/300 -- Iteration 56826 - Batch 9/963 - Train loss: 0.26671651  - Train acc: 0.8992 - Val loss: 0.97527385\n",
      "(11.93 min) Epoch 60/300 -- Iteration 56835 - Batch 18/963 - Train loss: 0.27027671  - Train acc: 0.8988 - Val loss: 0.97527385\n",
      "(11.93 min) Epoch 60/300 -- Iteration 56844 - Batch 27/963 - Train loss: 0.29146173  - Train acc: 0.8895 - Val loss: 0.97527385\n",
      "(11.93 min) Epoch 60/300 -- Iteration 56853 - Batch 36/963 - Train loss: 0.28770519  - Train acc: 0.8923 - Val loss: 0.97527385\n",
      "(11.93 min) Epoch 60/300 -- Iteration 56862 - Batch 45/963 - Train loss: 0.28433388  - Train acc: 0.8932 - Val loss: 0.97527385\n",
      "(11.93 min) Epoch 60/300 -- Iteration 56871 - Batch 54/963 - Train loss: 0.28252643  - Train acc: 0.8936 - Val loss: 0.97527385\n",
      "(11.94 min) Epoch 60/300 -- Iteration 56880 - Batch 63/963 - Train loss: 0.28746908  - Train acc: 0.8917 - Val loss: 0.97527385\n",
      "(11.94 min) Epoch 60/300 -- Iteration 56889 - Batch 72/963 - Train loss: 0.28381072  - Train acc: 0.8943 - Val loss: 0.97527385\n",
      "(11.94 min) Epoch 60/300 -- Iteration 56898 - Batch 81/963 - Train loss: 0.28447718  - Train acc: 0.8936 - Val loss: 0.97527385\n",
      "(11.94 min) Epoch 60/300 -- Iteration 56907 - Batch 90/963 - Train loss: 0.28514212  - Train acc: 0.8923 - Val loss: 0.97527385\n",
      "(11.94 min) Epoch 60/300 -- Iteration 56916 - Batch 99/963 - Train loss: 0.28643968  - Train acc: 0.8913 - Val loss: 0.97527385\n",
      "(11.94 min) Epoch 60/300 -- Iteration 56925 - Batch 108/963 - Train loss: 0.28437659  - Train acc: 0.8926 - Val loss: 0.97527385\n",
      "(11.95 min) Epoch 60/300 -- Iteration 56934 - Batch 117/963 - Train loss: 0.28351282  - Train acc: 0.8922 - Val loss: 0.97527385\n",
      "(11.95 min) Epoch 60/300 -- Iteration 56943 - Batch 126/963 - Train loss: 0.28341200  - Train acc: 0.8922 - Val loss: 0.97527385\n",
      "(11.95 min) Epoch 60/300 -- Iteration 56952 - Batch 135/963 - Train loss: 0.28113236  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(11.95 min) Epoch 60/300 -- Iteration 56961 - Batch 144/963 - Train loss: 0.28107973  - Train acc: 0.8922 - Val loss: 0.97527385\n",
      "(11.95 min) Epoch 60/300 -- Iteration 56970 - Batch 153/963 - Train loss: 0.28042311  - Train acc: 0.8932 - Val loss: 0.97527385\n",
      "(11.96 min) Epoch 60/300 -- Iteration 56979 - Batch 162/963 - Train loss: 0.28027731  - Train acc: 0.8934 - Val loss: 0.97527385\n",
      "(11.96 min) Epoch 60/300 -- Iteration 56988 - Batch 171/963 - Train loss: 0.28090010  - Train acc: 0.8933 - Val loss: 0.97527385\n",
      "(11.96 min) Epoch 60/300 -- Iteration 56997 - Batch 180/963 - Train loss: 0.28022703  - Train acc: 0.8936 - Val loss: 0.97527385\n",
      "(11.96 min) Epoch 60/300 -- Iteration 57006 - Batch 189/963 - Train loss: 0.27914507  - Train acc: 0.8944 - Val loss: 0.97527385\n",
      "(11.96 min) Epoch 60/300 -- Iteration 57015 - Batch 198/963 - Train loss: 0.27928979  - Train acc: 0.8948 - Val loss: 0.97527385\n",
      "(11.97 min) Epoch 60/300 -- Iteration 57024 - Batch 207/963 - Train loss: 0.28035504  - Train acc: 0.8944 - Val loss: 0.97527385\n",
      "(11.97 min) Epoch 60/300 -- Iteration 57033 - Batch 216/963 - Train loss: 0.28187999  - Train acc: 0.8938 - Val loss: 0.97527385\n",
      "(11.97 min) Epoch 60/300 -- Iteration 57042 - Batch 225/963 - Train loss: 0.28213082  - Train acc: 0.8939 - Val loss: 0.97527385\n",
      "(11.97 min) Epoch 60/300 -- Iteration 57051 - Batch 234/963 - Train loss: 0.28289911  - Train acc: 0.8935 - Val loss: 0.97527385\n",
      "(11.97 min) Epoch 60/300 -- Iteration 57060 - Batch 243/963 - Train loss: 0.28366470  - Train acc: 0.8933 - Val loss: 0.97527385\n",
      "(11.98 min) Epoch 60/300 -- Iteration 57069 - Batch 252/963 - Train loss: 0.28314990  - Train acc: 0.8932 - Val loss: 0.97527385\n",
      "(11.98 min) Epoch 60/300 -- Iteration 57078 - Batch 261/963 - Train loss: 0.28190076  - Train acc: 0.8940 - Val loss: 0.97527385\n",
      "(11.98 min) Epoch 60/300 -- Iteration 57087 - Batch 270/963 - Train loss: 0.28242736  - Train acc: 0.8937 - Val loss: 0.97527385\n",
      "(11.98 min) Epoch 60/300 -- Iteration 57096 - Batch 279/963 - Train loss: 0.28105711  - Train acc: 0.8943 - Val loss: 0.97527385\n",
      "(11.98 min) Epoch 60/300 -- Iteration 57105 - Batch 288/963 - Train loss: 0.28256244  - Train acc: 0.8932 - Val loss: 0.97527385\n",
      "(11.98 min) Epoch 60/300 -- Iteration 57114 - Batch 297/963 - Train loss: 0.28298300  - Train acc: 0.8930 - Val loss: 0.97527385\n",
      "(11.99 min) Epoch 60/300 -- Iteration 57123 - Batch 306/963 - Train loss: 0.28247165  - Train acc: 0.8930 - Val loss: 0.97527385\n",
      "(11.99 min) Epoch 60/300 -- Iteration 57132 - Batch 315/963 - Train loss: 0.28263228  - Train acc: 0.8929 - Val loss: 0.97527385\n",
      "(11.99 min) Epoch 60/300 -- Iteration 57141 - Batch 324/963 - Train loss: 0.28248204  - Train acc: 0.8930 - Val loss: 0.97527385\n",
      "(11.99 min) Epoch 60/300 -- Iteration 57150 - Batch 333/963 - Train loss: 0.28212696  - Train acc: 0.8931 - Val loss: 0.97527385\n",
      "(11.99 min) Epoch 60/300 -- Iteration 57159 - Batch 342/963 - Train loss: 0.28266298  - Train acc: 0.8929 - Val loss: 0.97527385\n",
      "(12.00 min) Epoch 60/300 -- Iteration 57168 - Batch 351/963 - Train loss: 0.28349488  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.00 min) Epoch 60/300 -- Iteration 57177 - Batch 360/963 - Train loss: 0.28350757  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.00 min) Epoch 60/300 -- Iteration 57186 - Batch 369/963 - Train loss: 0.28419794  - Train acc: 0.8924 - Val loss: 0.97527385\n",
      "(12.00 min) Epoch 60/300 -- Iteration 57195 - Batch 378/963 - Train loss: 0.28417414  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.00 min) Epoch 60/300 -- Iteration 57204 - Batch 387/963 - Train loss: 0.28367180  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.01 min) Epoch 60/300 -- Iteration 57213 - Batch 396/963 - Train loss: 0.28322008  - Train acc: 0.8928 - Val loss: 0.97527385\n",
      "(12.01 min) Epoch 60/300 -- Iteration 57222 - Batch 405/963 - Train loss: 0.28278639  - Train acc: 0.8929 - Val loss: 0.97527385\n",
      "(12.01 min) Epoch 60/300 -- Iteration 57231 - Batch 414/963 - Train loss: 0.28263379  - Train acc: 0.8932 - Val loss: 0.97527385\n",
      "(12.01 min) Epoch 60/300 -- Iteration 57240 - Batch 423/963 - Train loss: 0.28283617  - Train acc: 0.8931 - Val loss: 0.97527385\n",
      "(12.01 min) Epoch 60/300 -- Iteration 57249 - Batch 432/963 - Train loss: 0.28272892  - Train acc: 0.8930 - Val loss: 0.97527385\n",
      "(12.01 min) Epoch 60/300 -- Iteration 57258 - Batch 441/963 - Train loss: 0.28232736  - Train acc: 0.8929 - Val loss: 0.97527385\n",
      "(12.02 min) Epoch 60/300 -- Iteration 57267 - Batch 450/963 - Train loss: 0.28258008  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.02 min) Epoch 60/300 -- Iteration 57276 - Batch 459/963 - Train loss: 0.28288440  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.02 min) Epoch 60/300 -- Iteration 57285 - Batch 468/963 - Train loss: 0.28296580  - Train acc: 0.8926 - Val loss: 0.97527385\n",
      "(12.02 min) Epoch 60/300 -- Iteration 57294 - Batch 477/963 - Train loss: 0.28303515  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.02 min) Epoch 60/300 -- Iteration 57303 - Batch 486/963 - Train loss: 0.28277842  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.03 min) Epoch 60/300 -- Iteration 57312 - Batch 495/963 - Train loss: 0.28297536  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.03 min) Epoch 60/300 -- Iteration 57321 - Batch 504/963 - Train loss: 0.28344596  - Train acc: 0.8924 - Val loss: 0.97527385\n",
      "(12.03 min) Epoch 60/300 -- Iteration 57330 - Batch 513/963 - Train loss: 0.28365003  - Train acc: 0.8924 - Val loss: 0.97527385\n",
      "(12.03 min) Epoch 60/300 -- Iteration 57339 - Batch 522/963 - Train loss: 0.28319945  - Train acc: 0.8926 - Val loss: 0.97527385\n",
      "(12.03 min) Epoch 60/300 -- Iteration 57348 - Batch 531/963 - Train loss: 0.28368019  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.04 min) Epoch 60/300 -- Iteration 57357 - Batch 540/963 - Train loss: 0.28309357  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.04 min) Epoch 60/300 -- Iteration 57366 - Batch 549/963 - Train loss: 0.28322470  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.04 min) Epoch 60/300 -- Iteration 57375 - Batch 558/963 - Train loss: 0.28351241  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.04 min) Epoch 60/300 -- Iteration 57384 - Batch 567/963 - Train loss: 0.28368942  - Train acc: 0.8926 - Val loss: 0.97527385\n",
      "(12.04 min) Epoch 60/300 -- Iteration 57393 - Batch 576/963 - Train loss: 0.28448909  - Train acc: 0.8923 - Val loss: 0.97527385\n",
      "(12.04 min) Epoch 60/300 -- Iteration 57402 - Batch 585/963 - Train loss: 0.28460407  - Train acc: 0.8921 - Val loss: 0.97527385\n",
      "(12.05 min) Epoch 60/300 -- Iteration 57411 - Batch 594/963 - Train loss: 0.28456846  - Train acc: 0.8921 - Val loss: 0.97527385\n",
      "(12.05 min) Epoch 60/300 -- Iteration 57420 - Batch 603/963 - Train loss: 0.28431493  - Train acc: 0.8922 - Val loss: 0.97527385\n",
      "(12.05 min) Epoch 60/300 -- Iteration 57429 - Batch 612/963 - Train loss: 0.28416307  - Train acc: 0.8922 - Val loss: 0.97527385\n",
      "(12.05 min) Epoch 60/300 -- Iteration 57438 - Batch 621/963 - Train loss: 0.28373731  - Train acc: 0.8924 - Val loss: 0.97527385\n",
      "(12.05 min) Epoch 60/300 -- Iteration 57447 - Batch 630/963 - Train loss: 0.28358396  - Train acc: 0.8923 - Val loss: 0.97527385\n",
      "(12.06 min) Epoch 60/300 -- Iteration 57456 - Batch 639/963 - Train loss: 0.28319457  - Train acc: 0.8926 - Val loss: 0.97527385\n",
      "(12.06 min) Epoch 60/300 -- Iteration 57465 - Batch 648/963 - Train loss: 0.28356177  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.06 min) Epoch 60/300 -- Iteration 57474 - Batch 657/963 - Train loss: 0.28422395  - Train acc: 0.8922 - Val loss: 0.97527385\n",
      "(12.06 min) Epoch 60/300 -- Iteration 57483 - Batch 666/963 - Train loss: 0.28394881  - Train acc: 0.8923 - Val loss: 0.97527385\n",
      "(12.06 min) Epoch 60/300 -- Iteration 57492 - Batch 675/963 - Train loss: 0.28422410  - Train acc: 0.8922 - Val loss: 0.97527385\n",
      "(12.07 min) Epoch 60/300 -- Iteration 57501 - Batch 684/963 - Train loss: 0.28446331  - Train acc: 0.8921 - Val loss: 0.97527385\n",
      "(12.07 min) Epoch 60/300 -- Iteration 57510 - Batch 693/963 - Train loss: 0.28428747  - Train acc: 0.8922 - Val loss: 0.97527385\n",
      "(12.07 min) Epoch 60/300 -- Iteration 57519 - Batch 702/963 - Train loss: 0.28404877  - Train acc: 0.8923 - Val loss: 0.97527385\n",
      "(12.07 min) Epoch 60/300 -- Iteration 57528 - Batch 711/963 - Train loss: 0.28409483  - Train acc: 0.8923 - Val loss: 0.97527385\n",
      "(12.07 min) Epoch 60/300 -- Iteration 57537 - Batch 720/963 - Train loss: 0.28408973  - Train acc: 0.8924 - Val loss: 0.97527385\n",
      "(12.07 min) Epoch 60/300 -- Iteration 57546 - Batch 729/963 - Train loss: 0.28400000  - Train acc: 0.8924 - Val loss: 0.97527385\n",
      "(12.08 min) Epoch 60/300 -- Iteration 57555 - Batch 738/963 - Train loss: 0.28371984  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.08 min) Epoch 60/300 -- Iteration 57564 - Batch 747/963 - Train loss: 0.28357242  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.08 min) Epoch 60/300 -- Iteration 57573 - Batch 756/963 - Train loss: 0.28380717  - Train acc: 0.8924 - Val loss: 0.97527385\n",
      "(12.08 min) Epoch 60/300 -- Iteration 57582 - Batch 765/963 - Train loss: 0.28369751  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.08 min) Epoch 60/300 -- Iteration 57591 - Batch 774/963 - Train loss: 0.28375414  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.09 min) Epoch 60/300 -- Iteration 57600 - Batch 783/963 - Train loss: 0.28359550  - Train acc: 0.8926 - Val loss: 0.97527385\n",
      "(12.09 min) Epoch 60/300 -- Iteration 57609 - Batch 792/963 - Train loss: 0.28354109  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.09 min) Epoch 60/300 -- Iteration 57618 - Batch 801/963 - Train loss: 0.28337583  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.09 min) Epoch 60/300 -- Iteration 57627 - Batch 810/963 - Train loss: 0.28288813  - Train acc: 0.8929 - Val loss: 0.97527385\n",
      "(12.09 min) Epoch 60/300 -- Iteration 57636 - Batch 819/963 - Train loss: 0.28276383  - Train acc: 0.8930 - Val loss: 0.97527385\n",
      "(12.10 min) Epoch 60/300 -- Iteration 57645 - Batch 828/963 - Train loss: 0.28308984  - Train acc: 0.8928 - Val loss: 0.97527385\n",
      "(12.10 min) Epoch 60/300 -- Iteration 57654 - Batch 837/963 - Train loss: 0.28322336  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.10 min) Epoch 60/300 -- Iteration 57663 - Batch 846/963 - Train loss: 0.28352505  - Train acc: 0.8926 - Val loss: 0.97527385\n",
      "(12.10 min) Epoch 60/300 -- Iteration 57672 - Batch 855/963 - Train loss: 0.28335671  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.10 min) Epoch 60/300 -- Iteration 57681 - Batch 864/963 - Train loss: 0.28326515  - Train acc: 0.8928 - Val loss: 0.97527385\n",
      "(12.11 min) Epoch 60/300 -- Iteration 57690 - Batch 873/963 - Train loss: 0.28340798  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.11 min) Epoch 60/300 -- Iteration 57699 - Batch 882/963 - Train loss: 0.28324103  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.11 min) Epoch 60/300 -- Iteration 57708 - Batch 891/963 - Train loss: 0.28336383  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.11 min) Epoch 60/300 -- Iteration 57717 - Batch 900/963 - Train loss: 0.28361502  - Train acc: 0.8926 - Val loss: 0.97527385\n",
      "(12.11 min) Epoch 60/300 -- Iteration 57726 - Batch 909/963 - Train loss: 0.28364352  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.11 min) Epoch 60/300 -- Iteration 57735 - Batch 918/963 - Train loss: 0.28382160  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.12 min) Epoch 60/300 -- Iteration 57744 - Batch 927/963 - Train loss: 0.28376372  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.12 min) Epoch 60/300 -- Iteration 57753 - Batch 936/963 - Train loss: 0.28361608  - Train acc: 0.8925 - Val loss: 0.97527385\n",
      "(12.12 min) Epoch 60/300 -- Iteration 57762 - Batch 945/963 - Train loss: 0.28347227  - Train acc: 0.8926 - Val loss: 0.97527385\n",
      "(12.12 min) Epoch 60/300 -- Iteration 57771 - Batch 954/963 - Train loss: 0.28335709  - Train acc: 0.8927 - Val loss: 0.97527385\n",
      "(12.12 min) Epoch 60/300 -- Iteration 57780 - Batch 962/963 - Train loss: 0.28341806  - Train acc: 0.8927 - Val loss: 0.99673247 - Val acc: 0.5917\n",
      "(12.13 min) Epoch 61/300 -- Iteration 57789 - Batch 9/963 - Train loss: 0.28891502  - Train acc: 0.8938 - Val loss: 0.99673247\n",
      "(12.13 min) Epoch 61/300 -- Iteration 57798 - Batch 18/963 - Train loss: 0.30327182  - Train acc: 0.8882 - Val loss: 0.99673247\n",
      "(12.13 min) Epoch 61/300 -- Iteration 57807 - Batch 27/963 - Train loss: 0.28814758  - Train acc: 0.8948 - Val loss: 0.99673247\n",
      "(12.13 min) Epoch 61/300 -- Iteration 57816 - Batch 36/963 - Train loss: 0.28953452  - Train acc: 0.8927 - Val loss: 0.99673247\n",
      "(12.13 min) Epoch 61/300 -- Iteration 57825 - Batch 45/963 - Train loss: 0.28819492  - Train acc: 0.8927 - Val loss: 0.99673247\n",
      "(12.14 min) Epoch 61/300 -- Iteration 57834 - Batch 54/963 - Train loss: 0.28306567  - Train acc: 0.8929 - Val loss: 0.99673247\n",
      "(12.14 min) Epoch 61/300 -- Iteration 57843 - Batch 63/963 - Train loss: 0.28260750  - Train acc: 0.8915 - Val loss: 0.99673247\n",
      "(12.14 min) Epoch 61/300 -- Iteration 57852 - Batch 72/963 - Train loss: 0.28087245  - Train acc: 0.8922 - Val loss: 0.99673247\n",
      "(12.14 min) Epoch 61/300 -- Iteration 57861 - Batch 81/963 - Train loss: 0.27906029  - Train acc: 0.8927 - Val loss: 0.99673247\n",
      "(12.14 min) Epoch 61/300 -- Iteration 57870 - Batch 90/963 - Train loss: 0.28014602  - Train acc: 0.8929 - Val loss: 0.99673247\n",
      "(12.15 min) Epoch 61/300 -- Iteration 57879 - Batch 99/963 - Train loss: 0.28134690  - Train acc: 0.8923 - Val loss: 0.99673247\n",
      "(12.15 min) Epoch 61/300 -- Iteration 57888 - Batch 108/963 - Train loss: 0.28266178  - Train acc: 0.8916 - Val loss: 0.99673247\n",
      "(12.15 min) Epoch 61/300 -- Iteration 57897 - Batch 117/963 - Train loss: 0.28321893  - Train acc: 0.8919 - Val loss: 0.99673247\n",
      "(12.15 min) Epoch 61/300 -- Iteration 57906 - Batch 126/963 - Train loss: 0.28294786  - Train acc: 0.8918 - Val loss: 0.99673247\n",
      "(12.15 min) Epoch 61/300 -- Iteration 57915 - Batch 135/963 - Train loss: 0.28154162  - Train acc: 0.8925 - Val loss: 0.99673247\n",
      "(12.15 min) Epoch 61/300 -- Iteration 57924 - Batch 144/963 - Train loss: 0.28262270  - Train acc: 0.8915 - Val loss: 0.99673247\n",
      "(12.16 min) Epoch 61/300 -- Iteration 57933 - Batch 153/963 - Train loss: 0.28028892  - Train acc: 0.8928 - Val loss: 0.99673247\n",
      "(12.16 min) Epoch 61/300 -- Iteration 57942 - Batch 162/963 - Train loss: 0.28129831  - Train acc: 0.8927 - Val loss: 0.99673247\n",
      "(12.16 min) Epoch 61/300 -- Iteration 57951 - Batch 171/963 - Train loss: 0.28046801  - Train acc: 0.8931 - Val loss: 0.99673247\n",
      "(12.16 min) Epoch 61/300 -- Iteration 57960 - Batch 180/963 - Train loss: 0.28076658  - Train acc: 0.8928 - Val loss: 0.99673247\n",
      "(12.16 min) Epoch 61/300 -- Iteration 57969 - Batch 189/963 - Train loss: 0.28174073  - Train acc: 0.8926 - Val loss: 0.99673247\n",
      "(12.17 min) Epoch 61/300 -- Iteration 57978 - Batch 198/963 - Train loss: 0.28173757  - Train acc: 0.8925 - Val loss: 0.99673247\n",
      "(12.17 min) Epoch 61/300 -- Iteration 57987 - Batch 207/963 - Train loss: 0.28250636  - Train acc: 0.8923 - Val loss: 0.99673247\n",
      "(12.17 min) Epoch 61/300 -- Iteration 57996 - Batch 216/963 - Train loss: 0.28214673  - Train acc: 0.8926 - Val loss: 0.99673247\n",
      "(12.17 min) Epoch 61/300 -- Iteration 58005 - Batch 225/963 - Train loss: 0.28144468  - Train acc: 0.8927 - Val loss: 0.99673247\n",
      "(12.17 min) Epoch 61/300 -- Iteration 58014 - Batch 234/963 - Train loss: 0.28142041  - Train acc: 0.8928 - Val loss: 0.99673247\n",
      "(12.17 min) Epoch 61/300 -- Iteration 58023 - Batch 243/963 - Train loss: 0.28105871  - Train acc: 0.8927 - Val loss: 0.99673247\n",
      "(12.18 min) Epoch 61/300 -- Iteration 58032 - Batch 252/963 - Train loss: 0.28169481  - Train acc: 0.8927 - Val loss: 0.99673247\n",
      "(12.18 min) Epoch 61/300 -- Iteration 58041 - Batch 261/963 - Train loss: 0.28271513  - Train acc: 0.8924 - Val loss: 0.99673247\n",
      "(12.18 min) Epoch 61/300 -- Iteration 58050 - Batch 270/963 - Train loss: 0.28306912  - Train acc: 0.8922 - Val loss: 0.99673247\n",
      "(12.18 min) Epoch 61/300 -- Iteration 58059 - Batch 279/963 - Train loss: 0.28259061  - Train acc: 0.8922 - Val loss: 0.99673247\n",
      "(12.18 min) Epoch 61/300 -- Iteration 58068 - Batch 288/963 - Train loss: 0.28230944  - Train acc: 0.8923 - Val loss: 0.99673247\n",
      "(12.19 min) Epoch 61/300 -- Iteration 58077 - Batch 297/963 - Train loss: 0.28234025  - Train acc: 0.8922 - Val loss: 0.99673247\n",
      "(12.19 min) Epoch 61/300 -- Iteration 58086 - Batch 306/963 - Train loss: 0.28235261  - Train acc: 0.8922 - Val loss: 0.99673247\n",
      "(12.19 min) Epoch 61/300 -- Iteration 58095 - Batch 315/963 - Train loss: 0.28191967  - Train acc: 0.8925 - Val loss: 0.99673247\n",
      "(12.19 min) Epoch 61/300 -- Iteration 58104 - Batch 324/963 - Train loss: 0.28173452  - Train acc: 0.8923 - Val loss: 0.99673247\n",
      "(12.19 min) Epoch 61/300 -- Iteration 58113 - Batch 333/963 - Train loss: 0.28131339  - Train acc: 0.8922 - Val loss: 0.99673247\n",
      "(12.20 min) Epoch 61/300 -- Iteration 58122 - Batch 342/963 - Train loss: 0.28157503  - Train acc: 0.8920 - Val loss: 0.99673247\n",
      "(12.20 min) Epoch 61/300 -- Iteration 58131 - Batch 351/963 - Train loss: 0.28225969  - Train acc: 0.8918 - Val loss: 0.99673247\n",
      "(12.20 min) Epoch 61/300 -- Iteration 58140 - Batch 360/963 - Train loss: 0.28236978  - Train acc: 0.8917 - Val loss: 0.99673247\n",
      "(12.20 min) Epoch 61/300 -- Iteration 58149 - Batch 369/963 - Train loss: 0.28198398  - Train acc: 0.8915 - Val loss: 0.99673247\n",
      "(12.20 min) Epoch 61/300 -- Iteration 58158 - Batch 378/963 - Train loss: 0.28160392  - Train acc: 0.8917 - Val loss: 0.99673247\n",
      "(12.21 min) Epoch 61/300 -- Iteration 58167 - Batch 387/963 - Train loss: 0.28172342  - Train acc: 0.8915 - Val loss: 0.99673247\n",
      "(12.21 min) Epoch 61/300 -- Iteration 58176 - Batch 396/963 - Train loss: 0.28167033  - Train acc: 0.8914 - Val loss: 0.99673247\n",
      "(12.21 min) Epoch 61/300 -- Iteration 58185 - Batch 405/963 - Train loss: 0.28101747  - Train acc: 0.8918 - Val loss: 0.99673247\n",
      "(12.21 min) Epoch 61/300 -- Iteration 58194 - Batch 414/963 - Train loss: 0.28081401  - Train acc: 0.8918 - Val loss: 0.99673247\n",
      "(12.21 min) Epoch 61/300 -- Iteration 58203 - Batch 423/963 - Train loss: 0.28105709  - Train acc: 0.8917 - Val loss: 0.99673247\n",
      "(12.21 min) Epoch 61/300 -- Iteration 58212 - Batch 432/963 - Train loss: 0.28069107  - Train acc: 0.8921 - Val loss: 0.99673247\n",
      "(12.22 min) Epoch 61/300 -- Iteration 58221 - Batch 441/963 - Train loss: 0.28111879  - Train acc: 0.8918 - Val loss: 0.99673247\n",
      "(12.22 min) Epoch 61/300 -- Iteration 58230 - Batch 450/963 - Train loss: 0.28078386  - Train acc: 0.8919 - Val loss: 0.99673247\n",
      "(12.22 min) Epoch 61/300 -- Iteration 58239 - Batch 459/963 - Train loss: 0.28058580  - Train acc: 0.8920 - Val loss: 0.99673247\n",
      "(12.22 min) Epoch 61/300 -- Iteration 58248 - Batch 468/963 - Train loss: 0.28009286  - Train acc: 0.8924 - Val loss: 0.99673247\n",
      "(12.22 min) Epoch 61/300 -- Iteration 58257 - Batch 477/963 - Train loss: 0.28005329  - Train acc: 0.8927 - Val loss: 0.99673247\n",
      "(12.23 min) Epoch 61/300 -- Iteration 58266 - Batch 486/963 - Train loss: 0.27990057  - Train acc: 0.8929 - Val loss: 0.99673247\n",
      "(12.23 min) Epoch 61/300 -- Iteration 58275 - Batch 495/963 - Train loss: 0.27969359  - Train acc: 0.8931 - Val loss: 0.99673247\n",
      "(12.23 min) Epoch 61/300 -- Iteration 58284 - Batch 504/963 - Train loss: 0.27978810  - Train acc: 0.8929 - Val loss: 0.99673247\n",
      "(12.23 min) Epoch 61/300 -- Iteration 58293 - Batch 513/963 - Train loss: 0.27977674  - Train acc: 0.8927 - Val loss: 0.99673247\n",
      "(12.23 min) Epoch 61/300 -- Iteration 58302 - Batch 522/963 - Train loss: 0.27945808  - Train acc: 0.8930 - Val loss: 0.99673247\n",
      "(12.23 min) Epoch 61/300 -- Iteration 58311 - Batch 531/963 - Train loss: 0.27933171  - Train acc: 0.8930 - Val loss: 0.99673247\n",
      "(12.24 min) Epoch 61/300 -- Iteration 58320 - Batch 540/963 - Train loss: 0.27922323  - Train acc: 0.8931 - Val loss: 0.99673247\n",
      "(12.24 min) Epoch 61/300 -- Iteration 58329 - Batch 549/963 - Train loss: 0.27938318  - Train acc: 0.8930 - Val loss: 0.99673247\n",
      "(12.24 min) Epoch 61/300 -- Iteration 58338 - Batch 558/963 - Train loss: 0.27878452  - Train acc: 0.8932 - Val loss: 0.99673247\n",
      "(12.24 min) Epoch 61/300 -- Iteration 58347 - Batch 567/963 - Train loss: 0.27891333  - Train acc: 0.8933 - Val loss: 0.99673247\n",
      "(12.24 min) Epoch 61/300 -- Iteration 58356 - Batch 576/963 - Train loss: 0.27882455  - Train acc: 0.8933 - Val loss: 0.99673247\n",
      "(12.25 min) Epoch 61/300 -- Iteration 58365 - Batch 585/963 - Train loss: 0.27863091  - Train acc: 0.8934 - Val loss: 0.99673247\n",
      "(12.25 min) Epoch 61/300 -- Iteration 58374 - Batch 594/963 - Train loss: 0.27897431  - Train acc: 0.8933 - Val loss: 0.99673247\n",
      "(12.25 min) Epoch 61/300 -- Iteration 58383 - Batch 603/963 - Train loss: 0.27911540  - Train acc: 0.8933 - Val loss: 0.99673247\n",
      "(12.25 min) Epoch 61/300 -- Iteration 58392 - Batch 612/963 - Train loss: 0.27888230  - Train acc: 0.8934 - Val loss: 0.99673247\n",
      "(12.25 min) Epoch 61/300 -- Iteration 58401 - Batch 621/963 - Train loss: 0.27847451  - Train acc: 0.8936 - Val loss: 0.99673247\n",
      "(12.26 min) Epoch 61/300 -- Iteration 58410 - Batch 630/963 - Train loss: 0.27870229  - Train acc: 0.8935 - Val loss: 0.99673247\n",
      "(12.26 min) Epoch 61/300 -- Iteration 58419 - Batch 639/963 - Train loss: 0.27881389  - Train acc: 0.8935 - Val loss: 0.99673247\n",
      "(12.26 min) Epoch 61/300 -- Iteration 58428 - Batch 648/963 - Train loss: 0.27904005  - Train acc: 0.8934 - Val loss: 0.99673247\n",
      "(12.26 min) Epoch 61/300 -- Iteration 58437 - Batch 657/963 - Train loss: 0.27908601  - Train acc: 0.8933 - Val loss: 0.99673247\n",
      "(12.26 min) Epoch 61/300 -- Iteration 58446 - Batch 666/963 - Train loss: 0.27898336  - Train acc: 0.8932 - Val loss: 0.99673247\n",
      "(12.26 min) Epoch 61/300 -- Iteration 58455 - Batch 675/963 - Train loss: 0.27917033  - Train acc: 0.8931 - Val loss: 0.99673247\n",
      "(12.27 min) Epoch 61/300 -- Iteration 58464 - Batch 684/963 - Train loss: 0.27900819  - Train acc: 0.8932 - Val loss: 0.99673247\n",
      "(12.27 min) Epoch 61/300 -- Iteration 58473 - Batch 693/963 - Train loss: 0.27887766  - Train acc: 0.8933 - Val loss: 0.99673247\n",
      "(12.27 min) Epoch 61/300 -- Iteration 58482 - Batch 702/963 - Train loss: 0.27855249  - Train acc: 0.8935 - Val loss: 0.99673247\n",
      "(12.27 min) Epoch 61/300 -- Iteration 58491 - Batch 711/963 - Train loss: 0.27799318  - Train acc: 0.8937 - Val loss: 0.99673247\n",
      "(12.27 min) Epoch 61/300 -- Iteration 58500 - Batch 720/963 - Train loss: 0.27797573  - Train acc: 0.8937 - Val loss: 0.99673247\n",
      "(12.28 min) Epoch 61/300 -- Iteration 58509 - Batch 729/963 - Train loss: 0.27765201  - Train acc: 0.8937 - Val loss: 0.99673247\n",
      "(12.28 min) Epoch 61/300 -- Iteration 58518 - Batch 738/963 - Train loss: 0.27767312  - Train acc: 0.8938 - Val loss: 0.99673247\n",
      "(12.28 min) Epoch 61/300 -- Iteration 58527 - Batch 747/963 - Train loss: 0.27742070  - Train acc: 0.8940 - Val loss: 0.99673247\n",
      "(12.28 min) Epoch 61/300 -- Iteration 58536 - Batch 756/963 - Train loss: 0.27787879  - Train acc: 0.8938 - Val loss: 0.99673247\n",
      "(12.28 min) Epoch 61/300 -- Iteration 58545 - Batch 765/963 - Train loss: 0.27792837  - Train acc: 0.8938 - Val loss: 0.99673247\n",
      "(12.29 min) Epoch 61/300 -- Iteration 58554 - Batch 774/963 - Train loss: 0.27792886  - Train acc: 0.8938 - Val loss: 0.99673247\n",
      "(12.29 min) Epoch 61/300 -- Iteration 58563 - Batch 783/963 - Train loss: 0.27818055  - Train acc: 0.8937 - Val loss: 0.99673247\n",
      "(12.29 min) Epoch 61/300 -- Iteration 58572 - Batch 792/963 - Train loss: 0.27829640  - Train acc: 0.8937 - Val loss: 0.99673247\n",
      "(12.29 min) Epoch 61/300 -- Iteration 58581 - Batch 801/963 - Train loss: 0.27859034  - Train acc: 0.8936 - Val loss: 0.99673247\n",
      "(12.29 min) Epoch 61/300 -- Iteration 58590 - Batch 810/963 - Train loss: 0.27861876  - Train acc: 0.8936 - Val loss: 0.99673247\n",
      "(12.29 min) Epoch 61/300 -- Iteration 58599 - Batch 819/963 - Train loss: 0.27860286  - Train acc: 0.8937 - Val loss: 0.99673247\n",
      "(12.30 min) Epoch 61/300 -- Iteration 58608 - Batch 828/963 - Train loss: 0.27879448  - Train acc: 0.8937 - Val loss: 0.99673247\n",
      "(12.30 min) Epoch 61/300 -- Iteration 58617 - Batch 837/963 - Train loss: 0.27887905  - Train acc: 0.8937 - Val loss: 0.99673247\n",
      "(12.30 min) Epoch 61/300 -- Iteration 58626 - Batch 846/963 - Train loss: 0.27844561  - Train acc: 0.8939 - Val loss: 0.99673247\n",
      "(12.30 min) Epoch 61/300 -- Iteration 58635 - Batch 855/963 - Train loss: 0.27818515  - Train acc: 0.8939 - Val loss: 0.99673247\n",
      "(12.30 min) Epoch 61/300 -- Iteration 58644 - Batch 864/963 - Train loss: 0.27802054  - Train acc: 0.8940 - Val loss: 0.99673247\n",
      "(12.31 min) Epoch 61/300 -- Iteration 58653 - Batch 873/963 - Train loss: 0.27797547  - Train acc: 0.8941 - Val loss: 0.99673247\n",
      "(12.31 min) Epoch 61/300 -- Iteration 58662 - Batch 882/963 - Train loss: 0.27790649  - Train acc: 0.8941 - Val loss: 0.99673247\n",
      "(12.31 min) Epoch 61/300 -- Iteration 58671 - Batch 891/963 - Train loss: 0.27845137  - Train acc: 0.8940 - Val loss: 0.99673247\n",
      "(12.31 min) Epoch 61/300 -- Iteration 58680 - Batch 900/963 - Train loss: 0.27832626  - Train acc: 0.8940 - Val loss: 0.99673247\n",
      "(12.31 min) Epoch 61/300 -- Iteration 58689 - Batch 909/963 - Train loss: 0.27833779  - Train acc: 0.8940 - Val loss: 0.99673247\n",
      "(12.32 min) Epoch 61/300 -- Iteration 58698 - Batch 918/963 - Train loss: 0.27844099  - Train acc: 0.8939 - Val loss: 0.99673247\n",
      "(12.32 min) Epoch 61/300 -- Iteration 58707 - Batch 927/963 - Train loss: 0.27834387  - Train acc: 0.8940 - Val loss: 0.99673247\n",
      "(12.32 min) Epoch 61/300 -- Iteration 58716 - Batch 936/963 - Train loss: 0.27849310  - Train acc: 0.8939 - Val loss: 0.99673247\n",
      "(12.32 min) Epoch 61/300 -- Iteration 58725 - Batch 945/963 - Train loss: 0.27835165  - Train acc: 0.8940 - Val loss: 0.99673247\n",
      "(12.32 min) Epoch 61/300 -- Iteration 58734 - Batch 954/963 - Train loss: 0.27829636  - Train acc: 0.8940 - Val loss: 0.99673247\n",
      "(12.32 min) Epoch 61/300 -- Iteration 58743 - Batch 962/963 - Train loss: 0.27836026  - Train acc: 0.8940 - Val loss: 0.99409294 - Val acc: 0.5933\n",
      "(12.33 min) Epoch 62/300 -- Iteration 58752 - Batch 9/963 - Train loss: 0.26175537  - Train acc: 0.8992 - Val loss: 0.99409294\n",
      "(12.33 min) Epoch 62/300 -- Iteration 58761 - Batch 18/963 - Train loss: 0.28310728  - Train acc: 0.8939 - Val loss: 0.99409294\n",
      "(12.33 min) Epoch 62/300 -- Iteration 58770 - Batch 27/963 - Train loss: 0.28229963  - Train acc: 0.8940 - Val loss: 0.99409294\n",
      "(12.33 min) Epoch 62/300 -- Iteration 58779 - Batch 36/963 - Train loss: 0.28350034  - Train acc: 0.8929 - Val loss: 0.99409294\n",
      "(12.33 min) Epoch 62/300 -- Iteration 58788 - Batch 45/963 - Train loss: 0.29071111  - Train acc: 0.8888 - Val loss: 0.99409294\n",
      "(12.34 min) Epoch 62/300 -- Iteration 58797 - Batch 54/963 - Train loss: 0.29021886  - Train acc: 0.8889 - Val loss: 0.99409294\n",
      "(12.34 min) Epoch 62/300 -- Iteration 58806 - Batch 63/963 - Train loss: 0.28995021  - Train acc: 0.8884 - Val loss: 0.99409294\n",
      "(12.34 min) Epoch 62/300 -- Iteration 58815 - Batch 72/963 - Train loss: 0.28947913  - Train acc: 0.8878 - Val loss: 0.99409294\n",
      "(12.34 min) Epoch 62/300 -- Iteration 58824 - Batch 81/963 - Train loss: 0.28716824  - Train acc: 0.8892 - Val loss: 0.99409294\n",
      "(12.34 min) Epoch 62/300 -- Iteration 58833 - Batch 90/963 - Train loss: 0.28534869  - Train acc: 0.8899 - Val loss: 0.99409294\n",
      "(12.35 min) Epoch 62/300 -- Iteration 58842 - Batch 99/963 - Train loss: 0.28697126  - Train acc: 0.8891 - Val loss: 0.99409294\n",
      "(12.35 min) Epoch 62/300 -- Iteration 58851 - Batch 108/963 - Train loss: 0.28489190  - Train acc: 0.8898 - Val loss: 0.99409294\n",
      "(12.35 min) Epoch 62/300 -- Iteration 58860 - Batch 117/963 - Train loss: 0.28349804  - Train acc: 0.8905 - Val loss: 0.99409294\n",
      "(12.35 min) Epoch 62/300 -- Iteration 58869 - Batch 126/963 - Train loss: 0.28554351  - Train acc: 0.8898 - Val loss: 0.99409294\n",
      "(12.35 min) Epoch 62/300 -- Iteration 58878 - Batch 135/963 - Train loss: 0.28519432  - Train acc: 0.8902 - Val loss: 0.99409294\n",
      "(12.36 min) Epoch 62/300 -- Iteration 58887 - Batch 144/963 - Train loss: 0.28499894  - Train acc: 0.8908 - Val loss: 0.99409294\n",
      "(12.36 min) Epoch 62/300 -- Iteration 58896 - Batch 153/963 - Train loss: 0.28659417  - Train acc: 0.8902 - Val loss: 0.99409294\n",
      "(12.36 min) Epoch 62/300 -- Iteration 58905 - Batch 162/963 - Train loss: 0.28692766  - Train acc: 0.8902 - Val loss: 0.99409294\n",
      "(12.36 min) Epoch 62/300 -- Iteration 58914 - Batch 171/963 - Train loss: 0.28444260  - Train acc: 0.8914 - Val loss: 0.99409294\n",
      "(12.36 min) Epoch 62/300 -- Iteration 58923 - Batch 180/963 - Train loss: 0.28259608  - Train acc: 0.8922 - Val loss: 0.99409294\n",
      "(12.36 min) Epoch 62/300 -- Iteration 58932 - Batch 189/963 - Train loss: 0.28327886  - Train acc: 0.8912 - Val loss: 0.99409294\n",
      "(12.37 min) Epoch 62/300 -- Iteration 58941 - Batch 198/963 - Train loss: 0.28185202  - Train acc: 0.8919 - Val loss: 0.99409294\n",
      "(12.37 min) Epoch 62/300 -- Iteration 58950 - Batch 207/963 - Train loss: 0.28229479  - Train acc: 0.8916 - Val loss: 0.99409294\n",
      "(12.37 min) Epoch 62/300 -- Iteration 58959 - Batch 216/963 - Train loss: 0.28098459  - Train acc: 0.8921 - Val loss: 0.99409294\n",
      "(12.37 min) Epoch 62/300 -- Iteration 58968 - Batch 225/963 - Train loss: 0.28188847  - Train acc: 0.8920 - Val loss: 0.99409294\n",
      "(12.37 min) Epoch 62/300 -- Iteration 58977 - Batch 234/963 - Train loss: 0.28074917  - Train acc: 0.8928 - Val loss: 0.99409294\n",
      "(12.38 min) Epoch 62/300 -- Iteration 58986 - Batch 243/963 - Train loss: 0.27923001  - Train acc: 0.8931 - Val loss: 0.99409294\n",
      "(12.38 min) Epoch 62/300 -- Iteration 58995 - Batch 252/963 - Train loss: 0.28025847  - Train acc: 0.8927 - Val loss: 0.99409294\n",
      "(12.38 min) Epoch 62/300 -- Iteration 59004 - Batch 261/963 - Train loss: 0.28069585  - Train acc: 0.8926 - Val loss: 0.99409294\n",
      "(12.38 min) Epoch 62/300 -- Iteration 59013 - Batch 270/963 - Train loss: 0.27963305  - Train acc: 0.8933 - Val loss: 0.99409294\n",
      "(12.38 min) Epoch 62/300 -- Iteration 59022 - Batch 279/963 - Train loss: 0.27935254  - Train acc: 0.8933 - Val loss: 0.99409294\n",
      "(12.39 min) Epoch 62/300 -- Iteration 59031 - Batch 288/963 - Train loss: 0.28020136  - Train acc: 0.8927 - Val loss: 0.99409294\n",
      "(12.39 min) Epoch 62/300 -- Iteration 59040 - Batch 297/963 - Train loss: 0.28091338  - Train acc: 0.8924 - Val loss: 0.99409294\n",
      "(12.39 min) Epoch 62/300 -- Iteration 59049 - Batch 306/963 - Train loss: 0.28095205  - Train acc: 0.8926 - Val loss: 0.99409294\n",
      "(12.39 min) Epoch 62/300 -- Iteration 59058 - Batch 315/963 - Train loss: 0.28116160  - Train acc: 0.8924 - Val loss: 0.99409294\n",
      "(12.39 min) Epoch 62/300 -- Iteration 59067 - Batch 324/963 - Train loss: 0.28084545  - Train acc: 0.8924 - Val loss: 0.99409294\n",
      "(12.39 min) Epoch 62/300 -- Iteration 59076 - Batch 333/963 - Train loss: 0.28067965  - Train acc: 0.8925 - Val loss: 0.99409294\n",
      "(12.40 min) Epoch 62/300 -- Iteration 59085 - Batch 342/963 - Train loss: 0.28099176  - Train acc: 0.8924 - Val loss: 0.99409294\n",
      "(12.40 min) Epoch 62/300 -- Iteration 59094 - Batch 351/963 - Train loss: 0.28070849  - Train acc: 0.8925 - Val loss: 0.99409294\n",
      "(12.40 min) Epoch 62/300 -- Iteration 59103 - Batch 360/963 - Train loss: 0.28023714  - Train acc: 0.8927 - Val loss: 0.99409294\n",
      "(12.40 min) Epoch 62/300 -- Iteration 59112 - Batch 369/963 - Train loss: 0.28028773  - Train acc: 0.8928 - Val loss: 0.99409294\n",
      "(12.40 min) Epoch 62/300 -- Iteration 59121 - Batch 378/963 - Train loss: 0.28124729  - Train acc: 0.8925 - Val loss: 0.99409294\n",
      "(12.41 min) Epoch 62/300 -- Iteration 59130 - Batch 387/963 - Train loss: 0.28116326  - Train acc: 0.8925 - Val loss: 0.99409294\n",
      "(12.41 min) Epoch 62/300 -- Iteration 59139 - Batch 396/963 - Train loss: 0.28113993  - Train acc: 0.8925 - Val loss: 0.99409294\n",
      "(12.41 min) Epoch 62/300 -- Iteration 59148 - Batch 405/963 - Train loss: 0.28044010  - Train acc: 0.8929 - Val loss: 0.99409294\n",
      "(12.41 min) Epoch 62/300 -- Iteration 59157 - Batch 414/963 - Train loss: 0.28054955  - Train acc: 0.8929 - Val loss: 0.99409294\n",
      "(12.41 min) Epoch 62/300 -- Iteration 59166 - Batch 423/963 - Train loss: 0.28034873  - Train acc: 0.8928 - Val loss: 0.99409294\n",
      "(12.42 min) Epoch 62/300 -- Iteration 59175 - Batch 432/963 - Train loss: 0.28026671  - Train acc: 0.8928 - Val loss: 0.99409294\n",
      "(12.42 min) Epoch 62/300 -- Iteration 59184 - Batch 441/963 - Train loss: 0.28001693  - Train acc: 0.8929 - Val loss: 0.99409294\n",
      "(12.42 min) Epoch 62/300 -- Iteration 59193 - Batch 450/963 - Train loss: 0.27997730  - Train acc: 0.8929 - Val loss: 0.99409294\n",
      "(12.42 min) Epoch 62/300 -- Iteration 59202 - Batch 459/963 - Train loss: 0.28037492  - Train acc: 0.8927 - Val loss: 0.99409294\n",
      "(12.42 min) Epoch 62/300 -- Iteration 59211 - Batch 468/963 - Train loss: 0.28031057  - Train acc: 0.8928 - Val loss: 0.99409294\n",
      "(12.42 min) Epoch 62/300 -- Iteration 59220 - Batch 477/963 - Train loss: 0.27990394  - Train acc: 0.8930 - Val loss: 0.99409294\n",
      "(12.43 min) Epoch 62/300 -- Iteration 59229 - Batch 486/963 - Train loss: 0.27948231  - Train acc: 0.8933 - Val loss: 0.99409294\n",
      "(12.43 min) Epoch 62/300 -- Iteration 59238 - Batch 495/963 - Train loss: 0.27961724  - Train acc: 0.8934 - Val loss: 0.99409294\n",
      "(12.43 min) Epoch 62/300 -- Iteration 59247 - Batch 504/963 - Train loss: 0.27978686  - Train acc: 0.8934 - Val loss: 0.99409294\n",
      "(12.43 min) Epoch 62/300 -- Iteration 59256 - Batch 513/963 - Train loss: 0.27996049  - Train acc: 0.8933 - Val loss: 0.99409294\n",
      "(12.43 min) Epoch 62/300 -- Iteration 59265 - Batch 522/963 - Train loss: 0.28016320  - Train acc: 0.8931 - Val loss: 0.99409294\n",
      "(12.44 min) Epoch 62/300 -- Iteration 59274 - Batch 531/963 - Train loss: 0.28040378  - Train acc: 0.8930 - Val loss: 0.99409294\n",
      "(12.44 min) Epoch 62/300 -- Iteration 59283 - Batch 540/963 - Train loss: 0.28026113  - Train acc: 0.8931 - Val loss: 0.99409294\n",
      "(12.44 min) Epoch 62/300 -- Iteration 59292 - Batch 549/963 - Train loss: 0.27999102  - Train acc: 0.8932 - Val loss: 0.99409294\n",
      "(12.44 min) Epoch 62/300 -- Iteration 59301 - Batch 558/963 - Train loss: 0.27998774  - Train acc: 0.8933 - Val loss: 0.99409294\n",
      "(12.44 min) Epoch 62/300 -- Iteration 59310 - Batch 567/963 - Train loss: 0.27980634  - Train acc: 0.8934 - Val loss: 0.99409294\n",
      "(12.45 min) Epoch 62/300 -- Iteration 59319 - Batch 576/963 - Train loss: 0.27999891  - Train acc: 0.8934 - Val loss: 0.99409294\n",
      "(12.45 min) Epoch 62/300 -- Iteration 59328 - Batch 585/963 - Train loss: 0.28027861  - Train acc: 0.8934 - Val loss: 0.99409294\n",
      "(12.45 min) Epoch 62/300 -- Iteration 59337 - Batch 594/963 - Train loss: 0.28032080  - Train acc: 0.8934 - Val loss: 0.99409294\n",
      "(12.45 min) Epoch 62/300 -- Iteration 59346 - Batch 603/963 - Train loss: 0.28004282  - Train acc: 0.8935 - Val loss: 0.99409294\n",
      "(12.45 min) Epoch 62/300 -- Iteration 59355 - Batch 612/963 - Train loss: 0.27977126  - Train acc: 0.8937 - Val loss: 0.99409294\n",
      "(12.45 min) Epoch 62/300 -- Iteration 59364 - Batch 621/963 - Train loss: 0.27983642  - Train acc: 0.8937 - Val loss: 0.99409294\n",
      "(12.46 min) Epoch 62/300 -- Iteration 59373 - Batch 630/963 - Train loss: 0.27990814  - Train acc: 0.8936 - Val loss: 0.99409294\n",
      "(12.46 min) Epoch 62/300 -- Iteration 59382 - Batch 639/963 - Train loss: 0.27976915  - Train acc: 0.8936 - Val loss: 0.99409294\n",
      "(12.46 min) Epoch 62/300 -- Iteration 59391 - Batch 648/963 - Train loss: 0.27973297  - Train acc: 0.8937 - Val loss: 0.99409294\n",
      "(12.46 min) Epoch 62/300 -- Iteration 59400 - Batch 657/963 - Train loss: 0.27983873  - Train acc: 0.8936 - Val loss: 0.99409294\n",
      "(12.46 min) Epoch 62/300 -- Iteration 59409 - Batch 666/963 - Train loss: 0.27964637  - Train acc: 0.8937 - Val loss: 0.99409294\n",
      "(12.47 min) Epoch 62/300 -- Iteration 59418 - Batch 675/963 - Train loss: 0.27917834  - Train acc: 0.8940 - Val loss: 0.99409294\n",
      "(12.47 min) Epoch 62/300 -- Iteration 59427 - Batch 684/963 - Train loss: 0.27920054  - Train acc: 0.8940 - Val loss: 0.99409294\n",
      "(12.47 min) Epoch 62/300 -- Iteration 59436 - Batch 693/963 - Train loss: 0.27887005  - Train acc: 0.8941 - Val loss: 0.99409294\n",
      "(12.47 min) Epoch 62/300 -- Iteration 59445 - Batch 702/963 - Train loss: 0.27893807  - Train acc: 0.8940 - Val loss: 0.99409294\n",
      "(12.47 min) Epoch 62/300 -- Iteration 59454 - Batch 711/963 - Train loss: 0.27891246  - Train acc: 0.8941 - Val loss: 0.99409294\n",
      "(12.48 min) Epoch 62/300 -- Iteration 59463 - Batch 720/963 - Train loss: 0.27895373  - Train acc: 0.8942 - Val loss: 0.99409294\n",
      "(12.48 min) Epoch 62/300 -- Iteration 59472 - Batch 729/963 - Train loss: 0.27873763  - Train acc: 0.8943 - Val loss: 0.99409294\n",
      "(12.48 min) Epoch 62/300 -- Iteration 59481 - Batch 738/963 - Train loss: 0.27889570  - Train acc: 0.8942 - Val loss: 0.99409294\n",
      "(12.48 min) Epoch 62/300 -- Iteration 59490 - Batch 747/963 - Train loss: 0.27884288  - Train acc: 0.8943 - Val loss: 0.99409294\n",
      "(12.48 min) Epoch 62/300 -- Iteration 59499 - Batch 756/963 - Train loss: 0.27884067  - Train acc: 0.8943 - Val loss: 0.99409294\n",
      "(12.49 min) Epoch 62/300 -- Iteration 59508 - Batch 765/963 - Train loss: 0.27891703  - Train acc: 0.8943 - Val loss: 0.99409294\n",
      "(12.49 min) Epoch 62/300 -- Iteration 59517 - Batch 774/963 - Train loss: 0.27928939  - Train acc: 0.8942 - Val loss: 0.99409294\n",
      "(12.49 min) Epoch 62/300 -- Iteration 59526 - Batch 783/963 - Train loss: 0.27986384  - Train acc: 0.8939 - Val loss: 0.99409294\n",
      "(12.49 min) Epoch 62/300 -- Iteration 59535 - Batch 792/963 - Train loss: 0.27988896  - Train acc: 0.8939 - Val loss: 0.99409294\n",
      "(12.49 min) Epoch 62/300 -- Iteration 59544 - Batch 801/963 - Train loss: 0.28003585  - Train acc: 0.8939 - Val loss: 0.99409294\n",
      "(12.49 min) Epoch 62/300 -- Iteration 59553 - Batch 810/963 - Train loss: 0.27981380  - Train acc: 0.8940 - Val loss: 0.99409294\n",
      "(12.50 min) Epoch 62/300 -- Iteration 59562 - Batch 819/963 - Train loss: 0.27939665  - Train acc: 0.8941 - Val loss: 0.99409294\n",
      "(12.50 min) Epoch 62/300 -- Iteration 59571 - Batch 828/963 - Train loss: 0.27923785  - Train acc: 0.8942 - Val loss: 0.99409294\n",
      "(12.50 min) Epoch 62/300 -- Iteration 59580 - Batch 837/963 - Train loss: 0.27929861  - Train acc: 0.8941 - Val loss: 0.99409294\n",
      "(12.50 min) Epoch 62/300 -- Iteration 59589 - Batch 846/963 - Train loss: 0.27913921  - Train acc: 0.8941 - Val loss: 0.99409294\n",
      "(12.50 min) Epoch 62/300 -- Iteration 59598 - Batch 855/963 - Train loss: 0.27893110  - Train acc: 0.8942 - Val loss: 0.99409294\n",
      "(12.51 min) Epoch 62/300 -- Iteration 59607 - Batch 864/963 - Train loss: 0.27895512  - Train acc: 0.8942 - Val loss: 0.99409294\n",
      "(12.51 min) Epoch 62/300 -- Iteration 59616 - Batch 873/963 - Train loss: 0.27901266  - Train acc: 0.8942 - Val loss: 0.99409294\n",
      "(12.51 min) Epoch 62/300 -- Iteration 59625 - Batch 882/963 - Train loss: 0.27891347  - Train acc: 0.8942 - Val loss: 0.99409294\n",
      "(12.51 min) Epoch 62/300 -- Iteration 59634 - Batch 891/963 - Train loss: 0.27890970  - Train acc: 0.8942 - Val loss: 0.99409294\n",
      "(12.51 min) Epoch 62/300 -- Iteration 59643 - Batch 900/963 - Train loss: 0.27892908  - Train acc: 0.8941 - Val loss: 0.99409294\n",
      "(12.51 min) Epoch 62/300 -- Iteration 59652 - Batch 909/963 - Train loss: 0.27893181  - Train acc: 0.8941 - Val loss: 0.99409294\n",
      "(12.52 min) Epoch 62/300 -- Iteration 59661 - Batch 918/963 - Train loss: 0.27915004  - Train acc: 0.8940 - Val loss: 0.99409294\n",
      "(12.52 min) Epoch 62/300 -- Iteration 59670 - Batch 927/963 - Train loss: 0.27939394  - Train acc: 0.8939 - Val loss: 0.99409294\n",
      "(12.52 min) Epoch 62/300 -- Iteration 59679 - Batch 936/963 - Train loss: 0.27922791  - Train acc: 0.8940 - Val loss: 0.99409294\n",
      "(12.52 min) Epoch 62/300 -- Iteration 59688 - Batch 945/963 - Train loss: 0.27906991  - Train acc: 0.8941 - Val loss: 0.99409294\n",
      "(12.52 min) Epoch 62/300 -- Iteration 59697 - Batch 954/963 - Train loss: 0.27906057  - Train acc: 0.8941 - Val loss: 0.99409294\n",
      "(12.53 min) Epoch 62/300 -- Iteration 59706 - Batch 962/963 - Train loss: 0.27908547  - Train acc: 0.8941 - Val loss: 1.01162231 - Val acc: 0.5933\n",
      "(12.53 min) Epoch 63/300 -- Iteration 59715 - Batch 9/963 - Train loss: 0.30608066  - Train acc: 0.8750 - Val loss: 1.01162231\n",
      "(12.53 min) Epoch 63/300 -- Iteration 59724 - Batch 18/963 - Train loss: 0.29606164  - Train acc: 0.8828 - Val loss: 1.01162231\n",
      "(12.53 min) Epoch 63/300 -- Iteration 59733 - Batch 27/963 - Train loss: 0.28980684  - Train acc: 0.8878 - Val loss: 1.01162231\n",
      "(12.53 min) Epoch 63/300 -- Iteration 59742 - Batch 36/963 - Train loss: 0.28472621  - Train acc: 0.8915 - Val loss: 1.01162231\n",
      "(12.54 min) Epoch 63/300 -- Iteration 59751 - Batch 45/963 - Train loss: 0.28143702  - Train acc: 0.8935 - Val loss: 1.01162231\n",
      "(12.54 min) Epoch 63/300 -- Iteration 59760 - Batch 54/963 - Train loss: 0.27804874  - Train acc: 0.8939 - Val loss: 1.01162231\n",
      "(12.54 min) Epoch 63/300 -- Iteration 59769 - Batch 63/963 - Train loss: 0.27450832  - Train acc: 0.8951 - Val loss: 1.01162231\n",
      "(12.54 min) Epoch 63/300 -- Iteration 59778 - Batch 72/963 - Train loss: 0.27779118  - Train acc: 0.8927 - Val loss: 1.01162231\n",
      "(12.54 min) Epoch 63/300 -- Iteration 59787 - Batch 81/963 - Train loss: 0.27775291  - Train acc: 0.8921 - Val loss: 1.01162231\n",
      "(12.55 min) Epoch 63/300 -- Iteration 59796 - Batch 90/963 - Train loss: 0.27708431  - Train acc: 0.8925 - Val loss: 1.01162231\n",
      "(12.55 min) Epoch 63/300 -- Iteration 59805 - Batch 99/963 - Train loss: 0.27944372  - Train acc: 0.8913 - Val loss: 1.01162231\n",
      "(12.55 min) Epoch 63/300 -- Iteration 59814 - Batch 108/963 - Train loss: 0.27801495  - Train acc: 0.8916 - Val loss: 1.01162231\n",
      "(12.55 min) Epoch 63/300 -- Iteration 59823 - Batch 117/963 - Train loss: 0.27832408  - Train acc: 0.8920 - Val loss: 1.01162231\n",
      "(12.55 min) Epoch 63/300 -- Iteration 59832 - Batch 126/963 - Train loss: 0.27749501  - Train acc: 0.8928 - Val loss: 1.01162231\n",
      "(12.55 min) Epoch 63/300 -- Iteration 59841 - Batch 135/963 - Train loss: 0.27544954  - Train acc: 0.8940 - Val loss: 1.01162231\n",
      "(12.56 min) Epoch 63/300 -- Iteration 59850 - Batch 144/963 - Train loss: 0.27776499  - Train acc: 0.8930 - Val loss: 1.01162231\n",
      "(12.56 min) Epoch 63/300 -- Iteration 59859 - Batch 153/963 - Train loss: 0.27715366  - Train acc: 0.8934 - Val loss: 1.01162231\n",
      "(12.56 min) Epoch 63/300 -- Iteration 59868 - Batch 162/963 - Train loss: 0.27651142  - Train acc: 0.8937 - Val loss: 1.01162231\n",
      "(12.56 min) Epoch 63/300 -- Iteration 59877 - Batch 171/963 - Train loss: 0.27770844  - Train acc: 0.8935 - Val loss: 1.01162231\n",
      "(12.56 min) Epoch 63/300 -- Iteration 59886 - Batch 180/963 - Train loss: 0.27778388  - Train acc: 0.8938 - Val loss: 1.01162231\n",
      "(12.57 min) Epoch 63/300 -- Iteration 59895 - Batch 189/963 - Train loss: 0.27752852  - Train acc: 0.8942 - Val loss: 1.01162231\n",
      "(12.57 min) Epoch 63/300 -- Iteration 59904 - Batch 198/963 - Train loss: 0.27713870  - Train acc: 0.8944 - Val loss: 1.01162231\n",
      "(12.57 min) Epoch 63/300 -- Iteration 59913 - Batch 207/963 - Train loss: 0.27904198  - Train acc: 0.8937 - Val loss: 1.01162231\n",
      "(12.57 min) Epoch 63/300 -- Iteration 59922 - Batch 216/963 - Train loss: 0.27922190  - Train acc: 0.8939 - Val loss: 1.01162231\n",
      "(12.57 min) Epoch 63/300 -- Iteration 59931 - Batch 225/963 - Train loss: 0.28001056  - Train acc: 0.8937 - Val loss: 1.01162231\n",
      "(12.58 min) Epoch 63/300 -- Iteration 59940 - Batch 234/963 - Train loss: 0.27865753  - Train acc: 0.8941 - Val loss: 1.01162231\n",
      "(12.58 min) Epoch 63/300 -- Iteration 59949 - Batch 243/963 - Train loss: 0.27724470  - Train acc: 0.8950 - Val loss: 1.01162231\n",
      "(12.58 min) Epoch 63/300 -- Iteration 59958 - Batch 252/963 - Train loss: 0.27774718  - Train acc: 0.8949 - Val loss: 1.01162231\n",
      "(12.58 min) Epoch 63/300 -- Iteration 59967 - Batch 261/963 - Train loss: 0.27741430  - Train acc: 0.8947 - Val loss: 1.01162231\n",
      "(12.58 min) Epoch 63/300 -- Iteration 59976 - Batch 270/963 - Train loss: 0.27681231  - Train acc: 0.8949 - Val loss: 1.01162231\n",
      "(12.58 min) Epoch 63/300 -- Iteration 59985 - Batch 279/963 - Train loss: 0.27638861  - Train acc: 0.8952 - Val loss: 1.01162231\n",
      "(12.59 min) Epoch 63/300 -- Iteration 59994 - Batch 288/963 - Train loss: 0.27628114  - Train acc: 0.8952 - Val loss: 1.01162231\n",
      "(12.59 min) Epoch 63/300 -- Iteration 60003 - Batch 297/963 - Train loss: 0.27638932  - Train acc: 0.8951 - Val loss: 1.01162231\n",
      "(12.59 min) Epoch 63/300 -- Iteration 60012 - Batch 306/963 - Train loss: 0.27725525  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.59 min) Epoch 63/300 -- Iteration 60021 - Batch 315/963 - Train loss: 0.27692190  - Train acc: 0.8952 - Val loss: 1.01162231\n",
      "(12.59 min) Epoch 63/300 -- Iteration 60030 - Batch 324/963 - Train loss: 0.27750838  - Train acc: 0.8951 - Val loss: 1.01162231\n",
      "(12.60 min) Epoch 63/300 -- Iteration 60039 - Batch 333/963 - Train loss: 0.27764070  - Train acc: 0.8950 - Val loss: 1.01162231\n",
      "(12.60 min) Epoch 63/300 -- Iteration 60048 - Batch 342/963 - Train loss: 0.27723088  - Train acc: 0.8953 - Val loss: 1.01162231\n",
      "(12.60 min) Epoch 63/300 -- Iteration 60057 - Batch 351/963 - Train loss: 0.27753115  - Train acc: 0.8952 - Val loss: 1.01162231\n",
      "(12.60 min) Epoch 63/300 -- Iteration 60066 - Batch 360/963 - Train loss: 0.27732514  - Train acc: 0.8953 - Val loss: 1.01162231\n",
      "(12.60 min) Epoch 63/300 -- Iteration 60075 - Batch 369/963 - Train loss: 0.27817982  - Train acc: 0.8949 - Val loss: 1.01162231\n",
      "(12.61 min) Epoch 63/300 -- Iteration 60084 - Batch 378/963 - Train loss: 0.27841833  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.61 min) Epoch 63/300 -- Iteration 60093 - Batch 387/963 - Train loss: 0.27939756  - Train acc: 0.8945 - Val loss: 1.01162231\n",
      "(12.61 min) Epoch 63/300 -- Iteration 60102 - Batch 396/963 - Train loss: 0.27958731  - Train acc: 0.8943 - Val loss: 1.01162231\n",
      "(12.61 min) Epoch 63/300 -- Iteration 60111 - Batch 405/963 - Train loss: 0.27918548  - Train acc: 0.8944 - Val loss: 1.01162231\n",
      "(12.61 min) Epoch 63/300 -- Iteration 60120 - Batch 414/963 - Train loss: 0.27909251  - Train acc: 0.8944 - Val loss: 1.01162231\n",
      "(12.61 min) Epoch 63/300 -- Iteration 60129 - Batch 423/963 - Train loss: 0.27851997  - Train acc: 0.8946 - Val loss: 1.01162231\n",
      "(12.62 min) Epoch 63/300 -- Iteration 60138 - Batch 432/963 - Train loss: 0.27893549  - Train acc: 0.8943 - Val loss: 1.01162231\n",
      "(12.62 min) Epoch 63/300 -- Iteration 60147 - Batch 441/963 - Train loss: 0.27865023  - Train acc: 0.8944 - Val loss: 1.01162231\n",
      "(12.62 min) Epoch 63/300 -- Iteration 60156 - Batch 450/963 - Train loss: 0.27853470  - Train acc: 0.8943 - Val loss: 1.01162231\n",
      "(12.62 min) Epoch 63/300 -- Iteration 60165 - Batch 459/963 - Train loss: 0.27906975  - Train acc: 0.8941 - Val loss: 1.01162231\n",
      "(12.62 min) Epoch 63/300 -- Iteration 60174 - Batch 468/963 - Train loss: 0.27905209  - Train acc: 0.8940 - Val loss: 1.01162231\n",
      "(12.63 min) Epoch 63/300 -- Iteration 60183 - Batch 477/963 - Train loss: 0.27806287  - Train acc: 0.8942 - Val loss: 1.01162231\n",
      "(12.63 min) Epoch 63/300 -- Iteration 60192 - Batch 486/963 - Train loss: 0.27784690  - Train acc: 0.8944 - Val loss: 1.01162231\n",
      "(12.63 min) Epoch 63/300 -- Iteration 60201 - Batch 495/963 - Train loss: 0.27843803  - Train acc: 0.8942 - Val loss: 1.01162231\n",
      "(12.63 min) Epoch 63/300 -- Iteration 60210 - Batch 504/963 - Train loss: 0.27819972  - Train acc: 0.8943 - Val loss: 1.01162231\n",
      "(12.63 min) Epoch 63/300 -- Iteration 60219 - Batch 513/963 - Train loss: 0.27793874  - Train acc: 0.8944 - Val loss: 1.01162231\n",
      "(12.64 min) Epoch 63/300 -- Iteration 60228 - Batch 522/963 - Train loss: 0.27777170  - Train acc: 0.8944 - Val loss: 1.01162231\n",
      "(12.64 min) Epoch 63/300 -- Iteration 60237 - Batch 531/963 - Train loss: 0.27797636  - Train acc: 0.8943 - Val loss: 1.01162231\n",
      "(12.64 min) Epoch 63/300 -- Iteration 60246 - Batch 540/963 - Train loss: 0.27719090  - Train acc: 0.8946 - Val loss: 1.01162231\n",
      "(12.64 min) Epoch 63/300 -- Iteration 60255 - Batch 549/963 - Train loss: 0.27693010  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.64 min) Epoch 63/300 -- Iteration 60264 - Batch 558/963 - Train loss: 0.27673157  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.64 min) Epoch 63/300 -- Iteration 60273 - Batch 567/963 - Train loss: 0.27675142  - Train acc: 0.8947 - Val loss: 1.01162231\n",
      "(12.65 min) Epoch 63/300 -- Iteration 60282 - Batch 576/963 - Train loss: 0.27645209  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.65 min) Epoch 63/300 -- Iteration 60291 - Batch 585/963 - Train loss: 0.27654735  - Train acc: 0.8949 - Val loss: 1.01162231\n",
      "(12.65 min) Epoch 63/300 -- Iteration 60300 - Batch 594/963 - Train loss: 0.27644154  - Train acc: 0.8950 - Val loss: 1.01162231\n",
      "(12.65 min) Epoch 63/300 -- Iteration 60309 - Batch 603/963 - Train loss: 0.27661247  - Train acc: 0.8949 - Val loss: 1.01162231\n",
      "(12.65 min) Epoch 63/300 -- Iteration 60318 - Batch 612/963 - Train loss: 0.27693383  - Train acc: 0.8947 - Val loss: 1.01162231\n",
      "(12.66 min) Epoch 63/300 -- Iteration 60327 - Batch 621/963 - Train loss: 0.27687952  - Train acc: 0.8947 - Val loss: 1.01162231\n",
      "(12.66 min) Epoch 63/300 -- Iteration 60336 - Batch 630/963 - Train loss: 0.27670263  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.66 min) Epoch 63/300 -- Iteration 60345 - Batch 639/963 - Train loss: 0.27659924  - Train acc: 0.8947 - Val loss: 1.01162231\n",
      "(12.66 min) Epoch 63/300 -- Iteration 60354 - Batch 648/963 - Train loss: 0.27679988  - Train acc: 0.8947 - Val loss: 1.01162231\n",
      "(12.66 min) Epoch 63/300 -- Iteration 60363 - Batch 657/963 - Train loss: 0.27648524  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.67 min) Epoch 63/300 -- Iteration 60372 - Batch 666/963 - Train loss: 0.27656445  - Train acc: 0.8947 - Val loss: 1.01162231\n",
      "(12.67 min) Epoch 63/300 -- Iteration 60381 - Batch 675/963 - Train loss: 0.27646968  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.67 min) Epoch 63/300 -- Iteration 60390 - Batch 684/963 - Train loss: 0.27640079  - Train acc: 0.8947 - Val loss: 1.01162231\n",
      "(12.67 min) Epoch 63/300 -- Iteration 60399 - Batch 693/963 - Train loss: 0.27647593  - Train acc: 0.8947 - Val loss: 1.01162231\n",
      "(12.67 min) Epoch 63/300 -- Iteration 60408 - Batch 702/963 - Train loss: 0.27633907  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.67 min) Epoch 63/300 -- Iteration 60417 - Batch 711/963 - Train loss: 0.27638255  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.68 min) Epoch 63/300 -- Iteration 60426 - Batch 720/963 - Train loss: 0.27648148  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.68 min) Epoch 63/300 -- Iteration 60435 - Batch 729/963 - Train loss: 0.27641918  - Train acc: 0.8949 - Val loss: 1.01162231\n",
      "(12.68 min) Epoch 63/300 -- Iteration 60444 - Batch 738/963 - Train loss: 0.27688373  - Train acc: 0.8948 - Val loss: 1.01162231\n",
      "(12.68 min) Epoch 63/300 -- Iteration 60453 - Batch 747/963 - Train loss: 0.27668550  - Train acc: 0.8950 - Val loss: 1.01162231\n",
      "(12.68 min) Epoch 63/300 -- Iteration 60462 - Batch 756/963 - Train loss: 0.27680341  - Train acc: 0.8949 - Val loss: 1.01162231\n",
      "(12.69 min) Epoch 63/300 -- Iteration 60471 - Batch 765/963 - Train loss: 0.27658025  - Train acc: 0.8952 - Val loss: 1.01162231\n",
      "(12.69 min) Epoch 63/300 -- Iteration 60480 - Batch 774/963 - Train loss: 0.27631963  - Train acc: 0.8953 - Val loss: 1.01162231\n",
      "(12.69 min) Epoch 63/300 -- Iteration 60489 - Batch 783/963 - Train loss: 0.27623670  - Train acc: 0.8953 - Val loss: 1.01162231\n",
      "(12.69 min) Epoch 63/300 -- Iteration 60498 - Batch 792/963 - Train loss: 0.27613976  - Train acc: 0.8954 - Val loss: 1.01162231\n",
      "(12.69 min) Epoch 63/300 -- Iteration 60507 - Batch 801/963 - Train loss: 0.27597939  - Train acc: 0.8954 - Val loss: 1.01162231\n",
      "(12.70 min) Epoch 63/300 -- Iteration 60516 - Batch 810/963 - Train loss: 0.27609259  - Train acc: 0.8954 - Val loss: 1.01162231\n",
      "(12.70 min) Epoch 63/300 -- Iteration 60525 - Batch 819/963 - Train loss: 0.27645620  - Train acc: 0.8951 - Val loss: 1.01162231\n",
      "(12.70 min) Epoch 63/300 -- Iteration 60534 - Batch 828/963 - Train loss: 0.27660533  - Train acc: 0.8951 - Val loss: 1.01162231\n",
      "(12.70 min) Epoch 63/300 -- Iteration 60543 - Batch 837/963 - Train loss: 0.27677166  - Train acc: 0.8950 - Val loss: 1.01162231\n",
      "(12.70 min) Epoch 63/300 -- Iteration 60552 - Batch 846/963 - Train loss: 0.27674873  - Train acc: 0.8950 - Val loss: 1.01162231\n",
      "(12.70 min) Epoch 63/300 -- Iteration 60561 - Batch 855/963 - Train loss: 0.27719310  - Train acc: 0.8949 - Val loss: 1.01162231\n",
      "(12.71 min) Epoch 63/300 -- Iteration 60570 - Batch 864/963 - Train loss: 0.27719808  - Train acc: 0.8949 - Val loss: 1.01162231\n",
      "(12.71 min) Epoch 63/300 -- Iteration 60579 - Batch 873/963 - Train loss: 0.27672206  - Train acc: 0.8952 - Val loss: 1.01162231\n",
      "(12.71 min) Epoch 63/300 -- Iteration 60588 - Batch 882/963 - Train loss: 0.27647730  - Train acc: 0.8953 - Val loss: 1.01162231\n",
      "(12.71 min) Epoch 63/300 -- Iteration 60597 - Batch 891/963 - Train loss: 0.27643624  - Train acc: 0.8952 - Val loss: 1.01162231\n",
      "(12.71 min) Epoch 63/300 -- Iteration 60606 - Batch 900/963 - Train loss: 0.27682614  - Train acc: 0.8951 - Val loss: 1.01162231\n",
      "(12.72 min) Epoch 63/300 -- Iteration 60615 - Batch 909/963 - Train loss: 0.27698825  - Train acc: 0.8951 - Val loss: 1.01162231\n",
      "(12.72 min) Epoch 63/300 -- Iteration 60624 - Batch 918/963 - Train loss: 0.27702777  - Train acc: 0.8951 - Val loss: 1.01162231\n",
      "(12.72 min) Epoch 63/300 -- Iteration 60633 - Batch 927/963 - Train loss: 0.27721274  - Train acc: 0.8950 - Val loss: 1.01162231\n",
      "(12.72 min) Epoch 63/300 -- Iteration 60642 - Batch 936/963 - Train loss: 0.27731104  - Train acc: 0.8950 - Val loss: 1.01162231\n",
      "(12.72 min) Epoch 63/300 -- Iteration 60651 - Batch 945/963 - Train loss: 0.27704537  - Train acc: 0.8951 - Val loss: 1.01162231\n",
      "(12.73 min) Epoch 63/300 -- Iteration 60660 - Batch 954/963 - Train loss: 0.27718059  - Train acc: 0.8950 - Val loss: 1.01162231\n",
      "(12.73 min) Epoch 63/300 -- Iteration 60669 - Batch 962/963 - Train loss: 0.27707313  - Train acc: 0.8950 - Val loss: 1.00730550 - Val acc: 0.5917\n",
      "(12.73 min) Epoch 64/300 -- Iteration 60678 - Batch 9/963 - Train loss: 0.27022287  - Train acc: 0.9055 - Val loss: 1.00730550\n",
      "(12.73 min) Epoch 64/300 -- Iteration 60687 - Batch 18/963 - Train loss: 0.27823878  - Train acc: 0.8993 - Val loss: 1.00730550\n",
      "(12.73 min) Epoch 64/300 -- Iteration 60696 - Batch 27/963 - Train loss: 0.28451252  - Train acc: 0.8954 - Val loss: 1.00730550\n",
      "(12.74 min) Epoch 64/300 -- Iteration 60705 - Batch 36/963 - Train loss: 0.28649970  - Train acc: 0.8919 - Val loss: 1.00730550\n",
      "(12.74 min) Epoch 64/300 -- Iteration 60714 - Batch 45/963 - Train loss: 0.27948137  - Train acc: 0.8942 - Val loss: 1.00730550\n",
      "(12.74 min) Epoch 64/300 -- Iteration 60723 - Batch 54/963 - Train loss: 0.28323027  - Train acc: 0.8939 - Val loss: 1.00730550\n",
      "(12.74 min) Epoch 64/300 -- Iteration 60732 - Batch 63/963 - Train loss: 0.28325900  - Train acc: 0.8939 - Val loss: 1.00730550\n",
      "(12.74 min) Epoch 64/300 -- Iteration 60741 - Batch 72/963 - Train loss: 0.28310585  - Train acc: 0.8936 - Val loss: 1.00730550\n",
      "(12.74 min) Epoch 64/300 -- Iteration 60750 - Batch 81/963 - Train loss: 0.28334500  - Train acc: 0.8937 - Val loss: 1.00730550\n",
      "(12.75 min) Epoch 64/300 -- Iteration 60759 - Batch 90/963 - Train loss: 0.28219526  - Train acc: 0.8935 - Val loss: 1.00730550\n",
      "(12.75 min) Epoch 64/300 -- Iteration 60768 - Batch 99/963 - Train loss: 0.28353061  - Train acc: 0.8930 - Val loss: 1.00730550\n",
      "(12.75 min) Epoch 64/300 -- Iteration 60777 - Batch 108/963 - Train loss: 0.28165554  - Train acc: 0.8940 - Val loss: 1.00730550\n",
      "(12.75 min) Epoch 64/300 -- Iteration 60786 - Batch 117/963 - Train loss: 0.28051283  - Train acc: 0.8942 - Val loss: 1.00730550\n",
      "(12.75 min) Epoch 64/300 -- Iteration 60795 - Batch 126/963 - Train loss: 0.27947452  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.76 min) Epoch 64/300 -- Iteration 60804 - Batch 135/963 - Train loss: 0.27838587  - Train acc: 0.8951 - Val loss: 1.00730550\n",
      "(12.76 min) Epoch 64/300 -- Iteration 60813 - Batch 144/963 - Train loss: 0.27746675  - Train acc: 0.8953 - Val loss: 1.00730550\n",
      "(12.76 min) Epoch 64/300 -- Iteration 60822 - Batch 153/963 - Train loss: 0.27800697  - Train acc: 0.8954 - Val loss: 1.00730550\n",
      "(12.76 min) Epoch 64/300 -- Iteration 60831 - Batch 162/963 - Train loss: 0.27793482  - Train acc: 0.8951 - Val loss: 1.00730550\n",
      "(12.76 min) Epoch 64/300 -- Iteration 60840 - Batch 171/963 - Train loss: 0.27873899  - Train acc: 0.8947 - Val loss: 1.00730550\n",
      "(12.77 min) Epoch 64/300 -- Iteration 60849 - Batch 180/963 - Train loss: 0.27972594  - Train acc: 0.8940 - Val loss: 1.00730550\n",
      "(12.77 min) Epoch 64/300 -- Iteration 60858 - Batch 189/963 - Train loss: 0.27821147  - Train acc: 0.8949 - Val loss: 1.00730550\n",
      "(12.77 min) Epoch 64/300 -- Iteration 60867 - Batch 198/963 - Train loss: 0.27842256  - Train acc: 0.8947 - Val loss: 1.00730550\n",
      "(12.77 min) Epoch 64/300 -- Iteration 60876 - Batch 207/963 - Train loss: 0.27859575  - Train acc: 0.8945 - Val loss: 1.00730550\n",
      "(12.77 min) Epoch 64/300 -- Iteration 60885 - Batch 216/963 - Train loss: 0.27900844  - Train acc: 0.8944 - Val loss: 1.00730550\n",
      "(12.78 min) Epoch 64/300 -- Iteration 60894 - Batch 225/963 - Train loss: 0.27761457  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.78 min) Epoch 64/300 -- Iteration 60903 - Batch 234/963 - Train loss: 0.27825889  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.78 min) Epoch 64/300 -- Iteration 60912 - Batch 243/963 - Train loss: 0.27801892  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.78 min) Epoch 64/300 -- Iteration 60921 - Batch 252/963 - Train loss: 0.27775779  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.78 min) Epoch 64/300 -- Iteration 60930 - Batch 261/963 - Train loss: 0.27798199  - Train acc: 0.8943 - Val loss: 1.00730550\n",
      "(12.78 min) Epoch 64/300 -- Iteration 60939 - Batch 270/963 - Train loss: 0.27692861  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.79 min) Epoch 64/300 -- Iteration 60948 - Batch 279/963 - Train loss: 0.27662412  - Train acc: 0.8947 - Val loss: 1.00730550\n",
      "(12.79 min) Epoch 64/300 -- Iteration 60957 - Batch 288/963 - Train loss: 0.27671305  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.79 min) Epoch 64/300 -- Iteration 60966 - Batch 297/963 - Train loss: 0.27640159  - Train acc: 0.8949 - Val loss: 1.00730550\n",
      "(12.79 min) Epoch 64/300 -- Iteration 60975 - Batch 306/963 - Train loss: 0.27675809  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.79 min) Epoch 64/300 -- Iteration 60984 - Batch 315/963 - Train loss: 0.27585579  - Train acc: 0.8951 - Val loss: 1.00730550\n",
      "(12.80 min) Epoch 64/300 -- Iteration 60993 - Batch 324/963 - Train loss: 0.27583711  - Train acc: 0.8955 - Val loss: 1.00730550\n",
      "(12.80 min) Epoch 64/300 -- Iteration 61002 - Batch 333/963 - Train loss: 0.27682691  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.80 min) Epoch 64/300 -- Iteration 61011 - Batch 342/963 - Train loss: 0.27627733  - Train acc: 0.8952 - Val loss: 1.00730550\n",
      "(12.80 min) Epoch 64/300 -- Iteration 61020 - Batch 351/963 - Train loss: 0.27557273  - Train acc: 0.8957 - Val loss: 1.00730550\n",
      "(12.80 min) Epoch 64/300 -- Iteration 61029 - Batch 360/963 - Train loss: 0.27544362  - Train acc: 0.8958 - Val loss: 1.00730550\n",
      "(12.81 min) Epoch 64/300 -- Iteration 61038 - Batch 369/963 - Train loss: 0.27657785  - Train acc: 0.8953 - Val loss: 1.00730550\n",
      "(12.81 min) Epoch 64/300 -- Iteration 61047 - Batch 378/963 - Train loss: 0.27657786  - Train acc: 0.8955 - Val loss: 1.00730550\n",
      "(12.81 min) Epoch 64/300 -- Iteration 61056 - Batch 387/963 - Train loss: 0.27645955  - Train acc: 0.8956 - Val loss: 1.00730550\n",
      "(12.81 min) Epoch 64/300 -- Iteration 61065 - Batch 396/963 - Train loss: 0.27620368  - Train acc: 0.8957 - Val loss: 1.00730550\n",
      "(12.81 min) Epoch 64/300 -- Iteration 61074 - Batch 405/963 - Train loss: 0.27630678  - Train acc: 0.8956 - Val loss: 1.00730550\n",
      "(12.81 min) Epoch 64/300 -- Iteration 61083 - Batch 414/963 - Train loss: 0.27533295  - Train acc: 0.8961 - Val loss: 1.00730550\n",
      "(12.82 min) Epoch 64/300 -- Iteration 61092 - Batch 423/963 - Train loss: 0.27533002  - Train acc: 0.8962 - Val loss: 1.00730550\n",
      "(12.82 min) Epoch 64/300 -- Iteration 61101 - Batch 432/963 - Train loss: 0.27515255  - Train acc: 0.8963 - Val loss: 1.00730550\n",
      "(12.82 min) Epoch 64/300 -- Iteration 61110 - Batch 441/963 - Train loss: 0.27449790  - Train acc: 0.8964 - Val loss: 1.00730550\n",
      "(12.82 min) Epoch 64/300 -- Iteration 61119 - Batch 450/963 - Train loss: 0.27417709  - Train acc: 0.8966 - Val loss: 1.00730550\n",
      "(12.82 min) Epoch 64/300 -- Iteration 61128 - Batch 459/963 - Train loss: 0.27417558  - Train acc: 0.8966 - Val loss: 1.00730550\n",
      "(12.83 min) Epoch 64/300 -- Iteration 61137 - Batch 468/963 - Train loss: 0.27474272  - Train acc: 0.8964 - Val loss: 1.00730550\n",
      "(12.83 min) Epoch 64/300 -- Iteration 61146 - Batch 477/963 - Train loss: 0.27456273  - Train acc: 0.8966 - Val loss: 1.00730550\n",
      "(12.83 min) Epoch 64/300 -- Iteration 61155 - Batch 486/963 - Train loss: 0.27483304  - Train acc: 0.8965 - Val loss: 1.00730550\n",
      "(12.83 min) Epoch 64/300 -- Iteration 61164 - Batch 495/963 - Train loss: 0.27532504  - Train acc: 0.8962 - Val loss: 1.00730550\n",
      "(12.83 min) Epoch 64/300 -- Iteration 61173 - Batch 504/963 - Train loss: 0.27498423  - Train acc: 0.8964 - Val loss: 1.00730550\n",
      "(12.84 min) Epoch 64/300 -- Iteration 61182 - Batch 513/963 - Train loss: 0.27512852  - Train acc: 0.8962 - Val loss: 1.00730550\n",
      "(12.84 min) Epoch 64/300 -- Iteration 61191 - Batch 522/963 - Train loss: 0.27538928  - Train acc: 0.8960 - Val loss: 1.00730550\n",
      "(12.84 min) Epoch 64/300 -- Iteration 61200 - Batch 531/963 - Train loss: 0.27554006  - Train acc: 0.8959 - Val loss: 1.00730550\n",
      "(12.84 min) Epoch 64/300 -- Iteration 61209 - Batch 540/963 - Train loss: 0.27572923  - Train acc: 0.8956 - Val loss: 1.00730550\n",
      "(12.84 min) Epoch 64/300 -- Iteration 61218 - Batch 549/963 - Train loss: 0.27620320  - Train acc: 0.8954 - Val loss: 1.00730550\n",
      "(12.84 min) Epoch 64/300 -- Iteration 61227 - Batch 558/963 - Train loss: 0.27593157  - Train acc: 0.8954 - Val loss: 1.00730550\n",
      "(12.85 min) Epoch 64/300 -- Iteration 61236 - Batch 567/963 - Train loss: 0.27604414  - Train acc: 0.8952 - Val loss: 1.00730550\n",
      "(12.85 min) Epoch 64/300 -- Iteration 61245 - Batch 576/963 - Train loss: 0.27636322  - Train acc: 0.8951 - Val loss: 1.00730550\n",
      "(12.85 min) Epoch 64/300 -- Iteration 61254 - Batch 585/963 - Train loss: 0.27647105  - Train acc: 0.8951 - Val loss: 1.00730550\n",
      "(12.85 min) Epoch 64/300 -- Iteration 61263 - Batch 594/963 - Train loss: 0.27606528  - Train acc: 0.8952 - Val loss: 1.00730550\n",
      "(12.85 min) Epoch 64/300 -- Iteration 61272 - Batch 603/963 - Train loss: 0.27614665  - Train acc: 0.8951 - Val loss: 1.00730550\n",
      "(12.86 min) Epoch 64/300 -- Iteration 61281 - Batch 612/963 - Train loss: 0.27600250  - Train acc: 0.8951 - Val loss: 1.00730550\n",
      "(12.86 min) Epoch 64/300 -- Iteration 61290 - Batch 621/963 - Train loss: 0.27629082  - Train acc: 0.8950 - Val loss: 1.00730550\n",
      "(12.86 min) Epoch 64/300 -- Iteration 61299 - Batch 630/963 - Train loss: 0.27635162  - Train acc: 0.8949 - Val loss: 1.00730550\n",
      "(12.86 min) Epoch 64/300 -- Iteration 61308 - Batch 639/963 - Train loss: 0.27604207  - Train acc: 0.8949 - Val loss: 1.00730550\n",
      "(12.86 min) Epoch 64/300 -- Iteration 61317 - Batch 648/963 - Train loss: 0.27611631  - Train acc: 0.8949 - Val loss: 1.00730550\n",
      "(12.87 min) Epoch 64/300 -- Iteration 61326 - Batch 657/963 - Train loss: 0.27593921  - Train acc: 0.8950 - Val loss: 1.00730550\n",
      "(12.87 min) Epoch 64/300 -- Iteration 61335 - Batch 666/963 - Train loss: 0.27594831  - Train acc: 0.8952 - Val loss: 1.00730550\n",
      "(12.87 min) Epoch 64/300 -- Iteration 61344 - Batch 675/963 - Train loss: 0.27608779  - Train acc: 0.8951 - Val loss: 1.00730550\n",
      "(12.87 min) Epoch 64/300 -- Iteration 61353 - Batch 684/963 - Train loss: 0.27666724  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.87 min) Epoch 64/300 -- Iteration 61362 - Batch 693/963 - Train loss: 0.27638590  - Train acc: 0.8950 - Val loss: 1.00730550\n",
      "(12.87 min) Epoch 64/300 -- Iteration 61371 - Batch 702/963 - Train loss: 0.27641264  - Train acc: 0.8951 - Val loss: 1.00730550\n",
      "(12.88 min) Epoch 64/300 -- Iteration 61380 - Batch 711/963 - Train loss: 0.27660630  - Train acc: 0.8950 - Val loss: 1.00730550\n",
      "(12.88 min) Epoch 64/300 -- Iteration 61389 - Batch 720/963 - Train loss: 0.27699654  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.88 min) Epoch 64/300 -- Iteration 61398 - Batch 729/963 - Train loss: 0.27712356  - Train acc: 0.8947 - Val loss: 1.00730550\n",
      "(12.88 min) Epoch 64/300 -- Iteration 61407 - Batch 738/963 - Train loss: 0.27703791  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.88 min) Epoch 64/300 -- Iteration 61416 - Batch 747/963 - Train loss: 0.27732734  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.89 min) Epoch 64/300 -- Iteration 61425 - Batch 756/963 - Train loss: 0.27726028  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.89 min) Epoch 64/300 -- Iteration 61434 - Batch 765/963 - Train loss: 0.27731070  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.89 min) Epoch 64/300 -- Iteration 61443 - Batch 774/963 - Train loss: 0.27722764  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.89 min) Epoch 64/300 -- Iteration 61452 - Batch 783/963 - Train loss: 0.27713727  - Train acc: 0.8949 - Val loss: 1.00730550\n",
      "(12.89 min) Epoch 64/300 -- Iteration 61461 - Batch 792/963 - Train loss: 0.27731665  - Train acc: 0.8949 - Val loss: 1.00730550\n",
      "(12.90 min) Epoch 64/300 -- Iteration 61470 - Batch 801/963 - Train loss: 0.27711380  - Train acc: 0.8950 - Val loss: 1.00730550\n",
      "(12.90 min) Epoch 64/300 -- Iteration 61479 - Batch 810/963 - Train loss: 0.27744980  - Train acc: 0.8949 - Val loss: 1.00730550\n",
      "(12.90 min) Epoch 64/300 -- Iteration 61488 - Batch 819/963 - Train loss: 0.27765532  - Train acc: 0.8947 - Val loss: 1.00730550\n",
      "(12.90 min) Epoch 64/300 -- Iteration 61497 - Batch 828/963 - Train loss: 0.27779031  - Train acc: 0.8947 - Val loss: 1.00730550\n",
      "(12.90 min) Epoch 64/300 -- Iteration 61506 - Batch 837/963 - Train loss: 0.27815520  - Train acc: 0.8945 - Val loss: 1.00730550\n",
      "(12.90 min) Epoch 64/300 -- Iteration 61515 - Batch 846/963 - Train loss: 0.27778394  - Train acc: 0.8947 - Val loss: 1.00730550\n",
      "(12.91 min) Epoch 64/300 -- Iteration 61524 - Batch 855/963 - Train loss: 0.27806229  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.91 min) Epoch 64/300 -- Iteration 61533 - Batch 864/963 - Train loss: 0.27812033  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.91 min) Epoch 64/300 -- Iteration 61542 - Batch 873/963 - Train loss: 0.27791922  - Train acc: 0.8947 - Val loss: 1.00730550\n",
      "(12.91 min) Epoch 64/300 -- Iteration 61551 - Batch 882/963 - Train loss: 0.27799324  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.91 min) Epoch 64/300 -- Iteration 61560 - Batch 891/963 - Train loss: 0.27826118  - Train acc: 0.8945 - Val loss: 1.00730550\n",
      "(12.92 min) Epoch 64/300 -- Iteration 61569 - Batch 900/963 - Train loss: 0.27815154  - Train acc: 0.8945 - Val loss: 1.00730550\n",
      "(12.92 min) Epoch 64/300 -- Iteration 61578 - Batch 909/963 - Train loss: 0.27809630  - Train acc: 0.8944 - Val loss: 1.00730550\n",
      "(12.92 min) Epoch 64/300 -- Iteration 61587 - Batch 918/963 - Train loss: 0.27789627  - Train acc: 0.8945 - Val loss: 1.00730550\n",
      "(12.92 min) Epoch 64/300 -- Iteration 61596 - Batch 927/963 - Train loss: 0.27772054  - Train acc: 0.8946 - Val loss: 1.00730550\n",
      "(12.92 min) Epoch 64/300 -- Iteration 61605 - Batch 936/963 - Train loss: 0.27787643  - Train acc: 0.8945 - Val loss: 1.00730550\n",
      "(12.93 min) Epoch 64/300 -- Iteration 61614 - Batch 945/963 - Train loss: 0.27765574  - Train acc: 0.8947 - Val loss: 1.00730550\n",
      "(12.93 min) Epoch 64/300 -- Iteration 61623 - Batch 954/963 - Train loss: 0.27735160  - Train acc: 0.8948 - Val loss: 1.00730550\n",
      "(12.93 min) Epoch 64/300 -- Iteration 61632 - Batch 962/963 - Train loss: 0.27732474  - Train acc: 0.8948 - Val loss: 1.00001204 - Val acc: 0.5933\n",
      "(12.93 min) Epoch 65/300 -- Iteration 61641 - Batch 9/963 - Train loss: 0.27756820  - Train acc: 0.8898 - Val loss: 1.00001204\n",
      "(12.93 min) Epoch 65/300 -- Iteration 61650 - Batch 18/963 - Train loss: 0.26522134  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(12.94 min) Epoch 65/300 -- Iteration 61659 - Batch 27/963 - Train loss: 0.27785421  - Train acc: 0.8890 - Val loss: 1.00001204\n",
      "(12.94 min) Epoch 65/300 -- Iteration 61668 - Batch 36/963 - Train loss: 0.28221769  - Train acc: 0.8883 - Val loss: 1.00001204\n",
      "(12.94 min) Epoch 65/300 -- Iteration 61677 - Batch 45/963 - Train loss: 0.27991304  - Train acc: 0.8906 - Val loss: 1.00001204\n",
      "(12.94 min) Epoch 65/300 -- Iteration 61686 - Batch 54/963 - Train loss: 0.27967829  - Train acc: 0.8912 - Val loss: 1.00001204\n",
      "(12.94 min) Epoch 65/300 -- Iteration 61695 - Batch 63/963 - Train loss: 0.27768507  - Train acc: 0.8931 - Val loss: 1.00001204\n",
      "(12.94 min) Epoch 65/300 -- Iteration 61704 - Batch 72/963 - Train loss: 0.27509821  - Train acc: 0.8945 - Val loss: 1.00001204\n",
      "(12.95 min) Epoch 65/300 -- Iteration 61713 - Batch 81/963 - Train loss: 0.27474059  - Train acc: 0.8946 - Val loss: 1.00001204\n",
      "(12.95 min) Epoch 65/300 -- Iteration 61722 - Batch 90/963 - Train loss: 0.27425598  - Train acc: 0.8956 - Val loss: 1.00001204\n",
      "(12.95 min) Epoch 65/300 -- Iteration 61731 - Batch 99/963 - Train loss: 0.27421348  - Train acc: 0.8958 - Val loss: 1.00001204\n",
      "(12.95 min) Epoch 65/300 -- Iteration 61740 - Batch 108/963 - Train loss: 0.27657230  - Train acc: 0.8951 - Val loss: 1.00001204\n",
      "(12.95 min) Epoch 65/300 -- Iteration 61749 - Batch 117/963 - Train loss: 0.27664802  - Train acc: 0.8953 - Val loss: 1.00001204\n",
      "(12.96 min) Epoch 65/300 -- Iteration 61758 - Batch 126/963 - Train loss: 0.27765607  - Train acc: 0.8948 - Val loss: 1.00001204\n",
      "(12.96 min) Epoch 65/300 -- Iteration 61767 - Batch 135/963 - Train loss: 0.27953504  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(12.96 min) Epoch 65/300 -- Iteration 61776 - Batch 144/963 - Train loss: 0.27983644  - Train acc: 0.8938 - Val loss: 1.00001204\n",
      "(12.96 min) Epoch 65/300 -- Iteration 61785 - Batch 153/963 - Train loss: 0.28072130  - Train acc: 0.8934 - Val loss: 1.00001204\n",
      "(12.96 min) Epoch 65/300 -- Iteration 61794 - Batch 162/963 - Train loss: 0.28082454  - Train acc: 0.8932 - Val loss: 1.00001204\n",
      "(12.97 min) Epoch 65/300 -- Iteration 61803 - Batch 171/963 - Train loss: 0.28176703  - Train acc: 0.8927 - Val loss: 1.00001204\n",
      "(12.97 min) Epoch 65/300 -- Iteration 61812 - Batch 180/963 - Train loss: 0.28120030  - Train acc: 0.8930 - Val loss: 1.00001204\n",
      "(12.97 min) Epoch 65/300 -- Iteration 61821 - Batch 189/963 - Train loss: 0.28150247  - Train acc: 0.8929 - Val loss: 1.00001204\n",
      "(12.97 min) Epoch 65/300 -- Iteration 61830 - Batch 198/963 - Train loss: 0.28072530  - Train acc: 0.8932 - Val loss: 1.00001204\n",
      "(12.97 min) Epoch 65/300 -- Iteration 61839 - Batch 207/963 - Train loss: 0.27931100  - Train acc: 0.8938 - Val loss: 1.00001204\n",
      "(12.97 min) Epoch 65/300 -- Iteration 61848 - Batch 216/963 - Train loss: 0.27886979  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(12.98 min) Epoch 65/300 -- Iteration 61857 - Batch 225/963 - Train loss: 0.27857501  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(12.98 min) Epoch 65/300 -- Iteration 61866 - Batch 234/963 - Train loss: 0.27852664  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(12.98 min) Epoch 65/300 -- Iteration 61875 - Batch 243/963 - Train loss: 0.28036157  - Train acc: 0.8931 - Val loss: 1.00001204\n",
      "(12.98 min) Epoch 65/300 -- Iteration 61884 - Batch 252/963 - Train loss: 0.28041587  - Train acc: 0.8928 - Val loss: 1.00001204\n",
      "(12.98 min) Epoch 65/300 -- Iteration 61893 - Batch 261/963 - Train loss: 0.28021069  - Train acc: 0.8929 - Val loss: 1.00001204\n",
      "(12.99 min) Epoch 65/300 -- Iteration 61902 - Batch 270/963 - Train loss: 0.28018569  - Train acc: 0.8927 - Val loss: 1.00001204\n",
      "(12.99 min) Epoch 65/300 -- Iteration 61911 - Batch 279/963 - Train loss: 0.27976173  - Train acc: 0.8927 - Val loss: 1.00001204\n",
      "(12.99 min) Epoch 65/300 -- Iteration 61920 - Batch 288/963 - Train loss: 0.27922573  - Train acc: 0.8930 - Val loss: 1.00001204\n",
      "(12.99 min) Epoch 65/300 -- Iteration 61929 - Batch 297/963 - Train loss: 0.27841419  - Train acc: 0.8931 - Val loss: 1.00001204\n",
      "(12.99 min) Epoch 65/300 -- Iteration 61938 - Batch 306/963 - Train loss: 0.27776219  - Train acc: 0.8936 - Val loss: 1.00001204\n",
      "(13.00 min) Epoch 65/300 -- Iteration 61947 - Batch 315/963 - Train loss: 0.27810020  - Train acc: 0.8933 - Val loss: 1.00001204\n",
      "(13.00 min) Epoch 65/300 -- Iteration 61956 - Batch 324/963 - Train loss: 0.27817452  - Train acc: 0.8933 - Val loss: 1.00001204\n",
      "(13.00 min) Epoch 65/300 -- Iteration 61965 - Batch 333/963 - Train loss: 0.27833565  - Train acc: 0.8931 - Val loss: 1.00001204\n",
      "(13.00 min) Epoch 65/300 -- Iteration 61974 - Batch 342/963 - Train loss: 0.27763185  - Train acc: 0.8933 - Val loss: 1.00001204\n",
      "(13.00 min) Epoch 65/300 -- Iteration 61983 - Batch 351/963 - Train loss: 0.27741470  - Train acc: 0.8935 - Val loss: 1.00001204\n",
      "(13.01 min) Epoch 65/300 -- Iteration 61992 - Batch 360/963 - Train loss: 0.27727506  - Train acc: 0.8938 - Val loss: 1.00001204\n",
      "(13.01 min) Epoch 65/300 -- Iteration 62001 - Batch 369/963 - Train loss: 0.27726734  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.01 min) Epoch 65/300 -- Iteration 62010 - Batch 378/963 - Train loss: 0.27736725  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.01 min) Epoch 65/300 -- Iteration 62019 - Batch 387/963 - Train loss: 0.27634222  - Train acc: 0.8944 - Val loss: 1.00001204\n",
      "(13.01 min) Epoch 65/300 -- Iteration 62028 - Batch 396/963 - Train loss: 0.27605081  - Train acc: 0.8943 - Val loss: 1.00001204\n",
      "(13.01 min) Epoch 65/300 -- Iteration 62037 - Batch 405/963 - Train loss: 0.27665599  - Train acc: 0.8942 - Val loss: 1.00001204\n",
      "(13.02 min) Epoch 65/300 -- Iteration 62046 - Batch 414/963 - Train loss: 0.27704681  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.02 min) Epoch 65/300 -- Iteration 62055 - Batch 423/963 - Train loss: 0.27733853  - Train acc: 0.8938 - Val loss: 1.00001204\n",
      "(13.02 min) Epoch 65/300 -- Iteration 62064 - Batch 432/963 - Train loss: 0.27732970  - Train acc: 0.8937 - Val loss: 1.00001204\n",
      "(13.02 min) Epoch 65/300 -- Iteration 62073 - Batch 441/963 - Train loss: 0.27751722  - Train acc: 0.8937 - Val loss: 1.00001204\n",
      "(13.02 min) Epoch 65/300 -- Iteration 62082 - Batch 450/963 - Train loss: 0.27782554  - Train acc: 0.8935 - Val loss: 1.00001204\n",
      "(13.03 min) Epoch 65/300 -- Iteration 62091 - Batch 459/963 - Train loss: 0.27743291  - Train acc: 0.8936 - Val loss: 1.00001204\n",
      "(13.03 min) Epoch 65/300 -- Iteration 62100 - Batch 468/963 - Train loss: 0.27749732  - Train acc: 0.8936 - Val loss: 1.00001204\n",
      "(13.03 min) Epoch 65/300 -- Iteration 62109 - Batch 477/963 - Train loss: 0.27732556  - Train acc: 0.8936 - Val loss: 1.00001204\n",
      "(13.03 min) Epoch 65/300 -- Iteration 62118 - Batch 486/963 - Train loss: 0.27734721  - Train acc: 0.8936 - Val loss: 1.00001204\n",
      "(13.03 min) Epoch 65/300 -- Iteration 62127 - Batch 495/963 - Train loss: 0.27709875  - Train acc: 0.8937 - Val loss: 1.00001204\n",
      "(13.03 min) Epoch 65/300 -- Iteration 62136 - Batch 504/963 - Train loss: 0.27671401  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.04 min) Epoch 65/300 -- Iteration 62145 - Batch 513/963 - Train loss: 0.27665028  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.04 min) Epoch 65/300 -- Iteration 62154 - Batch 522/963 - Train loss: 0.27686003  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.04 min) Epoch 65/300 -- Iteration 62163 - Batch 531/963 - Train loss: 0.27666791  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.04 min) Epoch 65/300 -- Iteration 62172 - Batch 540/963 - Train loss: 0.27671024  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.04 min) Epoch 65/300 -- Iteration 62181 - Batch 549/963 - Train loss: 0.27667819  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.05 min) Epoch 65/300 -- Iteration 62190 - Batch 558/963 - Train loss: 0.27638058  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.05 min) Epoch 65/300 -- Iteration 62199 - Batch 567/963 - Train loss: 0.27629916  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.05 min) Epoch 65/300 -- Iteration 62208 - Batch 576/963 - Train loss: 0.27647651  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.05 min) Epoch 65/300 -- Iteration 62217 - Batch 585/963 - Train loss: 0.27631596  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.05 min) Epoch 65/300 -- Iteration 62226 - Batch 594/963 - Train loss: 0.27631647  - Train acc: 0.8938 - Val loss: 1.00001204\n",
      "(13.06 min) Epoch 65/300 -- Iteration 62235 - Batch 603/963 - Train loss: 0.27615954  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.06 min) Epoch 65/300 -- Iteration 62244 - Batch 612/963 - Train loss: 0.27672153  - Train acc: 0.8937 - Val loss: 1.00001204\n",
      "(13.06 min) Epoch 65/300 -- Iteration 62253 - Batch 621/963 - Train loss: 0.27654928  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.06 min) Epoch 65/300 -- Iteration 62262 - Batch 630/963 - Train loss: 0.27684630  - Train acc: 0.8937 - Val loss: 1.00001204\n",
      "(13.06 min) Epoch 65/300 -- Iteration 62271 - Batch 639/963 - Train loss: 0.27715743  - Train acc: 0.8936 - Val loss: 1.00001204\n",
      "(13.07 min) Epoch 65/300 -- Iteration 62280 - Batch 648/963 - Train loss: 0.27709158  - Train acc: 0.8937 - Val loss: 1.00001204\n",
      "(13.07 min) Epoch 65/300 -- Iteration 62289 - Batch 657/963 - Train loss: 0.27705676  - Train acc: 0.8937 - Val loss: 1.00001204\n",
      "(13.07 min) Epoch 65/300 -- Iteration 62298 - Batch 666/963 - Train loss: 0.27699462  - Train acc: 0.8938 - Val loss: 1.00001204\n",
      "(13.07 min) Epoch 65/300 -- Iteration 62307 - Batch 675/963 - Train loss: 0.27676008  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.07 min) Epoch 65/300 -- Iteration 62316 - Batch 684/963 - Train loss: 0.27655985  - Train acc: 0.8941 - Val loss: 1.00001204\n",
      "(13.07 min) Epoch 65/300 -- Iteration 62325 - Batch 693/963 - Train loss: 0.27638901  - Train acc: 0.8942 - Val loss: 1.00001204\n",
      "(13.08 min) Epoch 65/300 -- Iteration 62334 - Batch 702/963 - Train loss: 0.27649866  - Train acc: 0.8941 - Val loss: 1.00001204\n",
      "(13.08 min) Epoch 65/300 -- Iteration 62343 - Batch 711/963 - Train loss: 0.27656816  - Train acc: 0.8941 - Val loss: 1.00001204\n",
      "(13.08 min) Epoch 65/300 -- Iteration 62352 - Batch 720/963 - Train loss: 0.27674959  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.08 min) Epoch 65/300 -- Iteration 62361 - Batch 729/963 - Train loss: 0.27662118  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.08 min) Epoch 65/300 -- Iteration 62370 - Batch 738/963 - Train loss: 0.27629943  - Train acc: 0.8942 - Val loss: 1.00001204\n",
      "(13.09 min) Epoch 65/300 -- Iteration 62379 - Batch 747/963 - Train loss: 0.27639604  - Train acc: 0.8941 - Val loss: 1.00001204\n",
      "(13.09 min) Epoch 65/300 -- Iteration 62388 - Batch 756/963 - Train loss: 0.27656027  - Train acc: 0.8941 - Val loss: 1.00001204\n",
      "(13.09 min) Epoch 65/300 -- Iteration 62397 - Batch 765/963 - Train loss: 0.27654901  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.09 min) Epoch 65/300 -- Iteration 62406 - Batch 774/963 - Train loss: 0.27699298  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.09 min) Epoch 65/300 -- Iteration 62415 - Batch 783/963 - Train loss: 0.27733267  - Train acc: 0.8938 - Val loss: 1.00001204\n",
      "(13.10 min) Epoch 65/300 -- Iteration 62424 - Batch 792/963 - Train loss: 0.27751367  - Train acc: 0.8937 - Val loss: 1.00001204\n",
      "(13.10 min) Epoch 65/300 -- Iteration 62433 - Batch 801/963 - Train loss: 0.27747131  - Train acc: 0.8938 - Val loss: 1.00001204\n",
      "(13.10 min) Epoch 65/300 -- Iteration 62442 - Batch 810/963 - Train loss: 0.27735794  - Train acc: 0.8938 - Val loss: 1.00001204\n",
      "(13.10 min) Epoch 65/300 -- Iteration 62451 - Batch 819/963 - Train loss: 0.27716301  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.10 min) Epoch 65/300 -- Iteration 62460 - Batch 828/963 - Train loss: 0.27707119  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.11 min) Epoch 65/300 -- Iteration 62469 - Batch 837/963 - Train loss: 0.27721742  - Train acc: 0.8939 - Val loss: 1.00001204\n",
      "(13.11 min) Epoch 65/300 -- Iteration 62478 - Batch 846/963 - Train loss: 0.27706298  - Train acc: 0.8941 - Val loss: 1.00001204\n",
      "(13.11 min) Epoch 65/300 -- Iteration 62487 - Batch 855/963 - Train loss: 0.27724738  - Train acc: 0.8940 - Val loss: 1.00001204\n",
      "(13.11 min) Epoch 65/300 -- Iteration 62496 - Batch 864/963 - Train loss: 0.27721060  - Train acc: 0.8942 - Val loss: 1.00001204\n",
      "(13.11 min) Epoch 65/300 -- Iteration 62505 - Batch 873/963 - Train loss: 0.27712256  - Train acc: 0.8943 - Val loss: 1.00001204\n",
      "(13.12 min) Epoch 65/300 -- Iteration 62514 - Batch 882/963 - Train loss: 0.27717608  - Train acc: 0.8943 - Val loss: 1.00001204\n",
      "(13.12 min) Epoch 65/300 -- Iteration 62523 - Batch 891/963 - Train loss: 0.27698539  - Train acc: 0.8943 - Val loss: 1.00001204\n",
      "(13.12 min) Epoch 65/300 -- Iteration 62532 - Batch 900/963 - Train loss: 0.27657715  - Train acc: 0.8945 - Val loss: 1.00001204\n",
      "(13.12 min) Epoch 65/300 -- Iteration 62541 - Batch 909/963 - Train loss: 0.27645533  - Train acc: 0.8945 - Val loss: 1.00001204\n",
      "(13.12 min) Epoch 65/300 -- Iteration 62550 - Batch 918/963 - Train loss: 0.27622918  - Train acc: 0.8946 - Val loss: 1.00001204\n",
      "(13.13 min) Epoch 65/300 -- Iteration 62559 - Batch 927/963 - Train loss: 0.27597864  - Train acc: 0.8947 - Val loss: 1.00001204\n",
      "(13.13 min) Epoch 65/300 -- Iteration 62568 - Batch 936/963 - Train loss: 0.27622229  - Train acc: 0.8946 - Val loss: 1.00001204\n",
      "(13.13 min) Epoch 65/300 -- Iteration 62577 - Batch 945/963 - Train loss: 0.27598149  - Train acc: 0.8947 - Val loss: 1.00001204\n",
      "(13.13 min) Epoch 65/300 -- Iteration 62586 - Batch 954/963 - Train loss: 0.27614483  - Train acc: 0.8947 - Val loss: 1.00001204\n",
      "(13.13 min) Epoch 65/300 -- Iteration 62595 - Batch 962/963 - Train loss: 0.27640851  - Train acc: 0.8946 - Val loss: 0.99933392 - Val acc: 0.5917\n",
      "(13.14 min) Epoch 66/300 -- Iteration 62604 - Batch 9/963 - Train loss: 0.27679881  - Train acc: 0.8938 - Val loss: 0.99933392\n",
      "(13.14 min) Epoch 66/300 -- Iteration 62613 - Batch 18/963 - Train loss: 0.27613978  - Train acc: 0.8927 - Val loss: 0.99933392\n",
      "(13.14 min) Epoch 66/300 -- Iteration 62622 - Batch 27/963 - Train loss: 0.28668867  - Train acc: 0.8898 - Val loss: 0.99933392\n",
      "(13.14 min) Epoch 66/300 -- Iteration 62631 - Batch 36/963 - Train loss: 0.28325874  - Train acc: 0.8902 - Val loss: 0.99933392\n",
      "(13.14 min) Epoch 66/300 -- Iteration 62640 - Batch 45/963 - Train loss: 0.28737570  - Train acc: 0.8886 - Val loss: 0.99933392\n",
      "(13.14 min) Epoch 66/300 -- Iteration 62649 - Batch 54/963 - Train loss: 0.28537071  - Train acc: 0.8899 - Val loss: 0.99933392\n",
      "(13.15 min) Epoch 66/300 -- Iteration 62658 - Batch 63/963 - Train loss: 0.28119508  - Train acc: 0.8916 - Val loss: 0.99933392\n",
      "(13.15 min) Epoch 66/300 -- Iteration 62667 - Batch 72/963 - Train loss: 0.28172008  - Train acc: 0.8916 - Val loss: 0.99933392\n",
      "(13.15 min) Epoch 66/300 -- Iteration 62676 - Batch 81/963 - Train loss: 0.28185156  - Train acc: 0.8923 - Val loss: 0.99933392\n",
      "(13.15 min) Epoch 66/300 -- Iteration 62685 - Batch 90/963 - Train loss: 0.28362819  - Train acc: 0.8918 - Val loss: 0.99933392\n",
      "(13.15 min) Epoch 66/300 -- Iteration 62694 - Batch 99/963 - Train loss: 0.28456914  - Train acc: 0.8917 - Val loss: 0.99933392\n",
      "(13.16 min) Epoch 66/300 -- Iteration 62703 - Batch 108/963 - Train loss: 0.28356182  - Train acc: 0.8920 - Val loss: 0.99933392\n",
      "(13.16 min) Epoch 66/300 -- Iteration 62712 - Batch 117/963 - Train loss: 0.28347820  - Train acc: 0.8923 - Val loss: 0.99933392\n",
      "(13.16 min) Epoch 66/300 -- Iteration 62721 - Batch 126/963 - Train loss: 0.28147278  - Train acc: 0.8926 - Val loss: 0.99933392\n",
      "(13.16 min) Epoch 66/300 -- Iteration 62730 - Batch 135/963 - Train loss: 0.28141747  - Train acc: 0.8926 - Val loss: 0.99933392\n",
      "(13.16 min) Epoch 66/300 -- Iteration 62739 - Batch 144/963 - Train loss: 0.27971190  - Train acc: 0.8930 - Val loss: 0.99933392\n",
      "(13.17 min) Epoch 66/300 -- Iteration 62748 - Batch 153/963 - Train loss: 0.28059956  - Train acc: 0.8923 - Val loss: 0.99933392\n",
      "(13.17 min) Epoch 66/300 -- Iteration 62757 - Batch 162/963 - Train loss: 0.27992882  - Train acc: 0.8927 - Val loss: 0.99933392\n",
      "(13.17 min) Epoch 66/300 -- Iteration 62766 - Batch 171/963 - Train loss: 0.28052325  - Train acc: 0.8925 - Val loss: 0.99933392\n",
      "(13.17 min) Epoch 66/300 -- Iteration 62775 - Batch 180/963 - Train loss: 0.28190586  - Train acc: 0.8919 - Val loss: 0.99933392\n",
      "(13.17 min) Epoch 66/300 -- Iteration 62784 - Batch 189/963 - Train loss: 0.28117856  - Train acc: 0.8923 - Val loss: 0.99933392\n",
      "(13.18 min) Epoch 66/300 -- Iteration 62793 - Batch 198/963 - Train loss: 0.28053591  - Train acc: 0.8926 - Val loss: 0.99933392\n",
      "(13.18 min) Epoch 66/300 -- Iteration 62802 - Batch 207/963 - Train loss: 0.28071023  - Train acc: 0.8924 - Val loss: 0.99933392\n",
      "(13.18 min) Epoch 66/300 -- Iteration 62811 - Batch 216/963 - Train loss: 0.28094897  - Train acc: 0.8921 - Val loss: 0.99933392\n",
      "(13.18 min) Epoch 66/300 -- Iteration 62820 - Batch 225/963 - Train loss: 0.28174145  - Train acc: 0.8919 - Val loss: 0.99933392\n",
      "(13.18 min) Epoch 66/300 -- Iteration 62829 - Batch 234/963 - Train loss: 0.28110705  - Train acc: 0.8921 - Val loss: 0.99933392\n",
      "(13.19 min) Epoch 66/300 -- Iteration 62838 - Batch 243/963 - Train loss: 0.28121985  - Train acc: 0.8919 - Val loss: 0.99933392\n",
      "(13.19 min) Epoch 66/300 -- Iteration 62847 - Batch 252/963 - Train loss: 0.28089305  - Train acc: 0.8920 - Val loss: 0.99933392\n",
      "(13.19 min) Epoch 66/300 -- Iteration 62856 - Batch 261/963 - Train loss: 0.27985900  - Train acc: 0.8927 - Val loss: 0.99933392\n",
      "(13.19 min) Epoch 66/300 -- Iteration 62865 - Batch 270/963 - Train loss: 0.27962216  - Train acc: 0.8928 - Val loss: 0.99933392\n",
      "(13.19 min) Epoch 66/300 -- Iteration 62874 - Batch 279/963 - Train loss: 0.27938578  - Train acc: 0.8929 - Val loss: 0.99933392\n",
      "(13.19 min) Epoch 66/300 -- Iteration 62883 - Batch 288/963 - Train loss: 0.27856066  - Train acc: 0.8932 - Val loss: 0.99933392\n",
      "(13.20 min) Epoch 66/300 -- Iteration 62892 - Batch 297/963 - Train loss: 0.27958611  - Train acc: 0.8931 - Val loss: 0.99933392\n",
      "(13.20 min) Epoch 66/300 -- Iteration 62901 - Batch 306/963 - Train loss: 0.27996520  - Train acc: 0.8929 - Val loss: 0.99933392\n",
      "(13.20 min) Epoch 66/300 -- Iteration 62910 - Batch 315/963 - Train loss: 0.27987070  - Train acc: 0.8929 - Val loss: 0.99933392\n",
      "(13.20 min) Epoch 66/300 -- Iteration 62919 - Batch 324/963 - Train loss: 0.27964224  - Train acc: 0.8931 - Val loss: 0.99933392\n",
      "(13.20 min) Epoch 66/300 -- Iteration 62928 - Batch 333/963 - Train loss: 0.27924321  - Train acc: 0.8930 - Val loss: 0.99933392\n",
      "(13.21 min) Epoch 66/300 -- Iteration 62937 - Batch 342/963 - Train loss: 0.28002337  - Train acc: 0.8930 - Val loss: 0.99933392\n",
      "(13.21 min) Epoch 66/300 -- Iteration 62946 - Batch 351/963 - Train loss: 0.27915493  - Train acc: 0.8935 - Val loss: 0.99933392\n",
      "(13.21 min) Epoch 66/300 -- Iteration 62955 - Batch 360/963 - Train loss: 0.27847144  - Train acc: 0.8938 - Val loss: 0.99933392\n",
      "(13.21 min) Epoch 66/300 -- Iteration 62964 - Batch 369/963 - Train loss: 0.27824294  - Train acc: 0.8941 - Val loss: 0.99933392\n",
      "(13.21 min) Epoch 66/300 -- Iteration 62973 - Batch 378/963 - Train loss: 0.27870965  - Train acc: 0.8939 - Val loss: 0.99933392\n",
      "(13.22 min) Epoch 66/300 -- Iteration 62982 - Batch 387/963 - Train loss: 0.27900758  - Train acc: 0.8936 - Val loss: 0.99933392\n",
      "(13.22 min) Epoch 66/300 -- Iteration 62991 - Batch 396/963 - Train loss: 0.27916370  - Train acc: 0.8933 - Val loss: 0.99933392\n",
      "(13.22 min) Epoch 66/300 -- Iteration 63000 - Batch 405/963 - Train loss: 0.27965377  - Train acc: 0.8932 - Val loss: 0.99933392\n",
      "(13.22 min) Epoch 66/300 -- Iteration 63009 - Batch 414/963 - Train loss: 0.27945071  - Train acc: 0.8933 - Val loss: 0.99933392\n",
      "(13.22 min) Epoch 66/300 -- Iteration 63018 - Batch 423/963 - Train loss: 0.27943817  - Train acc: 0.8934 - Val loss: 0.99933392\n",
      "(13.22 min) Epoch 66/300 -- Iteration 63027 - Batch 432/963 - Train loss: 0.27937100  - Train acc: 0.8933 - Val loss: 0.99933392\n",
      "(13.23 min) Epoch 66/300 -- Iteration 63036 - Batch 441/963 - Train loss: 0.27927686  - Train acc: 0.8935 - Val loss: 0.99933392\n",
      "(13.23 min) Epoch 66/300 -- Iteration 63045 - Batch 450/963 - Train loss: 0.27980018  - Train acc: 0.8932 - Val loss: 0.99933392\n",
      "(13.23 min) Epoch 66/300 -- Iteration 63054 - Batch 459/963 - Train loss: 0.27913485  - Train acc: 0.8934 - Val loss: 0.99933392\n",
      "(13.23 min) Epoch 66/300 -- Iteration 63063 - Batch 468/963 - Train loss: 0.27839583  - Train acc: 0.8937 - Val loss: 0.99933392\n",
      "(13.23 min) Epoch 66/300 -- Iteration 63072 - Batch 477/963 - Train loss: 0.27833014  - Train acc: 0.8936 - Val loss: 0.99933392\n",
      "(13.24 min) Epoch 66/300 -- Iteration 63081 - Batch 486/963 - Train loss: 0.27829749  - Train acc: 0.8935 - Val loss: 0.99933392\n",
      "(13.24 min) Epoch 66/300 -- Iteration 63090 - Batch 495/963 - Train loss: 0.27821983  - Train acc: 0.8936 - Val loss: 0.99933392\n",
      "(13.24 min) Epoch 66/300 -- Iteration 63099 - Batch 504/963 - Train loss: 0.27777087  - Train acc: 0.8937 - Val loss: 0.99933392\n",
      "(13.24 min) Epoch 66/300 -- Iteration 63108 - Batch 513/963 - Train loss: 0.27751838  - Train acc: 0.8937 - Val loss: 0.99933392\n",
      "(13.24 min) Epoch 66/300 -- Iteration 63117 - Batch 522/963 - Train loss: 0.27740651  - Train acc: 0.8937 - Val loss: 0.99933392\n",
      "(13.25 min) Epoch 66/300 -- Iteration 63126 - Batch 531/963 - Train loss: 0.27688096  - Train acc: 0.8939 - Val loss: 0.99933392\n",
      "(13.25 min) Epoch 66/300 -- Iteration 63135 - Batch 540/963 - Train loss: 0.27696270  - Train acc: 0.8939 - Val loss: 0.99933392\n",
      "(13.25 min) Epoch 66/300 -- Iteration 63144 - Batch 549/963 - Train loss: 0.27739437  - Train acc: 0.8937 - Val loss: 0.99933392\n",
      "(13.25 min) Epoch 66/300 -- Iteration 63153 - Batch 558/963 - Train loss: 0.27721750  - Train acc: 0.8937 - Val loss: 0.99933392\n",
      "(13.25 min) Epoch 66/300 -- Iteration 63162 - Batch 567/963 - Train loss: 0.27737068  - Train acc: 0.8936 - Val loss: 0.99933392\n",
      "(13.25 min) Epoch 66/300 -- Iteration 63171 - Batch 576/963 - Train loss: 0.27749567  - Train acc: 0.8935 - Val loss: 0.99933392\n",
      "(13.26 min) Epoch 66/300 -- Iteration 63180 - Batch 585/963 - Train loss: 0.27741466  - Train acc: 0.8935 - Val loss: 0.99933392\n",
      "(13.26 min) Epoch 66/300 -- Iteration 63189 - Batch 594/963 - Train loss: 0.27736653  - Train acc: 0.8936 - Val loss: 0.99933392\n",
      "(13.26 min) Epoch 66/300 -- Iteration 63198 - Batch 603/963 - Train loss: 0.27735170  - Train acc: 0.8937 - Val loss: 0.99933392\n",
      "(13.26 min) Epoch 66/300 -- Iteration 63207 - Batch 612/963 - Train loss: 0.27695796  - Train acc: 0.8938 - Val loss: 0.99933392\n",
      "(13.26 min) Epoch 66/300 -- Iteration 63216 - Batch 621/963 - Train loss: 0.27688344  - Train acc: 0.8939 - Val loss: 0.99933392\n",
      "(13.27 min) Epoch 66/300 -- Iteration 63225 - Batch 630/963 - Train loss: 0.27637047  - Train acc: 0.8940 - Val loss: 0.99933392\n",
      "(13.27 min) Epoch 66/300 -- Iteration 63234 - Batch 639/963 - Train loss: 0.27615216  - Train acc: 0.8942 - Val loss: 0.99933392\n",
      "(13.27 min) Epoch 66/300 -- Iteration 63243 - Batch 648/963 - Train loss: 0.27619743  - Train acc: 0.8943 - Val loss: 0.99933392\n",
      "(13.27 min) Epoch 66/300 -- Iteration 63252 - Batch 657/963 - Train loss: 0.27594521  - Train acc: 0.8944 - Val loss: 0.99933392\n",
      "(13.27 min) Epoch 66/300 -- Iteration 63261 - Batch 666/963 - Train loss: 0.27579049  - Train acc: 0.8945 - Val loss: 0.99933392\n",
      "(13.28 min) Epoch 66/300 -- Iteration 63270 - Batch 675/963 - Train loss: 0.27551929  - Train acc: 0.8947 - Val loss: 0.99933392\n",
      "(13.28 min) Epoch 66/300 -- Iteration 63279 - Batch 684/963 - Train loss: 0.27501934  - Train acc: 0.8949 - Val loss: 0.99933392\n",
      "(13.28 min) Epoch 66/300 -- Iteration 63288 - Batch 693/963 - Train loss: 0.27497054  - Train acc: 0.8948 - Val loss: 0.99933392\n",
      "(13.28 min) Epoch 66/300 -- Iteration 63297 - Batch 702/963 - Train loss: 0.27507862  - Train acc: 0.8948 - Val loss: 0.99933392\n",
      "(13.28 min) Epoch 66/300 -- Iteration 63306 - Batch 711/963 - Train loss: 0.27458247  - Train acc: 0.8950 - Val loss: 0.99933392\n",
      "(13.29 min) Epoch 66/300 -- Iteration 63315 - Batch 720/963 - Train loss: 0.27430264  - Train acc: 0.8952 - Val loss: 0.99933392\n",
      "(13.29 min) Epoch 66/300 -- Iteration 63324 - Batch 729/963 - Train loss: 0.27418822  - Train acc: 0.8952 - Val loss: 0.99933392\n",
      "(13.29 min) Epoch 66/300 -- Iteration 63333 - Batch 738/963 - Train loss: 0.27414435  - Train acc: 0.8951 - Val loss: 0.99933392\n",
      "(13.29 min) Epoch 66/300 -- Iteration 63342 - Batch 747/963 - Train loss: 0.27418932  - Train acc: 0.8951 - Val loss: 0.99933392\n",
      "(13.29 min) Epoch 66/300 -- Iteration 63351 - Batch 756/963 - Train loss: 0.27446716  - Train acc: 0.8950 - Val loss: 0.99933392\n",
      "(13.30 min) Epoch 66/300 -- Iteration 63360 - Batch 765/963 - Train loss: 0.27459015  - Train acc: 0.8949 - Val loss: 0.99933392\n",
      "(13.30 min) Epoch 66/300 -- Iteration 63369 - Batch 774/963 - Train loss: 0.27468988  - Train acc: 0.8949 - Val loss: 0.99933392\n",
      "(13.30 min) Epoch 66/300 -- Iteration 63378 - Batch 783/963 - Train loss: 0.27475035  - Train acc: 0.8949 - Val loss: 0.99933392\n",
      "(13.30 min) Epoch 66/300 -- Iteration 63387 - Batch 792/963 - Train loss: 0.27496982  - Train acc: 0.8949 - Val loss: 0.99933392\n",
      "(13.31 min) Epoch 66/300 -- Iteration 63396 - Batch 801/963 - Train loss: 0.27509231  - Train acc: 0.8949 - Val loss: 0.99933392\n",
      "(13.31 min) Epoch 66/300 -- Iteration 63405 - Batch 810/963 - Train loss: 0.27505328  - Train acc: 0.8951 - Val loss: 0.99933392\n",
      "(13.31 min) Epoch 66/300 -- Iteration 63414 - Batch 819/963 - Train loss: 0.27487388  - Train acc: 0.8951 - Val loss: 0.99933392\n",
      "(13.31 min) Epoch 66/300 -- Iteration 63423 - Batch 828/963 - Train loss: 0.27491477  - Train acc: 0.8952 - Val loss: 0.99933392\n",
      "(13.31 min) Epoch 66/300 -- Iteration 63432 - Batch 837/963 - Train loss: 0.27473184  - Train acc: 0.8953 - Val loss: 0.99933392\n",
      "(13.32 min) Epoch 66/300 -- Iteration 63441 - Batch 846/963 - Train loss: 0.27470897  - Train acc: 0.8952 - Val loss: 0.99933392\n",
      "(13.32 min) Epoch 66/300 -- Iteration 63450 - Batch 855/963 - Train loss: 0.27453887  - Train acc: 0.8953 - Val loss: 0.99933392\n",
      "(13.32 min) Epoch 66/300 -- Iteration 63459 - Batch 864/963 - Train loss: 0.27444832  - Train acc: 0.8953 - Val loss: 0.99933392\n",
      "(13.32 min) Epoch 66/300 -- Iteration 63468 - Batch 873/963 - Train loss: 0.27446716  - Train acc: 0.8953 - Val loss: 0.99933392\n",
      "(13.32 min) Epoch 66/300 -- Iteration 63477 - Batch 882/963 - Train loss: 0.27439210  - Train acc: 0.8955 - Val loss: 0.99933392\n",
      "(13.33 min) Epoch 66/300 -- Iteration 63486 - Batch 891/963 - Train loss: 0.27447336  - Train acc: 0.8955 - Val loss: 0.99933392\n",
      "(13.33 min) Epoch 66/300 -- Iteration 63495 - Batch 900/963 - Train loss: 0.27440910  - Train acc: 0.8956 - Val loss: 0.99933392\n",
      "(13.33 min) Epoch 66/300 -- Iteration 63504 - Batch 909/963 - Train loss: 0.27423255  - Train acc: 0.8957 - Val loss: 0.99933392\n",
      "(13.33 min) Epoch 66/300 -- Iteration 63513 - Batch 918/963 - Train loss: 0.27443069  - Train acc: 0.8956 - Val loss: 0.99933392\n",
      "(13.33 min) Epoch 66/300 -- Iteration 63522 - Batch 927/963 - Train loss: 0.27452562  - Train acc: 0.8954 - Val loss: 0.99933392\n",
      "(13.34 min) Epoch 66/300 -- Iteration 63531 - Batch 936/963 - Train loss: 0.27435789  - Train acc: 0.8956 - Val loss: 0.99933392\n",
      "(13.34 min) Epoch 66/300 -- Iteration 63540 - Batch 945/963 - Train loss: 0.27430976  - Train acc: 0.8957 - Val loss: 0.99933392\n",
      "(13.34 min) Epoch 66/300 -- Iteration 63549 - Batch 954/963 - Train loss: 0.27437678  - Train acc: 0.8956 - Val loss: 0.99933392\n",
      "(13.34 min) Epoch 66/300 -- Iteration 63558 - Batch 962/963 - Train loss: 0.27441166  - Train acc: 0.8956 - Val loss: 0.99940604 - Val acc: 0.5950\n",
      "(13.34 min) Epoch 67/300 -- Iteration 63567 - Batch 9/963 - Train loss: 0.26463875  - Train acc: 0.9000 - Val loss: 0.99940604\n",
      "(13.35 min) Epoch 67/300 -- Iteration 63576 - Batch 18/963 - Train loss: 0.26003798  - Train acc: 0.8997 - Val loss: 0.99940604\n",
      "(13.35 min) Epoch 67/300 -- Iteration 63585 - Batch 27/963 - Train loss: 0.26738702  - Train acc: 0.8962 - Val loss: 0.99940604\n",
      "(13.35 min) Epoch 67/300 -- Iteration 63594 - Batch 36/963 - Train loss: 0.26573161  - Train acc: 0.8976 - Val loss: 0.99940604\n",
      "(13.35 min) Epoch 67/300 -- Iteration 63603 - Batch 45/963 - Train loss: 0.27076958  - Train acc: 0.8959 - Val loss: 0.99940604\n",
      "(13.36 min) Epoch 67/300 -- Iteration 63612 - Batch 54/963 - Train loss: 0.26869432  - Train acc: 0.8973 - Val loss: 0.99940604\n",
      "(13.36 min) Epoch 67/300 -- Iteration 63621 - Batch 63/963 - Train loss: 0.26783007  - Train acc: 0.8972 - Val loss: 0.99940604\n",
      "(13.36 min) Epoch 67/300 -- Iteration 63630 - Batch 72/963 - Train loss: 0.26858624  - Train acc: 0.8965 - Val loss: 0.99940604\n",
      "(13.36 min) Epoch 67/300 -- Iteration 63639 - Batch 81/963 - Train loss: 0.27127120  - Train acc: 0.8962 - Val loss: 0.99940604\n",
      "(13.36 min) Epoch 67/300 -- Iteration 63648 - Batch 90/963 - Train loss: 0.27169104  - Train acc: 0.8964 - Val loss: 0.99940604\n",
      "(13.37 min) Epoch 67/300 -- Iteration 63657 - Batch 99/963 - Train loss: 0.27009357  - Train acc: 0.8974 - Val loss: 0.99940604\n",
      "(13.37 min) Epoch 67/300 -- Iteration 63666 - Batch 108/963 - Train loss: 0.26999748  - Train acc: 0.8974 - Val loss: 0.99940604\n",
      "(13.37 min) Epoch 67/300 -- Iteration 63675 - Batch 117/963 - Train loss: 0.26990174  - Train acc: 0.8974 - Val loss: 0.99940604\n",
      "(13.37 min) Epoch 67/300 -- Iteration 63684 - Batch 126/963 - Train loss: 0.26812345  - Train acc: 0.8980 - Val loss: 0.99940604\n",
      "(13.37 min) Epoch 67/300 -- Iteration 63693 - Batch 135/963 - Train loss: 0.26952502  - Train acc: 0.8967 - Val loss: 0.99940604\n",
      "(13.38 min) Epoch 67/300 -- Iteration 63702 - Batch 144/963 - Train loss: 0.26979246  - Train acc: 0.8969 - Val loss: 0.99940604\n",
      "(13.38 min) Epoch 67/300 -- Iteration 63711 - Batch 153/963 - Train loss: 0.27072541  - Train acc: 0.8961 - Val loss: 0.99940604\n",
      "(13.38 min) Epoch 67/300 -- Iteration 63720 - Batch 162/963 - Train loss: 0.27314734  - Train acc: 0.8948 - Val loss: 0.99940604\n",
      "(13.38 min) Epoch 67/300 -- Iteration 63729 - Batch 171/963 - Train loss: 0.27326709  - Train acc: 0.8951 - Val loss: 0.99940604\n",
      "(13.38 min) Epoch 67/300 -- Iteration 63738 - Batch 180/963 - Train loss: 0.27452826  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.39 min) Epoch 67/300 -- Iteration 63747 - Batch 189/963 - Train loss: 0.27481293  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.39 min) Epoch 67/300 -- Iteration 63756 - Batch 198/963 - Train loss: 0.27578943  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.39 min) Epoch 67/300 -- Iteration 63765 - Batch 207/963 - Train loss: 0.27660104  - Train acc: 0.8945 - Val loss: 0.99940604\n",
      "(13.39 min) Epoch 67/300 -- Iteration 63774 - Batch 216/963 - Train loss: 0.27731044  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.39 min) Epoch 67/300 -- Iteration 63783 - Batch 225/963 - Train loss: 0.27736745  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.40 min) Epoch 67/300 -- Iteration 63792 - Batch 234/963 - Train loss: 0.27631749  - Train acc: 0.8949 - Val loss: 0.99940604\n",
      "(13.40 min) Epoch 67/300 -- Iteration 63801 - Batch 243/963 - Train loss: 0.27738031  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.40 min) Epoch 67/300 -- Iteration 63810 - Batch 252/963 - Train loss: 0.27717662  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.40 min) Epoch 67/300 -- Iteration 63819 - Batch 261/963 - Train loss: 0.27762398  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.40 min) Epoch 67/300 -- Iteration 63828 - Batch 270/963 - Train loss: 0.27738806  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.40 min) Epoch 67/300 -- Iteration 63837 - Batch 279/963 - Train loss: 0.27780365  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.41 min) Epoch 67/300 -- Iteration 63846 - Batch 288/963 - Train loss: 0.27733792  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.41 min) Epoch 67/300 -- Iteration 63855 - Batch 297/963 - Train loss: 0.27667432  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.41 min) Epoch 67/300 -- Iteration 63864 - Batch 306/963 - Train loss: 0.27705024  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.41 min) Epoch 67/300 -- Iteration 63873 - Batch 315/963 - Train loss: 0.27792749  - Train acc: 0.8940 - Val loss: 0.99940604\n",
      "(13.41 min) Epoch 67/300 -- Iteration 63882 - Batch 324/963 - Train loss: 0.27738083  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.42 min) Epoch 67/300 -- Iteration 63891 - Batch 333/963 - Train loss: 0.27897857  - Train acc: 0.8935 - Val loss: 0.99940604\n",
      "(13.42 min) Epoch 67/300 -- Iteration 63900 - Batch 342/963 - Train loss: 0.27853421  - Train acc: 0.8939 - Val loss: 0.99940604\n",
      "(13.42 min) Epoch 67/300 -- Iteration 63909 - Batch 351/963 - Train loss: 0.27886298  - Train acc: 0.8939 - Val loss: 0.99940604\n",
      "(13.42 min) Epoch 67/300 -- Iteration 63918 - Batch 360/963 - Train loss: 0.27852242  - Train acc: 0.8938 - Val loss: 0.99940604\n",
      "(13.42 min) Epoch 67/300 -- Iteration 63927 - Batch 369/963 - Train loss: 0.27844661  - Train acc: 0.8937 - Val loss: 0.99940604\n",
      "(13.43 min) Epoch 67/300 -- Iteration 63936 - Batch 378/963 - Train loss: 0.27806591  - Train acc: 0.8940 - Val loss: 0.99940604\n",
      "(13.43 min) Epoch 67/300 -- Iteration 63945 - Batch 387/963 - Train loss: 0.27752640  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.43 min) Epoch 67/300 -- Iteration 63954 - Batch 396/963 - Train loss: 0.27757308  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.43 min) Epoch 67/300 -- Iteration 63963 - Batch 405/963 - Train loss: 0.27753001  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.43 min) Epoch 67/300 -- Iteration 63972 - Batch 414/963 - Train loss: 0.27783293  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.44 min) Epoch 67/300 -- Iteration 63981 - Batch 423/963 - Train loss: 0.27816752  - Train acc: 0.8940 - Val loss: 0.99940604\n",
      "(13.44 min) Epoch 67/300 -- Iteration 63990 - Batch 432/963 - Train loss: 0.27858764  - Train acc: 0.8939 - Val loss: 0.99940604\n",
      "(13.44 min) Epoch 67/300 -- Iteration 63999 - Batch 441/963 - Train loss: 0.27809811  - Train acc: 0.8942 - Val loss: 0.99940604\n",
      "(13.44 min) Epoch 67/300 -- Iteration 64008 - Batch 450/963 - Train loss: 0.27743893  - Train acc: 0.8945 - Val loss: 0.99940604\n",
      "(13.44 min) Epoch 67/300 -- Iteration 64017 - Batch 459/963 - Train loss: 0.27800696  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.44 min) Epoch 67/300 -- Iteration 64026 - Batch 468/963 - Train loss: 0.27811047  - Train acc: 0.8942 - Val loss: 0.99940604\n",
      "(13.45 min) Epoch 67/300 -- Iteration 64035 - Batch 477/963 - Train loss: 0.27810231  - Train acc: 0.8941 - Val loss: 0.99940604\n",
      "(13.45 min) Epoch 67/300 -- Iteration 64044 - Batch 486/963 - Train loss: 0.27777821  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.45 min) Epoch 67/300 -- Iteration 64053 - Batch 495/963 - Train loss: 0.27858773  - Train acc: 0.8938 - Val loss: 0.99940604\n",
      "(13.45 min) Epoch 67/300 -- Iteration 64062 - Batch 504/963 - Train loss: 0.27803659  - Train acc: 0.8941 - Val loss: 0.99940604\n",
      "(13.46 min) Epoch 67/300 -- Iteration 64071 - Batch 513/963 - Train loss: 0.27809115  - Train acc: 0.8941 - Val loss: 0.99940604\n",
      "(13.46 min) Epoch 67/300 -- Iteration 64080 - Batch 522/963 - Train loss: 0.27812178  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.46 min) Epoch 67/300 -- Iteration 64089 - Batch 531/963 - Train loss: 0.27805459  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.46 min) Epoch 67/300 -- Iteration 64098 - Batch 540/963 - Train loss: 0.27775007  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.46 min) Epoch 67/300 -- Iteration 64107 - Batch 549/963 - Train loss: 0.27744276  - Train acc: 0.8945 - Val loss: 0.99940604\n",
      "(13.47 min) Epoch 67/300 -- Iteration 64116 - Batch 558/963 - Train loss: 0.27752329  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.47 min) Epoch 67/300 -- Iteration 64125 - Batch 567/963 - Train loss: 0.27727507  - Train acc: 0.8945 - Val loss: 0.99940604\n",
      "(13.47 min) Epoch 67/300 -- Iteration 64134 - Batch 576/963 - Train loss: 0.27711261  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.47 min) Epoch 67/300 -- Iteration 64143 - Batch 585/963 - Train loss: 0.27674310  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.47 min) Epoch 67/300 -- Iteration 64152 - Batch 594/963 - Train loss: 0.27683862  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.48 min) Epoch 67/300 -- Iteration 64161 - Batch 603/963 - Train loss: 0.27706697  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.48 min) Epoch 67/300 -- Iteration 64170 - Batch 612/963 - Train loss: 0.27699458  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.48 min) Epoch 67/300 -- Iteration 64179 - Batch 621/963 - Train loss: 0.27694532  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.48 min) Epoch 67/300 -- Iteration 64188 - Batch 630/963 - Train loss: 0.27705740  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.48 min) Epoch 67/300 -- Iteration 64197 - Batch 639/963 - Train loss: 0.27680083  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.49 min) Epoch 67/300 -- Iteration 64206 - Batch 648/963 - Train loss: 0.27690444  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.49 min) Epoch 67/300 -- Iteration 64215 - Batch 657/963 - Train loss: 0.27701060  - Train acc: 0.8945 - Val loss: 0.99940604\n",
      "(13.49 min) Epoch 67/300 -- Iteration 64224 - Batch 666/963 - Train loss: 0.27715670  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.49 min) Epoch 67/300 -- Iteration 64233 - Batch 675/963 - Train loss: 0.27726907  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.49 min) Epoch 67/300 -- Iteration 64242 - Batch 684/963 - Train loss: 0.27799806  - Train acc: 0.8941 - Val loss: 0.99940604\n",
      "(13.50 min) Epoch 67/300 -- Iteration 64251 - Batch 693/963 - Train loss: 0.27821119  - Train acc: 0.8940 - Val loss: 0.99940604\n",
      "(13.50 min) Epoch 67/300 -- Iteration 64260 - Batch 702/963 - Train loss: 0.27839485  - Train acc: 0.8940 - Val loss: 0.99940604\n",
      "(13.50 min) Epoch 67/300 -- Iteration 64269 - Batch 711/963 - Train loss: 0.27812202  - Train acc: 0.8941 - Val loss: 0.99940604\n",
      "(13.50 min) Epoch 67/300 -- Iteration 64278 - Batch 720/963 - Train loss: 0.27809737  - Train acc: 0.8941 - Val loss: 0.99940604\n",
      "(13.50 min) Epoch 67/300 -- Iteration 64287 - Batch 729/963 - Train loss: 0.27823879  - Train acc: 0.8941 - Val loss: 0.99940604\n",
      "(13.51 min) Epoch 67/300 -- Iteration 64296 - Batch 738/963 - Train loss: 0.27788380  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.51 min) Epoch 67/300 -- Iteration 64305 - Batch 747/963 - Train loss: 0.27774824  - Train acc: 0.8942 - Val loss: 0.99940604\n",
      "(13.51 min) Epoch 67/300 -- Iteration 64314 - Batch 756/963 - Train loss: 0.27751371  - Train acc: 0.8943 - Val loss: 0.99940604\n",
      "(13.51 min) Epoch 67/300 -- Iteration 64323 - Batch 765/963 - Train loss: 0.27716988  - Train acc: 0.8944 - Val loss: 0.99940604\n",
      "(13.51 min) Epoch 67/300 -- Iteration 64332 - Batch 774/963 - Train loss: 0.27696471  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.52 min) Epoch 67/300 -- Iteration 64341 - Batch 783/963 - Train loss: 0.27689101  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.52 min) Epoch 67/300 -- Iteration 64350 - Batch 792/963 - Train loss: 0.27642144  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.52 min) Epoch 67/300 -- Iteration 64359 - Batch 801/963 - Train loss: 0.27651475  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.52 min) Epoch 67/300 -- Iteration 64368 - Batch 810/963 - Train loss: 0.27648059  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.52 min) Epoch 67/300 -- Iteration 64377 - Batch 819/963 - Train loss: 0.27629593  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.53 min) Epoch 67/300 -- Iteration 64386 - Batch 828/963 - Train loss: 0.27620992  - Train acc: 0.8948 - Val loss: 0.99940604\n",
      "(13.53 min) Epoch 67/300 -- Iteration 64395 - Batch 837/963 - Train loss: 0.27635811  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.53 min) Epoch 67/300 -- Iteration 64404 - Batch 846/963 - Train loss: 0.27632463  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.53 min) Epoch 67/300 -- Iteration 64413 - Batch 855/963 - Train loss: 0.27621287  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.53 min) Epoch 67/300 -- Iteration 64422 - Batch 864/963 - Train loss: 0.27621201  - Train acc: 0.8946 - Val loss: 0.99940604\n",
      "(13.54 min) Epoch 67/300 -- Iteration 64431 - Batch 873/963 - Train loss: 0.27602748  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.54 min) Epoch 67/300 -- Iteration 64440 - Batch 882/963 - Train loss: 0.27601535  - Train acc: 0.8948 - Val loss: 0.99940604\n",
      "(13.54 min) Epoch 67/300 -- Iteration 64449 - Batch 891/963 - Train loss: 0.27607920  - Train acc: 0.8948 - Val loss: 0.99940604\n",
      "(13.54 min) Epoch 67/300 -- Iteration 64458 - Batch 900/963 - Train loss: 0.27613255  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.54 min) Epoch 67/300 -- Iteration 64467 - Batch 909/963 - Train loss: 0.27597681  - Train acc: 0.8948 - Val loss: 0.99940604\n",
      "(13.55 min) Epoch 67/300 -- Iteration 64476 - Batch 918/963 - Train loss: 0.27605968  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.55 min) Epoch 67/300 -- Iteration 64485 - Batch 927/963 - Train loss: 0.27621239  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.55 min) Epoch 67/300 -- Iteration 64494 - Batch 936/963 - Train loss: 0.27603468  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.55 min) Epoch 67/300 -- Iteration 64503 - Batch 945/963 - Train loss: 0.27604489  - Train acc: 0.8947 - Val loss: 0.99940604\n",
      "(13.55 min) Epoch 67/300 -- Iteration 64512 - Batch 954/963 - Train loss: 0.27578140  - Train acc: 0.8948 - Val loss: 0.99940604\n",
      "(13.56 min) Epoch 67/300 -- Iteration 64521 - Batch 962/963 - Train loss: 0.27571063  - Train acc: 0.8949 - Val loss: 0.99098569 - Val acc: 0.5950\n",
      "(13.56 min) Epoch 68/300 -- Iteration 64530 - Batch 9/963 - Train loss: 0.27675655  - Train acc: 0.8891 - Val loss: 0.99098569\n",
      "(13.56 min) Epoch 68/300 -- Iteration 64539 - Batch 18/963 - Train loss: 0.28380498  - Train acc: 0.8919 - Val loss: 0.99098569\n",
      "(13.56 min) Epoch 68/300 -- Iteration 64548 - Batch 27/963 - Train loss: 0.27511995  - Train acc: 0.8951 - Val loss: 0.99098569\n",
      "(13.56 min) Epoch 68/300 -- Iteration 64557 - Batch 36/963 - Train loss: 0.27757489  - Train acc: 0.8944 - Val loss: 0.99098569\n",
      "(13.57 min) Epoch 68/300 -- Iteration 64566 - Batch 45/963 - Train loss: 0.28164874  - Train acc: 0.8923 - Val loss: 0.99098569\n",
      "(13.57 min) Epoch 68/300 -- Iteration 64575 - Batch 54/963 - Train loss: 0.27716478  - Train acc: 0.8928 - Val loss: 0.99098569\n",
      "(13.57 min) Epoch 68/300 -- Iteration 64584 - Batch 63/963 - Train loss: 0.27766175  - Train acc: 0.8933 - Val loss: 0.99098569\n",
      "(13.57 min) Epoch 68/300 -- Iteration 64593 - Batch 72/963 - Train loss: 0.27483396  - Train acc: 0.8950 - Val loss: 0.99098569\n",
      "(13.57 min) Epoch 68/300 -- Iteration 64602 - Batch 81/963 - Train loss: 0.27532335  - Train acc: 0.8955 - Val loss: 0.99098569\n",
      "(13.58 min) Epoch 68/300 -- Iteration 64611 - Batch 90/963 - Train loss: 0.27326909  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.58 min) Epoch 68/300 -- Iteration 64620 - Batch 99/963 - Train loss: 0.27436386  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.58 min) Epoch 68/300 -- Iteration 64629 - Batch 108/963 - Train loss: 0.27894517  - Train acc: 0.8944 - Val loss: 0.99098569\n",
      "(13.58 min) Epoch 68/300 -- Iteration 64638 - Batch 117/963 - Train loss: 0.27499319  - Train acc: 0.8961 - Val loss: 0.99098569\n",
      "(13.58 min) Epoch 68/300 -- Iteration 64647 - Batch 126/963 - Train loss: 0.27382785  - Train acc: 0.8968 - Val loss: 0.99098569\n",
      "(13.59 min) Epoch 68/300 -- Iteration 64656 - Batch 135/963 - Train loss: 0.27273055  - Train acc: 0.8977 - Val loss: 0.99098569\n",
      "(13.59 min) Epoch 68/300 -- Iteration 64665 - Batch 144/963 - Train loss: 0.27288693  - Train acc: 0.8977 - Val loss: 0.99098569\n",
      "(13.59 min) Epoch 68/300 -- Iteration 64674 - Batch 153/963 - Train loss: 0.27319057  - Train acc: 0.8974 - Val loss: 0.99098569\n",
      "(13.59 min) Epoch 68/300 -- Iteration 64683 - Batch 162/963 - Train loss: 0.27564743  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.59 min) Epoch 68/300 -- Iteration 64692 - Batch 171/963 - Train loss: 0.27610172  - Train acc: 0.8961 - Val loss: 0.99098569\n",
      "(13.60 min) Epoch 68/300 -- Iteration 64701 - Batch 180/963 - Train loss: 0.27659430  - Train acc: 0.8956 - Val loss: 0.99098569\n",
      "(13.60 min) Epoch 68/300 -- Iteration 64710 - Batch 189/963 - Train loss: 0.27722968  - Train acc: 0.8956 - Val loss: 0.99098569\n",
      "(13.60 min) Epoch 68/300 -- Iteration 64719 - Batch 198/963 - Train loss: 0.27875444  - Train acc: 0.8948 - Val loss: 0.99098569\n",
      "(13.60 min) Epoch 68/300 -- Iteration 64728 - Batch 207/963 - Train loss: 0.27985389  - Train acc: 0.8945 - Val loss: 0.99098569\n",
      "(13.60 min) Epoch 68/300 -- Iteration 64737 - Batch 216/963 - Train loss: 0.27994148  - Train acc: 0.8949 - Val loss: 0.99098569\n",
      "(13.61 min) Epoch 68/300 -- Iteration 64746 - Batch 225/963 - Train loss: 0.27924420  - Train acc: 0.8954 - Val loss: 0.99098569\n",
      "(13.61 min) Epoch 68/300 -- Iteration 64755 - Batch 234/963 - Train loss: 0.27869070  - Train acc: 0.8953 - Val loss: 0.99098569\n",
      "(13.61 min) Epoch 68/300 -- Iteration 64764 - Batch 243/963 - Train loss: 0.27841405  - Train acc: 0.8955 - Val loss: 0.99098569\n",
      "(13.61 min) Epoch 68/300 -- Iteration 64773 - Batch 252/963 - Train loss: 0.27846614  - Train acc: 0.8957 - Val loss: 0.99098569\n",
      "(13.61 min) Epoch 68/300 -- Iteration 64782 - Batch 261/963 - Train loss: 0.27916689  - Train acc: 0.8952 - Val loss: 0.99098569\n",
      "(13.62 min) Epoch 68/300 -- Iteration 64791 - Batch 270/963 - Train loss: 0.27839145  - Train acc: 0.8956 - Val loss: 0.99098569\n",
      "(13.62 min) Epoch 68/300 -- Iteration 64800 - Batch 279/963 - Train loss: 0.27780556  - Train acc: 0.8957 - Val loss: 0.99098569\n",
      "(13.62 min) Epoch 68/300 -- Iteration 64809 - Batch 288/963 - Train loss: 0.27742252  - Train acc: 0.8958 - Val loss: 0.99098569\n",
      "(13.62 min) Epoch 68/300 -- Iteration 64818 - Batch 297/963 - Train loss: 0.27648862  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.62 min) Epoch 68/300 -- Iteration 64827 - Batch 306/963 - Train loss: 0.27659328  - Train acc: 0.8958 - Val loss: 0.99098569\n",
      "(13.63 min) Epoch 68/300 -- Iteration 64836 - Batch 315/963 - Train loss: 0.27606650  - Train acc: 0.8958 - Val loss: 0.99098569\n",
      "(13.63 min) Epoch 68/300 -- Iteration 64845 - Batch 324/963 - Train loss: 0.27694073  - Train acc: 0.8954 - Val loss: 0.99098569\n",
      "(13.63 min) Epoch 68/300 -- Iteration 64854 - Batch 333/963 - Train loss: 0.27676886  - Train acc: 0.8955 - Val loss: 0.99098569\n",
      "(13.63 min) Epoch 68/300 -- Iteration 64863 - Batch 342/963 - Train loss: 0.27668894  - Train acc: 0.8957 - Val loss: 0.99098569\n",
      "(13.63 min) Epoch 68/300 -- Iteration 64872 - Batch 351/963 - Train loss: 0.27649412  - Train acc: 0.8956 - Val loss: 0.99098569\n",
      "(13.64 min) Epoch 68/300 -- Iteration 64881 - Batch 360/963 - Train loss: 0.27559619  - Train acc: 0.8958 - Val loss: 0.99098569\n",
      "(13.64 min) Epoch 68/300 -- Iteration 64890 - Batch 369/963 - Train loss: 0.27536559  - Train acc: 0.8961 - Val loss: 0.99098569\n",
      "(13.64 min) Epoch 68/300 -- Iteration 64899 - Batch 378/963 - Train loss: 0.27588162  - Train acc: 0.8959 - Val loss: 0.99098569\n",
      "(13.64 min) Epoch 68/300 -- Iteration 64908 - Batch 387/963 - Train loss: 0.27588461  - Train acc: 0.8958 - Val loss: 0.99098569\n",
      "(13.64 min) Epoch 68/300 -- Iteration 64917 - Batch 396/963 - Train loss: 0.27595334  - Train acc: 0.8958 - Val loss: 0.99098569\n",
      "(13.64 min) Epoch 68/300 -- Iteration 64926 - Batch 405/963 - Train loss: 0.27573653  - Train acc: 0.8960 - Val loss: 0.99098569\n",
      "(13.65 min) Epoch 68/300 -- Iteration 64935 - Batch 414/963 - Train loss: 0.27564998  - Train acc: 0.8960 - Val loss: 0.99098569\n",
      "(13.65 min) Epoch 68/300 -- Iteration 64944 - Batch 423/963 - Train loss: 0.27515020  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.65 min) Epoch 68/300 -- Iteration 64953 - Batch 432/963 - Train loss: 0.27501738  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.65 min) Epoch 68/300 -- Iteration 64962 - Batch 441/963 - Train loss: 0.27470307  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.65 min) Epoch 68/300 -- Iteration 64971 - Batch 450/963 - Train loss: 0.27444898  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.66 min) Epoch 68/300 -- Iteration 64980 - Batch 459/963 - Train loss: 0.27474402  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.66 min) Epoch 68/300 -- Iteration 64989 - Batch 468/963 - Train loss: 0.27425583  - Train acc: 0.8966 - Val loss: 0.99098569\n",
      "(13.66 min) Epoch 68/300 -- Iteration 64998 - Batch 477/963 - Train loss: 0.27418457  - Train acc: 0.8967 - Val loss: 0.99098569\n",
      "(13.66 min) Epoch 68/300 -- Iteration 65007 - Batch 486/963 - Train loss: 0.27451688  - Train acc: 0.8966 - Val loss: 0.99098569\n",
      "(13.66 min) Epoch 68/300 -- Iteration 65016 - Batch 495/963 - Train loss: 0.27494530  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.67 min) Epoch 68/300 -- Iteration 65025 - Batch 504/963 - Train loss: 0.27477215  - Train acc: 0.8966 - Val loss: 0.99098569\n",
      "(13.67 min) Epoch 68/300 -- Iteration 65034 - Batch 513/963 - Train loss: 0.27457675  - Train acc: 0.8966 - Val loss: 0.99098569\n",
      "(13.67 min) Epoch 68/300 -- Iteration 65043 - Batch 522/963 - Train loss: 0.27415181  - Train acc: 0.8969 - Val loss: 0.99098569\n",
      "(13.67 min) Epoch 68/300 -- Iteration 65052 - Batch 531/963 - Train loss: 0.27431262  - Train acc: 0.8968 - Val loss: 0.99098569\n",
      "(13.67 min) Epoch 68/300 -- Iteration 65061 - Batch 540/963 - Train loss: 0.27445172  - Train acc: 0.8966 - Val loss: 0.99098569\n",
      "(13.68 min) Epoch 68/300 -- Iteration 65070 - Batch 549/963 - Train loss: 0.27438304  - Train acc: 0.8966 - Val loss: 0.99098569\n",
      "(13.68 min) Epoch 68/300 -- Iteration 65079 - Batch 558/963 - Train loss: 0.27445277  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.68 min) Epoch 68/300 -- Iteration 65088 - Batch 567/963 - Train loss: 0.27447171  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.68 min) Epoch 68/300 -- Iteration 65097 - Batch 576/963 - Train loss: 0.27460408  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.68 min) Epoch 68/300 -- Iteration 65106 - Batch 585/963 - Train loss: 0.27466236  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.69 min) Epoch 68/300 -- Iteration 65115 - Batch 594/963 - Train loss: 0.27488621  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.69 min) Epoch 68/300 -- Iteration 65124 - Batch 603/963 - Train loss: 0.27475106  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.69 min) Epoch 68/300 -- Iteration 65133 - Batch 612/963 - Train loss: 0.27477667  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.69 min) Epoch 68/300 -- Iteration 65142 - Batch 621/963 - Train loss: 0.27498382  - Train acc: 0.8962 - Val loss: 0.99098569\n",
      "(13.69 min) Epoch 68/300 -- Iteration 65151 - Batch 630/963 - Train loss: 0.27476501  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.69 min) Epoch 68/300 -- Iteration 65160 - Batch 639/963 - Train loss: 0.27447316  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.70 min) Epoch 68/300 -- Iteration 65169 - Batch 648/963 - Train loss: 0.27392516  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.70 min) Epoch 68/300 -- Iteration 65178 - Batch 657/963 - Train loss: 0.27379036  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.70 min) Epoch 68/300 -- Iteration 65187 - Batch 666/963 - Train loss: 0.27384125  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.70 min) Epoch 68/300 -- Iteration 65196 - Batch 675/963 - Train loss: 0.27375210  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.70 min) Epoch 68/300 -- Iteration 65205 - Batch 684/963 - Train loss: 0.27356785  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.71 min) Epoch 68/300 -- Iteration 65214 - Batch 693/963 - Train loss: 0.27366010  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.71 min) Epoch 68/300 -- Iteration 65223 - Batch 702/963 - Train loss: 0.27382091  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.71 min) Epoch 68/300 -- Iteration 65232 - Batch 711/963 - Train loss: 0.27365613  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.71 min) Epoch 68/300 -- Iteration 65241 - Batch 720/963 - Train loss: 0.27414119  - Train acc: 0.8962 - Val loss: 0.99098569\n",
      "(13.71 min) Epoch 68/300 -- Iteration 65250 - Batch 729/963 - Train loss: 0.27397233  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.72 min) Epoch 68/300 -- Iteration 65259 - Batch 738/963 - Train loss: 0.27387713  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.72 min) Epoch 68/300 -- Iteration 65268 - Batch 747/963 - Train loss: 0.27354014  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.72 min) Epoch 68/300 -- Iteration 65277 - Batch 756/963 - Train loss: 0.27360333  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.72 min) Epoch 68/300 -- Iteration 65286 - Batch 765/963 - Train loss: 0.27353454  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.72 min) Epoch 68/300 -- Iteration 65295 - Batch 774/963 - Train loss: 0.27386286  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.72 min) Epoch 68/300 -- Iteration 65304 - Batch 783/963 - Train loss: 0.27407904  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.73 min) Epoch 68/300 -- Iteration 65313 - Batch 792/963 - Train loss: 0.27437509  - Train acc: 0.8962 - Val loss: 0.99098569\n",
      "(13.73 min) Epoch 68/300 -- Iteration 65322 - Batch 801/963 - Train loss: 0.27434122  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.73 min) Epoch 68/300 -- Iteration 65331 - Batch 810/963 - Train loss: 0.27421512  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.73 min) Epoch 68/300 -- Iteration 65340 - Batch 819/963 - Train loss: 0.27423815  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.73 min) Epoch 68/300 -- Iteration 65349 - Batch 828/963 - Train loss: 0.27420288  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.74 min) Epoch 68/300 -- Iteration 65358 - Batch 837/963 - Train loss: 0.27442324  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.74 min) Epoch 68/300 -- Iteration 65367 - Batch 846/963 - Train loss: 0.27436070  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.74 min) Epoch 68/300 -- Iteration 65376 - Batch 855/963 - Train loss: 0.27440569  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.74 min) Epoch 68/300 -- Iteration 65385 - Batch 864/963 - Train loss: 0.27437455  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.74 min) Epoch 68/300 -- Iteration 65394 - Batch 873/963 - Train loss: 0.27472558  - Train acc: 0.8962 - Val loss: 0.99098569\n",
      "(13.75 min) Epoch 68/300 -- Iteration 65403 - Batch 882/963 - Train loss: 0.27465387  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.75 min) Epoch 68/300 -- Iteration 65412 - Batch 891/963 - Train loss: 0.27461047  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.75 min) Epoch 68/300 -- Iteration 65421 - Batch 900/963 - Train loss: 0.27447135  - Train acc: 0.8963 - Val loss: 0.99098569\n",
      "(13.75 min) Epoch 68/300 -- Iteration 65430 - Batch 909/963 - Train loss: 0.27430262  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.75 min) Epoch 68/300 -- Iteration 65439 - Batch 918/963 - Train loss: 0.27431762  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.75 min) Epoch 68/300 -- Iteration 65448 - Batch 927/963 - Train loss: 0.27417803  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.76 min) Epoch 68/300 -- Iteration 65457 - Batch 936/963 - Train loss: 0.27416941  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.76 min) Epoch 68/300 -- Iteration 65466 - Batch 945/963 - Train loss: 0.27405684  - Train acc: 0.8965 - Val loss: 0.99098569\n",
      "(13.76 min) Epoch 68/300 -- Iteration 65475 - Batch 954/963 - Train loss: 0.27431014  - Train acc: 0.8964 - Val loss: 0.99098569\n",
      "(13.76 min) Epoch 68/300 -- Iteration 65484 - Batch 962/963 - Train loss: 0.27442745  - Train acc: 0.8963 - Val loss: 1.00529540 - Val acc: 0.5933\n",
      "(13.76 min) Epoch 69/300 -- Iteration 65493 - Batch 9/963 - Train loss: 0.26026532  - Train acc: 0.9016 - Val loss: 1.00529540\n",
      "(13.77 min) Epoch 69/300 -- Iteration 65502 - Batch 18/963 - Train loss: 0.26107238  - Train acc: 0.8964 - Val loss: 1.00529540\n",
      "(13.77 min) Epoch 69/300 -- Iteration 65511 - Batch 27/963 - Train loss: 0.26467669  - Train acc: 0.8937 - Val loss: 1.00529540\n",
      "(13.77 min) Epoch 69/300 -- Iteration 65520 - Batch 36/963 - Train loss: 0.26680793  - Train acc: 0.8953 - Val loss: 1.00529540\n",
      "(13.77 min) Epoch 69/300 -- Iteration 65529 - Batch 45/963 - Train loss: 0.26763281  - Train acc: 0.8945 - Val loss: 1.00529540\n",
      "(13.77 min) Epoch 69/300 -- Iteration 65538 - Batch 54/963 - Train loss: 0.27206358  - Train acc: 0.8930 - Val loss: 1.00529540\n",
      "(13.78 min) Epoch 69/300 -- Iteration 65547 - Batch 63/963 - Train loss: 0.26768946  - Train acc: 0.8951 - Val loss: 1.00529540\n",
      "(13.78 min) Epoch 69/300 -- Iteration 65556 - Batch 72/963 - Train loss: 0.26831641  - Train acc: 0.8951 - Val loss: 1.00529540\n",
      "(13.78 min) Epoch 69/300 -- Iteration 65565 - Batch 81/963 - Train loss: 0.26949899  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.78 min) Epoch 69/300 -- Iteration 65574 - Batch 90/963 - Train loss: 0.27181664  - Train acc: 0.8932 - Val loss: 1.00529540\n",
      "(13.78 min) Epoch 69/300 -- Iteration 65583 - Batch 99/963 - Train loss: 0.27291876  - Train acc: 0.8928 - Val loss: 1.00529540\n",
      "(13.79 min) Epoch 69/300 -- Iteration 65592 - Batch 108/963 - Train loss: 0.27488402  - Train acc: 0.8918 - Val loss: 1.00529540\n",
      "(13.79 min) Epoch 69/300 -- Iteration 65601 - Batch 117/963 - Train loss: 0.27779547  - Train acc: 0.8913 - Val loss: 1.00529540\n",
      "(13.79 min) Epoch 69/300 -- Iteration 65610 - Batch 126/963 - Train loss: 0.27758943  - Train acc: 0.8914 - Val loss: 1.00529540\n",
      "(13.79 min) Epoch 69/300 -- Iteration 65619 - Batch 135/963 - Train loss: 0.27541370  - Train acc: 0.8921 - Val loss: 1.00529540\n",
      "(13.79 min) Epoch 69/300 -- Iteration 65628 - Batch 144/963 - Train loss: 0.27492707  - Train acc: 0.8929 - Val loss: 1.00529540\n",
      "(13.79 min) Epoch 69/300 -- Iteration 65637 - Batch 153/963 - Train loss: 0.27583986  - Train acc: 0.8931 - Val loss: 1.00529540\n",
      "(13.80 min) Epoch 69/300 -- Iteration 65646 - Batch 162/963 - Train loss: 0.27640139  - Train acc: 0.8929 - Val loss: 1.00529540\n",
      "(13.80 min) Epoch 69/300 -- Iteration 65655 - Batch 171/963 - Train loss: 0.27656225  - Train acc: 0.8925 - Val loss: 1.00529540\n",
      "(13.80 min) Epoch 69/300 -- Iteration 65664 - Batch 180/963 - Train loss: 0.27609562  - Train acc: 0.8928 - Val loss: 1.00529540\n",
      "(13.80 min) Epoch 69/300 -- Iteration 65673 - Batch 189/963 - Train loss: 0.27692936  - Train acc: 0.8927 - Val loss: 1.00529540\n",
      "(13.80 min) Epoch 69/300 -- Iteration 65682 - Batch 198/963 - Train loss: 0.27621772  - Train acc: 0.8930 - Val loss: 1.00529540\n",
      "(13.81 min) Epoch 69/300 -- Iteration 65691 - Batch 207/963 - Train loss: 0.27741799  - Train acc: 0.8924 - Val loss: 1.00529540\n",
      "(13.81 min) Epoch 69/300 -- Iteration 65700 - Batch 216/963 - Train loss: 0.27803409  - Train acc: 0.8922 - Val loss: 1.00529540\n",
      "(13.81 min) Epoch 69/300 -- Iteration 65709 - Batch 225/963 - Train loss: 0.27891660  - Train acc: 0.8920 - Val loss: 1.00529540\n",
      "(13.81 min) Epoch 69/300 -- Iteration 65718 - Batch 234/963 - Train loss: 0.27976276  - Train acc: 0.8919 - Val loss: 1.00529540\n",
      "(13.81 min) Epoch 69/300 -- Iteration 65727 - Batch 243/963 - Train loss: 0.27902211  - Train acc: 0.8923 - Val loss: 1.00529540\n",
      "(13.82 min) Epoch 69/300 -- Iteration 65736 - Batch 252/963 - Train loss: 0.27880095  - Train acc: 0.8921 - Val loss: 1.00529540\n",
      "(13.82 min) Epoch 69/300 -- Iteration 65745 - Batch 261/963 - Train loss: 0.27802489  - Train acc: 0.8924 - Val loss: 1.00529540\n",
      "(13.82 min) Epoch 69/300 -- Iteration 65754 - Batch 270/963 - Train loss: 0.27810410  - Train acc: 0.8928 - Val loss: 1.00529540\n",
      "(13.82 min) Epoch 69/300 -- Iteration 65763 - Batch 279/963 - Train loss: 0.27777288  - Train acc: 0.8930 - Val loss: 1.00529540\n",
      "(13.82 min) Epoch 69/300 -- Iteration 65772 - Batch 288/963 - Train loss: 0.27753252  - Train acc: 0.8931 - Val loss: 1.00529540\n",
      "(13.83 min) Epoch 69/300 -- Iteration 65781 - Batch 297/963 - Train loss: 0.27680391  - Train acc: 0.8934 - Val loss: 1.00529540\n",
      "(13.83 min) Epoch 69/300 -- Iteration 65790 - Batch 306/963 - Train loss: 0.27663213  - Train acc: 0.8934 - Val loss: 1.00529540\n",
      "(13.83 min) Epoch 69/300 -- Iteration 65799 - Batch 315/963 - Train loss: 0.27745655  - Train acc: 0.8929 - Val loss: 1.00529540\n",
      "(13.83 min) Epoch 69/300 -- Iteration 65808 - Batch 324/963 - Train loss: 0.27748998  - Train acc: 0.8931 - Val loss: 1.00529540\n",
      "(13.83 min) Epoch 69/300 -- Iteration 65817 - Batch 333/963 - Train loss: 0.27796967  - Train acc: 0.8929 - Val loss: 1.00529540\n",
      "(13.83 min) Epoch 69/300 -- Iteration 65826 - Batch 342/963 - Train loss: 0.27745659  - Train acc: 0.8933 - Val loss: 1.00529540\n",
      "(13.84 min) Epoch 69/300 -- Iteration 65835 - Batch 351/963 - Train loss: 0.27675153  - Train acc: 0.8938 - Val loss: 1.00529540\n",
      "(13.84 min) Epoch 69/300 -- Iteration 65844 - Batch 360/963 - Train loss: 0.27547345  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.84 min) Epoch 69/300 -- Iteration 65853 - Batch 369/963 - Train loss: 0.27544362  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.84 min) Epoch 69/300 -- Iteration 65862 - Batch 378/963 - Train loss: 0.27525356  - Train acc: 0.8949 - Val loss: 1.00529540\n",
      "(13.84 min) Epoch 69/300 -- Iteration 65871 - Batch 387/963 - Train loss: 0.27513988  - Train acc: 0.8950 - Val loss: 1.00529540\n",
      "(13.85 min) Epoch 69/300 -- Iteration 65880 - Batch 396/963 - Train loss: 0.27536961  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.85 min) Epoch 69/300 -- Iteration 65889 - Batch 405/963 - Train loss: 0.27535689  - Train acc: 0.8948 - Val loss: 1.00529540\n",
      "(13.85 min) Epoch 69/300 -- Iteration 65898 - Batch 414/963 - Train loss: 0.27547854  - Train acc: 0.8944 - Val loss: 1.00529540\n",
      "(13.85 min) Epoch 69/300 -- Iteration 65907 - Batch 423/963 - Train loss: 0.27571041  - Train acc: 0.8944 - Val loss: 1.00529540\n",
      "(13.85 min) Epoch 69/300 -- Iteration 65916 - Batch 432/963 - Train loss: 0.27568647  - Train acc: 0.8944 - Val loss: 1.00529540\n",
      "(13.86 min) Epoch 69/300 -- Iteration 65925 - Batch 441/963 - Train loss: 0.27516411  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.86 min) Epoch 69/300 -- Iteration 65934 - Batch 450/963 - Train loss: 0.27565268  - Train acc: 0.8944 - Val loss: 1.00529540\n",
      "(13.86 min) Epoch 69/300 -- Iteration 65943 - Batch 459/963 - Train loss: 0.27593621  - Train acc: 0.8943 - Val loss: 1.00529540\n",
      "(13.86 min) Epoch 69/300 -- Iteration 65952 - Batch 468/963 - Train loss: 0.27637505  - Train acc: 0.8942 - Val loss: 1.00529540\n",
      "(13.86 min) Epoch 69/300 -- Iteration 65961 - Batch 477/963 - Train loss: 0.27604826  - Train acc: 0.8944 - Val loss: 1.00529540\n",
      "(13.87 min) Epoch 69/300 -- Iteration 65970 - Batch 486/963 - Train loss: 0.27623558  - Train acc: 0.8942 - Val loss: 1.00529540\n",
      "(13.87 min) Epoch 69/300 -- Iteration 65979 - Batch 495/963 - Train loss: 0.27565468  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.87 min) Epoch 69/300 -- Iteration 65988 - Batch 504/963 - Train loss: 0.27573453  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.87 min) Epoch 69/300 -- Iteration 65997 - Batch 513/963 - Train loss: 0.27535453  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.87 min) Epoch 69/300 -- Iteration 66006 - Batch 522/963 - Train loss: 0.27509394  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.87 min) Epoch 69/300 -- Iteration 66015 - Batch 531/963 - Train loss: 0.27509641  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.88 min) Epoch 69/300 -- Iteration 66024 - Batch 540/963 - Train loss: 0.27507075  - Train acc: 0.8948 - Val loss: 1.00529540\n",
      "(13.88 min) Epoch 69/300 -- Iteration 66033 - Batch 549/963 - Train loss: 0.27524530  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.88 min) Epoch 69/300 -- Iteration 66042 - Batch 558/963 - Train loss: 0.27532898  - Train acc: 0.8945 - Val loss: 1.00529540\n",
      "(13.88 min) Epoch 69/300 -- Iteration 66051 - Batch 567/963 - Train loss: 0.27598565  - Train acc: 0.8941 - Val loss: 1.00529540\n",
      "(13.88 min) Epoch 69/300 -- Iteration 66060 - Batch 576/963 - Train loss: 0.27586269  - Train acc: 0.8942 - Val loss: 1.00529540\n",
      "(13.89 min) Epoch 69/300 -- Iteration 66069 - Batch 585/963 - Train loss: 0.27586658  - Train acc: 0.8943 - Val loss: 1.00529540\n",
      "(13.89 min) Epoch 69/300 -- Iteration 66078 - Batch 594/963 - Train loss: 0.27537260  - Train acc: 0.8945 - Val loss: 1.00529540\n",
      "(13.89 min) Epoch 69/300 -- Iteration 66087 - Batch 603/963 - Train loss: 0.27549123  - Train acc: 0.8945 - Val loss: 1.00529540\n",
      "(13.89 min) Epoch 69/300 -- Iteration 66096 - Batch 612/963 - Train loss: 0.27578074  - Train acc: 0.8945 - Val loss: 1.00529540\n",
      "(13.89 min) Epoch 69/300 -- Iteration 66105 - Batch 621/963 - Train loss: 0.27589341  - Train acc: 0.8945 - Val loss: 1.00529540\n",
      "(13.90 min) Epoch 69/300 -- Iteration 66114 - Batch 630/963 - Train loss: 0.27612424  - Train acc: 0.8942 - Val loss: 1.00529540\n",
      "(13.90 min) Epoch 69/300 -- Iteration 66123 - Batch 639/963 - Train loss: 0.27568292  - Train acc: 0.8944 - Val loss: 1.00529540\n",
      "(13.90 min) Epoch 69/300 -- Iteration 66132 - Batch 648/963 - Train loss: 0.27600144  - Train acc: 0.8944 - Val loss: 1.00529540\n",
      "(13.90 min) Epoch 69/300 -- Iteration 66141 - Batch 657/963 - Train loss: 0.27547667  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.90 min) Epoch 69/300 -- Iteration 66150 - Batch 666/963 - Train loss: 0.27567832  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.90 min) Epoch 69/300 -- Iteration 66159 - Batch 675/963 - Train loss: 0.27557389  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.91 min) Epoch 69/300 -- Iteration 66168 - Batch 684/963 - Train loss: 0.27558705  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.91 min) Epoch 69/300 -- Iteration 66177 - Batch 693/963 - Train loss: 0.27560945  - Train acc: 0.8945 - Val loss: 1.00529540\n",
      "(13.91 min) Epoch 69/300 -- Iteration 66186 - Batch 702/963 - Train loss: 0.27554387  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.91 min) Epoch 69/300 -- Iteration 66195 - Batch 711/963 - Train loss: 0.27541436  - Train acc: 0.8948 - Val loss: 1.00529540\n",
      "(13.91 min) Epoch 69/300 -- Iteration 66204 - Batch 720/963 - Train loss: 0.27569196  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.92 min) Epoch 69/300 -- Iteration 66213 - Batch 729/963 - Train loss: 0.27543920  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.92 min) Epoch 69/300 -- Iteration 66222 - Batch 738/963 - Train loss: 0.27577244  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.92 min) Epoch 69/300 -- Iteration 66231 - Batch 747/963 - Train loss: 0.27591699  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.92 min) Epoch 69/300 -- Iteration 66240 - Batch 756/963 - Train loss: 0.27611960  - Train acc: 0.8945 - Val loss: 1.00529540\n",
      "(13.92 min) Epoch 69/300 -- Iteration 66249 - Batch 765/963 - Train loss: 0.27631598  - Train acc: 0.8945 - Val loss: 1.00529540\n",
      "(13.93 min) Epoch 69/300 -- Iteration 66258 - Batch 774/963 - Train loss: 0.27627278  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.93 min) Epoch 69/300 -- Iteration 66267 - Batch 783/963 - Train loss: 0.27620552  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.93 min) Epoch 69/300 -- Iteration 66276 - Batch 792/963 - Train loss: 0.27602365  - Train acc: 0.8946 - Val loss: 1.00529540\n",
      "(13.93 min) Epoch 69/300 -- Iteration 66285 - Batch 801/963 - Train loss: 0.27586060  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.93 min) Epoch 69/300 -- Iteration 66294 - Batch 810/963 - Train loss: 0.27583858  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.93 min) Epoch 69/300 -- Iteration 66303 - Batch 819/963 - Train loss: 0.27575940  - Train acc: 0.8948 - Val loss: 1.00529540\n",
      "(13.94 min) Epoch 69/300 -- Iteration 66312 - Batch 828/963 - Train loss: 0.27572831  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.94 min) Epoch 69/300 -- Iteration 66321 - Batch 837/963 - Train loss: 0.27546915  - Train acc: 0.8948 - Val loss: 1.00529540\n",
      "(13.94 min) Epoch 69/300 -- Iteration 66330 - Batch 846/963 - Train loss: 0.27550639  - Train acc: 0.8948 - Val loss: 1.00529540\n",
      "(13.94 min) Epoch 69/300 -- Iteration 66339 - Batch 855/963 - Train loss: 0.27568757  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.94 min) Epoch 69/300 -- Iteration 66348 - Batch 864/963 - Train loss: 0.27566862  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.95 min) Epoch 69/300 -- Iteration 66357 - Batch 873/963 - Train loss: 0.27569937  - Train acc: 0.8948 - Val loss: 1.00529540\n",
      "(13.95 min) Epoch 69/300 -- Iteration 66366 - Batch 882/963 - Train loss: 0.27583970  - Train acc: 0.8947 - Val loss: 1.00529540\n",
      "(13.95 min) Epoch 69/300 -- Iteration 66375 - Batch 891/963 - Train loss: 0.27563212  - Train acc: 0.8949 - Val loss: 1.00529540\n",
      "(13.95 min) Epoch 69/300 -- Iteration 66384 - Batch 900/963 - Train loss: 0.27571360  - Train acc: 0.8948 - Val loss: 1.00529540\n",
      "(13.95 min) Epoch 69/300 -- Iteration 66393 - Batch 909/963 - Train loss: 0.27547009  - Train acc: 0.8950 - Val loss: 1.00529540\n",
      "(13.96 min) Epoch 69/300 -- Iteration 66402 - Batch 918/963 - Train loss: 0.27548965  - Train acc: 0.8951 - Val loss: 1.00529540\n",
      "(13.96 min) Epoch 69/300 -- Iteration 66411 - Batch 927/963 - Train loss: 0.27536289  - Train acc: 0.8952 - Val loss: 1.00529540\n",
      "(13.96 min) Epoch 69/300 -- Iteration 66420 - Batch 936/963 - Train loss: 0.27581864  - Train acc: 0.8950 - Val loss: 1.00529540\n",
      "(13.96 min) Epoch 69/300 -- Iteration 66429 - Batch 945/963 - Train loss: 0.27584642  - Train acc: 0.8950 - Val loss: 1.00529540\n",
      "(13.96 min) Epoch 69/300 -- Iteration 66438 - Batch 954/963 - Train loss: 0.27578263  - Train acc: 0.8950 - Val loss: 1.00529540\n",
      "(13.97 min) Epoch 69/300 -- Iteration 66447 - Batch 962/963 - Train loss: 0.27566042  - Train acc: 0.8950 - Val loss: 1.01785517 - Val acc: 0.5917\n",
      "(13.97 min) Epoch 70/300 -- Iteration 66456 - Batch 9/963 - Train loss: 0.26870209  - Train acc: 0.9055 - Val loss: 1.01785517\n",
      "(13.97 min) Epoch 70/300 -- Iteration 66465 - Batch 18/963 - Train loss: 0.26503789  - Train acc: 0.9021 - Val loss: 1.01785517\n",
      "(13.97 min) Epoch 70/300 -- Iteration 66474 - Batch 27/963 - Train loss: 0.26054232  - Train acc: 0.9037 - Val loss: 1.01785517\n",
      "(13.97 min) Epoch 70/300 -- Iteration 66483 - Batch 36/963 - Train loss: 0.26700071  - Train acc: 0.9008 - Val loss: 1.01785517\n",
      "(13.98 min) Epoch 70/300 -- Iteration 66492 - Batch 45/963 - Train loss: 0.27121423  - Train acc: 0.8983 - Val loss: 1.01785517\n",
      "(13.98 min) Epoch 70/300 -- Iteration 66501 - Batch 54/963 - Train loss: 0.27522171  - Train acc: 0.8969 - Val loss: 1.01785517\n",
      "(13.98 min) Epoch 70/300 -- Iteration 66510 - Batch 63/963 - Train loss: 0.27387630  - Train acc: 0.8966 - Val loss: 1.01785517\n",
      "(13.98 min) Epoch 70/300 -- Iteration 66519 - Batch 72/963 - Train loss: 0.27775614  - Train acc: 0.8952 - Val loss: 1.01785517\n",
      "(13.98 min) Epoch 70/300 -- Iteration 66528 - Batch 81/963 - Train loss: 0.27477763  - Train acc: 0.8975 - Val loss: 1.01785517\n",
      "(13.98 min) Epoch 70/300 -- Iteration 66537 - Batch 90/963 - Train loss: 0.27249556  - Train acc: 0.8990 - Val loss: 1.01785517\n",
      "(13.99 min) Epoch 70/300 -- Iteration 66546 - Batch 99/963 - Train loss: 0.27340715  - Train acc: 0.8980 - Val loss: 1.01785517\n",
      "(13.99 min) Epoch 70/300 -- Iteration 66555 - Batch 108/963 - Train loss: 0.27308010  - Train acc: 0.8978 - Val loss: 1.01785517\n",
      "(13.99 min) Epoch 70/300 -- Iteration 66564 - Batch 117/963 - Train loss: 0.27489223  - Train acc: 0.8969 - Val loss: 1.01785517\n",
      "(13.99 min) Epoch 70/300 -- Iteration 66573 - Batch 126/963 - Train loss: 0.27648757  - Train acc: 0.8961 - Val loss: 1.01785517\n",
      "(13.99 min) Epoch 70/300 -- Iteration 66582 - Batch 135/963 - Train loss: 0.27454664  - Train acc: 0.8970 - Val loss: 1.01785517\n",
      "(14.00 min) Epoch 70/300 -- Iteration 66591 - Batch 144/963 - Train loss: 0.27375134  - Train acc: 0.8974 - Val loss: 1.01785517\n",
      "(14.00 min) Epoch 70/300 -- Iteration 66600 - Batch 153/963 - Train loss: 0.27396293  - Train acc: 0.8966 - Val loss: 1.01785517\n",
      "(14.00 min) Epoch 70/300 -- Iteration 66609 - Batch 162/963 - Train loss: 0.27267254  - Train acc: 0.8969 - Val loss: 1.01785517\n",
      "(14.00 min) Epoch 70/300 -- Iteration 66618 - Batch 171/963 - Train loss: 0.27256317  - Train acc: 0.8969 - Val loss: 1.01785517\n",
      "(14.00 min) Epoch 70/300 -- Iteration 66627 - Batch 180/963 - Train loss: 0.27271567  - Train acc: 0.8971 - Val loss: 1.01785517\n",
      "(14.01 min) Epoch 70/300 -- Iteration 66636 - Batch 189/963 - Train loss: 0.27161570  - Train acc: 0.8975 - Val loss: 1.01785517\n",
      "(14.01 min) Epoch 70/300 -- Iteration 66645 - Batch 198/963 - Train loss: 0.27085953  - Train acc: 0.8979 - Val loss: 1.01785517\n",
      "(14.01 min) Epoch 70/300 -- Iteration 66654 - Batch 207/963 - Train loss: 0.27027001  - Train acc: 0.8980 - Val loss: 1.01785517\n",
      "(14.01 min) Epoch 70/300 -- Iteration 66663 - Batch 216/963 - Train loss: 0.27026519  - Train acc: 0.8981 - Val loss: 1.01785517\n",
      "(14.01 min) Epoch 70/300 -- Iteration 66672 - Batch 225/963 - Train loss: 0.26992231  - Train acc: 0.8981 - Val loss: 1.01785517\n",
      "(14.01 min) Epoch 70/300 -- Iteration 66681 - Batch 234/963 - Train loss: 0.27002063  - Train acc: 0.8980 - Val loss: 1.01785517\n",
      "(14.02 min) Epoch 70/300 -- Iteration 66690 - Batch 243/963 - Train loss: 0.26988331  - Train acc: 0.8982 - Val loss: 1.01785517\n",
      "(14.02 min) Epoch 70/300 -- Iteration 66699 - Batch 252/963 - Train loss: 0.26857307  - Train acc: 0.8991 - Val loss: 1.01785517\n",
      "(14.02 min) Epoch 70/300 -- Iteration 66708 - Batch 261/963 - Train loss: 0.26821652  - Train acc: 0.8992 - Val loss: 1.01785517\n",
      "(14.02 min) Epoch 70/300 -- Iteration 66717 - Batch 270/963 - Train loss: 0.26891247  - Train acc: 0.8988 - Val loss: 1.01785517\n",
      "(14.02 min) Epoch 70/300 -- Iteration 66726 - Batch 279/963 - Train loss: 0.26889057  - Train acc: 0.8987 - Val loss: 1.01785517\n",
      "(14.03 min) Epoch 70/300 -- Iteration 66735 - Batch 288/963 - Train loss: 0.26894776  - Train acc: 0.8988 - Val loss: 1.01785517\n",
      "(14.03 min) Epoch 70/300 -- Iteration 66744 - Batch 297/963 - Train loss: 0.26907916  - Train acc: 0.8988 - Val loss: 1.01785517\n",
      "(14.03 min) Epoch 70/300 -- Iteration 66753 - Batch 306/963 - Train loss: 0.26915786  - Train acc: 0.8988 - Val loss: 1.01785517\n",
      "(14.03 min) Epoch 70/300 -- Iteration 66762 - Batch 315/963 - Train loss: 0.26944398  - Train acc: 0.8986 - Val loss: 1.01785517\n",
      "(14.03 min) Epoch 70/300 -- Iteration 66771 - Batch 324/963 - Train loss: 0.26981609  - Train acc: 0.8986 - Val loss: 1.01785517\n",
      "(14.04 min) Epoch 70/300 -- Iteration 66780 - Batch 333/963 - Train loss: 0.27007530  - Train acc: 0.8985 - Val loss: 1.01785517\n",
      "(14.04 min) Epoch 70/300 -- Iteration 66789 - Batch 342/963 - Train loss: 0.27001933  - Train acc: 0.8985 - Val loss: 1.01785517\n",
      "(14.04 min) Epoch 70/300 -- Iteration 66798 - Batch 351/963 - Train loss: 0.27000573  - Train acc: 0.8984 - Val loss: 1.01785517\n",
      "(14.04 min) Epoch 70/300 -- Iteration 66807 - Batch 360/963 - Train loss: 0.27015359  - Train acc: 0.8985 - Val loss: 1.01785517\n",
      "(14.04 min) Epoch 70/300 -- Iteration 66816 - Batch 369/963 - Train loss: 0.27045042  - Train acc: 0.8986 - Val loss: 1.01785517\n",
      "(14.04 min) Epoch 70/300 -- Iteration 66825 - Batch 378/963 - Train loss: 0.27008257  - Train acc: 0.8989 - Val loss: 1.01785517\n",
      "(14.05 min) Epoch 70/300 -- Iteration 66834 - Batch 387/963 - Train loss: 0.27110102  - Train acc: 0.8984 - Val loss: 1.01785517\n",
      "(14.05 min) Epoch 70/300 -- Iteration 66843 - Batch 396/963 - Train loss: 0.27166145  - Train acc: 0.8982 - Val loss: 1.01785517\n",
      "(14.05 min) Epoch 70/300 -- Iteration 66852 - Batch 405/963 - Train loss: 0.27190916  - Train acc: 0.8981 - Val loss: 1.01785517\n",
      "(14.05 min) Epoch 70/300 -- Iteration 66861 - Batch 414/963 - Train loss: 0.27197586  - Train acc: 0.8980 - Val loss: 1.01785517\n",
      "(14.05 min) Epoch 70/300 -- Iteration 66870 - Batch 423/963 - Train loss: 0.27210965  - Train acc: 0.8978 - Val loss: 1.01785517\n",
      "(14.06 min) Epoch 70/300 -- Iteration 66879 - Batch 432/963 - Train loss: 0.27222963  - Train acc: 0.8978 - Val loss: 1.01785517\n",
      "(14.06 min) Epoch 70/300 -- Iteration 66888 - Batch 441/963 - Train loss: 0.27181855  - Train acc: 0.8979 - Val loss: 1.01785517\n",
      "(14.06 min) Epoch 70/300 -- Iteration 66897 - Batch 450/963 - Train loss: 0.27180452  - Train acc: 0.8978 - Val loss: 1.01785517\n",
      "(14.06 min) Epoch 70/300 -- Iteration 66906 - Batch 459/963 - Train loss: 0.27200125  - Train acc: 0.8977 - Val loss: 1.01785517\n",
      "(14.06 min) Epoch 70/300 -- Iteration 66915 - Batch 468/963 - Train loss: 0.27256454  - Train acc: 0.8974 - Val loss: 1.01785517\n",
      "(14.07 min) Epoch 70/300 -- Iteration 66924 - Batch 477/963 - Train loss: 0.27284283  - Train acc: 0.8972 - Val loss: 1.01785517\n",
      "(14.07 min) Epoch 70/300 -- Iteration 66933 - Batch 486/963 - Train loss: 0.27296507  - Train acc: 0.8972 - Val loss: 1.01785517\n",
      "(14.07 min) Epoch 70/300 -- Iteration 66942 - Batch 495/963 - Train loss: 0.27288672  - Train acc: 0.8969 - Val loss: 1.01785517\n",
      "(14.07 min) Epoch 70/300 -- Iteration 66951 - Batch 504/963 - Train loss: 0.27308941  - Train acc: 0.8968 - Val loss: 1.01785517\n",
      "(14.07 min) Epoch 70/300 -- Iteration 66960 - Batch 513/963 - Train loss: 0.27319334  - Train acc: 0.8968 - Val loss: 1.01785517\n",
      "(14.07 min) Epoch 70/300 -- Iteration 66969 - Batch 522/963 - Train loss: 0.27330907  - Train acc: 0.8967 - Val loss: 1.01785517\n",
      "(14.08 min) Epoch 70/300 -- Iteration 66978 - Batch 531/963 - Train loss: 0.27338029  - Train acc: 0.8967 - Val loss: 1.01785517\n",
      "(14.08 min) Epoch 70/300 -- Iteration 66987 - Batch 540/963 - Train loss: 0.27345402  - Train acc: 0.8966 - Val loss: 1.01785517\n",
      "(14.08 min) Epoch 70/300 -- Iteration 66996 - Batch 549/963 - Train loss: 0.27352663  - Train acc: 0.8965 - Val loss: 1.01785517\n",
      "(14.08 min) Epoch 70/300 -- Iteration 67005 - Batch 558/963 - Train loss: 0.27380639  - Train acc: 0.8964 - Val loss: 1.01785517\n",
      "(14.08 min) Epoch 70/300 -- Iteration 67014 - Batch 567/963 - Train loss: 0.27368060  - Train acc: 0.8963 - Val loss: 1.01785517\n",
      "(14.09 min) Epoch 70/300 -- Iteration 67023 - Batch 576/963 - Train loss: 0.27382791  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.09 min) Epoch 70/300 -- Iteration 67032 - Batch 585/963 - Train loss: 0.27370504  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.09 min) Epoch 70/300 -- Iteration 67041 - Batch 594/963 - Train loss: 0.27397554  - Train acc: 0.8963 - Val loss: 1.01785517\n",
      "(14.09 min) Epoch 70/300 -- Iteration 67050 - Batch 603/963 - Train loss: 0.27403898  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.09 min) Epoch 70/300 -- Iteration 67059 - Batch 612/963 - Train loss: 0.27402955  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.10 min) Epoch 70/300 -- Iteration 67068 - Batch 621/963 - Train loss: 0.27424025  - Train acc: 0.8960 - Val loss: 1.01785517\n",
      "(14.10 min) Epoch 70/300 -- Iteration 67077 - Batch 630/963 - Train loss: 0.27421653  - Train acc: 0.8961 - Val loss: 1.01785517\n",
      "(14.10 min) Epoch 70/300 -- Iteration 67086 - Batch 639/963 - Train loss: 0.27441278  - Train acc: 0.8960 - Val loss: 1.01785517\n",
      "(14.10 min) Epoch 70/300 -- Iteration 67095 - Batch 648/963 - Train loss: 0.27439009  - Train acc: 0.8960 - Val loss: 1.01785517\n",
      "(14.10 min) Epoch 70/300 -- Iteration 67104 - Batch 657/963 - Train loss: 0.27432829  - Train acc: 0.8960 - Val loss: 1.01785517\n",
      "(14.10 min) Epoch 70/300 -- Iteration 67113 - Batch 666/963 - Train loss: 0.27408884  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.11 min) Epoch 70/300 -- Iteration 67122 - Batch 675/963 - Train loss: 0.27399798  - Train acc: 0.8961 - Val loss: 1.01785517\n",
      "(14.11 min) Epoch 70/300 -- Iteration 67131 - Batch 684/963 - Train loss: 0.27384421  - Train acc: 0.8961 - Val loss: 1.01785517\n",
      "(14.11 min) Epoch 70/300 -- Iteration 67140 - Batch 693/963 - Train loss: 0.27371876  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.11 min) Epoch 70/300 -- Iteration 67149 - Batch 702/963 - Train loss: 0.27374738  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.11 min) Epoch 70/300 -- Iteration 67158 - Batch 711/963 - Train loss: 0.27379092  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.12 min) Epoch 70/300 -- Iteration 67167 - Batch 720/963 - Train loss: 0.27335749  - Train acc: 0.8963 - Val loss: 1.01785517\n",
      "(14.12 min) Epoch 70/300 -- Iteration 67176 - Batch 729/963 - Train loss: 0.27369428  - Train acc: 0.8963 - Val loss: 1.01785517\n",
      "(14.12 min) Epoch 70/300 -- Iteration 67185 - Batch 738/963 - Train loss: 0.27360384  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.12 min) Epoch 70/300 -- Iteration 67194 - Batch 747/963 - Train loss: 0.27353210  - Train acc: 0.8962 - Val loss: 1.01785517\n",
      "(14.12 min) Epoch 70/300 -- Iteration 67203 - Batch 756/963 - Train loss: 0.27367078  - Train acc: 0.8961 - Val loss: 1.01785517\n",
      "(14.13 min) Epoch 70/300 -- Iteration 67212 - Batch 765/963 - Train loss: 0.27402284  - Train acc: 0.8960 - Val loss: 1.01785517\n",
      "(14.13 min) Epoch 70/300 -- Iteration 67221 - Batch 774/963 - Train loss: 0.27412617  - Train acc: 0.8959 - Val loss: 1.01785517\n",
      "(14.13 min) Epoch 70/300 -- Iteration 67230 - Batch 783/963 - Train loss: 0.27397554  - Train acc: 0.8960 - Val loss: 1.01785517\n",
      "(14.13 min) Epoch 70/300 -- Iteration 67239 - Batch 792/963 - Train loss: 0.27384335  - Train acc: 0.8960 - Val loss: 1.01785517\n",
      "(14.13 min) Epoch 70/300 -- Iteration 67248 - Batch 801/963 - Train loss: 0.27399325  - Train acc: 0.8960 - Val loss: 1.01785517\n",
      "(14.13 min) Epoch 70/300 -- Iteration 67257 - Batch 810/963 - Train loss: 0.27416453  - Train acc: 0.8959 - Val loss: 1.01785517\n",
      "(14.14 min) Epoch 70/300 -- Iteration 67266 - Batch 819/963 - Train loss: 0.27417971  - Train acc: 0.8959 - Val loss: 1.01785517\n",
      "(14.14 min) Epoch 70/300 -- Iteration 67275 - Batch 828/963 - Train loss: 0.27425182  - Train acc: 0.8959 - Val loss: 1.01785517\n",
      "(14.14 min) Epoch 70/300 -- Iteration 67284 - Batch 837/963 - Train loss: 0.27469470  - Train acc: 0.8957 - Val loss: 1.01785517\n",
      "(14.14 min) Epoch 70/300 -- Iteration 67293 - Batch 846/963 - Train loss: 0.27474469  - Train acc: 0.8957 - Val loss: 1.01785517\n",
      "(14.14 min) Epoch 70/300 -- Iteration 67302 - Batch 855/963 - Train loss: 0.27451798  - Train acc: 0.8958 - Val loss: 1.01785517\n",
      "(14.15 min) Epoch 70/300 -- Iteration 67311 - Batch 864/963 - Train loss: 0.27469470  - Train acc: 0.8956 - Val loss: 1.01785517\n",
      "(14.15 min) Epoch 70/300 -- Iteration 67320 - Batch 873/963 - Train loss: 0.27464228  - Train acc: 0.8956 - Val loss: 1.01785517\n",
      "(14.15 min) Epoch 70/300 -- Iteration 67329 - Batch 882/963 - Train loss: 0.27444124  - Train acc: 0.8957 - Val loss: 1.01785517\n",
      "(14.15 min) Epoch 70/300 -- Iteration 67338 - Batch 891/963 - Train loss: 0.27440065  - Train acc: 0.8958 - Val loss: 1.01785517\n",
      "(14.15 min) Epoch 70/300 -- Iteration 67347 - Batch 900/963 - Train loss: 0.27412481  - Train acc: 0.8958 - Val loss: 1.01785517\n",
      "(14.16 min) Epoch 70/300 -- Iteration 67356 - Batch 909/963 - Train loss: 0.27417686  - Train acc: 0.8959 - Val loss: 1.01785517\n",
      "(14.16 min) Epoch 70/300 -- Iteration 67365 - Batch 918/963 - Train loss: 0.27439785  - Train acc: 0.8958 - Val loss: 1.01785517\n",
      "(14.16 min) Epoch 70/300 -- Iteration 67374 - Batch 927/963 - Train loss: 0.27418403  - Train acc: 0.8958 - Val loss: 1.01785517\n",
      "(14.16 min) Epoch 70/300 -- Iteration 67383 - Batch 936/963 - Train loss: 0.27440552  - Train acc: 0.8956 - Val loss: 1.01785517\n",
      "(14.16 min) Epoch 70/300 -- Iteration 67392 - Batch 945/963 - Train loss: 0.27458986  - Train acc: 0.8955 - Val loss: 1.01785517\n",
      "(14.17 min) Epoch 70/300 -- Iteration 67401 - Batch 954/963 - Train loss: 0.27457641  - Train acc: 0.8955 - Val loss: 1.01785517\n",
      "(14.17 min) Epoch 70/300 -- Iteration 67410 - Batch 962/963 - Train loss: 0.27456534  - Train acc: 0.8954 - Val loss: 1.01362765 - Val acc: 0.5950\n",
      "(14.17 min) Epoch 71/300 -- Iteration 67419 - Batch 9/963 - Train loss: 0.23435488  - Train acc: 0.9055 - Val loss: 1.01362765\n",
      "(14.17 min) Epoch 71/300 -- Iteration 67428 - Batch 18/963 - Train loss: 0.25733686  - Train acc: 0.8984 - Val loss: 1.01362765\n",
      "(14.17 min) Epoch 71/300 -- Iteration 67437 - Batch 27/963 - Train loss: 0.25779696  - Train acc: 0.9035 - Val loss: 1.01362765\n",
      "(14.18 min) Epoch 71/300 -- Iteration 67446 - Batch 36/963 - Train loss: 0.25851913  - Train acc: 0.9031 - Val loss: 1.01362765\n",
      "(14.18 min) Epoch 71/300 -- Iteration 67455 - Batch 45/963 - Train loss: 0.25535882  - Train acc: 0.9035 - Val loss: 1.01362765\n",
      "(14.18 min) Epoch 71/300 -- Iteration 67464 - Batch 54/963 - Train loss: 0.25738768  - Train acc: 0.9033 - Val loss: 1.01362765\n",
      "(14.18 min) Epoch 71/300 -- Iteration 67473 - Batch 63/963 - Train loss: 0.25756713  - Train acc: 0.9023 - Val loss: 1.01362765\n",
      "(14.18 min) Epoch 71/300 -- Iteration 67482 - Batch 72/963 - Train loss: 0.26261378  - Train acc: 0.8996 - Val loss: 1.01362765\n",
      "(14.18 min) Epoch 71/300 -- Iteration 67491 - Batch 81/963 - Train loss: 0.26476391  - Train acc: 0.8984 - Val loss: 1.01362765\n",
      "(14.19 min) Epoch 71/300 -- Iteration 67500 - Batch 90/963 - Train loss: 0.26192326  - Train acc: 0.9004 - Val loss: 1.01362765\n",
      "(14.19 min) Epoch 71/300 -- Iteration 67509 - Batch 99/963 - Train loss: 0.26617862  - Train acc: 0.8981 - Val loss: 1.01362765\n",
      "(14.19 min) Epoch 71/300 -- Iteration 67518 - Batch 108/963 - Train loss: 0.26396556  - Train acc: 0.8993 - Val loss: 1.01362765\n",
      "(14.19 min) Epoch 71/300 -- Iteration 67527 - Batch 117/963 - Train loss: 0.26376479  - Train acc: 0.8997 - Val loss: 1.01362765\n",
      "(14.19 min) Epoch 71/300 -- Iteration 67536 - Batch 126/963 - Train loss: 0.26498102  - Train acc: 0.8989 - Val loss: 1.01362765\n",
      "(14.20 min) Epoch 71/300 -- Iteration 67545 - Batch 135/963 - Train loss: 0.26526367  - Train acc: 0.8987 - Val loss: 1.01362765\n",
      "(14.20 min) Epoch 71/300 -- Iteration 67554 - Batch 144/963 - Train loss: 0.26514615  - Train acc: 0.8988 - Val loss: 1.01362765\n",
      "(14.20 min) Epoch 71/300 -- Iteration 67563 - Batch 153/963 - Train loss: 0.26354513  - Train acc: 0.8992 - Val loss: 1.01362765\n",
      "(14.20 min) Epoch 71/300 -- Iteration 67572 - Batch 162/963 - Train loss: 0.26407708  - Train acc: 0.8990 - Val loss: 1.01362765\n",
      "(14.20 min) Epoch 71/300 -- Iteration 67581 - Batch 171/963 - Train loss: 0.26516166  - Train acc: 0.8984 - Val loss: 1.01362765\n",
      "(14.20 min) Epoch 71/300 -- Iteration 67590 - Batch 180/963 - Train loss: 0.26532945  - Train acc: 0.8981 - Val loss: 1.01362765\n",
      "(14.21 min) Epoch 71/300 -- Iteration 67599 - Batch 189/963 - Train loss: 0.26312786  - Train acc: 0.8988 - Val loss: 1.01362765\n",
      "(14.21 min) Epoch 71/300 -- Iteration 67608 - Batch 198/963 - Train loss: 0.26409573  - Train acc: 0.8982 - Val loss: 1.01362765\n",
      "(14.21 min) Epoch 71/300 -- Iteration 67617 - Batch 207/963 - Train loss: 0.26495964  - Train acc: 0.8979 - Val loss: 1.01362765\n",
      "(14.21 min) Epoch 71/300 -- Iteration 67626 - Batch 216/963 - Train loss: 0.26417206  - Train acc: 0.8983 - Val loss: 1.01362765\n",
      "(14.21 min) Epoch 71/300 -- Iteration 67635 - Batch 225/963 - Train loss: 0.26343585  - Train acc: 0.8986 - Val loss: 1.01362765\n",
      "(14.22 min) Epoch 71/300 -- Iteration 67644 - Batch 234/963 - Train loss: 0.26427154  - Train acc: 0.8984 - Val loss: 1.01362765\n",
      "(14.22 min) Epoch 71/300 -- Iteration 67653 - Batch 243/963 - Train loss: 0.26503351  - Train acc: 0.8982 - Val loss: 1.01362765\n",
      "(14.22 min) Epoch 71/300 -- Iteration 67662 - Batch 252/963 - Train loss: 0.26424188  - Train acc: 0.8987 - Val loss: 1.01362765\n",
      "(14.22 min) Epoch 71/300 -- Iteration 67671 - Batch 261/963 - Train loss: 0.26383878  - Train acc: 0.8987 - Val loss: 1.01362765\n",
      "(14.22 min) Epoch 71/300 -- Iteration 67680 - Batch 270/963 - Train loss: 0.26407401  - Train acc: 0.8986 - Val loss: 1.01362765\n",
      "(14.23 min) Epoch 71/300 -- Iteration 67689 - Batch 279/963 - Train loss: 0.26444041  - Train acc: 0.8982 - Val loss: 1.01362765\n",
      "(14.23 min) Epoch 71/300 -- Iteration 67698 - Batch 288/963 - Train loss: 0.26466837  - Train acc: 0.8981 - Val loss: 1.01362765\n",
      "(14.23 min) Epoch 71/300 -- Iteration 67707 - Batch 297/963 - Train loss: 0.26461886  - Train acc: 0.8982 - Val loss: 1.01362765\n",
      "(14.23 min) Epoch 71/300 -- Iteration 67716 - Batch 306/963 - Train loss: 0.26562668  - Train acc: 0.8978 - Val loss: 1.01362765\n",
      "(14.23 min) Epoch 71/300 -- Iteration 67725 - Batch 315/963 - Train loss: 0.26528692  - Train acc: 0.8981 - Val loss: 1.01362765\n",
      "(14.24 min) Epoch 71/300 -- Iteration 67734 - Batch 324/963 - Train loss: 0.26536233  - Train acc: 0.8979 - Val loss: 1.01362765\n",
      "(14.24 min) Epoch 71/300 -- Iteration 67743 - Batch 333/963 - Train loss: 0.26480532  - Train acc: 0.8979 - Val loss: 1.01362765\n",
      "(14.24 min) Epoch 71/300 -- Iteration 67752 - Batch 342/963 - Train loss: 0.26487010  - Train acc: 0.8978 - Val loss: 1.01362765\n",
      "(14.24 min) Epoch 71/300 -- Iteration 67761 - Batch 351/963 - Train loss: 0.26484908  - Train acc: 0.8977 - Val loss: 1.01362765\n",
      "(14.24 min) Epoch 71/300 -- Iteration 67770 - Batch 360/963 - Train loss: 0.26453205  - Train acc: 0.8978 - Val loss: 1.01362765\n",
      "(14.24 min) Epoch 71/300 -- Iteration 67779 - Batch 369/963 - Train loss: 0.26423455  - Train acc: 0.8982 - Val loss: 1.01362765\n",
      "(14.25 min) Epoch 71/300 -- Iteration 67788 - Batch 378/963 - Train loss: 0.26445147  - Train acc: 0.8981 - Val loss: 1.01362765\n",
      "(14.25 min) Epoch 71/300 -- Iteration 67797 - Batch 387/963 - Train loss: 0.26461088  - Train acc: 0.8982 - Val loss: 1.01362765\n",
      "(14.25 min) Epoch 71/300 -- Iteration 67806 - Batch 396/963 - Train loss: 0.26456289  - Train acc: 0.8982 - Val loss: 1.01362765\n",
      "(14.25 min) Epoch 71/300 -- Iteration 67815 - Batch 405/963 - Train loss: 0.26529479  - Train acc: 0.8978 - Val loss: 1.01362765\n",
      "(14.25 min) Epoch 71/300 -- Iteration 67824 - Batch 414/963 - Train loss: 0.26495353  - Train acc: 0.8980 - Val loss: 1.01362765\n",
      "(14.26 min) Epoch 71/300 -- Iteration 67833 - Batch 423/963 - Train loss: 0.26551214  - Train acc: 0.8979 - Val loss: 1.01362765\n",
      "(14.26 min) Epoch 71/300 -- Iteration 67842 - Batch 432/963 - Train loss: 0.26587539  - Train acc: 0.8978 - Val loss: 1.01362765\n",
      "(14.26 min) Epoch 71/300 -- Iteration 67851 - Batch 441/963 - Train loss: 0.26630728  - Train acc: 0.8977 - Val loss: 1.01362765\n",
      "(14.26 min) Epoch 71/300 -- Iteration 67860 - Batch 450/963 - Train loss: 0.26671650  - Train acc: 0.8975 - Val loss: 1.01362765\n",
      "(14.26 min) Epoch 71/300 -- Iteration 67869 - Batch 459/963 - Train loss: 0.26671846  - Train acc: 0.8975 - Val loss: 1.01362765\n",
      "(14.26 min) Epoch 71/300 -- Iteration 67878 - Batch 468/963 - Train loss: 0.26736584  - Train acc: 0.8971 - Val loss: 1.01362765\n",
      "(14.27 min) Epoch 71/300 -- Iteration 67887 - Batch 477/963 - Train loss: 0.26736195  - Train acc: 0.8972 - Val loss: 1.01362765\n",
      "(14.27 min) Epoch 71/300 -- Iteration 67896 - Batch 486/963 - Train loss: 0.26696806  - Train acc: 0.8972 - Val loss: 1.01362765\n",
      "(14.27 min) Epoch 71/300 -- Iteration 67905 - Batch 495/963 - Train loss: 0.26743248  - Train acc: 0.8971 - Val loss: 1.01362765\n",
      "(14.27 min) Epoch 71/300 -- Iteration 67914 - Batch 504/963 - Train loss: 0.26759211  - Train acc: 0.8970 - Val loss: 1.01362765\n",
      "(14.27 min) Epoch 71/300 -- Iteration 67923 - Batch 513/963 - Train loss: 0.26743984  - Train acc: 0.8971 - Val loss: 1.01362765\n",
      "(14.28 min) Epoch 71/300 -- Iteration 67932 - Batch 522/963 - Train loss: 0.26714255  - Train acc: 0.8972 - Val loss: 1.01362765\n",
      "(14.28 min) Epoch 71/300 -- Iteration 67941 - Batch 531/963 - Train loss: 0.26703447  - Train acc: 0.8972 - Val loss: 1.01362765\n",
      "(14.28 min) Epoch 71/300 -- Iteration 67950 - Batch 540/963 - Train loss: 0.26709152  - Train acc: 0.8972 - Val loss: 1.01362765\n",
      "(14.28 min) Epoch 71/300 -- Iteration 67959 - Batch 549/963 - Train loss: 0.26704639  - Train acc: 0.8971 - Val loss: 1.01362765\n",
      "(14.28 min) Epoch 71/300 -- Iteration 67968 - Batch 558/963 - Train loss: 0.26740929  - Train acc: 0.8971 - Val loss: 1.01362765\n",
      "(14.29 min) Epoch 71/300 -- Iteration 67977 - Batch 567/963 - Train loss: 0.26759662  - Train acc: 0.8971 - Val loss: 1.01362765\n",
      "(14.29 min) Epoch 71/300 -- Iteration 67986 - Batch 576/963 - Train loss: 0.26750595  - Train acc: 0.8971 - Val loss: 1.01362765\n",
      "(14.29 min) Epoch 71/300 -- Iteration 67995 - Batch 585/963 - Train loss: 0.26772199  - Train acc: 0.8972 - Val loss: 1.01362765\n",
      "(14.29 min) Epoch 71/300 -- Iteration 68004 - Batch 594/963 - Train loss: 0.26847633  - Train acc: 0.8969 - Val loss: 1.01362765\n",
      "(14.29 min) Epoch 71/300 -- Iteration 68013 - Batch 603/963 - Train loss: 0.26896757  - Train acc: 0.8967 - Val loss: 1.01362765\n",
      "(14.30 min) Epoch 71/300 -- Iteration 68022 - Batch 612/963 - Train loss: 0.26901419  - Train acc: 0.8967 - Val loss: 1.01362765\n",
      "(14.30 min) Epoch 71/300 -- Iteration 68031 - Batch 621/963 - Train loss: 0.26922437  - Train acc: 0.8966 - Val loss: 1.01362765\n",
      "(14.30 min) Epoch 71/300 -- Iteration 68040 - Batch 630/963 - Train loss: 0.26944260  - Train acc: 0.8965 - Val loss: 1.01362765\n",
      "(14.30 min) Epoch 71/300 -- Iteration 68049 - Batch 639/963 - Train loss: 0.26929064  - Train acc: 0.8965 - Val loss: 1.01362765\n",
      "(14.30 min) Epoch 71/300 -- Iteration 68058 - Batch 648/963 - Train loss: 0.26898022  - Train acc: 0.8965 - Val loss: 1.01362765\n",
      "(14.30 min) Epoch 71/300 -- Iteration 68067 - Batch 657/963 - Train loss: 0.26907630  - Train acc: 0.8964 - Val loss: 1.01362765\n",
      "(14.31 min) Epoch 71/300 -- Iteration 68076 - Batch 666/963 - Train loss: 0.26940831  - Train acc: 0.8964 - Val loss: 1.01362765\n",
      "(14.31 min) Epoch 71/300 -- Iteration 68085 - Batch 675/963 - Train loss: 0.26932871  - Train acc: 0.8964 - Val loss: 1.01362765\n",
      "(14.31 min) Epoch 71/300 -- Iteration 68094 - Batch 684/963 - Train loss: 0.26917193  - Train acc: 0.8964 - Val loss: 1.01362765\n",
      "(14.31 min) Epoch 71/300 -- Iteration 68103 - Batch 693/963 - Train loss: 0.26950858  - Train acc: 0.8963 - Val loss: 1.01362765\n",
      "(14.31 min) Epoch 71/300 -- Iteration 68112 - Batch 702/963 - Train loss: 0.26934488  - Train acc: 0.8964 - Val loss: 1.01362765\n",
      "(14.32 min) Epoch 71/300 -- Iteration 68121 - Batch 711/963 - Train loss: 0.26909176  - Train acc: 0.8966 - Val loss: 1.01362765\n",
      "(14.32 min) Epoch 71/300 -- Iteration 68130 - Batch 720/963 - Train loss: 0.26915033  - Train acc: 0.8965 - Val loss: 1.01362765\n",
      "(14.32 min) Epoch 71/300 -- Iteration 68139 - Batch 729/963 - Train loss: 0.26946443  - Train acc: 0.8964 - Val loss: 1.01362765\n",
      "(14.32 min) Epoch 71/300 -- Iteration 68148 - Batch 738/963 - Train loss: 0.26970344  - Train acc: 0.8963 - Val loss: 1.01362765\n",
      "(14.32 min) Epoch 71/300 -- Iteration 68157 - Batch 747/963 - Train loss: 0.26999241  - Train acc: 0.8962 - Val loss: 1.01362765\n",
      "(14.33 min) Epoch 71/300 -- Iteration 68166 - Batch 756/963 - Train loss: 0.26982059  - Train acc: 0.8963 - Val loss: 1.01362765\n",
      "(14.33 min) Epoch 71/300 -- Iteration 68175 - Batch 765/963 - Train loss: 0.26986107  - Train acc: 0.8962 - Val loss: 1.01362765\n",
      "(14.33 min) Epoch 71/300 -- Iteration 68184 - Batch 774/963 - Train loss: 0.27004107  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.33 min) Epoch 71/300 -- Iteration 68193 - Batch 783/963 - Train loss: 0.26984526  - Train acc: 0.8962 - Val loss: 1.01362765\n",
      "(14.33 min) Epoch 71/300 -- Iteration 68202 - Batch 792/963 - Train loss: 0.26997332  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.34 min) Epoch 71/300 -- Iteration 68211 - Batch 801/963 - Train loss: 0.27008428  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.34 min) Epoch 71/300 -- Iteration 68220 - Batch 810/963 - Train loss: 0.27017128  - Train acc: 0.8962 - Val loss: 1.01362765\n",
      "(14.34 min) Epoch 71/300 -- Iteration 68229 - Batch 819/963 - Train loss: 0.27011259  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.34 min) Epoch 71/300 -- Iteration 68238 - Batch 828/963 - Train loss: 0.27027569  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.34 min) Epoch 71/300 -- Iteration 68247 - Batch 837/963 - Train loss: 0.27030543  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.34 min) Epoch 71/300 -- Iteration 68256 - Batch 846/963 - Train loss: 0.27022799  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.35 min) Epoch 71/300 -- Iteration 68265 - Batch 855/963 - Train loss: 0.27017930  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.35 min) Epoch 71/300 -- Iteration 68274 - Batch 864/963 - Train loss: 0.27038422  - Train acc: 0.8960 - Val loss: 1.01362765\n",
      "(14.35 min) Epoch 71/300 -- Iteration 68283 - Batch 873/963 - Train loss: 0.27029504  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.35 min) Epoch 71/300 -- Iteration 68292 - Batch 882/963 - Train loss: 0.27002213  - Train acc: 0.8962 - Val loss: 1.01362765\n",
      "(14.35 min) Epoch 71/300 -- Iteration 68301 - Batch 891/963 - Train loss: 0.27019121  - Train acc: 0.8961 - Val loss: 1.01362765\n",
      "(14.36 min) Epoch 71/300 -- Iteration 68310 - Batch 900/963 - Train loss: 0.26999365  - Train acc: 0.8962 - Val loss: 1.01362765\n",
      "(14.36 min) Epoch 71/300 -- Iteration 68319 - Batch 909/963 - Train loss: 0.27004638  - Train acc: 0.8962 - Val loss: 1.01362765\n",
      "(14.36 min) Epoch 71/300 -- Iteration 68328 - Batch 918/963 - Train loss: 0.26997279  - Train acc: 0.8962 - Val loss: 1.01362765\n",
      "(14.36 min) Epoch 71/300 -- Iteration 68337 - Batch 927/963 - Train loss: 0.27009867  - Train acc: 0.8963 - Val loss: 1.01362765\n",
      "(14.36 min) Epoch 71/300 -- Iteration 68346 - Batch 936/963 - Train loss: 0.26986190  - Train acc: 0.8964 - Val loss: 1.01362765\n",
      "(14.36 min) Epoch 71/300 -- Iteration 68355 - Batch 945/963 - Train loss: 0.26996205  - Train acc: 0.8964 - Val loss: 1.01362765\n",
      "(14.37 min) Epoch 71/300 -- Iteration 68364 - Batch 954/963 - Train loss: 0.26985522  - Train acc: 0.8965 - Val loss: 1.01362765\n",
      "(14.37 min) Epoch 71/300 -- Iteration 68373 - Batch 962/963 - Train loss: 0.27001695  - Train acc: 0.8964 - Val loss: 1.01165903 - Val acc: 0.5917\n",
      "(14.37 min) Epoch 72/300 -- Iteration 68382 - Batch 9/963 - Train loss: 0.27799227  - Train acc: 0.8820 - Val loss: 1.01165903\n",
      "(14.37 min) Epoch 72/300 -- Iteration 68391 - Batch 18/963 - Train loss: 0.27560551  - Train acc: 0.8894 - Val loss: 1.01165903\n",
      "(14.37 min) Epoch 72/300 -- Iteration 68400 - Batch 27/963 - Train loss: 0.27727896  - Train acc: 0.8892 - Val loss: 1.01165903\n",
      "(14.38 min) Epoch 72/300 -- Iteration 68409 - Batch 36/963 - Train loss: 0.28417176  - Train acc: 0.8883 - Val loss: 1.01165903\n",
      "(14.38 min) Epoch 72/300 -- Iteration 68418 - Batch 45/963 - Train loss: 0.27545928  - Train acc: 0.8927 - Val loss: 1.01165903\n",
      "(14.38 min) Epoch 72/300 -- Iteration 68427 - Batch 54/963 - Train loss: 0.27633742  - Train acc: 0.8929 - Val loss: 1.01165903\n",
      "(14.38 min) Epoch 72/300 -- Iteration 68436 - Batch 63/963 - Train loss: 0.27664086  - Train acc: 0.8912 - Val loss: 1.01165903\n",
      "(14.38 min) Epoch 72/300 -- Iteration 68445 - Batch 72/963 - Train loss: 0.27072048  - Train acc: 0.8943 - Val loss: 1.01165903\n",
      "(14.39 min) Epoch 72/300 -- Iteration 68454 - Batch 81/963 - Train loss: 0.27001210  - Train acc: 0.8940 - Val loss: 1.01165903\n",
      "(14.39 min) Epoch 72/300 -- Iteration 68463 - Batch 90/963 - Train loss: 0.26707270  - Train acc: 0.8960 - Val loss: 1.01165903\n",
      "(14.39 min) Epoch 72/300 -- Iteration 68472 - Batch 99/963 - Train loss: 0.26920978  - Train acc: 0.8956 - Val loss: 1.01165903\n",
      "(14.39 min) Epoch 72/300 -- Iteration 68481 - Batch 108/963 - Train loss: 0.27058572  - Train acc: 0.8949 - Val loss: 1.01165903\n",
      "(14.39 min) Epoch 72/300 -- Iteration 68490 - Batch 117/963 - Train loss: 0.27228496  - Train acc: 0.8937 - Val loss: 1.01165903\n",
      "(14.39 min) Epoch 72/300 -- Iteration 68499 - Batch 126/963 - Train loss: 0.27209965  - Train acc: 0.8937 - Val loss: 1.01165903\n",
      "(14.40 min) Epoch 72/300 -- Iteration 68508 - Batch 135/963 - Train loss: 0.27276915  - Train acc: 0.8937 - Val loss: 1.01165903\n",
      "(14.40 min) Epoch 72/300 -- Iteration 68517 - Batch 144/963 - Train loss: 0.27439636  - Train acc: 0.8933 - Val loss: 1.01165903\n",
      "(14.40 min) Epoch 72/300 -- Iteration 68526 - Batch 153/963 - Train loss: 0.27357939  - Train acc: 0.8942 - Val loss: 1.01165903\n",
      "(14.40 min) Epoch 72/300 -- Iteration 68535 - Batch 162/963 - Train loss: 0.27259880  - Train acc: 0.8945 - Val loss: 1.01165903\n",
      "(14.40 min) Epoch 72/300 -- Iteration 68544 - Batch 171/963 - Train loss: 0.27178848  - Train acc: 0.8950 - Val loss: 1.01165903\n",
      "(14.41 min) Epoch 72/300 -- Iteration 68553 - Batch 180/963 - Train loss: 0.27141310  - Train acc: 0.8955 - Val loss: 1.01165903\n",
      "(14.41 min) Epoch 72/300 -- Iteration 68562 - Batch 189/963 - Train loss: 0.27170106  - Train acc: 0.8952 - Val loss: 1.01165903\n",
      "(14.41 min) Epoch 72/300 -- Iteration 68571 - Batch 198/963 - Train loss: 0.27050372  - Train acc: 0.8957 - Val loss: 1.01165903\n",
      "(14.41 min) Epoch 72/300 -- Iteration 68580 - Batch 207/963 - Train loss: 0.27060023  - Train acc: 0.8961 - Val loss: 1.01165903\n",
      "(14.41 min) Epoch 72/300 -- Iteration 68589 - Batch 216/963 - Train loss: 0.27027567  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.42 min) Epoch 72/300 -- Iteration 68598 - Batch 225/963 - Train loss: 0.27098061  - Train acc: 0.8961 - Val loss: 1.01165903\n",
      "(14.42 min) Epoch 72/300 -- Iteration 68607 - Batch 234/963 - Train loss: 0.26978815  - Train acc: 0.8970 - Val loss: 1.01165903\n",
      "(14.42 min) Epoch 72/300 -- Iteration 68616 - Batch 243/963 - Train loss: 0.26952652  - Train acc: 0.8970 - Val loss: 1.01165903\n",
      "(14.42 min) Epoch 72/300 -- Iteration 68625 - Batch 252/963 - Train loss: 0.26933988  - Train acc: 0.8972 - Val loss: 1.01165903\n",
      "(14.42 min) Epoch 72/300 -- Iteration 68634 - Batch 261/963 - Train loss: 0.26848994  - Train acc: 0.8978 - Val loss: 1.01165903\n",
      "(14.42 min) Epoch 72/300 -- Iteration 68643 - Batch 270/963 - Train loss: 0.26834271  - Train acc: 0.8976 - Val loss: 1.01165903\n",
      "(14.43 min) Epoch 72/300 -- Iteration 68652 - Batch 279/963 - Train loss: 0.26784733  - Train acc: 0.8979 - Val loss: 1.01165903\n",
      "(14.43 min) Epoch 72/300 -- Iteration 68661 - Batch 288/963 - Train loss: 0.26818049  - Train acc: 0.8980 - Val loss: 1.01165903\n",
      "(14.43 min) Epoch 72/300 -- Iteration 68670 - Batch 297/963 - Train loss: 0.26814968  - Train acc: 0.8980 - Val loss: 1.01165903\n",
      "(14.43 min) Epoch 72/300 -- Iteration 68679 - Batch 306/963 - Train loss: 0.26881615  - Train acc: 0.8978 - Val loss: 1.01165903\n",
      "(14.43 min) Epoch 72/300 -- Iteration 68688 - Batch 315/963 - Train loss: 0.26861429  - Train acc: 0.8980 - Val loss: 1.01165903\n",
      "(14.44 min) Epoch 72/300 -- Iteration 68697 - Batch 324/963 - Train loss: 0.26910867  - Train acc: 0.8977 - Val loss: 1.01165903\n",
      "(14.44 min) Epoch 72/300 -- Iteration 68706 - Batch 333/963 - Train loss: 0.26889840  - Train acc: 0.8979 - Val loss: 1.01165903\n",
      "(14.44 min) Epoch 72/300 -- Iteration 68715 - Batch 342/963 - Train loss: 0.26923818  - Train acc: 0.8980 - Val loss: 1.01165903\n",
      "(14.44 min) Epoch 72/300 -- Iteration 68724 - Batch 351/963 - Train loss: 0.26956239  - Train acc: 0.8978 - Val loss: 1.01165903\n",
      "(14.44 min) Epoch 72/300 -- Iteration 68733 - Batch 360/963 - Train loss: 0.26907944  - Train acc: 0.8979 - Val loss: 1.01165903\n",
      "(14.45 min) Epoch 72/300 -- Iteration 68742 - Batch 369/963 - Train loss: 0.26949638  - Train acc: 0.8977 - Val loss: 1.01165903\n",
      "(14.45 min) Epoch 72/300 -- Iteration 68751 - Batch 378/963 - Train loss: 0.26995265  - Train acc: 0.8975 - Val loss: 1.01165903\n",
      "(14.45 min) Epoch 72/300 -- Iteration 68760 - Batch 387/963 - Train loss: 0.27054263  - Train acc: 0.8974 - Val loss: 1.01165903\n",
      "(14.45 min) Epoch 72/300 -- Iteration 68769 - Batch 396/963 - Train loss: 0.27007933  - Train acc: 0.8976 - Val loss: 1.01165903\n",
      "(14.45 min) Epoch 72/300 -- Iteration 68778 - Batch 405/963 - Train loss: 0.27049682  - Train acc: 0.8975 - Val loss: 1.01165903\n",
      "(14.46 min) Epoch 72/300 -- Iteration 68787 - Batch 414/963 - Train loss: 0.27039433  - Train acc: 0.8977 - Val loss: 1.01165903\n",
      "(14.46 min) Epoch 72/300 -- Iteration 68796 - Batch 423/963 - Train loss: 0.27027284  - Train acc: 0.8978 - Val loss: 1.01165903\n",
      "(14.46 min) Epoch 72/300 -- Iteration 68805 - Batch 432/963 - Train loss: 0.27025530  - Train acc: 0.8979 - Val loss: 1.01165903\n",
      "(14.46 min) Epoch 72/300 -- Iteration 68814 - Batch 441/963 - Train loss: 0.27130995  - Train acc: 0.8973 - Val loss: 1.01165903\n",
      "(14.46 min) Epoch 72/300 -- Iteration 68823 - Batch 450/963 - Train loss: 0.27074741  - Train acc: 0.8975 - Val loss: 1.01165903\n",
      "(14.46 min) Epoch 72/300 -- Iteration 68832 - Batch 459/963 - Train loss: 0.27111320  - Train acc: 0.8972 - Val loss: 1.01165903\n",
      "(14.47 min) Epoch 72/300 -- Iteration 68841 - Batch 468/963 - Train loss: 0.27121759  - Train acc: 0.8970 - Val loss: 1.01165903\n",
      "(14.47 min) Epoch 72/300 -- Iteration 68850 - Batch 477/963 - Train loss: 0.27127200  - Train acc: 0.8970 - Val loss: 1.01165903\n",
      "(14.47 min) Epoch 72/300 -- Iteration 68859 - Batch 486/963 - Train loss: 0.27194871  - Train acc: 0.8968 - Val loss: 1.01165903\n",
      "(14.47 min) Epoch 72/300 -- Iteration 68868 - Batch 495/963 - Train loss: 0.27233109  - Train acc: 0.8966 - Val loss: 1.01165903\n",
      "(14.47 min) Epoch 72/300 -- Iteration 68877 - Batch 504/963 - Train loss: 0.27211401  - Train acc: 0.8966 - Val loss: 1.01165903\n",
      "(14.48 min) Epoch 72/300 -- Iteration 68886 - Batch 513/963 - Train loss: 0.27189973  - Train acc: 0.8966 - Val loss: 1.01165903\n",
      "(14.48 min) Epoch 72/300 -- Iteration 68895 - Batch 522/963 - Train loss: 0.27208856  - Train acc: 0.8965 - Val loss: 1.01165903\n",
      "(14.48 min) Epoch 72/300 -- Iteration 68904 - Batch 531/963 - Train loss: 0.27215746  - Train acc: 0.8966 - Val loss: 1.01165903\n",
      "(14.48 min) Epoch 72/300 -- Iteration 68913 - Batch 540/963 - Train loss: 0.27175145  - Train acc: 0.8968 - Val loss: 1.01165903\n",
      "(14.48 min) Epoch 72/300 -- Iteration 68922 - Batch 549/963 - Train loss: 0.27186412  - Train acc: 0.8968 - Val loss: 1.01165903\n",
      "(14.49 min) Epoch 72/300 -- Iteration 68931 - Batch 558/963 - Train loss: 0.27209669  - Train acc: 0.8967 - Val loss: 1.01165903\n",
      "(14.49 min) Epoch 72/300 -- Iteration 68940 - Batch 567/963 - Train loss: 0.27230727  - Train acc: 0.8966 - Val loss: 1.01165903\n",
      "(14.49 min) Epoch 72/300 -- Iteration 68949 - Batch 576/963 - Train loss: 0.27233449  - Train acc: 0.8966 - Val loss: 1.01165903\n",
      "(14.49 min) Epoch 72/300 -- Iteration 68958 - Batch 585/963 - Train loss: 0.27244493  - Train acc: 0.8965 - Val loss: 1.01165903\n",
      "(14.49 min) Epoch 72/300 -- Iteration 68967 - Batch 594/963 - Train loss: 0.27203236  - Train acc: 0.8966 - Val loss: 1.01165903\n",
      "(14.49 min) Epoch 72/300 -- Iteration 68976 - Batch 603/963 - Train loss: 0.27238832  - Train acc: 0.8963 - Val loss: 1.01165903\n",
      "(14.50 min) Epoch 72/300 -- Iteration 68985 - Batch 612/963 - Train loss: 0.27232538  - Train acc: 0.8961 - Val loss: 1.01165903\n",
      "(14.50 min) Epoch 72/300 -- Iteration 68994 - Batch 621/963 - Train loss: 0.27235116  - Train acc: 0.8961 - Val loss: 1.01165903\n",
      "(14.50 min) Epoch 72/300 -- Iteration 69003 - Batch 630/963 - Train loss: 0.27263084  - Train acc: 0.8959 - Val loss: 1.01165903\n",
      "(14.50 min) Epoch 72/300 -- Iteration 69012 - Batch 639/963 - Train loss: 0.27268974  - Train acc: 0.8960 - Val loss: 1.01165903\n",
      "(14.50 min) Epoch 72/300 -- Iteration 69021 - Batch 648/963 - Train loss: 0.27246116  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.51 min) Epoch 72/300 -- Iteration 69030 - Batch 657/963 - Train loss: 0.27242885  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.51 min) Epoch 72/300 -- Iteration 69039 - Batch 666/963 - Train loss: 0.27269257  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.51 min) Epoch 72/300 -- Iteration 69048 - Batch 675/963 - Train loss: 0.27259755  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.51 min) Epoch 72/300 -- Iteration 69057 - Batch 684/963 - Train loss: 0.27266823  - Train acc: 0.8961 - Val loss: 1.01165903\n",
      "(14.51 min) Epoch 72/300 -- Iteration 69066 - Batch 693/963 - Train loss: 0.27248513  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.52 min) Epoch 72/300 -- Iteration 69075 - Batch 702/963 - Train loss: 0.27213529  - Train acc: 0.8964 - Val loss: 1.01165903\n",
      "(14.52 min) Epoch 72/300 -- Iteration 69084 - Batch 711/963 - Train loss: 0.27262938  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.52 min) Epoch 72/300 -- Iteration 69093 - Batch 720/963 - Train loss: 0.27237936  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.52 min) Epoch 72/300 -- Iteration 69102 - Batch 729/963 - Train loss: 0.27212591  - Train acc: 0.8963 - Val loss: 1.01165903\n",
      "(14.52 min) Epoch 72/300 -- Iteration 69111 - Batch 738/963 - Train loss: 0.27198381  - Train acc: 0.8964 - Val loss: 1.01165903\n",
      "(14.53 min) Epoch 72/300 -- Iteration 69120 - Batch 747/963 - Train loss: 0.27173487  - Train acc: 0.8965 - Val loss: 1.01165903\n",
      "(14.53 min) Epoch 72/300 -- Iteration 69129 - Batch 756/963 - Train loss: 0.27225992  - Train acc: 0.8963 - Val loss: 1.01165903\n",
      "(14.53 min) Epoch 72/300 -- Iteration 69138 - Batch 765/963 - Train loss: 0.27237232  - Train acc: 0.8963 - Val loss: 1.01165903\n",
      "(14.53 min) Epoch 72/300 -- Iteration 69147 - Batch 774/963 - Train loss: 0.27194136  - Train acc: 0.8964 - Val loss: 1.01165903\n",
      "(14.53 min) Epoch 72/300 -- Iteration 69156 - Batch 783/963 - Train loss: 0.27206768  - Train acc: 0.8964 - Val loss: 1.01165903\n",
      "(14.54 min) Epoch 72/300 -- Iteration 69165 - Batch 792/963 - Train loss: 0.27202922  - Train acc: 0.8964 - Val loss: 1.01165903\n",
      "(14.54 min) Epoch 72/300 -- Iteration 69174 - Batch 801/963 - Train loss: 0.27167216  - Train acc: 0.8966 - Val loss: 1.01165903\n",
      "(14.54 min) Epoch 72/300 -- Iteration 69183 - Batch 810/963 - Train loss: 0.27184074  - Train acc: 0.8965 - Val loss: 1.01165903\n",
      "(14.54 min) Epoch 72/300 -- Iteration 69192 - Batch 819/963 - Train loss: 0.27208260  - Train acc: 0.8964 - Val loss: 1.01165903\n",
      "(14.54 min) Epoch 72/300 -- Iteration 69201 - Batch 828/963 - Train loss: 0.27230565  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.55 min) Epoch 72/300 -- Iteration 69210 - Batch 837/963 - Train loss: 0.27226907  - Train acc: 0.8962 - Val loss: 1.01165903\n",
      "(14.55 min) Epoch 72/300 -- Iteration 69219 - Batch 846/963 - Train loss: 0.27199877  - Train acc: 0.8964 - Val loss: 1.01165903\n",
      "(14.55 min) Epoch 72/300 -- Iteration 69228 - Batch 855/963 - Train loss: 0.27175998  - Train acc: 0.8964 - Val loss: 1.01165903\n",
      "(14.55 min) Epoch 72/300 -- Iteration 69237 - Batch 864/963 - Train loss: 0.27172423  - Train acc: 0.8964 - Val loss: 1.01165903\n",
      "(14.55 min) Epoch 72/300 -- Iteration 69246 - Batch 873/963 - Train loss: 0.27173000  - Train acc: 0.8965 - Val loss: 1.01165903\n",
      "(14.55 min) Epoch 72/300 -- Iteration 69255 - Batch 882/963 - Train loss: 0.27148463  - Train acc: 0.8968 - Val loss: 1.01165903\n",
      "(14.56 min) Epoch 72/300 -- Iteration 69264 - Batch 891/963 - Train loss: 0.27146032  - Train acc: 0.8968 - Val loss: 1.01165903\n",
      "(14.56 min) Epoch 72/300 -- Iteration 69273 - Batch 900/963 - Train loss: 0.27132776  - Train acc: 0.8969 - Val loss: 1.01165903\n",
      "(14.56 min) Epoch 72/300 -- Iteration 69282 - Batch 909/963 - Train loss: 0.27109031  - Train acc: 0.8970 - Val loss: 1.01165903\n",
      "(14.56 min) Epoch 72/300 -- Iteration 69291 - Batch 918/963 - Train loss: 0.27108013  - Train acc: 0.8970 - Val loss: 1.01165903\n",
      "(14.56 min) Epoch 72/300 -- Iteration 69300 - Batch 927/963 - Train loss: 0.27092772  - Train acc: 0.8972 - Val loss: 1.01165903\n",
      "(14.57 min) Epoch 72/300 -- Iteration 69309 - Batch 936/963 - Train loss: 0.27080959  - Train acc: 0.8972 - Val loss: 1.01165903\n",
      "(14.57 min) Epoch 72/300 -- Iteration 69318 - Batch 945/963 - Train loss: 0.27082890  - Train acc: 0.8972 - Val loss: 1.01165903\n",
      "(14.57 min) Epoch 72/300 -- Iteration 69327 - Batch 954/963 - Train loss: 0.27075453  - Train acc: 0.8972 - Val loss: 1.01165903\n",
      "(14.57 min) Epoch 72/300 -- Iteration 69336 - Batch 962/963 - Train loss: 0.27042641  - Train acc: 0.8973 - Val loss: 1.03249180 - Val acc: 0.5883\n",
      "(14.57 min) Epoch 73/300 -- Iteration 69345 - Batch 9/963 - Train loss: 0.28050358  - Train acc: 0.8930 - Val loss: 1.03249180\n",
      "(14.58 min) Epoch 73/300 -- Iteration 69354 - Batch 18/963 - Train loss: 0.27685296  - Train acc: 0.8988 - Val loss: 1.03249180\n",
      "(14.58 min) Epoch 73/300 -- Iteration 69363 - Batch 27/963 - Train loss: 0.28422427  - Train acc: 0.8931 - Val loss: 1.03249180\n",
      "(14.58 min) Epoch 73/300 -- Iteration 69372 - Batch 36/963 - Train loss: 0.29239738  - Train acc: 0.8896 - Val loss: 1.03249180\n",
      "(14.58 min) Epoch 73/300 -- Iteration 69381 - Batch 45/963 - Train loss: 0.28387196  - Train acc: 0.8942 - Val loss: 1.03249180\n",
      "(14.58 min) Epoch 73/300 -- Iteration 69390 - Batch 54/963 - Train loss: 0.28454638  - Train acc: 0.8947 - Val loss: 1.03249180\n",
      "(14.59 min) Epoch 73/300 -- Iteration 69399 - Batch 63/963 - Train loss: 0.28311961  - Train acc: 0.8955 - Val loss: 1.03249180\n",
      "(14.59 min) Epoch 73/300 -- Iteration 69408 - Batch 72/963 - Train loss: 0.27869690  - Train acc: 0.8977 - Val loss: 1.03249180\n",
      "(14.59 min) Epoch 73/300 -- Iteration 69417 - Batch 81/963 - Train loss: 0.27694588  - Train acc: 0.8975 - Val loss: 1.03249180\n",
      "(14.59 min) Epoch 73/300 -- Iteration 69426 - Batch 90/963 - Train loss: 0.27542317  - Train acc: 0.8977 - Val loss: 1.03249180\n",
      "(14.59 min) Epoch 73/300 -- Iteration 69435 - Batch 99/963 - Train loss: 0.27525965  - Train acc: 0.8976 - Val loss: 1.03249180\n",
      "(14.59 min) Epoch 73/300 -- Iteration 69444 - Batch 108/963 - Train loss: 0.27324601  - Train acc: 0.8984 - Val loss: 1.03249180\n",
      "(14.60 min) Epoch 73/300 -- Iteration 69453 - Batch 117/963 - Train loss: 0.27482790  - Train acc: 0.8980 - Val loss: 1.03249180\n",
      "(14.60 min) Epoch 73/300 -- Iteration 69462 - Batch 126/963 - Train loss: 0.27250153  - Train acc: 0.8983 - Val loss: 1.03249180\n",
      "(14.60 min) Epoch 73/300 -- Iteration 69471 - Batch 135/963 - Train loss: 0.27005451  - Train acc: 0.8995 - Val loss: 1.03249180\n",
      "(14.60 min) Epoch 73/300 -- Iteration 69480 - Batch 144/963 - Train loss: 0.27147972  - Train acc: 0.8990 - Val loss: 1.03249180\n",
      "(14.60 min) Epoch 73/300 -- Iteration 69489 - Batch 153/963 - Train loss: 0.27083369  - Train acc: 0.8990 - Val loss: 1.03249180\n",
      "(14.61 min) Epoch 73/300 -- Iteration 69498 - Batch 162/963 - Train loss: 0.26937726  - Train acc: 0.8993 - Val loss: 1.03249180\n",
      "(14.61 min) Epoch 73/300 -- Iteration 69507 - Batch 171/963 - Train loss: 0.26815406  - Train acc: 0.9001 - Val loss: 1.03249180\n",
      "(14.61 min) Epoch 73/300 -- Iteration 69516 - Batch 180/963 - Train loss: 0.26954658  - Train acc: 0.8995 - Val loss: 1.03249180\n",
      "(14.61 min) Epoch 73/300 -- Iteration 69525 - Batch 189/963 - Train loss: 0.27016187  - Train acc: 0.8992 - Val loss: 1.03249180\n",
      "(14.61 min) Epoch 73/300 -- Iteration 69534 - Batch 198/963 - Train loss: 0.27013718  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.62 min) Epoch 73/300 -- Iteration 69543 - Batch 207/963 - Train loss: 0.26994897  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.62 min) Epoch 73/300 -- Iteration 69552 - Batch 216/963 - Train loss: 0.26944436  - Train acc: 0.8997 - Val loss: 1.03249180\n",
      "(14.62 min) Epoch 73/300 -- Iteration 69561 - Batch 225/963 - Train loss: 0.27117123  - Train acc: 0.8987 - Val loss: 1.03249180\n",
      "(14.62 min) Epoch 73/300 -- Iteration 69570 - Batch 234/963 - Train loss: 0.27061096  - Train acc: 0.8990 - Val loss: 1.03249180\n",
      "(14.62 min) Epoch 73/300 -- Iteration 69579 - Batch 243/963 - Train loss: 0.27134447  - Train acc: 0.8988 - Val loss: 1.03249180\n",
      "(14.62 min) Epoch 73/300 -- Iteration 69588 - Batch 252/963 - Train loss: 0.27138566  - Train acc: 0.8985 - Val loss: 1.03249180\n",
      "(14.63 min) Epoch 73/300 -- Iteration 69597 - Batch 261/963 - Train loss: 0.27031271  - Train acc: 0.8990 - Val loss: 1.03249180\n",
      "(14.63 min) Epoch 73/300 -- Iteration 69606 - Batch 270/963 - Train loss: 0.27011732  - Train acc: 0.8990 - Val loss: 1.03249180\n",
      "(14.63 min) Epoch 73/300 -- Iteration 69615 - Batch 279/963 - Train loss: 0.26986325  - Train acc: 0.8994 - Val loss: 1.03249180\n",
      "(14.63 min) Epoch 73/300 -- Iteration 69624 - Batch 288/963 - Train loss: 0.26879632  - Train acc: 0.8997 - Val loss: 1.03249180\n",
      "(14.63 min) Epoch 73/300 -- Iteration 69633 - Batch 297/963 - Train loss: 0.26853958  - Train acc: 0.8997 - Val loss: 1.03249180\n",
      "(14.64 min) Epoch 73/300 -- Iteration 69642 - Batch 306/963 - Train loss: 0.26811398  - Train acc: 0.8999 - Val loss: 1.03249180\n",
      "(14.64 min) Epoch 73/300 -- Iteration 69651 - Batch 315/963 - Train loss: 0.26817174  - Train acc: 0.8998 - Val loss: 1.03249180\n",
      "(14.64 min) Epoch 73/300 -- Iteration 69660 - Batch 324/963 - Train loss: 0.26783548  - Train acc: 0.8998 - Val loss: 1.03249180\n",
      "(14.64 min) Epoch 73/300 -- Iteration 69669 - Batch 333/963 - Train loss: 0.26823047  - Train acc: 0.8997 - Val loss: 1.03249180\n",
      "(14.64 min) Epoch 73/300 -- Iteration 69678 - Batch 342/963 - Train loss: 0.26795256  - Train acc: 0.9001 - Val loss: 1.03249180\n",
      "(14.65 min) Epoch 73/300 -- Iteration 69687 - Batch 351/963 - Train loss: 0.26828076  - Train acc: 0.8997 - Val loss: 1.03249180\n",
      "(14.65 min) Epoch 73/300 -- Iteration 69696 - Batch 360/963 - Train loss: 0.26832733  - Train acc: 0.8998 - Val loss: 1.03249180\n",
      "(14.65 min) Epoch 73/300 -- Iteration 69705 - Batch 369/963 - Train loss: 0.26810559  - Train acc: 0.8999 - Val loss: 1.03249180\n",
      "(14.65 min) Epoch 73/300 -- Iteration 69714 - Batch 378/963 - Train loss: 0.26872076  - Train acc: 0.8997 - Val loss: 1.03249180\n",
      "(14.65 min) Epoch 73/300 -- Iteration 69723 - Batch 387/963 - Train loss: 0.26942761  - Train acc: 0.8994 - Val loss: 1.03249180\n",
      "(14.66 min) Epoch 73/300 -- Iteration 69732 - Batch 396/963 - Train loss: 0.26908903  - Train acc: 0.8995 - Val loss: 1.03249180\n",
      "(14.66 min) Epoch 73/300 -- Iteration 69741 - Batch 405/963 - Train loss: 0.26987998  - Train acc: 0.8992 - Val loss: 1.03249180\n",
      "(14.66 min) Epoch 73/300 -- Iteration 69750 - Batch 414/963 - Train loss: 0.26985613  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.66 min) Epoch 73/300 -- Iteration 69759 - Batch 423/963 - Train loss: 0.26949759  - Train acc: 0.8992 - Val loss: 1.03249180\n",
      "(14.66 min) Epoch 73/300 -- Iteration 69768 - Batch 432/963 - Train loss: 0.26953232  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.67 min) Epoch 73/300 -- Iteration 69777 - Batch 441/963 - Train loss: 0.26950979  - Train acc: 0.8992 - Val loss: 1.03249180\n",
      "(14.67 min) Epoch 73/300 -- Iteration 69786 - Batch 450/963 - Train loss: 0.26950273  - Train acc: 0.8993 - Val loss: 1.03249180\n",
      "(14.67 min) Epoch 73/300 -- Iteration 69795 - Batch 459/963 - Train loss: 0.26991737  - Train acc: 0.8992 - Val loss: 1.03249180\n",
      "(14.67 min) Epoch 73/300 -- Iteration 69804 - Batch 468/963 - Train loss: 0.26966642  - Train acc: 0.8993 - Val loss: 1.03249180\n",
      "(14.67 min) Epoch 73/300 -- Iteration 69813 - Batch 477/963 - Train loss: 0.26991945  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.67 min) Epoch 73/300 -- Iteration 69822 - Batch 486/963 - Train loss: 0.27014118  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.68 min) Epoch 73/300 -- Iteration 69831 - Batch 495/963 - Train loss: 0.27029312  - Train acc: 0.8989 - Val loss: 1.03249180\n",
      "(14.68 min) Epoch 73/300 -- Iteration 69840 - Batch 504/963 - Train loss: 0.27015744  - Train acc: 0.8990 - Val loss: 1.03249180\n",
      "(14.68 min) Epoch 73/300 -- Iteration 69849 - Batch 513/963 - Train loss: 0.27047045  - Train acc: 0.8988 - Val loss: 1.03249180\n",
      "(14.68 min) Epoch 73/300 -- Iteration 69858 - Batch 522/963 - Train loss: 0.26990902  - Train acc: 0.8990 - Val loss: 1.03249180\n",
      "(14.68 min) Epoch 73/300 -- Iteration 69867 - Batch 531/963 - Train loss: 0.26993112  - Train acc: 0.8990 - Val loss: 1.03249180\n",
      "(14.69 min) Epoch 73/300 -- Iteration 69876 - Batch 540/963 - Train loss: 0.26950166  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.69 min) Epoch 73/300 -- Iteration 69885 - Batch 549/963 - Train loss: 0.26975845  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.69 min) Epoch 73/300 -- Iteration 69894 - Batch 558/963 - Train loss: 0.26965521  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.69 min) Epoch 73/300 -- Iteration 69903 - Batch 567/963 - Train loss: 0.26954794  - Train acc: 0.8992 - Val loss: 1.03249180\n",
      "(14.69 min) Epoch 73/300 -- Iteration 69912 - Batch 576/963 - Train loss: 0.26911694  - Train acc: 0.8994 - Val loss: 1.03249180\n",
      "(14.70 min) Epoch 73/300 -- Iteration 69921 - Batch 585/963 - Train loss: 0.26920576  - Train acc: 0.8993 - Val loss: 1.03249180\n",
      "(14.70 min) Epoch 73/300 -- Iteration 69930 - Batch 594/963 - Train loss: 0.26875196  - Train acc: 0.8994 - Val loss: 1.03249180\n",
      "(14.70 min) Epoch 73/300 -- Iteration 69939 - Batch 603/963 - Train loss: 0.26841516  - Train acc: 0.8996 - Val loss: 1.03249180\n",
      "(14.70 min) Epoch 73/300 -- Iteration 69948 - Batch 612/963 - Train loss: 0.26854416  - Train acc: 0.8996 - Val loss: 1.03249180\n",
      "(14.70 min) Epoch 73/300 -- Iteration 69957 - Batch 621/963 - Train loss: 0.26878935  - Train acc: 0.8995 - Val loss: 1.03249180\n",
      "(14.71 min) Epoch 73/300 -- Iteration 69966 - Batch 630/963 - Train loss: 0.26858277  - Train acc: 0.8996 - Val loss: 1.03249180\n",
      "(14.71 min) Epoch 73/300 -- Iteration 69975 - Batch 639/963 - Train loss: 0.26831674  - Train acc: 0.8996 - Val loss: 1.03249180\n",
      "(14.71 min) Epoch 73/300 -- Iteration 69984 - Batch 648/963 - Train loss: 0.26810975  - Train acc: 0.8997 - Val loss: 1.03249180\n",
      "(14.71 min) Epoch 73/300 -- Iteration 69993 - Batch 657/963 - Train loss: 0.26826620  - Train acc: 0.8996 - Val loss: 1.03249180\n",
      "(14.71 min) Epoch 73/300 -- Iteration 70002 - Batch 666/963 - Train loss: 0.26826038  - Train acc: 0.8996 - Val loss: 1.03249180\n",
      "(14.71 min) Epoch 73/300 -- Iteration 70011 - Batch 675/963 - Train loss: 0.26827577  - Train acc: 0.8995 - Val loss: 1.03249180\n",
      "(14.72 min) Epoch 73/300 -- Iteration 70020 - Batch 684/963 - Train loss: 0.26821489  - Train acc: 0.8995 - Val loss: 1.03249180\n",
      "(14.72 min) Epoch 73/300 -- Iteration 70029 - Batch 693/963 - Train loss: 0.26802477  - Train acc: 0.8996 - Val loss: 1.03249180\n",
      "(14.72 min) Epoch 73/300 -- Iteration 70038 - Batch 702/963 - Train loss: 0.26816641  - Train acc: 0.8993 - Val loss: 1.03249180\n",
      "(14.72 min) Epoch 73/300 -- Iteration 70047 - Batch 711/963 - Train loss: 0.26825039  - Train acc: 0.8992 - Val loss: 1.03249180\n",
      "(14.72 min) Epoch 73/300 -- Iteration 70056 - Batch 720/963 - Train loss: 0.26845170  - Train acc: 0.8991 - Val loss: 1.03249180\n",
      "(14.73 min) Epoch 73/300 -- Iteration 70065 - Batch 729/963 - Train loss: 0.26873123  - Train acc: 0.8989 - Val loss: 1.03249180\n",
      "(14.73 min) Epoch 73/300 -- Iteration 70074 - Batch 738/963 - Train loss: 0.26912506  - Train acc: 0.8986 - Val loss: 1.03249180\n",
      "(14.73 min) Epoch 73/300 -- Iteration 70083 - Batch 747/963 - Train loss: 0.26957237  - Train acc: 0.8984 - Val loss: 1.03249180\n",
      "(14.73 min) Epoch 73/300 -- Iteration 70092 - Batch 756/963 - Train loss: 0.26946178  - Train acc: 0.8985 - Val loss: 1.03249180\n",
      "(14.73 min) Epoch 73/300 -- Iteration 70101 - Batch 765/963 - Train loss: 0.26966930  - Train acc: 0.8983 - Val loss: 1.03249180\n",
      "(14.74 min) Epoch 73/300 -- Iteration 70110 - Batch 774/963 - Train loss: 0.26964964  - Train acc: 0.8984 - Val loss: 1.03249180\n",
      "(14.74 min) Epoch 73/300 -- Iteration 70119 - Batch 783/963 - Train loss: 0.26961282  - Train acc: 0.8985 - Val loss: 1.03249180\n",
      "(14.74 min) Epoch 73/300 -- Iteration 70128 - Batch 792/963 - Train loss: 0.26937806  - Train acc: 0.8986 - Val loss: 1.03249180\n",
      "(14.74 min) Epoch 73/300 -- Iteration 70137 - Batch 801/963 - Train loss: 0.26928692  - Train acc: 0.8986 - Val loss: 1.03249180\n",
      "(14.74 min) Epoch 73/300 -- Iteration 70146 - Batch 810/963 - Train loss: 0.26919063  - Train acc: 0.8986 - Val loss: 1.03249180\n",
      "(14.75 min) Epoch 73/300 -- Iteration 70155 - Batch 819/963 - Train loss: 0.26953107  - Train acc: 0.8985 - Val loss: 1.03249180\n",
      "(14.75 min) Epoch 73/300 -- Iteration 70164 - Batch 828/963 - Train loss: 0.26984224  - Train acc: 0.8984 - Val loss: 1.03249180\n",
      "(14.75 min) Epoch 73/300 -- Iteration 70173 - Batch 837/963 - Train loss: 0.27011441  - Train acc: 0.8984 - Val loss: 1.03249180\n",
      "(14.75 min) Epoch 73/300 -- Iteration 70182 - Batch 846/963 - Train loss: 0.27012450  - Train acc: 0.8984 - Val loss: 1.03249180\n",
      "(14.75 min) Epoch 73/300 -- Iteration 70191 - Batch 855/963 - Train loss: 0.27028521  - Train acc: 0.8983 - Val loss: 1.03249180\n",
      "(14.75 min) Epoch 73/300 -- Iteration 70200 - Batch 864/963 - Train loss: 0.27033374  - Train acc: 0.8983 - Val loss: 1.03249180\n",
      "(14.76 min) Epoch 73/300 -- Iteration 70209 - Batch 873/963 - Train loss: 0.26995199  - Train acc: 0.8985 - Val loss: 1.03249180\n",
      "(14.76 min) Epoch 73/300 -- Iteration 70218 - Batch 882/963 - Train loss: 0.26982487  - Train acc: 0.8985 - Val loss: 1.03249180\n",
      "(14.76 min) Epoch 73/300 -- Iteration 70227 - Batch 891/963 - Train loss: 0.26963753  - Train acc: 0.8986 - Val loss: 1.03249180\n",
      "(14.76 min) Epoch 73/300 -- Iteration 70236 - Batch 900/963 - Train loss: 0.26987533  - Train acc: 0.8984 - Val loss: 1.03249180\n",
      "(14.76 min) Epoch 73/300 -- Iteration 70245 - Batch 909/963 - Train loss: 0.26976572  - Train acc: 0.8985 - Val loss: 1.03249180\n",
      "(14.77 min) Epoch 73/300 -- Iteration 70254 - Batch 918/963 - Train loss: 0.26955985  - Train acc: 0.8986 - Val loss: 1.03249180\n",
      "(14.77 min) Epoch 73/300 -- Iteration 70263 - Batch 927/963 - Train loss: 0.26944223  - Train acc: 0.8985 - Val loss: 1.03249180\n",
      "(14.77 min) Epoch 73/300 -- Iteration 70272 - Batch 936/963 - Train loss: 0.26945690  - Train acc: 0.8984 - Val loss: 1.03249180\n",
      "(14.77 min) Epoch 73/300 -- Iteration 70281 - Batch 945/963 - Train loss: 0.26958490  - Train acc: 0.8984 - Val loss: 1.03249180\n",
      "(14.77 min) Epoch 73/300 -- Iteration 70290 - Batch 954/963 - Train loss: 0.26968121  - Train acc: 0.8985 - Val loss: 1.03249180\n",
      "(14.78 min) Epoch 73/300 -- Iteration 70299 - Batch 962/963 - Train loss: 0.26962609  - Train acc: 0.8985 - Val loss: 1.01957929 - Val acc: 0.5900\n",
      "(14.78 min) Epoch 74/300 -- Iteration 70308 - Batch 9/963 - Train loss: 0.27857445  - Train acc: 0.8844 - Val loss: 1.01957929\n",
      "(14.78 min) Epoch 74/300 -- Iteration 70317 - Batch 18/963 - Train loss: 0.29062557  - Train acc: 0.8898 - Val loss: 1.01957929\n",
      "(14.78 min) Epoch 74/300 -- Iteration 70326 - Batch 27/963 - Train loss: 0.28498948  - Train acc: 0.8934 - Val loss: 1.01957929\n",
      "(14.78 min) Epoch 74/300 -- Iteration 70335 - Batch 36/963 - Train loss: 0.28853940  - Train acc: 0.8913 - Val loss: 1.01957929\n",
      "(14.78 min) Epoch 74/300 -- Iteration 70344 - Batch 45/963 - Train loss: 0.28878514  - Train acc: 0.8903 - Val loss: 1.01957929\n",
      "(14.79 min) Epoch 74/300 -- Iteration 70353 - Batch 54/963 - Train loss: 0.28951042  - Train acc: 0.8896 - Val loss: 1.01957929\n",
      "(14.79 min) Epoch 74/300 -- Iteration 70362 - Batch 63/963 - Train loss: 0.28647888  - Train acc: 0.8912 - Val loss: 1.01957929\n",
      "(14.79 min) Epoch 74/300 -- Iteration 70371 - Batch 72/963 - Train loss: 0.28441760  - Train acc: 0.8917 - Val loss: 1.01957929\n",
      "(14.79 min) Epoch 74/300 -- Iteration 70380 - Batch 81/963 - Train loss: 0.28607650  - Train acc: 0.8911 - Val loss: 1.01957929\n",
      "(14.79 min) Epoch 74/300 -- Iteration 70389 - Batch 90/963 - Train loss: 0.28527797  - Train acc: 0.8916 - Val loss: 1.01957929\n",
      "(14.80 min) Epoch 74/300 -- Iteration 70398 - Batch 99/963 - Train loss: 0.28426631  - Train acc: 0.8922 - Val loss: 1.01957929\n",
      "(14.80 min) Epoch 74/300 -- Iteration 70407 - Batch 108/963 - Train loss: 0.28530870  - Train acc: 0.8926 - Val loss: 1.01957929\n",
      "(14.80 min) Epoch 74/300 -- Iteration 70416 - Batch 117/963 - Train loss: 0.28539410  - Train acc: 0.8933 - Val loss: 1.01957929\n",
      "(14.80 min) Epoch 74/300 -- Iteration 70425 - Batch 126/963 - Train loss: 0.28466108  - Train acc: 0.8933 - Val loss: 1.01957929\n",
      "(14.80 min) Epoch 74/300 -- Iteration 70434 - Batch 135/963 - Train loss: 0.28453137  - Train acc: 0.8931 - Val loss: 1.01957929\n",
      "(14.81 min) Epoch 74/300 -- Iteration 70443 - Batch 144/963 - Train loss: 0.28420675  - Train acc: 0.8936 - Val loss: 1.01957929\n",
      "(14.81 min) Epoch 74/300 -- Iteration 70452 - Batch 153/963 - Train loss: 0.28460453  - Train acc: 0.8930 - Val loss: 1.01957929\n",
      "(14.81 min) Epoch 74/300 -- Iteration 70461 - Batch 162/963 - Train loss: 0.28350842  - Train acc: 0.8932 - Val loss: 1.01957929\n",
      "(14.81 min) Epoch 74/300 -- Iteration 70470 - Batch 171/963 - Train loss: 0.28316894  - Train acc: 0.8932 - Val loss: 1.01957929\n",
      "(14.81 min) Epoch 74/300 -- Iteration 70479 - Batch 180/963 - Train loss: 0.28285239  - Train acc: 0.8929 - Val loss: 1.01957929\n",
      "(14.82 min) Epoch 74/300 -- Iteration 70488 - Batch 189/963 - Train loss: 0.28136162  - Train acc: 0.8935 - Val loss: 1.01957929\n",
      "(14.82 min) Epoch 74/300 -- Iteration 70497 - Batch 198/963 - Train loss: 0.28081024  - Train acc: 0.8937 - Val loss: 1.01957929\n",
      "(14.82 min) Epoch 74/300 -- Iteration 70506 - Batch 207/963 - Train loss: 0.27970287  - Train acc: 0.8942 - Val loss: 1.01957929\n",
      "(14.82 min) Epoch 74/300 -- Iteration 70515 - Batch 216/963 - Train loss: 0.27861371  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.82 min) Epoch 74/300 -- Iteration 70524 - Batch 225/963 - Train loss: 0.27827074  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.82 min) Epoch 74/300 -- Iteration 70533 - Batch 234/963 - Train loss: 0.27677232  - Train acc: 0.8952 - Val loss: 1.01957929\n",
      "(14.83 min) Epoch 74/300 -- Iteration 70542 - Batch 243/963 - Train loss: 0.27627179  - Train acc: 0.8954 - Val loss: 1.01957929\n",
      "(14.83 min) Epoch 74/300 -- Iteration 70551 - Batch 252/963 - Train loss: 0.27685943  - Train acc: 0.8948 - Val loss: 1.01957929\n",
      "(14.83 min) Epoch 74/300 -- Iteration 70560 - Batch 261/963 - Train loss: 0.27732681  - Train acc: 0.8950 - Val loss: 1.01957929\n",
      "(14.83 min) Epoch 74/300 -- Iteration 70569 - Batch 270/963 - Train loss: 0.27859519  - Train acc: 0.8945 - Val loss: 1.01957929\n",
      "(14.83 min) Epoch 74/300 -- Iteration 70578 - Batch 279/963 - Train loss: 0.27831777  - Train acc: 0.8945 - Val loss: 1.01957929\n",
      "(14.84 min) Epoch 74/300 -- Iteration 70587 - Batch 288/963 - Train loss: 0.27930315  - Train acc: 0.8942 - Val loss: 1.01957929\n",
      "(14.84 min) Epoch 74/300 -- Iteration 70596 - Batch 297/963 - Train loss: 0.27909501  - Train acc: 0.8943 - Val loss: 1.01957929\n",
      "(14.84 min) Epoch 74/300 -- Iteration 70605 - Batch 306/963 - Train loss: 0.27907117  - Train acc: 0.8943 - Val loss: 1.01957929\n",
      "(14.84 min) Epoch 74/300 -- Iteration 70614 - Batch 315/963 - Train loss: 0.27836105  - Train acc: 0.8942 - Val loss: 1.01957929\n",
      "(14.84 min) Epoch 74/300 -- Iteration 70623 - Batch 324/963 - Train loss: 0.27811511  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.85 min) Epoch 74/300 -- Iteration 70632 - Batch 333/963 - Train loss: 0.27794362  - Train acc: 0.8947 - Val loss: 1.01957929\n",
      "(14.85 min) Epoch 74/300 -- Iteration 70641 - Batch 342/963 - Train loss: 0.27803423  - Train acc: 0.8946 - Val loss: 1.01957929\n",
      "(14.85 min) Epoch 74/300 -- Iteration 70650 - Batch 351/963 - Train loss: 0.27786272  - Train acc: 0.8946 - Val loss: 1.01957929\n",
      "(14.85 min) Epoch 74/300 -- Iteration 70659 - Batch 360/963 - Train loss: 0.27760722  - Train acc: 0.8946 - Val loss: 1.01957929\n",
      "(14.85 min) Epoch 74/300 -- Iteration 70668 - Batch 369/963 - Train loss: 0.27765285  - Train acc: 0.8947 - Val loss: 1.01957929\n",
      "(14.85 min) Epoch 74/300 -- Iteration 70677 - Batch 378/963 - Train loss: 0.27732730  - Train acc: 0.8947 - Val loss: 1.01957929\n",
      "(14.86 min) Epoch 74/300 -- Iteration 70686 - Batch 387/963 - Train loss: 0.27719339  - Train acc: 0.8949 - Val loss: 1.01957929\n",
      "(14.86 min) Epoch 74/300 -- Iteration 70695 - Batch 396/963 - Train loss: 0.27729571  - Train acc: 0.8947 - Val loss: 1.01957929\n",
      "(14.86 min) Epoch 74/300 -- Iteration 70704 - Batch 405/963 - Train loss: 0.27786010  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.86 min) Epoch 74/300 -- Iteration 70713 - Batch 414/963 - Train loss: 0.27771443  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.86 min) Epoch 74/300 -- Iteration 70722 - Batch 423/963 - Train loss: 0.27780310  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.87 min) Epoch 74/300 -- Iteration 70731 - Batch 432/963 - Train loss: 0.27805922  - Train acc: 0.8942 - Val loss: 1.01957929\n",
      "(14.87 min) Epoch 74/300 -- Iteration 70740 - Batch 441/963 - Train loss: 0.27805991  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.87 min) Epoch 74/300 -- Iteration 70749 - Batch 450/963 - Train loss: 0.27843976  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.87 min) Epoch 74/300 -- Iteration 70758 - Batch 459/963 - Train loss: 0.27875497  - Train acc: 0.8942 - Val loss: 1.01957929\n",
      "(14.87 min) Epoch 74/300 -- Iteration 70767 - Batch 468/963 - Train loss: 0.27855420  - Train acc: 0.8943 - Val loss: 1.01957929\n",
      "(14.88 min) Epoch 74/300 -- Iteration 70776 - Batch 477/963 - Train loss: 0.27870996  - Train acc: 0.8941 - Val loss: 1.01957929\n",
      "(14.88 min) Epoch 74/300 -- Iteration 70785 - Batch 486/963 - Train loss: 0.27863534  - Train acc: 0.8942 - Val loss: 1.01957929\n",
      "(14.88 min) Epoch 74/300 -- Iteration 70794 - Batch 495/963 - Train loss: 0.27841826  - Train acc: 0.8943 - Val loss: 1.01957929\n",
      "(14.88 min) Epoch 74/300 -- Iteration 70803 - Batch 504/963 - Train loss: 0.27831692  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.88 min) Epoch 74/300 -- Iteration 70812 - Batch 513/963 - Train loss: 0.27815691  - Train acc: 0.8946 - Val loss: 1.01957929\n",
      "(14.88 min) Epoch 74/300 -- Iteration 70821 - Batch 522/963 - Train loss: 0.27818862  - Train acc: 0.8945 - Val loss: 1.01957929\n",
      "(14.89 min) Epoch 74/300 -- Iteration 70830 - Batch 531/963 - Train loss: 0.27816601  - Train acc: 0.8945 - Val loss: 1.01957929\n",
      "(14.89 min) Epoch 74/300 -- Iteration 70839 - Batch 540/963 - Train loss: 0.27856476  - Train acc: 0.8944 - Val loss: 1.01957929\n",
      "(14.89 min) Epoch 74/300 -- Iteration 70848 - Batch 549/963 - Train loss: 0.27853110  - Train acc: 0.8942 - Val loss: 1.01957929\n",
      "(14.89 min) Epoch 74/300 -- Iteration 70857 - Batch 558/963 - Train loss: 0.27899536  - Train acc: 0.8940 - Val loss: 1.01957929\n",
      "(14.89 min) Epoch 74/300 -- Iteration 70866 - Batch 567/963 - Train loss: 0.27900323  - Train acc: 0.8940 - Val loss: 1.01957929\n",
      "(14.90 min) Epoch 74/300 -- Iteration 70875 - Batch 576/963 - Train loss: 0.27887147  - Train acc: 0.8940 - Val loss: 1.01957929\n",
      "(14.90 min) Epoch 74/300 -- Iteration 70884 - Batch 585/963 - Train loss: 0.27842723  - Train acc: 0.8942 - Val loss: 1.01957929\n",
      "(14.90 min) Epoch 74/300 -- Iteration 70893 - Batch 594/963 - Train loss: 0.27844658  - Train acc: 0.8942 - Val loss: 1.01957929\n",
      "(14.90 min) Epoch 74/300 -- Iteration 70902 - Batch 603/963 - Train loss: 0.27797305  - Train acc: 0.8943 - Val loss: 1.01957929\n",
      "(14.90 min) Epoch 74/300 -- Iteration 70911 - Batch 612/963 - Train loss: 0.27744207  - Train acc: 0.8945 - Val loss: 1.01957929\n",
      "(14.91 min) Epoch 74/300 -- Iteration 70920 - Batch 621/963 - Train loss: 0.27732168  - Train acc: 0.8945 - Val loss: 1.01957929\n",
      "(14.91 min) Epoch 74/300 -- Iteration 70929 - Batch 630/963 - Train loss: 0.27709410  - Train acc: 0.8945 - Val loss: 1.01957929\n",
      "(14.91 min) Epoch 74/300 -- Iteration 70938 - Batch 639/963 - Train loss: 0.27687491  - Train acc: 0.8946 - Val loss: 1.01957929\n",
      "(14.91 min) Epoch 74/300 -- Iteration 70947 - Batch 648/963 - Train loss: 0.27671400  - Train acc: 0.8947 - Val loss: 1.01957929\n",
      "(14.91 min) Epoch 74/300 -- Iteration 70956 - Batch 657/963 - Train loss: 0.27642279  - Train acc: 0.8949 - Val loss: 1.01957929\n",
      "(14.92 min) Epoch 74/300 -- Iteration 70965 - Batch 666/963 - Train loss: 0.27620676  - Train acc: 0.8951 - Val loss: 1.01957929\n",
      "(14.92 min) Epoch 74/300 -- Iteration 70974 - Batch 675/963 - Train loss: 0.27617595  - Train acc: 0.8950 - Val loss: 1.01957929\n",
      "(14.92 min) Epoch 74/300 -- Iteration 70983 - Batch 684/963 - Train loss: 0.27571956  - Train acc: 0.8952 - Val loss: 1.01957929\n",
      "(14.92 min) Epoch 74/300 -- Iteration 70992 - Batch 693/963 - Train loss: 0.27544120  - Train acc: 0.8952 - Val loss: 1.01957929\n",
      "(14.92 min) Epoch 74/300 -- Iteration 71001 - Batch 702/963 - Train loss: 0.27506470  - Train acc: 0.8954 - Val loss: 1.01957929\n",
      "(14.92 min) Epoch 74/300 -- Iteration 71010 - Batch 711/963 - Train loss: 0.27533915  - Train acc: 0.8953 - Val loss: 1.01957929\n",
      "(14.93 min) Epoch 74/300 -- Iteration 71019 - Batch 720/963 - Train loss: 0.27520422  - Train acc: 0.8954 - Val loss: 1.01957929\n",
      "(14.93 min) Epoch 74/300 -- Iteration 71028 - Batch 729/963 - Train loss: 0.27547760  - Train acc: 0.8953 - Val loss: 1.01957929\n",
      "(14.93 min) Epoch 74/300 -- Iteration 71037 - Batch 738/963 - Train loss: 0.27591305  - Train acc: 0.8951 - Val loss: 1.01957929\n",
      "(14.93 min) Epoch 74/300 -- Iteration 71046 - Batch 747/963 - Train loss: 0.27566804  - Train acc: 0.8953 - Val loss: 1.01957929\n",
      "(14.93 min) Epoch 74/300 -- Iteration 71055 - Batch 756/963 - Train loss: 0.27573069  - Train acc: 0.8952 - Val loss: 1.01957929\n",
      "(14.94 min) Epoch 74/300 -- Iteration 71064 - Batch 765/963 - Train loss: 0.27548931  - Train acc: 0.8953 - Val loss: 1.01957929\n",
      "(14.94 min) Epoch 74/300 -- Iteration 71073 - Batch 774/963 - Train loss: 0.27534619  - Train acc: 0.8954 - Val loss: 1.01957929\n",
      "(14.94 min) Epoch 74/300 -- Iteration 71082 - Batch 783/963 - Train loss: 0.27532799  - Train acc: 0.8955 - Val loss: 1.01957929\n",
      "(14.94 min) Epoch 74/300 -- Iteration 71091 - Batch 792/963 - Train loss: 0.27521493  - Train acc: 0.8956 - Val loss: 1.01957929\n",
      "(14.94 min) Epoch 74/300 -- Iteration 71100 - Batch 801/963 - Train loss: 0.27488892  - Train acc: 0.8957 - Val loss: 1.01957929\n",
      "(14.95 min) Epoch 74/300 -- Iteration 71109 - Batch 810/963 - Train loss: 0.27517775  - Train acc: 0.8957 - Val loss: 1.01957929\n",
      "(14.95 min) Epoch 74/300 -- Iteration 71118 - Batch 819/963 - Train loss: 0.27459558  - Train acc: 0.8960 - Val loss: 1.01957929\n",
      "(14.95 min) Epoch 74/300 -- Iteration 71127 - Batch 828/963 - Train loss: 0.27466671  - Train acc: 0.8959 - Val loss: 1.01957929\n",
      "(14.95 min) Epoch 74/300 -- Iteration 71136 - Batch 837/963 - Train loss: 0.27433475  - Train acc: 0.8960 - Val loss: 1.01957929\n",
      "(14.95 min) Epoch 74/300 -- Iteration 71145 - Batch 846/963 - Train loss: 0.27437264  - Train acc: 0.8959 - Val loss: 1.01957929\n",
      "(14.96 min) Epoch 74/300 -- Iteration 71154 - Batch 855/963 - Train loss: 0.27422814  - Train acc: 0.8960 - Val loss: 1.01957929\n",
      "(14.96 min) Epoch 74/300 -- Iteration 71163 - Batch 864/963 - Train loss: 0.27419119  - Train acc: 0.8960 - Val loss: 1.01957929\n",
      "(14.96 min) Epoch 74/300 -- Iteration 71172 - Batch 873/963 - Train loss: 0.27405329  - Train acc: 0.8961 - Val loss: 1.01957929\n",
      "(14.96 min) Epoch 74/300 -- Iteration 71181 - Batch 882/963 - Train loss: 0.27401047  - Train acc: 0.8962 - Val loss: 1.01957929\n",
      "(14.96 min) Epoch 74/300 -- Iteration 71190 - Batch 891/963 - Train loss: 0.27371797  - Train acc: 0.8963 - Val loss: 1.01957929\n",
      "(14.96 min) Epoch 74/300 -- Iteration 71199 - Batch 900/963 - Train loss: 0.27348918  - Train acc: 0.8964 - Val loss: 1.01957929\n",
      "(14.97 min) Epoch 74/300 -- Iteration 71208 - Batch 909/963 - Train loss: 0.27316368  - Train acc: 0.8966 - Val loss: 1.01957929\n",
      "(14.97 min) Epoch 74/300 -- Iteration 71217 - Batch 918/963 - Train loss: 0.27340423  - Train acc: 0.8965 - Val loss: 1.01957929\n",
      "(14.97 min) Epoch 74/300 -- Iteration 71226 - Batch 927/963 - Train loss: 0.27353644  - Train acc: 0.8966 - Val loss: 1.01957929\n",
      "(14.97 min) Epoch 74/300 -- Iteration 71235 - Batch 936/963 - Train loss: 0.27351571  - Train acc: 0.8966 - Val loss: 1.01957929\n",
      "(14.97 min) Epoch 74/300 -- Iteration 71244 - Batch 945/963 - Train loss: 0.27339120  - Train acc: 0.8966 - Val loss: 1.01957929\n",
      "(14.98 min) Epoch 74/300 -- Iteration 71253 - Batch 954/963 - Train loss: 0.27340056  - Train acc: 0.8966 - Val loss: 1.01957929\n",
      "(14.98 min) Epoch 74/300 -- Iteration 71262 - Batch 962/963 - Train loss: 0.27343454  - Train acc: 0.8966 - Val loss: 1.01272130 - Val acc: 0.5933\n",
      "(14.98 min) Epoch 75/300 -- Iteration 71271 - Batch 9/963 - Train loss: 0.25038720  - Train acc: 0.9039 - Val loss: 1.01272130\n",
      "(14.98 min) Epoch 75/300 -- Iteration 71280 - Batch 18/963 - Train loss: 0.26115344  - Train acc: 0.9005 - Val loss: 1.01272130\n",
      "(14.98 min) Epoch 75/300 -- Iteration 71289 - Batch 27/963 - Train loss: 0.26317953  - Train acc: 0.8996 - Val loss: 1.01272130\n",
      "(14.99 min) Epoch 75/300 -- Iteration 71298 - Batch 36/963 - Train loss: 0.27122473  - Train acc: 0.8967 - Val loss: 1.01272130\n",
      "(14.99 min) Epoch 75/300 -- Iteration 71307 - Batch 45/963 - Train loss: 0.26985016  - Train acc: 0.8972 - Val loss: 1.01272130\n",
      "(14.99 min) Epoch 75/300 -- Iteration 71316 - Batch 54/963 - Train loss: 0.27567188  - Train acc: 0.8967 - Val loss: 1.01272130\n",
      "(14.99 min) Epoch 75/300 -- Iteration 71325 - Batch 63/963 - Train loss: 0.27487866  - Train acc: 0.8976 - Val loss: 1.01272130\n",
      "(14.99 min) Epoch 75/300 -- Iteration 71334 - Batch 72/963 - Train loss: 0.27208878  - Train acc: 0.8982 - Val loss: 1.01272130\n",
      "(15.00 min) Epoch 75/300 -- Iteration 71343 - Batch 81/963 - Train loss: 0.26819879  - Train acc: 0.8997 - Val loss: 1.01272130\n",
      "(15.00 min) Epoch 75/300 -- Iteration 71352 - Batch 90/963 - Train loss: 0.26989507  - Train acc: 0.8990 - Val loss: 1.01272130\n",
      "(15.00 min) Epoch 75/300 -- Iteration 71361 - Batch 99/963 - Train loss: 0.27060966  - Train acc: 0.8987 - Val loss: 1.01272130\n",
      "(15.00 min) Epoch 75/300 -- Iteration 71370 - Batch 108/963 - Train loss: 0.27061149  - Train acc: 0.8991 - Val loss: 1.01272130\n",
      "(15.00 min) Epoch 75/300 -- Iteration 71379 - Batch 117/963 - Train loss: 0.26807211  - Train acc: 0.9001 - Val loss: 1.01272130\n",
      "(15.00 min) Epoch 75/300 -- Iteration 71388 - Batch 126/963 - Train loss: 0.26511056  - Train acc: 0.9011 - Val loss: 1.01272130\n",
      "(15.01 min) Epoch 75/300 -- Iteration 71397 - Batch 135/963 - Train loss: 0.26622688  - Train acc: 0.9004 - Val loss: 1.01272130\n",
      "(15.01 min) Epoch 75/300 -- Iteration 71406 - Batch 144/963 - Train loss: 0.26853835  - Train acc: 0.8991 - Val loss: 1.01272130\n",
      "(15.01 min) Epoch 75/300 -- Iteration 71415 - Batch 153/963 - Train loss: 0.26950768  - Train acc: 0.8997 - Val loss: 1.01272130\n",
      "(15.01 min) Epoch 75/300 -- Iteration 71424 - Batch 162/963 - Train loss: 0.26864749  - Train acc: 0.9000 - Val loss: 1.01272130\n",
      "(15.01 min) Epoch 75/300 -- Iteration 71433 - Batch 171/963 - Train loss: 0.26914894  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.02 min) Epoch 75/300 -- Iteration 71442 - Batch 180/963 - Train loss: 0.26776388  - Train acc: 0.9001 - Val loss: 1.01272130\n",
      "(15.02 min) Epoch 75/300 -- Iteration 71451 - Batch 189/963 - Train loss: 0.26768433  - Train acc: 0.9001 - Val loss: 1.01272130\n",
      "(15.02 min) Epoch 75/300 -- Iteration 71460 - Batch 198/963 - Train loss: 0.26724108  - Train acc: 0.9005 - Val loss: 1.01272130\n",
      "(15.02 min) Epoch 75/300 -- Iteration 71469 - Batch 207/963 - Train loss: 0.26861518  - Train acc: 0.8998 - Val loss: 1.01272130\n",
      "(15.02 min) Epoch 75/300 -- Iteration 71478 - Batch 216/963 - Train loss: 0.26750293  - Train acc: 0.9001 - Val loss: 1.01272130\n",
      "(15.03 min) Epoch 75/300 -- Iteration 71487 - Batch 225/963 - Train loss: 0.26595932  - Train acc: 0.9010 - Val loss: 1.01272130\n",
      "(15.03 min) Epoch 75/300 -- Iteration 71496 - Batch 234/963 - Train loss: 0.26621994  - Train acc: 0.9010 - Val loss: 1.01272130\n",
      "(15.03 min) Epoch 75/300 -- Iteration 71505 - Batch 243/963 - Train loss: 0.26702241  - Train acc: 0.9006 - Val loss: 1.01272130\n",
      "(15.03 min) Epoch 75/300 -- Iteration 71514 - Batch 252/963 - Train loss: 0.26726876  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.03 min) Epoch 75/300 -- Iteration 71523 - Batch 261/963 - Train loss: 0.26718279  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.04 min) Epoch 75/300 -- Iteration 71532 - Batch 270/963 - Train loss: 0.26694856  - Train acc: 0.9009 - Val loss: 1.01272130\n",
      "(15.04 min) Epoch 75/300 -- Iteration 71541 - Batch 279/963 - Train loss: 0.26732472  - Train acc: 0.9006 - Val loss: 1.01272130\n",
      "(15.04 min) Epoch 75/300 -- Iteration 71550 - Batch 288/963 - Train loss: 0.26705602  - Train acc: 0.9010 - Val loss: 1.01272130\n",
      "(15.04 min) Epoch 75/300 -- Iteration 71559 - Batch 297/963 - Train loss: 0.26570997  - Train acc: 0.9018 - Val loss: 1.01272130\n",
      "(15.04 min) Epoch 75/300 -- Iteration 71568 - Batch 306/963 - Train loss: 0.26561224  - Train acc: 0.9016 - Val loss: 1.01272130\n",
      "(15.04 min) Epoch 75/300 -- Iteration 71577 - Batch 315/963 - Train loss: 0.26593546  - Train acc: 0.9015 - Val loss: 1.01272130\n",
      "(15.05 min) Epoch 75/300 -- Iteration 71586 - Batch 324/963 - Train loss: 0.26563062  - Train acc: 0.9019 - Val loss: 1.01272130\n",
      "(15.05 min) Epoch 75/300 -- Iteration 71595 - Batch 333/963 - Train loss: 0.26616145  - Train acc: 0.9015 - Val loss: 1.01272130\n",
      "(15.05 min) Epoch 75/300 -- Iteration 71604 - Batch 342/963 - Train loss: 0.26653661  - Train acc: 0.9014 - Val loss: 1.01272130\n",
      "(15.05 min) Epoch 75/300 -- Iteration 71613 - Batch 351/963 - Train loss: 0.26721876  - Train acc: 0.9010 - Val loss: 1.01272130\n",
      "(15.05 min) Epoch 75/300 -- Iteration 71622 - Batch 360/963 - Train loss: 0.26694185  - Train acc: 0.9012 - Val loss: 1.01272130\n",
      "(15.06 min) Epoch 75/300 -- Iteration 71631 - Batch 369/963 - Train loss: 0.26675282  - Train acc: 0.9010 - Val loss: 1.01272130\n",
      "(15.06 min) Epoch 75/300 -- Iteration 71640 - Batch 378/963 - Train loss: 0.26680741  - Train acc: 0.9009 - Val loss: 1.01272130\n",
      "(15.06 min) Epoch 75/300 -- Iteration 71649 - Batch 387/963 - Train loss: 0.26714153  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.06 min) Epoch 75/300 -- Iteration 71658 - Batch 396/963 - Train loss: 0.26757809  - Train acc: 0.9005 - Val loss: 1.01272130\n",
      "(15.06 min) Epoch 75/300 -- Iteration 71667 - Batch 405/963 - Train loss: 0.26766558  - Train acc: 0.9003 - Val loss: 1.01272130\n",
      "(15.06 min) Epoch 75/300 -- Iteration 71676 - Batch 414/963 - Train loss: 0.26720593  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.07 min) Epoch 75/300 -- Iteration 71685 - Batch 423/963 - Train loss: 0.26692134  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.07 min) Epoch 75/300 -- Iteration 71694 - Batch 432/963 - Train loss: 0.26702606  - Train acc: 0.9008 - Val loss: 1.01272130\n",
      "(15.07 min) Epoch 75/300 -- Iteration 71703 - Batch 441/963 - Train loss: 0.26692280  - Train acc: 0.9009 - Val loss: 1.01272130\n",
      "(15.07 min) Epoch 75/300 -- Iteration 71712 - Batch 450/963 - Train loss: 0.26653045  - Train acc: 0.9011 - Val loss: 1.01272130\n",
      "(15.07 min) Epoch 75/300 -- Iteration 71721 - Batch 459/963 - Train loss: 0.26643750  - Train acc: 0.9012 - Val loss: 1.01272130\n",
      "(15.08 min) Epoch 75/300 -- Iteration 71730 - Batch 468/963 - Train loss: 0.26647576  - Train acc: 0.9011 - Val loss: 1.01272130\n",
      "(15.08 min) Epoch 75/300 -- Iteration 71739 - Batch 477/963 - Train loss: 0.26680629  - Train acc: 0.9008 - Val loss: 1.01272130\n",
      "(15.08 min) Epoch 75/300 -- Iteration 71748 - Batch 486/963 - Train loss: 0.26685818  - Train acc: 0.9008 - Val loss: 1.01272130\n",
      "(15.08 min) Epoch 75/300 -- Iteration 71757 - Batch 495/963 - Train loss: 0.26704678  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.08 min) Epoch 75/300 -- Iteration 71766 - Batch 504/963 - Train loss: 0.26695656  - Train acc: 0.9006 - Val loss: 1.01272130\n",
      "(15.09 min) Epoch 75/300 -- Iteration 71775 - Batch 513/963 - Train loss: 0.26683212  - Train acc: 0.9006 - Val loss: 1.01272130\n",
      "(15.09 min) Epoch 75/300 -- Iteration 71784 - Batch 522/963 - Train loss: 0.26615820  - Train acc: 0.9008 - Val loss: 1.01272130\n",
      "(15.09 min) Epoch 75/300 -- Iteration 71793 - Batch 531/963 - Train loss: 0.26663734  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.09 min) Epoch 75/300 -- Iteration 71802 - Batch 540/963 - Train loss: 0.26630980  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.09 min) Epoch 75/300 -- Iteration 71811 - Batch 549/963 - Train loss: 0.26621107  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.09 min) Epoch 75/300 -- Iteration 71820 - Batch 558/963 - Train loss: 0.26595930  - Train acc: 0.9007 - Val loss: 1.01272130\n",
      "(15.10 min) Epoch 75/300 -- Iteration 71829 - Batch 567/963 - Train loss: 0.26612633  - Train acc: 0.9005 - Val loss: 1.01272130\n",
      "(15.10 min) Epoch 75/300 -- Iteration 71838 - Batch 576/963 - Train loss: 0.26702312  - Train acc: 0.9001 - Val loss: 1.01272130\n",
      "(15.10 min) Epoch 75/300 -- Iteration 71847 - Batch 585/963 - Train loss: 0.26671862  - Train acc: 0.9002 - Val loss: 1.01272130\n",
      "(15.10 min) Epoch 75/300 -- Iteration 71856 - Batch 594/963 - Train loss: 0.26658783  - Train acc: 0.9002 - Val loss: 1.01272130\n",
      "(15.10 min) Epoch 75/300 -- Iteration 71865 - Batch 603/963 - Train loss: 0.26651681  - Train acc: 0.9004 - Val loss: 1.01272130\n",
      "(15.11 min) Epoch 75/300 -- Iteration 71874 - Batch 612/963 - Train loss: 0.26694215  - Train acc: 0.9002 - Val loss: 1.01272130\n",
      "(15.11 min) Epoch 75/300 -- Iteration 71883 - Batch 621/963 - Train loss: 0.26708505  - Train acc: 0.9001 - Val loss: 1.01272130\n",
      "(15.11 min) Epoch 75/300 -- Iteration 71892 - Batch 630/963 - Train loss: 0.26658292  - Train acc: 0.9004 - Val loss: 1.01272130\n",
      "(15.11 min) Epoch 75/300 -- Iteration 71901 - Batch 639/963 - Train loss: 0.26639390  - Train acc: 0.9004 - Val loss: 1.01272130\n",
      "(15.11 min) Epoch 75/300 -- Iteration 71910 - Batch 648/963 - Train loss: 0.26650553  - Train acc: 0.9003 - Val loss: 1.01272130\n",
      "(15.12 min) Epoch 75/300 -- Iteration 71919 - Batch 657/963 - Train loss: 0.26656807  - Train acc: 0.9002 - Val loss: 1.01272130\n",
      "(15.12 min) Epoch 75/300 -- Iteration 71928 - Batch 666/963 - Train loss: 0.26643528  - Train acc: 0.9002 - Val loss: 1.01272130\n",
      "(15.12 min) Epoch 75/300 -- Iteration 71937 - Batch 675/963 - Train loss: 0.26691898  - Train acc: 0.9001 - Val loss: 1.01272130\n",
      "(15.12 min) Epoch 75/300 -- Iteration 71946 - Batch 684/963 - Train loss: 0.26720394  - Train acc: 0.8999 - Val loss: 1.01272130\n",
      "(15.12 min) Epoch 75/300 -- Iteration 71955 - Batch 693/963 - Train loss: 0.26764015  - Train acc: 0.8996 - Val loss: 1.01272130\n",
      "(15.13 min) Epoch 75/300 -- Iteration 71964 - Batch 702/963 - Train loss: 0.26795250  - Train acc: 0.8995 - Val loss: 1.01272130\n",
      "(15.13 min) Epoch 75/300 -- Iteration 71973 - Batch 711/963 - Train loss: 0.26772043  - Train acc: 0.8996 - Val loss: 1.01272130\n",
      "(15.13 min) Epoch 75/300 -- Iteration 71982 - Batch 720/963 - Train loss: 0.26793237  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.13 min) Epoch 75/300 -- Iteration 71991 - Batch 729/963 - Train loss: 0.26853655  - Train acc: 0.8992 - Val loss: 1.01272130\n",
      "(15.13 min) Epoch 75/300 -- Iteration 72000 - Batch 738/963 - Train loss: 0.26841346  - Train acc: 0.8993 - Val loss: 1.01272130\n",
      "(15.13 min) Epoch 75/300 -- Iteration 72009 - Batch 747/963 - Train loss: 0.26859937  - Train acc: 0.8992 - Val loss: 1.01272130\n",
      "(15.14 min) Epoch 75/300 -- Iteration 72018 - Batch 756/963 - Train loss: 0.26870039  - Train acc: 0.8992 - Val loss: 1.01272130\n",
      "(15.14 min) Epoch 75/300 -- Iteration 72027 - Batch 765/963 - Train loss: 0.26880862  - Train acc: 0.8992 - Val loss: 1.01272130\n",
      "(15.14 min) Epoch 75/300 -- Iteration 72036 - Batch 774/963 - Train loss: 0.26887414  - Train acc: 0.8991 - Val loss: 1.01272130\n",
      "(15.14 min) Epoch 75/300 -- Iteration 72045 - Batch 783/963 - Train loss: 0.26832545  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.14 min) Epoch 75/300 -- Iteration 72054 - Batch 792/963 - Train loss: 0.26832003  - Train acc: 0.8993 - Val loss: 1.01272130\n",
      "(15.15 min) Epoch 75/300 -- Iteration 72063 - Batch 801/963 - Train loss: 0.26807324  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.15 min) Epoch 75/300 -- Iteration 72072 - Batch 810/963 - Train loss: 0.26795777  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.15 min) Epoch 75/300 -- Iteration 72081 - Batch 819/963 - Train loss: 0.26787984  - Train acc: 0.8995 - Val loss: 1.01272130\n",
      "(15.15 min) Epoch 75/300 -- Iteration 72090 - Batch 828/963 - Train loss: 0.26836951  - Train acc: 0.8993 - Val loss: 1.01272130\n",
      "(15.15 min) Epoch 75/300 -- Iteration 72099 - Batch 837/963 - Train loss: 0.26821655  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.16 min) Epoch 75/300 -- Iteration 72108 - Batch 846/963 - Train loss: 0.26812386  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.16 min) Epoch 75/300 -- Iteration 72117 - Batch 855/963 - Train loss: 0.26817044  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.16 min) Epoch 75/300 -- Iteration 72126 - Batch 864/963 - Train loss: 0.26798906  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.16 min) Epoch 75/300 -- Iteration 72135 - Batch 873/963 - Train loss: 0.26835901  - Train acc: 0.8992 - Val loss: 1.01272130\n",
      "(15.16 min) Epoch 75/300 -- Iteration 72144 - Batch 882/963 - Train loss: 0.26843887  - Train acc: 0.8992 - Val loss: 1.01272130\n",
      "(15.17 min) Epoch 75/300 -- Iteration 72153 - Batch 891/963 - Train loss: 0.26840298  - Train acc: 0.8992 - Val loss: 1.01272130\n",
      "(15.17 min) Epoch 75/300 -- Iteration 72162 - Batch 900/963 - Train loss: 0.26848037  - Train acc: 0.8991 - Val loss: 1.01272130\n",
      "(15.17 min) Epoch 75/300 -- Iteration 72171 - Batch 909/963 - Train loss: 0.26843810  - Train acc: 0.8992 - Val loss: 1.01272130\n",
      "(15.17 min) Epoch 75/300 -- Iteration 72180 - Batch 918/963 - Train loss: 0.26825269  - Train acc: 0.8993 - Val loss: 1.01272130\n",
      "(15.17 min) Epoch 75/300 -- Iteration 72189 - Batch 927/963 - Train loss: 0.26823935  - Train acc: 0.8993 - Val loss: 1.01272130\n",
      "(15.17 min) Epoch 75/300 -- Iteration 72198 - Batch 936/963 - Train loss: 0.26824627  - Train acc: 0.8993 - Val loss: 1.01272130\n",
      "(15.18 min) Epoch 75/300 -- Iteration 72207 - Batch 945/963 - Train loss: 0.26795995  - Train acc: 0.8994 - Val loss: 1.01272130\n",
      "(15.18 min) Epoch 75/300 -- Iteration 72216 - Batch 954/963 - Train loss: 0.26784221  - Train acc: 0.8995 - Val loss: 1.01272130\n",
      "(15.18 min) Epoch 75/300 -- Iteration 72225 - Batch 962/963 - Train loss: 0.26806100  - Train acc: 0.8994 - Val loss: 1.00318754 - Val acc: 0.5933\n",
      "(15.18 min) Epoch 76/300 -- Iteration 72234 - Batch 9/963 - Train loss: 0.24560878  - Train acc: 0.9125 - Val loss: 1.00318754\n",
      "(15.18 min) Epoch 76/300 -- Iteration 72243 - Batch 18/963 - Train loss: 0.24588453  - Train acc: 0.9067 - Val loss: 1.00318754\n",
      "(15.19 min) Epoch 76/300 -- Iteration 72252 - Batch 27/963 - Train loss: 0.25117318  - Train acc: 0.9043 - Val loss: 1.00318754\n",
      "(15.19 min) Epoch 76/300 -- Iteration 72261 - Batch 36/963 - Train loss: 0.26104384  - Train acc: 0.9014 - Val loss: 1.00318754\n",
      "(15.19 min) Epoch 76/300 -- Iteration 72270 - Batch 45/963 - Train loss: 0.26526391  - Train acc: 0.9003 - Val loss: 1.00318754\n",
      "(15.19 min) Epoch 76/300 -- Iteration 72279 - Batch 54/963 - Train loss: 0.26300905  - Train acc: 0.9003 - Val loss: 1.00318754\n",
      "(15.19 min) Epoch 76/300 -- Iteration 72288 - Batch 63/963 - Train loss: 0.26148200  - Train acc: 0.9006 - Val loss: 1.00318754\n",
      "(15.20 min) Epoch 76/300 -- Iteration 72297 - Batch 72/963 - Train loss: 0.26016506  - Train acc: 0.9005 - Val loss: 1.00318754\n",
      "(15.20 min) Epoch 76/300 -- Iteration 72306 - Batch 81/963 - Train loss: 0.26357838  - Train acc: 0.8994 - Val loss: 1.00318754\n",
      "(15.20 min) Epoch 76/300 -- Iteration 72315 - Batch 90/963 - Train loss: 0.26323382  - Train acc: 0.9002 - Val loss: 1.00318754\n",
      "(15.20 min) Epoch 76/300 -- Iteration 72324 - Batch 99/963 - Train loss: 0.26249258  - Train acc: 0.9009 - Val loss: 1.00318754\n",
      "(15.20 min) Epoch 76/300 -- Iteration 72333 - Batch 108/963 - Train loss: 0.26473017  - Train acc: 0.9007 - Val loss: 1.00318754\n",
      "(15.21 min) Epoch 76/300 -- Iteration 72342 - Batch 117/963 - Train loss: 0.26639183  - Train acc: 0.9002 - Val loss: 1.00318754\n",
      "(15.21 min) Epoch 76/300 -- Iteration 72351 - Batch 126/963 - Train loss: 0.26646471  - Train acc: 0.9002 - Val loss: 1.00318754\n",
      "(15.21 min) Epoch 76/300 -- Iteration 72360 - Batch 135/963 - Train loss: 0.26835290  - Train acc: 0.8994 - Val loss: 1.00318754\n",
      "(15.21 min) Epoch 76/300 -- Iteration 72369 - Batch 144/963 - Train loss: 0.26967730  - Train acc: 0.8989 - Val loss: 1.00318754\n",
      "(15.21 min) Epoch 76/300 -- Iteration 72378 - Batch 153/963 - Train loss: 0.26966540  - Train acc: 0.8991 - Val loss: 1.00318754\n",
      "(15.21 min) Epoch 76/300 -- Iteration 72387 - Batch 162/963 - Train loss: 0.26957101  - Train acc: 0.8993 - Val loss: 1.00318754\n",
      "(15.22 min) Epoch 76/300 -- Iteration 72396 - Batch 171/963 - Train loss: 0.26998339  - Train acc: 0.8988 - Val loss: 1.00318754\n",
      "(15.22 min) Epoch 76/300 -- Iteration 72405 - Batch 180/963 - Train loss: 0.27137749  - Train acc: 0.8982 - Val loss: 1.00318754\n",
      "(15.22 min) Epoch 76/300 -- Iteration 72414 - Batch 189/963 - Train loss: 0.27051786  - Train acc: 0.8988 - Val loss: 1.00318754\n",
      "(15.22 min) Epoch 76/300 -- Iteration 72423 - Batch 198/963 - Train loss: 0.27193600  - Train acc: 0.8977 - Val loss: 1.00318754\n",
      "(15.22 min) Epoch 76/300 -- Iteration 72432 - Batch 207/963 - Train loss: 0.27172199  - Train acc: 0.8977 - Val loss: 1.00318754\n",
      "(15.23 min) Epoch 76/300 -- Iteration 72441 - Batch 216/963 - Train loss: 0.27248110  - Train acc: 0.8972 - Val loss: 1.00318754\n",
      "(15.23 min) Epoch 76/300 -- Iteration 72450 - Batch 225/963 - Train loss: 0.27214347  - Train acc: 0.8975 - Val loss: 1.00318754\n",
      "(15.23 min) Epoch 76/300 -- Iteration 72459 - Batch 234/963 - Train loss: 0.27164061  - Train acc: 0.8980 - Val loss: 1.00318754\n",
      "(15.23 min) Epoch 76/300 -- Iteration 72468 - Batch 243/963 - Train loss: 0.27178733  - Train acc: 0.8980 - Val loss: 1.00318754\n",
      "(15.23 min) Epoch 76/300 -- Iteration 72477 - Batch 252/963 - Train loss: 0.27208853  - Train acc: 0.8979 - Val loss: 1.00318754\n",
      "(15.24 min) Epoch 76/300 -- Iteration 72486 - Batch 261/963 - Train loss: 0.27135341  - Train acc: 0.8978 - Val loss: 1.00318754\n",
      "(15.24 min) Epoch 76/300 -- Iteration 72495 - Batch 270/963 - Train loss: 0.27155789  - Train acc: 0.8979 - Val loss: 1.00318754\n",
      "(15.24 min) Epoch 76/300 -- Iteration 72504 - Batch 279/963 - Train loss: 0.27152121  - Train acc: 0.8980 - Val loss: 1.00318754\n",
      "(15.24 min) Epoch 76/300 -- Iteration 72513 - Batch 288/963 - Train loss: 0.27191533  - Train acc: 0.8977 - Val loss: 1.00318754\n",
      "(15.24 min) Epoch 76/300 -- Iteration 72522 - Batch 297/963 - Train loss: 0.27245637  - Train acc: 0.8977 - Val loss: 1.00318754\n",
      "(15.24 min) Epoch 76/300 -- Iteration 72531 - Batch 306/963 - Train loss: 0.27196904  - Train acc: 0.8978 - Val loss: 1.00318754\n",
      "(15.25 min) Epoch 76/300 -- Iteration 72540 - Batch 315/963 - Train loss: 0.27190285  - Train acc: 0.8978 - Val loss: 1.00318754\n",
      "(15.25 min) Epoch 76/300 -- Iteration 72549 - Batch 324/963 - Train loss: 0.27166384  - Train acc: 0.8980 - Val loss: 1.00318754\n",
      "(15.25 min) Epoch 76/300 -- Iteration 72558 - Batch 333/963 - Train loss: 0.27182892  - Train acc: 0.8978 - Val loss: 1.00318754\n",
      "(15.25 min) Epoch 76/300 -- Iteration 72567 - Batch 342/963 - Train loss: 0.27156962  - Train acc: 0.8976 - Val loss: 1.00318754\n",
      "(15.25 min) Epoch 76/300 -- Iteration 72576 - Batch 351/963 - Train loss: 0.27156507  - Train acc: 0.8977 - Val loss: 1.00318754\n",
      "(15.26 min) Epoch 76/300 -- Iteration 72585 - Batch 360/963 - Train loss: 0.27161094  - Train acc: 0.8975 - Val loss: 1.00318754\n",
      "(15.26 min) Epoch 76/300 -- Iteration 72594 - Batch 369/963 - Train loss: 0.27057055  - Train acc: 0.8979 - Val loss: 1.00318754\n",
      "(15.26 min) Epoch 76/300 -- Iteration 72603 - Batch 378/963 - Train loss: 0.27032109  - Train acc: 0.8980 - Val loss: 1.00318754\n",
      "(15.26 min) Epoch 76/300 -- Iteration 72612 - Batch 387/963 - Train loss: 0.27022987  - Train acc: 0.8979 - Val loss: 1.00318754\n",
      "(15.26 min) Epoch 76/300 -- Iteration 72621 - Batch 396/963 - Train loss: 0.27000667  - Train acc: 0.8979 - Val loss: 1.00318754\n",
      "(15.27 min) Epoch 76/300 -- Iteration 72630 - Batch 405/963 - Train loss: 0.26954348  - Train acc: 0.8980 - Val loss: 1.00318754\n",
      "(15.27 min) Epoch 76/300 -- Iteration 72639 - Batch 414/963 - Train loss: 0.26874808  - Train acc: 0.8984 - Val loss: 1.00318754\n",
      "(15.27 min) Epoch 76/300 -- Iteration 72648 - Batch 423/963 - Train loss: 0.26887701  - Train acc: 0.8983 - Val loss: 1.00318754\n",
      "(15.27 min) Epoch 76/300 -- Iteration 72657 - Batch 432/963 - Train loss: 0.26872151  - Train acc: 0.8983 - Val loss: 1.00318754\n",
      "(15.27 min) Epoch 76/300 -- Iteration 72666 - Batch 441/963 - Train loss: 0.26915778  - Train acc: 0.8982 - Val loss: 1.00318754\n",
      "(15.27 min) Epoch 76/300 -- Iteration 72675 - Batch 450/963 - Train loss: 0.26866901  - Train acc: 0.8984 - Val loss: 1.00318754\n",
      "(15.28 min) Epoch 76/300 -- Iteration 72684 - Batch 459/963 - Train loss: 0.26808238  - Train acc: 0.8987 - Val loss: 1.00318754\n",
      "(15.28 min) Epoch 76/300 -- Iteration 72693 - Batch 468/963 - Train loss: 0.26846712  - Train acc: 0.8986 - Val loss: 1.00318754\n",
      "(15.28 min) Epoch 76/300 -- Iteration 72702 - Batch 477/963 - Train loss: 0.26841359  - Train acc: 0.8986 - Val loss: 1.00318754\n",
      "(15.28 min) Epoch 76/300 -- Iteration 72711 - Batch 486/963 - Train loss: 0.26803245  - Train acc: 0.8987 - Val loss: 1.00318754\n",
      "(15.28 min) Epoch 76/300 -- Iteration 72720 - Batch 495/963 - Train loss: 0.26927914  - Train acc: 0.8981 - Val loss: 1.00318754\n",
      "(15.29 min) Epoch 76/300 -- Iteration 72729 - Batch 504/963 - Train loss: 0.26945530  - Train acc: 0.8981 - Val loss: 1.00318754\n",
      "(15.29 min) Epoch 76/300 -- Iteration 72738 - Batch 513/963 - Train loss: 0.26932684  - Train acc: 0.8982 - Val loss: 1.00318754\n",
      "(15.29 min) Epoch 76/300 -- Iteration 72747 - Batch 522/963 - Train loss: 0.26916125  - Train acc: 0.8982 - Val loss: 1.00318754\n",
      "(15.29 min) Epoch 76/300 -- Iteration 72756 - Batch 531/963 - Train loss: 0.26874795  - Train acc: 0.8984 - Val loss: 1.00318754\n",
      "(15.29 min) Epoch 76/300 -- Iteration 72765 - Batch 540/963 - Train loss: 0.26882882  - Train acc: 0.8984 - Val loss: 1.00318754\n",
      "(15.30 min) Epoch 76/300 -- Iteration 72774 - Batch 549/963 - Train loss: 0.26911654  - Train acc: 0.8984 - Val loss: 1.00318754\n",
      "(15.30 min) Epoch 76/300 -- Iteration 72783 - Batch 558/963 - Train loss: 0.26925762  - Train acc: 0.8985 - Val loss: 1.00318754\n",
      "(15.30 min) Epoch 76/300 -- Iteration 72792 - Batch 567/963 - Train loss: 0.26927790  - Train acc: 0.8985 - Val loss: 1.00318754\n",
      "(15.30 min) Epoch 76/300 -- Iteration 72801 - Batch 576/963 - Train loss: 0.26937545  - Train acc: 0.8985 - Val loss: 1.00318754\n",
      "(15.30 min) Epoch 76/300 -- Iteration 72810 - Batch 585/963 - Train loss: 0.26999315  - Train acc: 0.8982 - Val loss: 1.00318754\n",
      "(15.30 min) Epoch 76/300 -- Iteration 72819 - Batch 594/963 - Train loss: 0.27040674  - Train acc: 0.8980 - Val loss: 1.00318754\n",
      "(15.31 min) Epoch 76/300 -- Iteration 72828 - Batch 603/963 - Train loss: 0.27024252  - Train acc: 0.8982 - Val loss: 1.00318754\n",
      "(15.31 min) Epoch 76/300 -- Iteration 72837 - Batch 612/963 - Train loss: 0.26974114  - Train acc: 0.8984 - Val loss: 1.00318754\n",
      "(15.31 min) Epoch 76/300 -- Iteration 72846 - Batch 621/963 - Train loss: 0.27010012  - Train acc: 0.8983 - Val loss: 1.00318754\n",
      "(15.31 min) Epoch 76/300 -- Iteration 72855 - Batch 630/963 - Train loss: 0.27020261  - Train acc: 0.8981 - Val loss: 1.00318754\n",
      "(15.31 min) Epoch 76/300 -- Iteration 72864 - Batch 639/963 - Train loss: 0.26958271  - Train acc: 0.8984 - Val loss: 1.00318754\n",
      "(15.32 min) Epoch 76/300 -- Iteration 72873 - Batch 648/963 - Train loss: 0.26960018  - Train acc: 0.8983 - Val loss: 1.00318754\n",
      "(15.32 min) Epoch 76/300 -- Iteration 72882 - Batch 657/963 - Train loss: 0.26946260  - Train acc: 0.8984 - Val loss: 1.00318754\n",
      "(15.32 min) Epoch 76/300 -- Iteration 72891 - Batch 666/963 - Train loss: 0.26917656  - Train acc: 0.8986 - Val loss: 1.00318754\n",
      "(15.32 min) Epoch 76/300 -- Iteration 72900 - Batch 675/963 - Train loss: 0.26886721  - Train acc: 0.8987 - Val loss: 1.00318754\n",
      "(15.32 min) Epoch 76/300 -- Iteration 72909 - Batch 684/963 - Train loss: 0.26896057  - Train acc: 0.8987 - Val loss: 1.00318754\n",
      "(15.33 min) Epoch 76/300 -- Iteration 72918 - Batch 693/963 - Train loss: 0.26893498  - Train acc: 0.8987 - Val loss: 1.00318754\n",
      "(15.33 min) Epoch 76/300 -- Iteration 72927 - Batch 702/963 - Train loss: 0.26906162  - Train acc: 0.8986 - Val loss: 1.00318754\n",
      "(15.33 min) Epoch 76/300 -- Iteration 72936 - Batch 711/963 - Train loss: 0.26890947  - Train acc: 0.8987 - Val loss: 1.00318754\n",
      "(15.33 min) Epoch 76/300 -- Iteration 72945 - Batch 720/963 - Train loss: 0.26923550  - Train acc: 0.8986 - Val loss: 1.00318754\n",
      "(15.33 min) Epoch 76/300 -- Iteration 72954 - Batch 729/963 - Train loss: 0.26910697  - Train acc: 0.8987 - Val loss: 1.00318754\n",
      "(15.34 min) Epoch 76/300 -- Iteration 72963 - Batch 738/963 - Train loss: 0.26908586  - Train acc: 0.8988 - Val loss: 1.00318754\n",
      "(15.34 min) Epoch 76/300 -- Iteration 72972 - Batch 747/963 - Train loss: 0.26927518  - Train acc: 0.8988 - Val loss: 1.00318754\n",
      "(15.34 min) Epoch 76/300 -- Iteration 72981 - Batch 756/963 - Train loss: 0.26921600  - Train acc: 0.8988 - Val loss: 1.00318754\n",
      "(15.34 min) Epoch 76/300 -- Iteration 72990 - Batch 765/963 - Train loss: 0.26932110  - Train acc: 0.8988 - Val loss: 1.00318754\n",
      "(15.34 min) Epoch 76/300 -- Iteration 72999 - Batch 774/963 - Train loss: 0.26908359  - Train acc: 0.8990 - Val loss: 1.00318754\n",
      "(15.34 min) Epoch 76/300 -- Iteration 73008 - Batch 783/963 - Train loss: 0.26933986  - Train acc: 0.8989 - Val loss: 1.00318754\n",
      "(15.35 min) Epoch 76/300 -- Iteration 73017 - Batch 792/963 - Train loss: 0.26964308  - Train acc: 0.8987 - Val loss: 1.00318754\n",
      "(15.35 min) Epoch 76/300 -- Iteration 73026 - Batch 801/963 - Train loss: 0.26970157  - Train acc: 0.8986 - Val loss: 1.00318754\n",
      "(15.35 min) Epoch 76/300 -- Iteration 73035 - Batch 810/963 - Train loss: 0.26925613  - Train acc: 0.8989 - Val loss: 1.00318754\n",
      "(15.35 min) Epoch 76/300 -- Iteration 73044 - Batch 819/963 - Train loss: 0.26969640  - Train acc: 0.8987 - Val loss: 1.00318754\n",
      "(15.35 min) Epoch 76/300 -- Iteration 73053 - Batch 828/963 - Train loss: 0.26953860  - Train acc: 0.8988 - Val loss: 1.00318754\n",
      "(15.36 min) Epoch 76/300 -- Iteration 73062 - Batch 837/963 - Train loss: 0.26929011  - Train acc: 0.8989 - Val loss: 1.00318754\n",
      "(15.36 min) Epoch 76/300 -- Iteration 73071 - Batch 846/963 - Train loss: 0.26934052  - Train acc: 0.8989 - Val loss: 1.00318754\n",
      "(15.36 min) Epoch 76/300 -- Iteration 73080 - Batch 855/963 - Train loss: 0.26913256  - Train acc: 0.8990 - Val loss: 1.00318754\n",
      "(15.36 min) Epoch 76/300 -- Iteration 73089 - Batch 864/963 - Train loss: 0.26894683  - Train acc: 0.8991 - Val loss: 1.00318754\n",
      "(15.36 min) Epoch 76/300 -- Iteration 73098 - Batch 873/963 - Train loss: 0.26874795  - Train acc: 0.8992 - Val loss: 1.00318754\n",
      "(15.37 min) Epoch 76/300 -- Iteration 73107 - Batch 882/963 - Train loss: 0.26878914  - Train acc: 0.8992 - Val loss: 1.00318754\n",
      "(15.37 min) Epoch 76/300 -- Iteration 73116 - Batch 891/963 - Train loss: 0.26877603  - Train acc: 0.8992 - Val loss: 1.00318754\n",
      "(15.37 min) Epoch 76/300 -- Iteration 73125 - Batch 900/963 - Train loss: 0.26884059  - Train acc: 0.8991 - Val loss: 1.00318754\n",
      "(15.37 min) Epoch 76/300 -- Iteration 73134 - Batch 909/963 - Train loss: 0.26878433  - Train acc: 0.8991 - Val loss: 1.00318754\n",
      "(15.37 min) Epoch 76/300 -- Iteration 73143 - Batch 918/963 - Train loss: 0.26891093  - Train acc: 0.8991 - Val loss: 1.00318754\n",
      "(15.38 min) Epoch 76/300 -- Iteration 73152 - Batch 927/963 - Train loss: 0.26896278  - Train acc: 0.8991 - Val loss: 1.00318754\n",
      "(15.38 min) Epoch 76/300 -- Iteration 73161 - Batch 936/963 - Train loss: 0.26928862  - Train acc: 0.8990 - Val loss: 1.00318754\n",
      "(15.38 min) Epoch 76/300 -- Iteration 73170 - Batch 945/963 - Train loss: 0.26943717  - Train acc: 0.8988 - Val loss: 1.00318754\n",
      "(15.38 min) Epoch 76/300 -- Iteration 73179 - Batch 954/963 - Train loss: 0.26923015  - Train acc: 0.8989 - Val loss: 1.00318754\n",
      "(15.38 min) Epoch 76/300 -- Iteration 73188 - Batch 962/963 - Train loss: 0.26928128  - Train acc: 0.8989 - Val loss: 1.01023543 - Val acc: 0.5900\n",
      "(15.39 min) Epoch 77/300 -- Iteration 73197 - Batch 9/963 - Train loss: 0.30059343  - Train acc: 0.8750 - Val loss: 1.01023543\n",
      "(15.39 min) Epoch 77/300 -- Iteration 73206 - Batch 18/963 - Train loss: 0.30122258  - Train acc: 0.8787 - Val loss: 1.01023543\n",
      "(15.39 min) Epoch 77/300 -- Iteration 73215 - Batch 27/963 - Train loss: 0.29166538  - Train acc: 0.8862 - Val loss: 1.01023543\n",
      "(15.39 min) Epoch 77/300 -- Iteration 73224 - Batch 36/963 - Train loss: 0.28984336  - Train acc: 0.8896 - Val loss: 1.01023543\n",
      "(15.39 min) Epoch 77/300 -- Iteration 73233 - Batch 45/963 - Train loss: 0.28103757  - Train acc: 0.8939 - Val loss: 1.01023543\n",
      "(15.40 min) Epoch 77/300 -- Iteration 73242 - Batch 54/963 - Train loss: 0.27809672  - Train acc: 0.8938 - Val loss: 1.01023543\n",
      "(15.40 min) Epoch 77/300 -- Iteration 73251 - Batch 63/963 - Train loss: 0.27968165  - Train acc: 0.8925 - Val loss: 1.01023543\n",
      "(15.40 min) Epoch 77/300 -- Iteration 73260 - Batch 72/963 - Train loss: 0.28022806  - Train acc: 0.8932 - Val loss: 1.01023543\n",
      "(15.40 min) Epoch 77/300 -- Iteration 73269 - Batch 81/963 - Train loss: 0.28228876  - Train acc: 0.8930 - Val loss: 1.01023543\n",
      "(15.40 min) Epoch 77/300 -- Iteration 73278 - Batch 90/963 - Train loss: 0.28386248  - Train acc: 0.8923 - Val loss: 1.01023543\n",
      "(15.40 min) Epoch 77/300 -- Iteration 73287 - Batch 99/963 - Train loss: 0.28254123  - Train acc: 0.8930 - Val loss: 1.01023543\n",
      "(15.41 min) Epoch 77/300 -- Iteration 73296 - Batch 108/963 - Train loss: 0.28269277  - Train acc: 0.8934 - Val loss: 1.01023543\n",
      "(15.41 min) Epoch 77/300 -- Iteration 73305 - Batch 117/963 - Train loss: 0.28090638  - Train acc: 0.8941 - Val loss: 1.01023543\n",
      "(15.41 min) Epoch 77/300 -- Iteration 73314 - Batch 126/963 - Train loss: 0.27958571  - Train acc: 0.8952 - Val loss: 1.01023543\n",
      "(15.41 min) Epoch 77/300 -- Iteration 73323 - Batch 135/963 - Train loss: 0.27763915  - Train acc: 0.8959 - Val loss: 1.01023543\n",
      "(15.41 min) Epoch 77/300 -- Iteration 73332 - Batch 144/963 - Train loss: 0.27756120  - Train acc: 0.8959 - Val loss: 1.01023543\n",
      "(15.42 min) Epoch 77/300 -- Iteration 73341 - Batch 153/963 - Train loss: 0.27679790  - Train acc: 0.8958 - Val loss: 1.01023543\n",
      "(15.42 min) Epoch 77/300 -- Iteration 73350 - Batch 162/963 - Train loss: 0.27412394  - Train acc: 0.8965 - Val loss: 1.01023543\n",
      "(15.42 min) Epoch 77/300 -- Iteration 73359 - Batch 171/963 - Train loss: 0.27459326  - Train acc: 0.8958 - Val loss: 1.01023543\n",
      "(15.42 min) Epoch 77/300 -- Iteration 73368 - Batch 180/963 - Train loss: 0.27404435  - Train acc: 0.8957 - Val loss: 1.01023543\n",
      "(15.42 min) Epoch 77/300 -- Iteration 73377 - Batch 189/963 - Train loss: 0.27182444  - Train acc: 0.8965 - Val loss: 1.01023543\n",
      "(15.43 min) Epoch 77/300 -- Iteration 73386 - Batch 198/963 - Train loss: 0.27205187  - Train acc: 0.8963 - Val loss: 1.01023543\n",
      "(15.43 min) Epoch 77/300 -- Iteration 73395 - Batch 207/963 - Train loss: 0.27193867  - Train acc: 0.8963 - Val loss: 1.01023543\n",
      "(15.43 min) Epoch 77/300 -- Iteration 73404 - Batch 216/963 - Train loss: 0.27101322  - Train acc: 0.8965 - Val loss: 1.01023543\n",
      "(15.43 min) Epoch 77/300 -- Iteration 73413 - Batch 225/963 - Train loss: 0.27020717  - Train acc: 0.8968 - Val loss: 1.01023543\n",
      "(15.43 min) Epoch 77/300 -- Iteration 73422 - Batch 234/963 - Train loss: 0.27092985  - Train acc: 0.8966 - Val loss: 1.01023543\n",
      "(15.44 min) Epoch 77/300 -- Iteration 73431 - Batch 243/963 - Train loss: 0.27116454  - Train acc: 0.8967 - Val loss: 1.01023543\n",
      "(15.44 min) Epoch 77/300 -- Iteration 73440 - Batch 252/963 - Train loss: 0.27053510  - Train acc: 0.8970 - Val loss: 1.01023543\n",
      "(15.44 min) Epoch 77/300 -- Iteration 73449 - Batch 261/963 - Train loss: 0.26980556  - Train acc: 0.8974 - Val loss: 1.01023543\n",
      "(15.44 min) Epoch 77/300 -- Iteration 73458 - Batch 270/963 - Train loss: 0.27016889  - Train acc: 0.8975 - Val loss: 1.01023543\n",
      "(15.45 min) Epoch 77/300 -- Iteration 73467 - Batch 279/963 - Train loss: 0.27107672  - Train acc: 0.8974 - Val loss: 1.01023543\n",
      "(15.45 min) Epoch 77/300 -- Iteration 73476 - Batch 288/963 - Train loss: 0.27175582  - Train acc: 0.8974 - Val loss: 1.01023543\n",
      "(15.45 min) Epoch 77/300 -- Iteration 73485 - Batch 297/963 - Train loss: 0.27157043  - Train acc: 0.8973 - Val loss: 1.01023543\n",
      "(15.45 min) Epoch 77/300 -- Iteration 73494 - Batch 306/963 - Train loss: 0.27083610  - Train acc: 0.8977 - Val loss: 1.01023543\n",
      "(15.45 min) Epoch 77/300 -- Iteration 73503 - Batch 315/963 - Train loss: 0.27029067  - Train acc: 0.8979 - Val loss: 1.01023543\n",
      "(15.45 min) Epoch 77/300 -- Iteration 73512 - Batch 324/963 - Train loss: 0.27019525  - Train acc: 0.8979 - Val loss: 1.01023543\n",
      "(15.46 min) Epoch 77/300 -- Iteration 73521 - Batch 333/963 - Train loss: 0.27087538  - Train acc: 0.8977 - Val loss: 1.01023543\n",
      "(15.46 min) Epoch 77/300 -- Iteration 73530 - Batch 342/963 - Train loss: 0.27111320  - Train acc: 0.8975 - Val loss: 1.01023543\n",
      "(15.46 min) Epoch 77/300 -- Iteration 73539 - Batch 351/963 - Train loss: 0.27029036  - Train acc: 0.8977 - Val loss: 1.01023543\n",
      "(15.46 min) Epoch 77/300 -- Iteration 73548 - Batch 360/963 - Train loss: 0.27137494  - Train acc: 0.8973 - Val loss: 1.01023543\n",
      "(15.46 min) Epoch 77/300 -- Iteration 73557 - Batch 369/963 - Train loss: 0.27132338  - Train acc: 0.8974 - Val loss: 1.01023543\n",
      "(15.47 min) Epoch 77/300 -- Iteration 73566 - Batch 378/963 - Train loss: 0.27097520  - Train acc: 0.8976 - Val loss: 1.01023543\n",
      "(15.47 min) Epoch 77/300 -- Iteration 73575 - Batch 387/963 - Train loss: 0.27148725  - Train acc: 0.8974 - Val loss: 1.01023543\n",
      "(15.47 min) Epoch 77/300 -- Iteration 73584 - Batch 396/963 - Train loss: 0.27113789  - Train acc: 0.8975 - Val loss: 1.01023543\n",
      "(15.47 min) Epoch 77/300 -- Iteration 73593 - Batch 405/963 - Train loss: 0.27163515  - Train acc: 0.8974 - Val loss: 1.01023543\n",
      "(15.47 min) Epoch 77/300 -- Iteration 73602 - Batch 414/963 - Train loss: 0.27144847  - Train acc: 0.8975 - Val loss: 1.01023543\n",
      "(15.48 min) Epoch 77/300 -- Iteration 73611 - Batch 423/963 - Train loss: 0.27160591  - Train acc: 0.8974 - Val loss: 1.01023543\n",
      "(15.48 min) Epoch 77/300 -- Iteration 73620 - Batch 432/963 - Train loss: 0.27144283  - Train acc: 0.8975 - Val loss: 1.01023543\n",
      "(15.48 min) Epoch 77/300 -- Iteration 73629 - Batch 441/963 - Train loss: 0.27156630  - Train acc: 0.8976 - Val loss: 1.01023543\n",
      "(15.48 min) Epoch 77/300 -- Iteration 73638 - Batch 450/963 - Train loss: 0.27198601  - Train acc: 0.8974 - Val loss: 1.01023543\n",
      "(15.48 min) Epoch 77/300 -- Iteration 73647 - Batch 459/963 - Train loss: 0.27152419  - Train acc: 0.8978 - Val loss: 1.01023543\n",
      "(15.49 min) Epoch 77/300 -- Iteration 73656 - Batch 468/963 - Train loss: 0.27156267  - Train acc: 0.8977 - Val loss: 1.01023543\n",
      "(15.49 min) Epoch 77/300 -- Iteration 73665 - Batch 477/963 - Train loss: 0.27153292  - Train acc: 0.8976 - Val loss: 1.01023543\n",
      "(15.49 min) Epoch 77/300 -- Iteration 73674 - Batch 486/963 - Train loss: 0.27145290  - Train acc: 0.8977 - Val loss: 1.01023543\n",
      "(15.49 min) Epoch 77/300 -- Iteration 73683 - Batch 495/963 - Train loss: 0.27112698  - Train acc: 0.8977 - Val loss: 1.01023543\n",
      "(15.49 min) Epoch 77/300 -- Iteration 73692 - Batch 504/963 - Train loss: 0.27095338  - Train acc: 0.8979 - Val loss: 1.01023543\n",
      "(15.49 min) Epoch 77/300 -- Iteration 73701 - Batch 513/963 - Train loss: 0.27077346  - Train acc: 0.8979 - Val loss: 1.01023543\n",
      "(15.50 min) Epoch 77/300 -- Iteration 73710 - Batch 522/963 - Train loss: 0.27122680  - Train acc: 0.8978 - Val loss: 1.01023543\n",
      "(15.50 min) Epoch 77/300 -- Iteration 73719 - Batch 531/963 - Train loss: 0.27148892  - Train acc: 0.8977 - Val loss: 1.01023543\n",
      "(15.50 min) Epoch 77/300 -- Iteration 73728 - Batch 540/963 - Train loss: 0.27173281  - Train acc: 0.8976 - Val loss: 1.01023543\n",
      "(15.50 min) Epoch 77/300 -- Iteration 73737 - Batch 549/963 - Train loss: 0.27165750  - Train acc: 0.8978 - Val loss: 1.01023543\n",
      "(15.50 min) Epoch 77/300 -- Iteration 73746 - Batch 558/963 - Train loss: 0.27167382  - Train acc: 0.8978 - Val loss: 1.01023543\n",
      "(15.51 min) Epoch 77/300 -- Iteration 73755 - Batch 567/963 - Train loss: 0.27164220  - Train acc: 0.8978 - Val loss: 1.01023543\n",
      "(15.51 min) Epoch 77/300 -- Iteration 73764 - Batch 576/963 - Train loss: 0.27160653  - Train acc: 0.8978 - Val loss: 1.01023543\n",
      "(15.51 min) Epoch 77/300 -- Iteration 73773 - Batch 585/963 - Train loss: 0.27159242  - Train acc: 0.8978 - Val loss: 1.01023543\n",
      "(15.51 min) Epoch 77/300 -- Iteration 73782 - Batch 594/963 - Train loss: 0.27135362  - Train acc: 0.8979 - Val loss: 1.01023543\n",
      "(15.51 min) Epoch 77/300 -- Iteration 73791 - Batch 603/963 - Train loss: 0.27091123  - Train acc: 0.8980 - Val loss: 1.01023543\n",
      "(15.52 min) Epoch 77/300 -- Iteration 73800 - Batch 612/963 - Train loss: 0.27125552  - Train acc: 0.8980 - Val loss: 1.01023543\n",
      "(15.52 min) Epoch 77/300 -- Iteration 73809 - Batch 621/963 - Train loss: 0.27106438  - Train acc: 0.8980 - Val loss: 1.01023543\n",
      "(15.52 min) Epoch 77/300 -- Iteration 73818 - Batch 630/963 - Train loss: 0.27048764  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.52 min) Epoch 77/300 -- Iteration 73827 - Batch 639/963 - Train loss: 0.27036229  - Train acc: 0.8982 - Val loss: 1.01023543\n",
      "(15.52 min) Epoch 77/300 -- Iteration 73836 - Batch 648/963 - Train loss: 0.27031622  - Train acc: 0.8981 - Val loss: 1.01023543\n",
      "(15.53 min) Epoch 77/300 -- Iteration 73845 - Batch 657/963 - Train loss: 0.27026777  - Train acc: 0.8982 - Val loss: 1.01023543\n",
      "(15.53 min) Epoch 77/300 -- Iteration 73854 - Batch 666/963 - Train loss: 0.26989320  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.53 min) Epoch 77/300 -- Iteration 73863 - Batch 675/963 - Train loss: 0.26982600  - Train acc: 0.8982 - Val loss: 1.01023543\n",
      "(15.53 min) Epoch 77/300 -- Iteration 73872 - Batch 684/963 - Train loss: 0.27007499  - Train acc: 0.8981 - Val loss: 1.01023543\n",
      "(15.53 min) Epoch 77/300 -- Iteration 73881 - Batch 693/963 - Train loss: 0.27053463  - Train acc: 0.8979 - Val loss: 1.01023543\n",
      "(15.54 min) Epoch 77/300 -- Iteration 73890 - Batch 702/963 - Train loss: 0.27055330  - Train acc: 0.8979 - Val loss: 1.01023543\n",
      "(15.54 min) Epoch 77/300 -- Iteration 73899 - Batch 711/963 - Train loss: 0.27013837  - Train acc: 0.8980 - Val loss: 1.01023543\n",
      "(15.54 min) Epoch 77/300 -- Iteration 73908 - Batch 720/963 - Train loss: 0.27031625  - Train acc: 0.8980 - Val loss: 1.01023543\n",
      "(15.54 min) Epoch 77/300 -- Iteration 73917 - Batch 729/963 - Train loss: 0.27002852  - Train acc: 0.8981 - Val loss: 1.01023543\n",
      "(15.55 min) Epoch 77/300 -- Iteration 73926 - Batch 738/963 - Train loss: 0.26990407  - Train acc: 0.8982 - Val loss: 1.01023543\n",
      "(15.55 min) Epoch 77/300 -- Iteration 73935 - Batch 747/963 - Train loss: 0.26989407  - Train acc: 0.8982 - Val loss: 1.01023543\n",
      "(15.55 min) Epoch 77/300 -- Iteration 73944 - Batch 756/963 - Train loss: 0.27015767  - Train acc: 0.8981 - Val loss: 1.01023543\n",
      "(15.55 min) Epoch 77/300 -- Iteration 73953 - Batch 765/963 - Train loss: 0.26977486  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.55 min) Epoch 77/300 -- Iteration 73962 - Batch 774/963 - Train loss: 0.26975158  - Train acc: 0.8984 - Val loss: 1.01023543\n",
      "(15.56 min) Epoch 77/300 -- Iteration 73971 - Batch 783/963 - Train loss: 0.26975419  - Train acc: 0.8984 - Val loss: 1.01023543\n",
      "(15.56 min) Epoch 77/300 -- Iteration 73980 - Batch 792/963 - Train loss: 0.26966361  - Train acc: 0.8985 - Val loss: 1.01023543\n",
      "(15.56 min) Epoch 77/300 -- Iteration 73989 - Batch 801/963 - Train loss: 0.26974508  - Train acc: 0.8984 - Val loss: 1.01023543\n",
      "(15.56 min) Epoch 77/300 -- Iteration 73998 - Batch 810/963 - Train loss: 0.26981313  - Train acc: 0.8984 - Val loss: 1.01023543\n",
      "(15.56 min) Epoch 77/300 -- Iteration 74007 - Batch 819/963 - Train loss: 0.26971813  - Train acc: 0.8984 - Val loss: 1.01023543\n",
      "(15.57 min) Epoch 77/300 -- Iteration 74016 - Batch 828/963 - Train loss: 0.26993721  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.57 min) Epoch 77/300 -- Iteration 74025 - Batch 837/963 - Train loss: 0.26949255  - Train acc: 0.8985 - Val loss: 1.01023543\n",
      "(15.57 min) Epoch 77/300 -- Iteration 74034 - Batch 846/963 - Train loss: 0.26974334  - Train acc: 0.8985 - Val loss: 1.01023543\n",
      "(15.57 min) Epoch 77/300 -- Iteration 74043 - Batch 855/963 - Train loss: 0.26974115  - Train acc: 0.8984 - Val loss: 1.01023543\n",
      "(15.58 min) Epoch 77/300 -- Iteration 74052 - Batch 864/963 - Train loss: 0.26965215  - Train acc: 0.8984 - Val loss: 1.01023543\n",
      "(15.58 min) Epoch 77/300 -- Iteration 74061 - Batch 873/963 - Train loss: 0.26951376  - Train acc: 0.8985 - Val loss: 1.01023543\n",
      "(15.58 min) Epoch 77/300 -- Iteration 74070 - Batch 882/963 - Train loss: 0.26962822  - Train acc: 0.8984 - Val loss: 1.01023543\n",
      "(15.58 min) Epoch 77/300 -- Iteration 74079 - Batch 891/963 - Train loss: 0.26960469  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.58 min) Epoch 77/300 -- Iteration 74088 - Batch 900/963 - Train loss: 0.26977489  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.59 min) Epoch 77/300 -- Iteration 74097 - Batch 909/963 - Train loss: 0.26963857  - Train acc: 0.8982 - Val loss: 1.01023543\n",
      "(15.59 min) Epoch 77/300 -- Iteration 74106 - Batch 918/963 - Train loss: 0.26950411  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.59 min) Epoch 77/300 -- Iteration 74115 - Batch 927/963 - Train loss: 0.26942755  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.59 min) Epoch 77/300 -- Iteration 74124 - Batch 936/963 - Train loss: 0.26947436  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.59 min) Epoch 77/300 -- Iteration 74133 - Batch 945/963 - Train loss: 0.26953644  - Train acc: 0.8983 - Val loss: 1.01023543\n",
      "(15.59 min) Epoch 77/300 -- Iteration 74142 - Batch 954/963 - Train loss: 0.26972142  - Train acc: 0.8982 - Val loss: 1.01023543\n",
      "(15.60 min) Epoch 77/300 -- Iteration 74151 - Batch 962/963 - Train loss: 0.26964030  - Train acc: 0.8983 - Val loss: 1.02331364 - Val acc: 0.5833\n",
      "(15.60 min) Epoch 78/300 -- Iteration 74160 - Batch 9/963 - Train loss: 0.27859807  - Train acc: 0.8930 - Val loss: 1.02331364\n",
      "(15.60 min) Epoch 78/300 -- Iteration 74169 - Batch 18/963 - Train loss: 0.27428753  - Train acc: 0.8972 - Val loss: 1.02331364\n",
      "(15.60 min) Epoch 78/300 -- Iteration 74178 - Batch 27/963 - Train loss: 0.27258472  - Train acc: 0.8993 - Val loss: 1.02331364\n",
      "(15.61 min) Epoch 78/300 -- Iteration 74187 - Batch 36/963 - Train loss: 0.27352086  - Train acc: 0.8980 - Val loss: 1.02331364\n",
      "(15.61 min) Epoch 78/300 -- Iteration 74196 - Batch 45/963 - Train loss: 0.27683012  - Train acc: 0.8976 - Val loss: 1.02331364\n",
      "(15.61 min) Epoch 78/300 -- Iteration 74205 - Batch 54/963 - Train loss: 0.27227441  - Train acc: 0.8997 - Val loss: 1.02331364\n",
      "(15.61 min) Epoch 78/300 -- Iteration 74214 - Batch 63/963 - Train loss: 0.27613832  - Train acc: 0.8971 - Val loss: 1.02331364\n",
      "(15.61 min) Epoch 78/300 -- Iteration 74223 - Batch 72/963 - Train loss: 0.27582846  - Train acc: 0.8967 - Val loss: 1.02331364\n",
      "(15.61 min) Epoch 78/300 -- Iteration 74232 - Batch 81/963 - Train loss: 0.27518303  - Train acc: 0.8959 - Val loss: 1.02331364\n",
      "(15.62 min) Epoch 78/300 -- Iteration 74241 - Batch 90/963 - Train loss: 0.27419085  - Train acc: 0.8959 - Val loss: 1.02331364\n",
      "(15.62 min) Epoch 78/300 -- Iteration 74250 - Batch 99/963 - Train loss: 0.27456728  - Train acc: 0.8954 - Val loss: 1.02331364\n",
      "(15.62 min) Epoch 78/300 -- Iteration 74259 - Batch 108/963 - Train loss: 0.27369812  - Train acc: 0.8957 - Val loss: 1.02331364\n",
      "(15.62 min) Epoch 78/300 -- Iteration 74268 - Batch 117/963 - Train loss: 0.27138668  - Train acc: 0.8967 - Val loss: 1.02331364\n",
      "(15.63 min) Epoch 78/300 -- Iteration 74277 - Batch 126/963 - Train loss: 0.27368439  - Train acc: 0.8956 - Val loss: 1.02331364\n",
      "(15.63 min) Epoch 78/300 -- Iteration 74286 - Batch 135/963 - Train loss: 0.27207321  - Train acc: 0.8951 - Val loss: 1.02331364\n",
      "(15.63 min) Epoch 78/300 -- Iteration 74295 - Batch 144/963 - Train loss: 0.27174463  - Train acc: 0.8955 - Val loss: 1.02331364\n",
      "(15.63 min) Epoch 78/300 -- Iteration 74304 - Batch 153/963 - Train loss: 0.27112570  - Train acc: 0.8960 - Val loss: 1.02331364\n",
      "(15.63 min) Epoch 78/300 -- Iteration 74313 - Batch 162/963 - Train loss: 0.27029605  - Train acc: 0.8966 - Val loss: 1.02331364\n",
      "(15.64 min) Epoch 78/300 -- Iteration 74322 - Batch 171/963 - Train loss: 0.26904520  - Train acc: 0.8968 - Val loss: 1.02331364\n",
      "(15.64 min) Epoch 78/300 -- Iteration 74331 - Batch 180/963 - Train loss: 0.26905311  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.64 min) Epoch 78/300 -- Iteration 74340 - Batch 189/963 - Train loss: 0.27119988  - Train acc: 0.8956 - Val loss: 1.02331364\n",
      "(15.64 min) Epoch 78/300 -- Iteration 74349 - Batch 198/963 - Train loss: 0.27026600  - Train acc: 0.8959 - Val loss: 1.02331364\n",
      "(15.64 min) Epoch 78/300 -- Iteration 74358 - Batch 207/963 - Train loss: 0.27037594  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.65 min) Epoch 78/300 -- Iteration 74367 - Batch 216/963 - Train loss: 0.27071324  - Train acc: 0.8962 - Val loss: 1.02331364\n",
      "(15.65 min) Epoch 78/300 -- Iteration 74376 - Batch 225/963 - Train loss: 0.27035351  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.65 min) Epoch 78/300 -- Iteration 74385 - Batch 234/963 - Train loss: 0.27093970  - Train acc: 0.8960 - Val loss: 1.02331364\n",
      "(15.65 min) Epoch 78/300 -- Iteration 74394 - Batch 243/963 - Train loss: 0.27155526  - Train acc: 0.8955 - Val loss: 1.02331364\n",
      "(15.65 min) Epoch 78/300 -- Iteration 74403 - Batch 252/963 - Train loss: 0.27092447  - Train acc: 0.8958 - Val loss: 1.02331364\n",
      "(15.66 min) Epoch 78/300 -- Iteration 74412 - Batch 261/963 - Train loss: 0.27015635  - Train acc: 0.8960 - Val loss: 1.02331364\n",
      "(15.66 min) Epoch 78/300 -- Iteration 74421 - Batch 270/963 - Train loss: 0.27030619  - Train acc: 0.8960 - Val loss: 1.02331364\n",
      "(15.66 min) Epoch 78/300 -- Iteration 74430 - Batch 279/963 - Train loss: 0.27048638  - Train acc: 0.8958 - Val loss: 1.02331364\n",
      "(15.66 min) Epoch 78/300 -- Iteration 74439 - Batch 288/963 - Train loss: 0.27098596  - Train acc: 0.8959 - Val loss: 1.02331364\n",
      "(15.67 min) Epoch 78/300 -- Iteration 74448 - Batch 297/963 - Train loss: 0.27063869  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.67 min) Epoch 78/300 -- Iteration 74457 - Batch 306/963 - Train loss: 0.27040042  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.67 min) Epoch 78/300 -- Iteration 74466 - Batch 315/963 - Train loss: 0.27054535  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.67 min) Epoch 78/300 -- Iteration 74475 - Batch 324/963 - Train loss: 0.27043035  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.67 min) Epoch 78/300 -- Iteration 74484 - Batch 333/963 - Train loss: 0.27019062  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.68 min) Epoch 78/300 -- Iteration 74493 - Batch 342/963 - Train loss: 0.26986753  - Train acc: 0.8965 - Val loss: 1.02331364\n",
      "(15.68 min) Epoch 78/300 -- Iteration 74502 - Batch 351/963 - Train loss: 0.27013487  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.68 min) Epoch 78/300 -- Iteration 74511 - Batch 360/963 - Train loss: 0.26993086  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.68 min) Epoch 78/300 -- Iteration 74520 - Batch 369/963 - Train loss: 0.26983608  - Train acc: 0.8965 - Val loss: 1.02331364\n",
      "(15.68 min) Epoch 78/300 -- Iteration 74529 - Batch 378/963 - Train loss: 0.26917353  - Train acc: 0.8965 - Val loss: 1.02331364\n",
      "(15.69 min) Epoch 78/300 -- Iteration 74538 - Batch 387/963 - Train loss: 0.26878590  - Train acc: 0.8966 - Val loss: 1.02331364\n",
      "(15.69 min) Epoch 78/300 -- Iteration 74547 - Batch 396/963 - Train loss: 0.26888366  - Train acc: 0.8965 - Val loss: 1.02331364\n",
      "(15.69 min) Epoch 78/300 -- Iteration 74556 - Batch 405/963 - Train loss: 0.26945978  - Train acc: 0.8962 - Val loss: 1.02331364\n",
      "(15.69 min) Epoch 78/300 -- Iteration 74565 - Batch 414/963 - Train loss: 0.27003547  - Train acc: 0.8960 - Val loss: 1.02331364\n",
      "(15.70 min) Epoch 78/300 -- Iteration 74574 - Batch 423/963 - Train loss: 0.26966933  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.70 min) Epoch 78/300 -- Iteration 74583 - Batch 432/963 - Train loss: 0.26953131  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.70 min) Epoch 78/300 -- Iteration 74592 - Batch 441/963 - Train loss: 0.26952473  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.70 min) Epoch 78/300 -- Iteration 74601 - Batch 450/963 - Train loss: 0.26945779  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.70 min) Epoch 78/300 -- Iteration 74610 - Batch 459/963 - Train loss: 0.26978517  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.71 min) Epoch 78/300 -- Iteration 74619 - Batch 468/963 - Train loss: 0.26974265  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.71 min) Epoch 78/300 -- Iteration 74628 - Batch 477/963 - Train loss: 0.26988993  - Train acc: 0.8960 - Val loss: 1.02331364\n",
      "(15.71 min) Epoch 78/300 -- Iteration 74637 - Batch 486/963 - Train loss: 0.27065130  - Train acc: 0.8958 - Val loss: 1.02331364\n",
      "(15.71 min) Epoch 78/300 -- Iteration 74646 - Batch 495/963 - Train loss: 0.27054187  - Train acc: 0.8958 - Val loss: 1.02331364\n",
      "(15.71 min) Epoch 78/300 -- Iteration 74655 - Batch 504/963 - Train loss: 0.27074173  - Train acc: 0.8957 - Val loss: 1.02331364\n",
      "(15.72 min) Epoch 78/300 -- Iteration 74664 - Batch 513/963 - Train loss: 0.27105760  - Train acc: 0.8957 - Val loss: 1.02331364\n",
      "(15.72 min) Epoch 78/300 -- Iteration 74673 - Batch 522/963 - Train loss: 0.27087919  - Train acc: 0.8957 - Val loss: 1.02331364\n",
      "(15.72 min) Epoch 78/300 -- Iteration 74682 - Batch 531/963 - Train loss: 0.27045703  - Train acc: 0.8960 - Val loss: 1.02331364\n",
      "(15.72 min) Epoch 78/300 -- Iteration 74691 - Batch 540/963 - Train loss: 0.27014070  - Train acc: 0.8960 - Val loss: 1.02331364\n",
      "(15.72 min) Epoch 78/300 -- Iteration 74700 - Batch 549/963 - Train loss: 0.27025067  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.73 min) Epoch 78/300 -- Iteration 74709 - Batch 558/963 - Train loss: 0.27022572  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.73 min) Epoch 78/300 -- Iteration 74718 - Batch 567/963 - Train loss: 0.27032662  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.73 min) Epoch 78/300 -- Iteration 74727 - Batch 576/963 - Train loss: 0.26988271  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.73 min) Epoch 78/300 -- Iteration 74736 - Batch 585/963 - Train loss: 0.26998831  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.73 min) Epoch 78/300 -- Iteration 74745 - Batch 594/963 - Train loss: 0.26968913  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.74 min) Epoch 78/300 -- Iteration 74754 - Batch 603/963 - Train loss: 0.27032913  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.74 min) Epoch 78/300 -- Iteration 74763 - Batch 612/963 - Train loss: 0.27033404  - Train acc: 0.8962 - Val loss: 1.02331364\n",
      "(15.74 min) Epoch 78/300 -- Iteration 74772 - Batch 621/963 - Train loss: 0.27047231  - Train acc: 0.8962 - Val loss: 1.02331364\n",
      "(15.74 min) Epoch 78/300 -- Iteration 74781 - Batch 630/963 - Train loss: 0.27006958  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.74 min) Epoch 78/300 -- Iteration 74790 - Batch 639/963 - Train loss: 0.27050249  - Train acc: 0.8962 - Val loss: 1.02331364\n",
      "(15.75 min) Epoch 78/300 -- Iteration 74799 - Batch 648/963 - Train loss: 0.27057900  - Train acc: 0.8961 - Val loss: 1.02331364\n",
      "(15.75 min) Epoch 78/300 -- Iteration 74808 - Batch 657/963 - Train loss: 0.27029289  - Train acc: 0.8962 - Val loss: 1.02331364\n",
      "(15.75 min) Epoch 78/300 -- Iteration 74817 - Batch 666/963 - Train loss: 0.27005057  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.75 min) Epoch 78/300 -- Iteration 74826 - Batch 675/963 - Train loss: 0.27014583  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.75 min) Epoch 78/300 -- Iteration 74835 - Batch 684/963 - Train loss: 0.27013887  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.76 min) Epoch 78/300 -- Iteration 74844 - Batch 693/963 - Train loss: 0.27005867  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.76 min) Epoch 78/300 -- Iteration 74853 - Batch 702/963 - Train loss: 0.26984501  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.76 min) Epoch 78/300 -- Iteration 74862 - Batch 711/963 - Train loss: 0.27019154  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.76 min) Epoch 78/300 -- Iteration 74871 - Batch 720/963 - Train loss: 0.26989434  - Train acc: 0.8965 - Val loss: 1.02331364\n",
      "(15.76 min) Epoch 78/300 -- Iteration 74880 - Batch 729/963 - Train loss: 0.27014768  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.76 min) Epoch 78/300 -- Iteration 74889 - Batch 738/963 - Train loss: 0.27025164  - Train acc: 0.8962 - Val loss: 1.02331364\n",
      "(15.77 min) Epoch 78/300 -- Iteration 74898 - Batch 747/963 - Train loss: 0.27002587  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.77 min) Epoch 78/300 -- Iteration 74907 - Batch 756/963 - Train loss: 0.26987727  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.77 min) Epoch 78/300 -- Iteration 74916 - Batch 765/963 - Train loss: 0.26982623  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.77 min) Epoch 78/300 -- Iteration 74925 - Batch 774/963 - Train loss: 0.26971355  - Train acc: 0.8965 - Val loss: 1.02331364\n",
      "(15.77 min) Epoch 78/300 -- Iteration 74934 - Batch 783/963 - Train loss: 0.26940758  - Train acc: 0.8967 - Val loss: 1.02331364\n",
      "(15.78 min) Epoch 78/300 -- Iteration 74943 - Batch 792/963 - Train loss: 0.26929725  - Train acc: 0.8968 - Val loss: 1.02331364\n",
      "(15.78 min) Epoch 78/300 -- Iteration 74952 - Batch 801/963 - Train loss: 0.26931608  - Train acc: 0.8967 - Val loss: 1.02331364\n",
      "(15.78 min) Epoch 78/300 -- Iteration 74961 - Batch 810/963 - Train loss: 0.26944514  - Train acc: 0.8965 - Val loss: 1.02331364\n",
      "(15.78 min) Epoch 78/300 -- Iteration 74970 - Batch 819/963 - Train loss: 0.26943354  - Train acc: 0.8965 - Val loss: 1.02331364\n",
      "(15.78 min) Epoch 78/300 -- Iteration 74979 - Batch 828/963 - Train loss: 0.26956658  - Train acc: 0.8965 - Val loss: 1.02331364\n",
      "(15.79 min) Epoch 78/300 -- Iteration 74988 - Batch 837/963 - Train loss: 0.26993552  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.79 min) Epoch 78/300 -- Iteration 74997 - Batch 846/963 - Train loss: 0.26983404  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.79 min) Epoch 78/300 -- Iteration 75006 - Batch 855/963 - Train loss: 0.26981116  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.79 min) Epoch 78/300 -- Iteration 75015 - Batch 864/963 - Train loss: 0.26990784  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.79 min) Epoch 78/300 -- Iteration 75024 - Batch 873/963 - Train loss: 0.26982119  - Train acc: 0.8963 - Val loss: 1.02331364\n",
      "(15.79 min) Epoch 78/300 -- Iteration 75033 - Batch 882/963 - Train loss: 0.26973893  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.80 min) Epoch 78/300 -- Iteration 75042 - Batch 891/963 - Train loss: 0.26982148  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.80 min) Epoch 78/300 -- Iteration 75051 - Batch 900/963 - Train loss: 0.26967329  - Train acc: 0.8964 - Val loss: 1.02331364\n",
      "(15.80 min) Epoch 78/300 -- Iteration 75060 - Batch 909/963 - Train loss: 0.26941782  - Train acc: 0.8966 - Val loss: 1.02331364\n",
      "(15.80 min) Epoch 78/300 -- Iteration 75069 - Batch 918/963 - Train loss: 0.26920010  - Train acc: 0.8968 - Val loss: 1.02331364\n",
      "(15.80 min) Epoch 78/300 -- Iteration 75078 - Batch 927/963 - Train loss: 0.26922408  - Train acc: 0.8968 - Val loss: 1.02331364\n",
      "(15.81 min) Epoch 78/300 -- Iteration 75087 - Batch 936/963 - Train loss: 0.26927111  - Train acc: 0.8969 - Val loss: 1.02331364\n",
      "(15.81 min) Epoch 78/300 -- Iteration 75096 - Batch 945/963 - Train loss: 0.26917037  - Train acc: 0.8970 - Val loss: 1.02331364\n",
      "(15.81 min) Epoch 78/300 -- Iteration 75105 - Batch 954/963 - Train loss: 0.26902214  - Train acc: 0.8971 - Val loss: 1.02331364\n",
      "(15.81 min) Epoch 78/300 -- Iteration 75114 - Batch 962/963 - Train loss: 0.26910574  - Train acc: 0.8971 - Val loss: 0.98656464 - Val acc: 0.5917\n",
      "(15.81 min) Epoch 79/300 -- Iteration 75123 - Batch 9/963 - Train loss: 0.28347774  - Train acc: 0.8938 - Val loss: 0.98656464\n",
      "(15.82 min) Epoch 79/300 -- Iteration 75132 - Batch 18/963 - Train loss: 0.27404360  - Train acc: 0.8968 - Val loss: 0.98656464\n",
      "(15.82 min) Epoch 79/300 -- Iteration 75141 - Batch 27/963 - Train loss: 0.27451931  - Train acc: 0.8965 - Val loss: 0.98656464\n",
      "(15.82 min) Epoch 79/300 -- Iteration 75150 - Batch 36/963 - Train loss: 0.27322625  - Train acc: 0.8963 - Val loss: 0.98656464\n",
      "(15.82 min) Epoch 79/300 -- Iteration 75159 - Batch 45/963 - Train loss: 0.26829268  - Train acc: 0.8983 - Val loss: 0.98656464\n",
      "(15.82 min) Epoch 79/300 -- Iteration 75168 - Batch 54/963 - Train loss: 0.26316472  - Train acc: 0.9000 - Val loss: 0.98656464\n",
      "(15.83 min) Epoch 79/300 -- Iteration 75177 - Batch 63/963 - Train loss: 0.26558586  - Train acc: 0.8988 - Val loss: 0.98656464\n",
      "(15.83 min) Epoch 79/300 -- Iteration 75186 - Batch 72/963 - Train loss: 0.26678650  - Train acc: 0.8987 - Val loss: 0.98656464\n",
      "(15.83 min) Epoch 79/300 -- Iteration 75195 - Batch 81/963 - Train loss: 0.26869314  - Train acc: 0.8976 - Val loss: 0.98656464\n",
      "(15.83 min) Epoch 79/300 -- Iteration 75204 - Batch 90/963 - Train loss: 0.26954623  - Train acc: 0.8971 - Val loss: 0.98656464\n",
      "(15.83 min) Epoch 79/300 -- Iteration 75213 - Batch 99/963 - Train loss: 0.26802893  - Train acc: 0.8973 - Val loss: 0.98656464\n",
      "(15.83 min) Epoch 79/300 -- Iteration 75222 - Batch 108/963 - Train loss: 0.27008350  - Train acc: 0.8964 - Val loss: 0.98656464\n",
      "(15.84 min) Epoch 79/300 -- Iteration 75231 - Batch 117/963 - Train loss: 0.26667993  - Train acc: 0.8984 - Val loss: 0.98656464\n",
      "(15.84 min) Epoch 79/300 -- Iteration 75240 - Batch 126/963 - Train loss: 0.26625173  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.84 min) Epoch 79/300 -- Iteration 75249 - Batch 135/963 - Train loss: 0.26655175  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(15.84 min) Epoch 79/300 -- Iteration 75258 - Batch 144/963 - Train loss: 0.26635944  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.84 min) Epoch 79/300 -- Iteration 75267 - Batch 153/963 - Train loss: 0.26602187  - Train acc: 0.8976 - Val loss: 0.98656464\n",
      "(15.85 min) Epoch 79/300 -- Iteration 75276 - Batch 162/963 - Train loss: 0.26649895  - Train acc: 0.8970 - Val loss: 0.98656464\n",
      "(15.85 min) Epoch 79/300 -- Iteration 75285 - Batch 171/963 - Train loss: 0.26753930  - Train acc: 0.8966 - Val loss: 0.98656464\n",
      "(15.85 min) Epoch 79/300 -- Iteration 75294 - Batch 180/963 - Train loss: 0.26659346  - Train acc: 0.8975 - Val loss: 0.98656464\n",
      "(15.85 min) Epoch 79/300 -- Iteration 75303 - Batch 189/963 - Train loss: 0.26744999  - Train acc: 0.8973 - Val loss: 0.98656464\n",
      "(15.85 min) Epoch 79/300 -- Iteration 75312 - Batch 198/963 - Train loss: 0.26866198  - Train acc: 0.8968 - Val loss: 0.98656464\n",
      "(15.86 min) Epoch 79/300 -- Iteration 75321 - Batch 207/963 - Train loss: 0.26919548  - Train acc: 0.8967 - Val loss: 0.98656464\n",
      "(15.86 min) Epoch 79/300 -- Iteration 75330 - Batch 216/963 - Train loss: 0.26945455  - Train acc: 0.8967 - Val loss: 0.98656464\n",
      "(15.86 min) Epoch 79/300 -- Iteration 75339 - Batch 225/963 - Train loss: 0.26909884  - Train acc: 0.8971 - Val loss: 0.98656464\n",
      "(15.86 min) Epoch 79/300 -- Iteration 75348 - Batch 234/963 - Train loss: 0.26895117  - Train acc: 0.8974 - Val loss: 0.98656464\n",
      "(15.86 min) Epoch 79/300 -- Iteration 75357 - Batch 243/963 - Train loss: 0.26901936  - Train acc: 0.8972 - Val loss: 0.98656464\n",
      "(15.86 min) Epoch 79/300 -- Iteration 75366 - Batch 252/963 - Train loss: 0.27035505  - Train acc: 0.8968 - Val loss: 0.98656464\n",
      "(15.87 min) Epoch 79/300 -- Iteration 75375 - Batch 261/963 - Train loss: 0.26952564  - Train acc: 0.8971 - Val loss: 0.98656464\n",
      "(15.87 min) Epoch 79/300 -- Iteration 75384 - Batch 270/963 - Train loss: 0.26932232  - Train acc: 0.8971 - Val loss: 0.98656464\n",
      "(15.87 min) Epoch 79/300 -- Iteration 75393 - Batch 279/963 - Train loss: 0.26822349  - Train acc: 0.8975 - Val loss: 0.98656464\n",
      "(15.87 min) Epoch 79/300 -- Iteration 75402 - Batch 288/963 - Train loss: 0.26847218  - Train acc: 0.8974 - Val loss: 0.98656464\n",
      "(15.87 min) Epoch 79/300 -- Iteration 75411 - Batch 297/963 - Train loss: 0.26939797  - Train acc: 0.8972 - Val loss: 0.98656464\n",
      "(15.88 min) Epoch 79/300 -- Iteration 75420 - Batch 306/963 - Train loss: 0.26918334  - Train acc: 0.8973 - Val loss: 0.98656464\n",
      "(15.88 min) Epoch 79/300 -- Iteration 75429 - Batch 315/963 - Train loss: 0.26884930  - Train acc: 0.8975 - Val loss: 0.98656464\n",
      "(15.88 min) Epoch 79/300 -- Iteration 75438 - Batch 324/963 - Train loss: 0.26842944  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(15.88 min) Epoch 79/300 -- Iteration 75447 - Batch 333/963 - Train loss: 0.26892210  - Train acc: 0.8976 - Val loss: 0.98656464\n",
      "(15.88 min) Epoch 79/300 -- Iteration 75456 - Batch 342/963 - Train loss: 0.26888353  - Train acc: 0.8979 - Val loss: 0.98656464\n",
      "(15.89 min) Epoch 79/300 -- Iteration 75465 - Batch 351/963 - Train loss: 0.26891744  - Train acc: 0.8977 - Val loss: 0.98656464\n",
      "(15.89 min) Epoch 79/300 -- Iteration 75474 - Batch 360/963 - Train loss: 0.26943764  - Train acc: 0.8975 - Val loss: 0.98656464\n",
      "(15.89 min) Epoch 79/300 -- Iteration 75483 - Batch 369/963 - Train loss: 0.26944948  - Train acc: 0.8974 - Val loss: 0.98656464\n",
      "(15.89 min) Epoch 79/300 -- Iteration 75492 - Batch 378/963 - Train loss: 0.26941178  - Train acc: 0.8974 - Val loss: 0.98656464\n",
      "(15.89 min) Epoch 79/300 -- Iteration 75501 - Batch 387/963 - Train loss: 0.26952159  - Train acc: 0.8974 - Val loss: 0.98656464\n",
      "(15.89 min) Epoch 79/300 -- Iteration 75510 - Batch 396/963 - Train loss: 0.27024360  - Train acc: 0.8974 - Val loss: 0.98656464\n",
      "(15.90 min) Epoch 79/300 -- Iteration 75519 - Batch 405/963 - Train loss: 0.27052676  - Train acc: 0.8971 - Val loss: 0.98656464\n",
      "(15.90 min) Epoch 79/300 -- Iteration 75528 - Batch 414/963 - Train loss: 0.27051124  - Train acc: 0.8973 - Val loss: 0.98656464\n",
      "(15.90 min) Epoch 79/300 -- Iteration 75537 - Batch 423/963 - Train loss: 0.27076901  - Train acc: 0.8973 - Val loss: 0.98656464\n",
      "(15.90 min) Epoch 79/300 -- Iteration 75546 - Batch 432/963 - Train loss: 0.27060481  - Train acc: 0.8974 - Val loss: 0.98656464\n",
      "(15.90 min) Epoch 79/300 -- Iteration 75555 - Batch 441/963 - Train loss: 0.27037236  - Train acc: 0.8974 - Val loss: 0.98656464\n",
      "(15.91 min) Epoch 79/300 -- Iteration 75564 - Batch 450/963 - Train loss: 0.27019751  - Train acc: 0.8975 - Val loss: 0.98656464\n",
      "(15.91 min) Epoch 79/300 -- Iteration 75573 - Batch 459/963 - Train loss: 0.26984636  - Train acc: 0.8978 - Val loss: 0.98656464\n",
      "(15.91 min) Epoch 79/300 -- Iteration 75582 - Batch 468/963 - Train loss: 0.26975223  - Train acc: 0.8979 - Val loss: 0.98656464\n",
      "(15.91 min) Epoch 79/300 -- Iteration 75591 - Batch 477/963 - Train loss: 0.26988836  - Train acc: 0.8978 - Val loss: 0.98656464\n",
      "(15.91 min) Epoch 79/300 -- Iteration 75600 - Batch 486/963 - Train loss: 0.27024438  - Train acc: 0.8978 - Val loss: 0.98656464\n",
      "(15.92 min) Epoch 79/300 -- Iteration 75609 - Batch 495/963 - Train loss: 0.26952465  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(15.92 min) Epoch 79/300 -- Iteration 75618 - Batch 504/963 - Train loss: 0.26936591  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.92 min) Epoch 79/300 -- Iteration 75627 - Batch 513/963 - Train loss: 0.26985324  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.92 min) Epoch 79/300 -- Iteration 75636 - Batch 522/963 - Train loss: 0.26970201  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.92 min) Epoch 79/300 -- Iteration 75645 - Batch 531/963 - Train loss: 0.26963226  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.92 min) Epoch 79/300 -- Iteration 75654 - Batch 540/963 - Train loss: 0.27002599  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.93 min) Epoch 79/300 -- Iteration 75663 - Batch 549/963 - Train loss: 0.26972473  - Train acc: 0.8983 - Val loss: 0.98656464\n",
      "(15.93 min) Epoch 79/300 -- Iteration 75672 - Batch 558/963 - Train loss: 0.26974419  - Train acc: 0.8983 - Val loss: 0.98656464\n",
      "(15.93 min) Epoch 79/300 -- Iteration 75681 - Batch 567/963 - Train loss: 0.26972012  - Train acc: 0.8983 - Val loss: 0.98656464\n",
      "(15.93 min) Epoch 79/300 -- Iteration 75690 - Batch 576/963 - Train loss: 0.26964856  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.93 min) Epoch 79/300 -- Iteration 75699 - Batch 585/963 - Train loss: 0.26908399  - Train acc: 0.8984 - Val loss: 0.98656464\n",
      "(15.94 min) Epoch 79/300 -- Iteration 75708 - Batch 594/963 - Train loss: 0.26892066  - Train acc: 0.8985 - Val loss: 0.98656464\n",
      "(15.94 min) Epoch 79/300 -- Iteration 75717 - Batch 603/963 - Train loss: 0.26884283  - Train acc: 0.8986 - Val loss: 0.98656464\n",
      "(15.94 min) Epoch 79/300 -- Iteration 75726 - Batch 612/963 - Train loss: 0.26877625  - Train acc: 0.8986 - Val loss: 0.98656464\n",
      "(15.94 min) Epoch 79/300 -- Iteration 75735 - Batch 621/963 - Train loss: 0.26863192  - Train acc: 0.8987 - Val loss: 0.98656464\n",
      "(15.94 min) Epoch 79/300 -- Iteration 75744 - Batch 630/963 - Train loss: 0.26873706  - Train acc: 0.8986 - Val loss: 0.98656464\n",
      "(15.95 min) Epoch 79/300 -- Iteration 75753 - Batch 639/963 - Train loss: 0.26886606  - Train acc: 0.8984 - Val loss: 0.98656464\n",
      "(15.95 min) Epoch 79/300 -- Iteration 75762 - Batch 648/963 - Train loss: 0.26915465  - Train acc: 0.8983 - Val loss: 0.98656464\n",
      "(15.95 min) Epoch 79/300 -- Iteration 75771 - Batch 657/963 - Train loss: 0.26959961  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.95 min) Epoch 79/300 -- Iteration 75780 - Batch 666/963 - Train loss: 0.26956775  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.95 min) Epoch 79/300 -- Iteration 75789 - Batch 675/963 - Train loss: 0.26954846  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.96 min) Epoch 79/300 -- Iteration 75798 - Batch 684/963 - Train loss: 0.26962625  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.96 min) Epoch 79/300 -- Iteration 75807 - Batch 693/963 - Train loss: 0.26915793  - Train acc: 0.8983 - Val loss: 0.98656464\n",
      "(15.96 min) Epoch 79/300 -- Iteration 75816 - Batch 702/963 - Train loss: 0.26910765  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.96 min) Epoch 79/300 -- Iteration 75825 - Batch 711/963 - Train loss: 0.26911357  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.96 min) Epoch 79/300 -- Iteration 75834 - Batch 720/963 - Train loss: 0.26929976  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(15.97 min) Epoch 79/300 -- Iteration 75843 - Batch 729/963 - Train loss: 0.26943109  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(15.97 min) Epoch 79/300 -- Iteration 75852 - Batch 738/963 - Train loss: 0.26927063  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(15.97 min) Epoch 79/300 -- Iteration 75861 - Batch 747/963 - Train loss: 0.26931146  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(15.97 min) Epoch 79/300 -- Iteration 75870 - Batch 756/963 - Train loss: 0.26894947  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.97 min) Epoch 79/300 -- Iteration 75879 - Batch 765/963 - Train loss: 0.26899267  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.98 min) Epoch 79/300 -- Iteration 75888 - Batch 774/963 - Train loss: 0.26911549  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(15.98 min) Epoch 79/300 -- Iteration 75897 - Batch 783/963 - Train loss: 0.26908533  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.98 min) Epoch 79/300 -- Iteration 75906 - Batch 792/963 - Train loss: 0.26905039  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.98 min) Epoch 79/300 -- Iteration 75915 - Batch 801/963 - Train loss: 0.26902300  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.98 min) Epoch 79/300 -- Iteration 75924 - Batch 810/963 - Train loss: 0.26872801  - Train acc: 0.8983 - Val loss: 0.98656464\n",
      "(15.99 min) Epoch 79/300 -- Iteration 75933 - Batch 819/963 - Train loss: 0.26874550  - Train acc: 0.8982 - Val loss: 0.98656464\n",
      "(15.99 min) Epoch 79/300 -- Iteration 75942 - Batch 828/963 - Train loss: 0.26906256  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(15.99 min) Epoch 79/300 -- Iteration 75951 - Batch 837/963 - Train loss: 0.26922074  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.99 min) Epoch 79/300 -- Iteration 75960 - Batch 846/963 - Train loss: 0.26924848  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(15.99 min) Epoch 79/300 -- Iteration 75969 - Batch 855/963 - Train loss: 0.26922949  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(16.00 min) Epoch 79/300 -- Iteration 75978 - Batch 864/963 - Train loss: 0.26881358  - Train acc: 0.8984 - Val loss: 0.98656464\n",
      "(16.00 min) Epoch 79/300 -- Iteration 75987 - Batch 873/963 - Train loss: 0.26879470  - Train acc: 0.8984 - Val loss: 0.98656464\n",
      "(16.00 min) Epoch 79/300 -- Iteration 75996 - Batch 882/963 - Train loss: 0.26897145  - Train acc: 0.8984 - Val loss: 0.98656464\n",
      "(16.00 min) Epoch 79/300 -- Iteration 76005 - Batch 891/963 - Train loss: 0.26871859  - Train acc: 0.8984 - Val loss: 0.98656464\n",
      "(16.00 min) Epoch 79/300 -- Iteration 76014 - Batch 900/963 - Train loss: 0.26865674  - Train acc: 0.8985 - Val loss: 0.98656464\n",
      "(16.01 min) Epoch 79/300 -- Iteration 76023 - Batch 909/963 - Train loss: 0.26878599  - Train acc: 0.8984 - Val loss: 0.98656464\n",
      "(16.01 min) Epoch 79/300 -- Iteration 76032 - Batch 918/963 - Train loss: 0.26880832  - Train acc: 0.8983 - Val loss: 0.98656464\n",
      "(16.01 min) Epoch 79/300 -- Iteration 76041 - Batch 927/963 - Train loss: 0.26853020  - Train acc: 0.8984 - Val loss: 0.98656464\n",
      "(16.01 min) Epoch 79/300 -- Iteration 76050 - Batch 936/963 - Train loss: 0.26885357  - Train acc: 0.8983 - Val loss: 0.98656464\n",
      "(16.01 min) Epoch 79/300 -- Iteration 76059 - Batch 945/963 - Train loss: 0.26913544  - Train acc: 0.8981 - Val loss: 0.98656464\n",
      "(16.02 min) Epoch 79/300 -- Iteration 76068 - Batch 954/963 - Train loss: 0.26912814  - Train acc: 0.8980 - Val loss: 0.98656464\n",
      "(16.02 min) Epoch 79/300 -- Iteration 76077 - Batch 962/963 - Train loss: 0.26940131  - Train acc: 0.8980 - Val loss: 0.99539113 - Val acc: 0.5883\n",
      "(16.02 min) Epoch 80/300 -- Iteration 76086 - Batch 9/963 - Train loss: 0.24712982  - Train acc: 0.9094 - Val loss: 0.99539113\n",
      "(16.02 min) Epoch 80/300 -- Iteration 76095 - Batch 18/963 - Train loss: 0.26245526  - Train acc: 0.9042 - Val loss: 0.99539113\n",
      "(16.03 min) Epoch 80/300 -- Iteration 76104 - Batch 27/963 - Train loss: 0.26654184  - Train acc: 0.9001 - Val loss: 0.99539113\n",
      "(16.03 min) Epoch 80/300 -- Iteration 76113 - Batch 36/963 - Train loss: 0.26454547  - Train acc: 0.9018 - Val loss: 0.99539113\n",
      "(16.03 min) Epoch 80/300 -- Iteration 76122 - Batch 45/963 - Train loss: 0.26486831  - Train acc: 0.9017 - Val loss: 0.99539113\n",
      "(16.03 min) Epoch 80/300 -- Iteration 76131 - Batch 54/963 - Train loss: 0.26959965  - Train acc: 0.8977 - Val loss: 0.99539113\n",
      "(16.03 min) Epoch 80/300 -- Iteration 76140 - Batch 63/963 - Train loss: 0.27358033  - Train acc: 0.8956 - Val loss: 0.99539113\n",
      "(16.04 min) Epoch 80/300 -- Iteration 76149 - Batch 72/963 - Train loss: 0.27372993  - Train acc: 0.8957 - Val loss: 0.99539113\n",
      "(16.04 min) Epoch 80/300 -- Iteration 76158 - Batch 81/963 - Train loss: 0.26965524  - Train acc: 0.8973 - Val loss: 0.99539113\n",
      "(16.04 min) Epoch 80/300 -- Iteration 76167 - Batch 90/963 - Train loss: 0.27116775  - Train acc: 0.8962 - Val loss: 0.99539113\n",
      "(16.04 min) Epoch 80/300 -- Iteration 76176 - Batch 99/963 - Train loss: 0.27436325  - Train acc: 0.8950 - Val loss: 0.99539113\n",
      "(16.04 min) Epoch 80/300 -- Iteration 76185 - Batch 108/963 - Train loss: 0.27482067  - Train acc: 0.8950 - Val loss: 0.99539113\n",
      "(16.05 min) Epoch 80/300 -- Iteration 76194 - Batch 117/963 - Train loss: 0.27284550  - Train acc: 0.8961 - Val loss: 0.99539113\n",
      "(16.05 min) Epoch 80/300 -- Iteration 76203 - Batch 126/963 - Train loss: 0.27272317  - Train acc: 0.8960 - Val loss: 0.99539113\n",
      "(16.05 min) Epoch 80/300 -- Iteration 76212 - Batch 135/963 - Train loss: 0.27211133  - Train acc: 0.8968 - Val loss: 0.99539113\n",
      "(16.05 min) Epoch 80/300 -- Iteration 76221 - Batch 144/963 - Train loss: 0.27263753  - Train acc: 0.8969 - Val loss: 0.99539113\n",
      "(16.06 min) Epoch 80/300 -- Iteration 76230 - Batch 153/963 - Train loss: 0.27266866  - Train acc: 0.8967 - Val loss: 0.99539113\n",
      "(16.06 min) Epoch 80/300 -- Iteration 76239 - Batch 162/963 - Train loss: 0.27425162  - Train acc: 0.8956 - Val loss: 0.99539113\n",
      "(16.06 min) Epoch 80/300 -- Iteration 76248 - Batch 171/963 - Train loss: 0.27495306  - Train acc: 0.8957 - Val loss: 0.99539113\n",
      "(16.06 min) Epoch 80/300 -- Iteration 76257 - Batch 180/963 - Train loss: 0.27408709  - Train acc: 0.8959 - Val loss: 0.99539113\n",
      "(16.06 min) Epoch 80/300 -- Iteration 76266 - Batch 189/963 - Train loss: 0.27366678  - Train acc: 0.8957 - Val loss: 0.99539113\n",
      "(16.07 min) Epoch 80/300 -- Iteration 76275 - Batch 198/963 - Train loss: 0.27323338  - Train acc: 0.8956 - Val loss: 0.99539113\n",
      "(16.07 min) Epoch 80/300 -- Iteration 76284 - Batch 207/963 - Train loss: 0.27276410  - Train acc: 0.8961 - Val loss: 0.99539113\n",
      "(16.07 min) Epoch 80/300 -- Iteration 76293 - Batch 216/963 - Train loss: 0.27244542  - Train acc: 0.8964 - Val loss: 0.99539113\n",
      "(16.07 min) Epoch 80/300 -- Iteration 76302 - Batch 225/963 - Train loss: 0.27172098  - Train acc: 0.8967 - Val loss: 0.99539113\n",
      "(16.08 min) Epoch 80/300 -- Iteration 76311 - Batch 234/963 - Train loss: 0.27185321  - Train acc: 0.8970 - Val loss: 0.99539113\n",
      "(16.08 min) Epoch 80/300 -- Iteration 76320 - Batch 243/963 - Train loss: 0.27227715  - Train acc: 0.8969 - Val loss: 0.99539113\n",
      "(16.08 min) Epoch 80/300 -- Iteration 76329 - Batch 252/963 - Train loss: 0.27231467  - Train acc: 0.8970 - Val loss: 0.99539113\n",
      "(16.08 min) Epoch 80/300 -- Iteration 76338 - Batch 261/963 - Train loss: 0.27300775  - Train acc: 0.8967 - Val loss: 0.99539113\n",
      "(16.08 min) Epoch 80/300 -- Iteration 76347 - Batch 270/963 - Train loss: 0.27232050  - Train acc: 0.8968 - Val loss: 0.99539113\n",
      "(16.08 min) Epoch 80/300 -- Iteration 76356 - Batch 279/963 - Train loss: 0.27296863  - Train acc: 0.8964 - Val loss: 0.99539113\n",
      "(16.09 min) Epoch 80/300 -- Iteration 76365 - Batch 288/963 - Train loss: 0.27327550  - Train acc: 0.8964 - Val loss: 0.99539113\n",
      "(16.09 min) Epoch 80/300 -- Iteration 76374 - Batch 297/963 - Train loss: 0.27336048  - Train acc: 0.8963 - Val loss: 0.99539113\n",
      "(16.09 min) Epoch 80/300 -- Iteration 76383 - Batch 306/963 - Train loss: 0.27344682  - Train acc: 0.8962 - Val loss: 0.99539113\n",
      "(16.09 min) Epoch 80/300 -- Iteration 76392 - Batch 315/963 - Train loss: 0.27379389  - Train acc: 0.8961 - Val loss: 0.99539113\n",
      "(16.09 min) Epoch 80/300 -- Iteration 76401 - Batch 324/963 - Train loss: 0.27321358  - Train acc: 0.8964 - Val loss: 0.99539113\n",
      "(16.10 min) Epoch 80/300 -- Iteration 76410 - Batch 333/963 - Train loss: 0.27253892  - Train acc: 0.8969 - Val loss: 0.99539113\n",
      "(16.10 min) Epoch 80/300 -- Iteration 76419 - Batch 342/963 - Train loss: 0.27196574  - Train acc: 0.8971 - Val loss: 0.99539113\n",
      "(16.10 min) Epoch 80/300 -- Iteration 76428 - Batch 351/963 - Train loss: 0.27250721  - Train acc: 0.8969 - Val loss: 0.99539113\n",
      "(16.10 min) Epoch 80/300 -- Iteration 76437 - Batch 360/963 - Train loss: 0.27222854  - Train acc: 0.8971 - Val loss: 0.99539113\n",
      "(16.10 min) Epoch 80/300 -- Iteration 76446 - Batch 369/963 - Train loss: 0.27219607  - Train acc: 0.8971 - Val loss: 0.99539113\n",
      "(16.11 min) Epoch 80/300 -- Iteration 76455 - Batch 378/963 - Train loss: 0.27155370  - Train acc: 0.8976 - Val loss: 0.99539113\n",
      "(16.11 min) Epoch 80/300 -- Iteration 76464 - Batch 387/963 - Train loss: 0.27091738  - Train acc: 0.8978 - Val loss: 0.99539113\n",
      "(16.11 min) Epoch 80/300 -- Iteration 76473 - Batch 396/963 - Train loss: 0.27075629  - Train acc: 0.8979 - Val loss: 0.99539113\n",
      "(16.11 min) Epoch 80/300 -- Iteration 76482 - Batch 405/963 - Train loss: 0.27063828  - Train acc: 0.8979 - Val loss: 0.99539113\n",
      "(16.11 min) Epoch 80/300 -- Iteration 76491 - Batch 414/963 - Train loss: 0.27110160  - Train acc: 0.8976 - Val loss: 0.99539113\n",
      "(16.12 min) Epoch 80/300 -- Iteration 76500 - Batch 423/963 - Train loss: 0.27159683  - Train acc: 0.8974 - Val loss: 0.99539113\n",
      "(16.12 min) Epoch 80/300 -- Iteration 76509 - Batch 432/963 - Train loss: 0.27139378  - Train acc: 0.8975 - Val loss: 0.99539113\n",
      "(16.12 min) Epoch 80/300 -- Iteration 76518 - Batch 441/963 - Train loss: 0.27127805  - Train acc: 0.8976 - Val loss: 0.99539113\n",
      "(16.12 min) Epoch 80/300 -- Iteration 76527 - Batch 450/963 - Train loss: 0.27156056  - Train acc: 0.8975 - Val loss: 0.99539113\n",
      "(16.12 min) Epoch 80/300 -- Iteration 76536 - Batch 459/963 - Train loss: 0.27105859  - Train acc: 0.8979 - Val loss: 0.99539113\n",
      "(16.13 min) Epoch 80/300 -- Iteration 76545 - Batch 468/963 - Train loss: 0.27060941  - Train acc: 0.8981 - Val loss: 0.99539113\n",
      "(16.13 min) Epoch 80/300 -- Iteration 76554 - Batch 477/963 - Train loss: 0.27115641  - Train acc: 0.8977 - Val loss: 0.99539113\n",
      "(16.13 min) Epoch 80/300 -- Iteration 76563 - Batch 486/963 - Train loss: 0.27058676  - Train acc: 0.8979 - Val loss: 0.99539113\n",
      "(16.13 min) Epoch 80/300 -- Iteration 76572 - Batch 495/963 - Train loss: 0.27096781  - Train acc: 0.8977 - Val loss: 0.99539113\n",
      "(16.13 min) Epoch 80/300 -- Iteration 76581 - Batch 504/963 - Train loss: 0.27086215  - Train acc: 0.8976 - Val loss: 0.99539113\n",
      "(16.13 min) Epoch 80/300 -- Iteration 76590 - Batch 513/963 - Train loss: 0.27075510  - Train acc: 0.8977 - Val loss: 0.99539113\n",
      "(16.14 min) Epoch 80/300 -- Iteration 76599 - Batch 522/963 - Train loss: 0.27118274  - Train acc: 0.8975 - Val loss: 0.99539113\n",
      "(16.14 min) Epoch 80/300 -- Iteration 76608 - Batch 531/963 - Train loss: 0.27097058  - Train acc: 0.8976 - Val loss: 0.99539113\n",
      "(16.14 min) Epoch 80/300 -- Iteration 76617 - Batch 540/963 - Train loss: 0.27077713  - Train acc: 0.8977 - Val loss: 0.99539113\n",
      "(16.14 min) Epoch 80/300 -- Iteration 76626 - Batch 549/963 - Train loss: 0.27049245  - Train acc: 0.8977 - Val loss: 0.99539113\n",
      "(16.14 min) Epoch 80/300 -- Iteration 76635 - Batch 558/963 - Train loss: 0.27021561  - Train acc: 0.8978 - Val loss: 0.99539113\n",
      "(16.15 min) Epoch 80/300 -- Iteration 76644 - Batch 567/963 - Train loss: 0.26962764  - Train acc: 0.8980 - Val loss: 0.99539113\n",
      "(16.15 min) Epoch 80/300 -- Iteration 76653 - Batch 576/963 - Train loss: 0.26951562  - Train acc: 0.8979 - Val loss: 0.99539113\n",
      "(16.15 min) Epoch 80/300 -- Iteration 76662 - Batch 585/963 - Train loss: 0.26914731  - Train acc: 0.8981 - Val loss: 0.99539113\n",
      "(16.15 min) Epoch 80/300 -- Iteration 76671 - Batch 594/963 - Train loss: 0.26934415  - Train acc: 0.8979 - Val loss: 0.99539113\n",
      "(16.15 min) Epoch 80/300 -- Iteration 76680 - Batch 603/963 - Train loss: 0.26933641  - Train acc: 0.8981 - Val loss: 0.99539113\n",
      "(16.16 min) Epoch 80/300 -- Iteration 76689 - Batch 612/963 - Train loss: 0.26961798  - Train acc: 0.8980 - Val loss: 0.99539113\n",
      "(16.16 min) Epoch 80/300 -- Iteration 76698 - Batch 621/963 - Train loss: 0.26945707  - Train acc: 0.8982 - Val loss: 0.99539113\n",
      "(16.16 min) Epoch 80/300 -- Iteration 76707 - Batch 630/963 - Train loss: 0.26866852  - Train acc: 0.8985 - Val loss: 0.99539113\n",
      "(16.16 min) Epoch 80/300 -- Iteration 76716 - Batch 639/963 - Train loss: 0.26831036  - Train acc: 0.8986 - Val loss: 0.99539113\n",
      "(16.16 min) Epoch 80/300 -- Iteration 76725 - Batch 648/963 - Train loss: 0.26832443  - Train acc: 0.8987 - Val loss: 0.99539113\n",
      "(16.17 min) Epoch 80/300 -- Iteration 76734 - Batch 657/963 - Train loss: 0.26820792  - Train acc: 0.8988 - Val loss: 0.99539113\n",
      "(16.17 min) Epoch 80/300 -- Iteration 76743 - Batch 666/963 - Train loss: 0.26812654  - Train acc: 0.8988 - Val loss: 0.99539113\n",
      "(16.17 min) Epoch 80/300 -- Iteration 76752 - Batch 675/963 - Train loss: 0.26820626  - Train acc: 0.8987 - Val loss: 0.99539113\n",
      "(16.17 min) Epoch 80/300 -- Iteration 76761 - Batch 684/963 - Train loss: 0.26810814  - Train acc: 0.8988 - Val loss: 0.99539113\n",
      "(16.17 min) Epoch 80/300 -- Iteration 76770 - Batch 693/963 - Train loss: 0.26798580  - Train acc: 0.8989 - Val loss: 0.99539113\n",
      "(16.18 min) Epoch 80/300 -- Iteration 76779 - Batch 702/963 - Train loss: 0.26795459  - Train acc: 0.8989 - Val loss: 0.99539113\n",
      "(16.18 min) Epoch 80/300 -- Iteration 76788 - Batch 711/963 - Train loss: 0.26816556  - Train acc: 0.8989 - Val loss: 0.99539113\n",
      "(16.18 min) Epoch 80/300 -- Iteration 76797 - Batch 720/963 - Train loss: 0.26812384  - Train acc: 0.8989 - Val loss: 0.99539113\n",
      "(16.18 min) Epoch 80/300 -- Iteration 76806 - Batch 729/963 - Train loss: 0.26787568  - Train acc: 0.8990 - Val loss: 0.99539113\n",
      "(16.18 min) Epoch 80/300 -- Iteration 76815 - Batch 738/963 - Train loss: 0.26839313  - Train acc: 0.8989 - Val loss: 0.99539113\n",
      "(16.19 min) Epoch 80/300 -- Iteration 76824 - Batch 747/963 - Train loss: 0.26820760  - Train acc: 0.8990 - Val loss: 0.99539113\n",
      "(16.19 min) Epoch 80/300 -- Iteration 76833 - Batch 756/963 - Train loss: 0.26777610  - Train acc: 0.8991 - Val loss: 0.99539113\n",
      "(16.19 min) Epoch 80/300 -- Iteration 76842 - Batch 765/963 - Train loss: 0.26798605  - Train acc: 0.8990 - Val loss: 0.99539113\n",
      "(16.19 min) Epoch 80/300 -- Iteration 76851 - Batch 774/963 - Train loss: 0.26769953  - Train acc: 0.8991 - Val loss: 0.99539113\n",
      "(16.19 min) Epoch 80/300 -- Iteration 76860 - Batch 783/963 - Train loss: 0.26752834  - Train acc: 0.8992 - Val loss: 0.99539113\n",
      "(16.20 min) Epoch 80/300 -- Iteration 76869 - Batch 792/963 - Train loss: 0.26736565  - Train acc: 0.8992 - Val loss: 0.99539113\n",
      "(16.20 min) Epoch 80/300 -- Iteration 76878 - Batch 801/963 - Train loss: 0.26733036  - Train acc: 0.8992 - Val loss: 0.99539113\n",
      "(16.20 min) Epoch 80/300 -- Iteration 76887 - Batch 810/963 - Train loss: 0.26737187  - Train acc: 0.8992 - Val loss: 0.99539113\n",
      "(16.20 min) Epoch 80/300 -- Iteration 76896 - Batch 819/963 - Train loss: 0.26759284  - Train acc: 0.8991 - Val loss: 0.99539113\n",
      "(16.20 min) Epoch 80/300 -- Iteration 76905 - Batch 828/963 - Train loss: 0.26779067  - Train acc: 0.8990 - Val loss: 0.99539113\n",
      "(16.21 min) Epoch 80/300 -- Iteration 76914 - Batch 837/963 - Train loss: 0.26768699  - Train acc: 0.8991 - Val loss: 0.99539113\n",
      "(16.21 min) Epoch 80/300 -- Iteration 76923 - Batch 846/963 - Train loss: 0.26757743  - Train acc: 0.8991 - Val loss: 0.99539113\n",
      "(16.21 min) Epoch 80/300 -- Iteration 76932 - Batch 855/963 - Train loss: 0.26732627  - Train acc: 0.8992 - Val loss: 0.99539113\n",
      "(16.21 min) Epoch 80/300 -- Iteration 76941 - Batch 864/963 - Train loss: 0.26716730  - Train acc: 0.8993 - Val loss: 0.99539113\n",
      "(16.21 min) Epoch 80/300 -- Iteration 76950 - Batch 873/963 - Train loss: 0.26723005  - Train acc: 0.8993 - Val loss: 0.99539113\n",
      "(16.22 min) Epoch 80/300 -- Iteration 76959 - Batch 882/963 - Train loss: 0.26710848  - Train acc: 0.8992 - Val loss: 0.99539113\n",
      "(16.22 min) Epoch 80/300 -- Iteration 76968 - Batch 891/963 - Train loss: 0.26712194  - Train acc: 0.8991 - Val loss: 0.99539113\n",
      "(16.22 min) Epoch 80/300 -- Iteration 76977 - Batch 900/963 - Train loss: 0.26708761  - Train acc: 0.8990 - Val loss: 0.99539113\n",
      "(16.22 min) Epoch 80/300 -- Iteration 76986 - Batch 909/963 - Train loss: 0.26727628  - Train acc: 0.8988 - Val loss: 0.99539113\n",
      "(16.22 min) Epoch 80/300 -- Iteration 76995 - Batch 918/963 - Train loss: 0.26737610  - Train acc: 0.8988 - Val loss: 0.99539113\n",
      "(16.23 min) Epoch 80/300 -- Iteration 77004 - Batch 927/963 - Train loss: 0.26732400  - Train acc: 0.8989 - Val loss: 0.99539113\n",
      "(16.23 min) Epoch 80/300 -- Iteration 77013 - Batch 936/963 - Train loss: 0.26739702  - Train acc: 0.8989 - Val loss: 0.99539113\n",
      "(16.23 min) Epoch 80/300 -- Iteration 77022 - Batch 945/963 - Train loss: 0.26727190  - Train acc: 0.8990 - Val loss: 0.99539113\n",
      "(16.23 min) Epoch 80/300 -- Iteration 77031 - Batch 954/963 - Train loss: 0.26708541  - Train acc: 0.8990 - Val loss: 0.99539113\n",
      "(16.23 min) Epoch 80/300 -- Iteration 77040 - Batch 962/963 - Train loss: 0.26701104  - Train acc: 0.8991 - Val loss: 1.00701332 - Val acc: 0.5883\n",
      "(16.24 min) Epoch 81/300 -- Iteration 77049 - Batch 9/963 - Train loss: 0.27985115  - Train acc: 0.8930 - Val loss: 1.00701332\n",
      "(16.24 min) Epoch 81/300 -- Iteration 77058 - Batch 18/963 - Train loss: 0.26295415  - Train acc: 0.8972 - Val loss: 1.00701332\n",
      "(16.24 min) Epoch 81/300 -- Iteration 77067 - Batch 27/963 - Train loss: 0.26502649  - Train acc: 0.8979 - Val loss: 1.00701332\n",
      "(16.24 min) Epoch 81/300 -- Iteration 77076 - Batch 36/963 - Train loss: 0.26089003  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.24 min) Epoch 81/300 -- Iteration 77085 - Batch 45/963 - Train loss: 0.26682600  - Train acc: 0.8974 - Val loss: 1.00701332\n",
      "(16.25 min) Epoch 81/300 -- Iteration 77094 - Batch 54/963 - Train loss: 0.26391422  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.25 min) Epoch 81/300 -- Iteration 77103 - Batch 63/963 - Train loss: 0.26441424  - Train acc: 0.8989 - Val loss: 1.00701332\n",
      "(16.25 min) Epoch 81/300 -- Iteration 77112 - Batch 72/963 - Train loss: 0.26467232  - Train acc: 0.8983 - Val loss: 1.00701332\n",
      "(16.25 min) Epoch 81/300 -- Iteration 77121 - Batch 81/963 - Train loss: 0.26730678  - Train acc: 0.8981 - Val loss: 1.00701332\n",
      "(16.25 min) Epoch 81/300 -- Iteration 77130 - Batch 90/963 - Train loss: 0.26489370  - Train acc: 0.8994 - Val loss: 1.00701332\n",
      "(16.26 min) Epoch 81/300 -- Iteration 77139 - Batch 99/963 - Train loss: 0.26471546  - Train acc: 0.8997 - Val loss: 1.00701332\n",
      "(16.26 min) Epoch 81/300 -- Iteration 77148 - Batch 108/963 - Train loss: 0.26264443  - Train acc: 0.9008 - Val loss: 1.00701332\n",
      "(16.26 min) Epoch 81/300 -- Iteration 77157 - Batch 117/963 - Train loss: 0.26052865  - Train acc: 0.9014 - Val loss: 1.00701332\n",
      "(16.26 min) Epoch 81/300 -- Iteration 77166 - Batch 126/963 - Train loss: 0.26066206  - Train acc: 0.9011 - Val loss: 1.00701332\n",
      "(16.26 min) Epoch 81/300 -- Iteration 77175 - Batch 135/963 - Train loss: 0.26039132  - Train acc: 0.9015 - Val loss: 1.00701332\n",
      "(16.26 min) Epoch 81/300 -- Iteration 77184 - Batch 144/963 - Train loss: 0.26190484  - Train acc: 0.9007 - Val loss: 1.00701332\n",
      "(16.27 min) Epoch 81/300 -- Iteration 77193 - Batch 153/963 - Train loss: 0.26311816  - Train acc: 0.9006 - Val loss: 1.00701332\n",
      "(16.27 min) Epoch 81/300 -- Iteration 77202 - Batch 162/963 - Train loss: 0.26225630  - Train acc: 0.9014 - Val loss: 1.00701332\n",
      "(16.27 min) Epoch 81/300 -- Iteration 77211 - Batch 171/963 - Train loss: 0.26296133  - Train acc: 0.9010 - Val loss: 1.00701332\n",
      "(16.27 min) Epoch 81/300 -- Iteration 77220 - Batch 180/963 - Train loss: 0.26396732  - Train acc: 0.9010 - Val loss: 1.00701332\n",
      "(16.27 min) Epoch 81/300 -- Iteration 77229 - Batch 189/963 - Train loss: 0.26487179  - Train acc: 0.9004 - Val loss: 1.00701332\n",
      "(16.28 min) Epoch 81/300 -- Iteration 77238 - Batch 198/963 - Train loss: 0.26450035  - Train acc: 0.9003 - Val loss: 1.00701332\n",
      "(16.28 min) Epoch 81/300 -- Iteration 77247 - Batch 207/963 - Train loss: 0.26529422  - Train acc: 0.8998 - Val loss: 1.00701332\n",
      "(16.28 min) Epoch 81/300 -- Iteration 77256 - Batch 216/963 - Train loss: 0.26592034  - Train acc: 0.8996 - Val loss: 1.00701332\n",
      "(16.28 min) Epoch 81/300 -- Iteration 77265 - Batch 225/963 - Train loss: 0.26653066  - Train acc: 0.8998 - Val loss: 1.00701332\n",
      "(16.28 min) Epoch 81/300 -- Iteration 77274 - Batch 234/963 - Train loss: 0.26704687  - Train acc: 0.8998 - Val loss: 1.00701332\n",
      "(16.29 min) Epoch 81/300 -- Iteration 77283 - Batch 243/963 - Train loss: 0.26724960  - Train acc: 0.8997 - Val loss: 1.00701332\n",
      "(16.29 min) Epoch 81/300 -- Iteration 77292 - Batch 252/963 - Train loss: 0.26730752  - Train acc: 0.8997 - Val loss: 1.00701332\n",
      "(16.29 min) Epoch 81/300 -- Iteration 77301 - Batch 261/963 - Train loss: 0.26808507  - Train acc: 0.8994 - Val loss: 1.00701332\n",
      "(16.29 min) Epoch 81/300 -- Iteration 77310 - Batch 270/963 - Train loss: 0.26837503  - Train acc: 0.8992 - Val loss: 1.00701332\n",
      "(16.29 min) Epoch 81/300 -- Iteration 77319 - Batch 279/963 - Train loss: 0.26974899  - Train acc: 0.8989 - Val loss: 1.00701332\n",
      "(16.30 min) Epoch 81/300 -- Iteration 77328 - Batch 288/963 - Train loss: 0.26944766  - Train acc: 0.8989 - Val loss: 1.00701332\n",
      "(16.30 min) Epoch 81/300 -- Iteration 77337 - Batch 297/963 - Train loss: 0.26899107  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.30 min) Epoch 81/300 -- Iteration 77346 - Batch 306/963 - Train loss: 0.26894605  - Train acc: 0.8993 - Val loss: 1.00701332\n",
      "(16.30 min) Epoch 81/300 -- Iteration 77355 - Batch 315/963 - Train loss: 0.26799761  - Train acc: 0.8996 - Val loss: 1.00701332\n",
      "(16.30 min) Epoch 81/300 -- Iteration 77364 - Batch 324/963 - Train loss: 0.26762218  - Train acc: 0.9000 - Val loss: 1.00701332\n",
      "(16.30 min) Epoch 81/300 -- Iteration 77373 - Batch 333/963 - Train loss: 0.26705507  - Train acc: 0.9001 - Val loss: 1.00701332\n",
      "(16.31 min) Epoch 81/300 -- Iteration 77382 - Batch 342/963 - Train loss: 0.26771809  - Train acc: 0.8999 - Val loss: 1.00701332\n",
      "(16.31 min) Epoch 81/300 -- Iteration 77391 - Batch 351/963 - Train loss: 0.26759695  - Train acc: 0.8999 - Val loss: 1.00701332\n",
      "(16.31 min) Epoch 81/300 -- Iteration 77400 - Batch 360/963 - Train loss: 0.26808086  - Train acc: 0.8998 - Val loss: 1.00701332\n",
      "(16.31 min) Epoch 81/300 -- Iteration 77409 - Batch 369/963 - Train loss: 0.26837217  - Train acc: 0.8997 - Val loss: 1.00701332\n",
      "(16.31 min) Epoch 81/300 -- Iteration 77418 - Batch 378/963 - Train loss: 0.26898324  - Train acc: 0.8996 - Val loss: 1.00701332\n",
      "(16.32 min) Epoch 81/300 -- Iteration 77427 - Batch 387/963 - Train loss: 0.26875749  - Train acc: 0.8996 - Val loss: 1.00701332\n",
      "(16.32 min) Epoch 81/300 -- Iteration 77436 - Batch 396/963 - Train loss: 0.26936059  - Train acc: 0.8992 - Val loss: 1.00701332\n",
      "(16.32 min) Epoch 81/300 -- Iteration 77445 - Batch 405/963 - Train loss: 0.26944028  - Train acc: 0.8994 - Val loss: 1.00701332\n",
      "(16.32 min) Epoch 81/300 -- Iteration 77454 - Batch 414/963 - Train loss: 0.26886670  - Train acc: 0.8997 - Val loss: 1.00701332\n",
      "(16.32 min) Epoch 81/300 -- Iteration 77463 - Batch 423/963 - Train loss: 0.26860874  - Train acc: 0.8995 - Val loss: 1.00701332\n",
      "(16.33 min) Epoch 81/300 -- Iteration 77472 - Batch 432/963 - Train loss: 0.26856617  - Train acc: 0.8994 - Val loss: 1.00701332\n",
      "(16.33 min) Epoch 81/300 -- Iteration 77481 - Batch 441/963 - Train loss: 0.26865341  - Train acc: 0.8994 - Val loss: 1.00701332\n",
      "(16.33 min) Epoch 81/300 -- Iteration 77490 - Batch 450/963 - Train loss: 0.26865817  - Train acc: 0.8995 - Val loss: 1.00701332\n",
      "(16.33 min) Epoch 81/300 -- Iteration 77499 - Batch 459/963 - Train loss: 0.26896334  - Train acc: 0.8992 - Val loss: 1.00701332\n",
      "(16.33 min) Epoch 81/300 -- Iteration 77508 - Batch 468/963 - Train loss: 0.26909651  - Train acc: 0.8992 - Val loss: 1.00701332\n",
      "(16.33 min) Epoch 81/300 -- Iteration 77517 - Batch 477/963 - Train loss: 0.26952620  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.34 min) Epoch 81/300 -- Iteration 77526 - Batch 486/963 - Train loss: 0.26962002  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.34 min) Epoch 81/300 -- Iteration 77535 - Batch 495/963 - Train loss: 0.26957466  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.34 min) Epoch 81/300 -- Iteration 77544 - Batch 504/963 - Train loss: 0.26999673  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.34 min) Epoch 81/300 -- Iteration 77553 - Batch 513/963 - Train loss: 0.26982250  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.34 min) Epoch 81/300 -- Iteration 77562 - Batch 522/963 - Train loss: 0.26996885  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.35 min) Epoch 81/300 -- Iteration 77571 - Batch 531/963 - Train loss: 0.27022051  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.35 min) Epoch 81/300 -- Iteration 77580 - Batch 540/963 - Train loss: 0.27029305  - Train acc: 0.8989 - Val loss: 1.00701332\n",
      "(16.35 min) Epoch 81/300 -- Iteration 77589 - Batch 549/963 - Train loss: 0.26993385  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.35 min) Epoch 81/300 -- Iteration 77598 - Batch 558/963 - Train loss: 0.26995996  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.35 min) Epoch 81/300 -- Iteration 77607 - Batch 567/963 - Train loss: 0.26978272  - Train acc: 0.8992 - Val loss: 1.00701332\n",
      "(16.36 min) Epoch 81/300 -- Iteration 77616 - Batch 576/963 - Train loss: 0.26972259  - Train acc: 0.8992 - Val loss: 1.00701332\n",
      "(16.36 min) Epoch 81/300 -- Iteration 77625 - Batch 585/963 - Train loss: 0.26984447  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.36 min) Epoch 81/300 -- Iteration 77634 - Batch 594/963 - Train loss: 0.26976668  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.36 min) Epoch 81/300 -- Iteration 77643 - Batch 603/963 - Train loss: 0.26985934  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.36 min) Epoch 81/300 -- Iteration 77652 - Batch 612/963 - Train loss: 0.27033713  - Train acc: 0.8987 - Val loss: 1.00701332\n",
      "(16.37 min) Epoch 81/300 -- Iteration 77661 - Batch 621/963 - Train loss: 0.27032247  - Train acc: 0.8987 - Val loss: 1.00701332\n",
      "(16.37 min) Epoch 81/300 -- Iteration 77670 - Batch 630/963 - Train loss: 0.27015675  - Train acc: 0.8988 - Val loss: 1.00701332\n",
      "(16.37 min) Epoch 81/300 -- Iteration 77679 - Batch 639/963 - Train loss: 0.27005933  - Train acc: 0.8988 - Val loss: 1.00701332\n",
      "(16.37 min) Epoch 81/300 -- Iteration 77688 - Batch 648/963 - Train loss: 0.26971461  - Train acc: 0.8989 - Val loss: 1.00701332\n",
      "(16.37 min) Epoch 81/300 -- Iteration 77697 - Batch 657/963 - Train loss: 0.26967999  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.37 min) Epoch 81/300 -- Iteration 77706 - Batch 666/963 - Train loss: 0.26924073  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.38 min) Epoch 81/300 -- Iteration 77715 - Batch 675/963 - Train loss: 0.26939298  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.38 min) Epoch 81/300 -- Iteration 77724 - Batch 684/963 - Train loss: 0.26925838  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.38 min) Epoch 81/300 -- Iteration 77733 - Batch 693/963 - Train loss: 0.26890822  - Train acc: 0.8993 - Val loss: 1.00701332\n",
      "(16.38 min) Epoch 81/300 -- Iteration 77742 - Batch 702/963 - Train loss: 0.26873949  - Train acc: 0.8992 - Val loss: 1.00701332\n",
      "(16.38 min) Epoch 81/300 -- Iteration 77751 - Batch 711/963 - Train loss: 0.26882214  - Train acc: 0.8991 - Val loss: 1.00701332\n",
      "(16.39 min) Epoch 81/300 -- Iteration 77760 - Batch 720/963 - Train loss: 0.26930517  - Train acc: 0.8988 - Val loss: 1.00701332\n",
      "(16.39 min) Epoch 81/300 -- Iteration 77769 - Batch 729/963 - Train loss: 0.26927514  - Train acc: 0.8988 - Val loss: 1.00701332\n",
      "(16.39 min) Epoch 81/300 -- Iteration 77778 - Batch 738/963 - Train loss: 0.26911940  - Train acc: 0.8988 - Val loss: 1.00701332\n",
      "(16.39 min) Epoch 81/300 -- Iteration 77787 - Batch 747/963 - Train loss: 0.26902910  - Train acc: 0.8989 - Val loss: 1.00701332\n",
      "(16.39 min) Epoch 81/300 -- Iteration 77796 - Batch 756/963 - Train loss: 0.26870008  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.40 min) Epoch 81/300 -- Iteration 77805 - Batch 765/963 - Train loss: 0.26857288  - Train acc: 0.8990 - Val loss: 1.00701332\n",
      "(16.40 min) Epoch 81/300 -- Iteration 77814 - Batch 774/963 - Train loss: 0.26838632  - Train acc: 0.8992 - Val loss: 1.00701332\n",
      "(16.40 min) Epoch 81/300 -- Iteration 77823 - Batch 783/963 - Train loss: 0.26805701  - Train acc: 0.8993 - Val loss: 1.00701332\n",
      "(16.40 min) Epoch 81/300 -- Iteration 77832 - Batch 792/963 - Train loss: 0.26819904  - Train acc: 0.8993 - Val loss: 1.00701332\n",
      "(16.40 min) Epoch 81/300 -- Iteration 77841 - Batch 801/963 - Train loss: 0.26825368  - Train acc: 0.8993 - Val loss: 1.00701332\n",
      "(16.41 min) Epoch 81/300 -- Iteration 77850 - Batch 810/963 - Train loss: 0.26809830  - Train acc: 0.8993 - Val loss: 1.00701332\n",
      "(16.41 min) Epoch 81/300 -- Iteration 77859 - Batch 819/963 - Train loss: 0.26794090  - Train acc: 0.8994 - Val loss: 1.00701332\n",
      "(16.41 min) Epoch 81/300 -- Iteration 77868 - Batch 828/963 - Train loss: 0.26790756  - Train acc: 0.8993 - Val loss: 1.00701332\n",
      "(16.41 min) Epoch 81/300 -- Iteration 77877 - Batch 837/963 - Train loss: 0.26744114  - Train acc: 0.8995 - Val loss: 1.00701332\n",
      "(16.41 min) Epoch 81/300 -- Iteration 77886 - Batch 846/963 - Train loss: 0.26711933  - Train acc: 0.8997 - Val loss: 1.00701332\n",
      "(16.41 min) Epoch 81/300 -- Iteration 77895 - Batch 855/963 - Train loss: 0.26669438  - Train acc: 0.8999 - Val loss: 1.00701332\n",
      "(16.42 min) Epoch 81/300 -- Iteration 77904 - Batch 864/963 - Train loss: 0.26671115  - Train acc: 0.8999 - Val loss: 1.00701332\n",
      "(16.42 min) Epoch 81/300 -- Iteration 77913 - Batch 873/963 - Train loss: 0.26665546  - Train acc: 0.8999 - Val loss: 1.00701332\n",
      "(16.42 min) Epoch 81/300 -- Iteration 77922 - Batch 882/963 - Train loss: 0.26649499  - Train acc: 0.9000 - Val loss: 1.00701332\n",
      "(16.42 min) Epoch 81/300 -- Iteration 77931 - Batch 891/963 - Train loss: 0.26658374  - Train acc: 0.9000 - Val loss: 1.00701332\n",
      "(16.42 min) Epoch 81/300 -- Iteration 77940 - Batch 900/963 - Train loss: 0.26659338  - Train acc: 0.9000 - Val loss: 1.00701332\n",
      "(16.43 min) Epoch 81/300 -- Iteration 77949 - Batch 909/963 - Train loss: 0.26651841  - Train acc: 0.9001 - Val loss: 1.00701332\n",
      "(16.43 min) Epoch 81/300 -- Iteration 77958 - Batch 918/963 - Train loss: 0.26648508  - Train acc: 0.9001 - Val loss: 1.00701332\n",
      "(16.43 min) Epoch 81/300 -- Iteration 77967 - Batch 927/963 - Train loss: 0.26659914  - Train acc: 0.9002 - Val loss: 1.00701332\n",
      "(16.43 min) Epoch 81/300 -- Iteration 77976 - Batch 936/963 - Train loss: 0.26656056  - Train acc: 0.9001 - Val loss: 1.00701332\n",
      "(16.43 min) Epoch 81/300 -- Iteration 77985 - Batch 945/963 - Train loss: 0.26666592  - Train acc: 0.9001 - Val loss: 1.00701332\n",
      "(16.44 min) Epoch 81/300 -- Iteration 77994 - Batch 954/963 - Train loss: 0.26673264  - Train acc: 0.9000 - Val loss: 1.00701332\n",
      "(16.44 min) Epoch 81/300 -- Iteration 78003 - Batch 962/963 - Train loss: 0.26672537  - Train acc: 0.9000 - Val loss: 0.99629629 - Val acc: 0.5933\n",
      "(16.44 min) Epoch 82/300 -- Iteration 78012 - Batch 9/963 - Train loss: 0.25915305  - Train acc: 0.8969 - Val loss: 0.99629629\n",
      "(16.44 min) Epoch 82/300 -- Iteration 78021 - Batch 18/963 - Train loss: 0.26262710  - Train acc: 0.9009 - Val loss: 0.99629629\n",
      "(16.44 min) Epoch 82/300 -- Iteration 78030 - Batch 27/963 - Train loss: 0.26392991  - Train acc: 0.9046 - Val loss: 0.99629629\n",
      "(16.45 min) Epoch 82/300 -- Iteration 78039 - Batch 36/963 - Train loss: 0.25891231  - Train acc: 0.9082 - Val loss: 0.99629629\n",
      "(16.45 min) Epoch 82/300 -- Iteration 78048 - Batch 45/963 - Train loss: 0.25745585  - Train acc: 0.9071 - Val loss: 0.99629629\n",
      "(16.45 min) Epoch 82/300 -- Iteration 78057 - Batch 54/963 - Train loss: 0.25947727  - Train acc: 0.9044 - Val loss: 0.99629629\n",
      "(16.45 min) Epoch 82/300 -- Iteration 78066 - Batch 63/963 - Train loss: 0.25884789  - Train acc: 0.9033 - Val loss: 0.99629629\n",
      "(16.45 min) Epoch 82/300 -- Iteration 78075 - Batch 72/963 - Train loss: 0.26228400  - Train acc: 0.9015 - Val loss: 0.99629629\n",
      "(16.45 min) Epoch 82/300 -- Iteration 78084 - Batch 81/963 - Train loss: 0.26580526  - Train acc: 0.9006 - Val loss: 0.99629629\n",
      "(16.46 min) Epoch 82/300 -- Iteration 78093 - Batch 90/963 - Train loss: 0.26698071  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.46 min) Epoch 82/300 -- Iteration 78102 - Batch 99/963 - Train loss: 0.26540719  - Train acc: 0.8999 - Val loss: 0.99629629\n",
      "(16.46 min) Epoch 82/300 -- Iteration 78111 - Batch 108/963 - Train loss: 0.26525785  - Train acc: 0.9003 - Val loss: 0.99629629\n",
      "(16.46 min) Epoch 82/300 -- Iteration 78120 - Batch 117/963 - Train loss: 0.26680939  - Train acc: 0.8994 - Val loss: 0.99629629\n",
      "(16.46 min) Epoch 82/300 -- Iteration 78129 - Batch 126/963 - Train loss: 0.26659162  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.47 min) Epoch 82/300 -- Iteration 78138 - Batch 135/963 - Train loss: 0.26707967  - Train acc: 0.8992 - Val loss: 0.99629629\n",
      "(16.47 min) Epoch 82/300 -- Iteration 78147 - Batch 144/963 - Train loss: 0.26647626  - Train acc: 0.9001 - Val loss: 0.99629629\n",
      "(16.47 min) Epoch 82/300 -- Iteration 78156 - Batch 153/963 - Train loss: 0.26712397  - Train acc: 0.8991 - Val loss: 0.99629629\n",
      "(16.47 min) Epoch 82/300 -- Iteration 78165 - Batch 162/963 - Train loss: 0.26773488  - Train acc: 0.8990 - Val loss: 0.99629629\n",
      "(16.47 min) Epoch 82/300 -- Iteration 78174 - Batch 171/963 - Train loss: 0.26701702  - Train acc: 0.8993 - Val loss: 0.99629629\n",
      "(16.48 min) Epoch 82/300 -- Iteration 78183 - Batch 180/963 - Train loss: 0.26690745  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.48 min) Epoch 82/300 -- Iteration 78192 - Batch 189/963 - Train loss: 0.26629597  - Train acc: 0.9000 - Val loss: 0.99629629\n",
      "(16.48 min) Epoch 82/300 -- Iteration 78201 - Batch 198/963 - Train loss: 0.26734029  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.48 min) Epoch 82/300 -- Iteration 78210 - Batch 207/963 - Train loss: 0.26812638  - Train acc: 0.8992 - Val loss: 0.99629629\n",
      "(16.48 min) Epoch 82/300 -- Iteration 78219 - Batch 216/963 - Train loss: 0.26638108  - Train acc: 0.9000 - Val loss: 0.99629629\n",
      "(16.49 min) Epoch 82/300 -- Iteration 78228 - Batch 225/963 - Train loss: 0.26676857  - Train acc: 0.8995 - Val loss: 0.99629629\n",
      "(16.49 min) Epoch 82/300 -- Iteration 78237 - Batch 234/963 - Train loss: 0.26636804  - Train acc: 0.8995 - Val loss: 0.99629629\n",
      "(16.49 min) Epoch 82/300 -- Iteration 78246 - Batch 243/963 - Train loss: 0.26684007  - Train acc: 0.8993 - Val loss: 0.99629629\n",
      "(16.49 min) Epoch 82/300 -- Iteration 78255 - Batch 252/963 - Train loss: 0.26710593  - Train acc: 0.8992 - Val loss: 0.99629629\n",
      "(16.49 min) Epoch 82/300 -- Iteration 78264 - Batch 261/963 - Train loss: 0.26705846  - Train acc: 0.8994 - Val loss: 0.99629629\n",
      "(16.49 min) Epoch 82/300 -- Iteration 78273 - Batch 270/963 - Train loss: 0.26749289  - Train acc: 0.8994 - Val loss: 0.99629629\n",
      "(16.50 min) Epoch 82/300 -- Iteration 78282 - Batch 279/963 - Train loss: 0.26763623  - Train acc: 0.8992 - Val loss: 0.99629629\n",
      "(16.50 min) Epoch 82/300 -- Iteration 78291 - Batch 288/963 - Train loss: 0.26740865  - Train acc: 0.8994 - Val loss: 0.99629629\n",
      "(16.50 min) Epoch 82/300 -- Iteration 78300 - Batch 297/963 - Train loss: 0.26744797  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.50 min) Epoch 82/300 -- Iteration 78309 - Batch 306/963 - Train loss: 0.26691218  - Train acc: 0.8999 - Val loss: 0.99629629\n",
      "(16.50 min) Epoch 82/300 -- Iteration 78318 - Batch 315/963 - Train loss: 0.26644391  - Train acc: 0.9000 - Val loss: 0.99629629\n",
      "(16.51 min) Epoch 82/300 -- Iteration 78327 - Batch 324/963 - Train loss: 0.26784103  - Train acc: 0.8995 - Val loss: 0.99629629\n",
      "(16.51 min) Epoch 82/300 -- Iteration 78336 - Batch 333/963 - Train loss: 0.26857992  - Train acc: 0.8990 - Val loss: 0.99629629\n",
      "(16.51 min) Epoch 82/300 -- Iteration 78345 - Batch 342/963 - Train loss: 0.26823889  - Train acc: 0.8991 - Val loss: 0.99629629\n",
      "(16.51 min) Epoch 82/300 -- Iteration 78354 - Batch 351/963 - Train loss: 0.26910881  - Train acc: 0.8987 - Val loss: 0.99629629\n",
      "(16.51 min) Epoch 82/300 -- Iteration 78363 - Batch 360/963 - Train loss: 0.26843906  - Train acc: 0.8989 - Val loss: 0.99629629\n",
      "(16.52 min) Epoch 82/300 -- Iteration 78372 - Batch 369/963 - Train loss: 0.26833033  - Train acc: 0.8989 - Val loss: 0.99629629\n",
      "(16.52 min) Epoch 82/300 -- Iteration 78381 - Batch 378/963 - Train loss: 0.26869388  - Train acc: 0.8987 - Val loss: 0.99629629\n",
      "(16.52 min) Epoch 82/300 -- Iteration 78390 - Batch 387/963 - Train loss: 0.26851467  - Train acc: 0.8988 - Val loss: 0.99629629\n",
      "(16.52 min) Epoch 82/300 -- Iteration 78399 - Batch 396/963 - Train loss: 0.26889543  - Train acc: 0.8986 - Val loss: 0.99629629\n",
      "(16.52 min) Epoch 82/300 -- Iteration 78408 - Batch 405/963 - Train loss: 0.26825514  - Train acc: 0.8988 - Val loss: 0.99629629\n",
      "(16.53 min) Epoch 82/300 -- Iteration 78417 - Batch 414/963 - Train loss: 0.26800344  - Train acc: 0.8990 - Val loss: 0.99629629\n",
      "(16.53 min) Epoch 82/300 -- Iteration 78426 - Batch 423/963 - Train loss: 0.26780504  - Train acc: 0.8992 - Val loss: 0.99629629\n",
      "(16.53 min) Epoch 82/300 -- Iteration 78435 - Batch 432/963 - Train loss: 0.26717527  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.53 min) Epoch 82/300 -- Iteration 78444 - Batch 441/963 - Train loss: 0.26670961  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.53 min) Epoch 82/300 -- Iteration 78453 - Batch 450/963 - Train loss: 0.26661297  - Train acc: 0.8999 - Val loss: 0.99629629\n",
      "(16.53 min) Epoch 82/300 -- Iteration 78462 - Batch 459/963 - Train loss: 0.26664205  - Train acc: 0.8999 - Val loss: 0.99629629\n",
      "(16.54 min) Epoch 82/300 -- Iteration 78471 - Batch 468/963 - Train loss: 0.26665528  - Train acc: 0.9001 - Val loss: 0.99629629\n",
      "(16.54 min) Epoch 82/300 -- Iteration 78480 - Batch 477/963 - Train loss: 0.26701819  - Train acc: 0.9000 - Val loss: 0.99629629\n",
      "(16.54 min) Epoch 82/300 -- Iteration 78489 - Batch 486/963 - Train loss: 0.26689557  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.54 min) Epoch 82/300 -- Iteration 78498 - Batch 495/963 - Train loss: 0.26673847  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.54 min) Epoch 82/300 -- Iteration 78507 - Batch 504/963 - Train loss: 0.26660426  - Train acc: 0.8999 - Val loss: 0.99629629\n",
      "(16.55 min) Epoch 82/300 -- Iteration 78516 - Batch 513/963 - Train loss: 0.26672618  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.55 min) Epoch 82/300 -- Iteration 78525 - Batch 522/963 - Train loss: 0.26690664  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.55 min) Epoch 82/300 -- Iteration 78534 - Batch 531/963 - Train loss: 0.26688187  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.55 min) Epoch 82/300 -- Iteration 78543 - Batch 540/963 - Train loss: 0.26675154  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.55 min) Epoch 82/300 -- Iteration 78552 - Batch 549/963 - Train loss: 0.26738663  - Train acc: 0.8994 - Val loss: 0.99629629\n",
      "(16.56 min) Epoch 82/300 -- Iteration 78561 - Batch 558/963 - Train loss: 0.26701198  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.56 min) Epoch 82/300 -- Iteration 78570 - Batch 567/963 - Train loss: 0.26705187  - Train acc: 0.8995 - Val loss: 0.99629629\n",
      "(16.56 min) Epoch 82/300 -- Iteration 78579 - Batch 576/963 - Train loss: 0.26700956  - Train acc: 0.8994 - Val loss: 0.99629629\n",
      "(16.56 min) Epoch 82/300 -- Iteration 78588 - Batch 585/963 - Train loss: 0.26642073  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.56 min) Epoch 82/300 -- Iteration 78597 - Batch 594/963 - Train loss: 0.26621913  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.56 min) Epoch 82/300 -- Iteration 78606 - Batch 603/963 - Train loss: 0.26617852  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.57 min) Epoch 82/300 -- Iteration 78615 - Batch 612/963 - Train loss: 0.26645256  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.57 min) Epoch 82/300 -- Iteration 78624 - Batch 621/963 - Train loss: 0.26657294  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.57 min) Epoch 82/300 -- Iteration 78633 - Batch 630/963 - Train loss: 0.26663506  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.57 min) Epoch 82/300 -- Iteration 78642 - Batch 639/963 - Train loss: 0.26650068  - Train acc: 0.8999 - Val loss: 0.99629629\n",
      "(16.57 min) Epoch 82/300 -- Iteration 78651 - Batch 648/963 - Train loss: 0.26655432  - Train acc: 0.8999 - Val loss: 0.99629629\n",
      "(16.58 min) Epoch 82/300 -- Iteration 78660 - Batch 657/963 - Train loss: 0.26675582  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.58 min) Epoch 82/300 -- Iteration 78669 - Batch 666/963 - Train loss: 0.26658289  - Train acc: 0.8999 - Val loss: 0.99629629\n",
      "(16.58 min) Epoch 82/300 -- Iteration 78678 - Batch 675/963 - Train loss: 0.26679162  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.58 min) Epoch 82/300 -- Iteration 78687 - Batch 684/963 - Train loss: 0.26695891  - Train acc: 0.8995 - Val loss: 0.99629629\n",
      "(16.58 min) Epoch 82/300 -- Iteration 78696 - Batch 693/963 - Train loss: 0.26685037  - Train acc: 0.8995 - Val loss: 0.99629629\n",
      "(16.59 min) Epoch 82/300 -- Iteration 78705 - Batch 702/963 - Train loss: 0.26623971  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.59 min) Epoch 82/300 -- Iteration 78714 - Batch 711/963 - Train loss: 0.26624169  - Train acc: 0.8995 - Val loss: 0.99629629\n",
      "(16.59 min) Epoch 82/300 -- Iteration 78723 - Batch 720/963 - Train loss: 0.26652734  - Train acc: 0.8994 - Val loss: 0.99629629\n",
      "(16.59 min) Epoch 82/300 -- Iteration 78732 - Batch 729/963 - Train loss: 0.26620827  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.59 min) Epoch 82/300 -- Iteration 78741 - Batch 738/963 - Train loss: 0.26612876  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.59 min) Epoch 82/300 -- Iteration 78750 - Batch 747/963 - Train loss: 0.26646224  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.60 min) Epoch 82/300 -- Iteration 78759 - Batch 756/963 - Train loss: 0.26666690  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.60 min) Epoch 82/300 -- Iteration 78768 - Batch 765/963 - Train loss: 0.26664546  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.60 min) Epoch 82/300 -- Iteration 78777 - Batch 774/963 - Train loss: 0.26677523  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.60 min) Epoch 82/300 -- Iteration 78786 - Batch 783/963 - Train loss: 0.26675893  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.60 min) Epoch 82/300 -- Iteration 78795 - Batch 792/963 - Train loss: 0.26667705  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.61 min) Epoch 82/300 -- Iteration 78804 - Batch 801/963 - Train loss: 0.26656181  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.61 min) Epoch 82/300 -- Iteration 78813 - Batch 810/963 - Train loss: 0.26638275  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.61 min) Epoch 82/300 -- Iteration 78822 - Batch 819/963 - Train loss: 0.26618487  - Train acc: 0.8999 - Val loss: 0.99629629\n",
      "(16.61 min) Epoch 82/300 -- Iteration 78831 - Batch 828/963 - Train loss: 0.26633669  - Train acc: 0.8998 - Val loss: 0.99629629\n",
      "(16.61 min) Epoch 82/300 -- Iteration 78840 - Batch 837/963 - Train loss: 0.26650378  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.62 min) Epoch 82/300 -- Iteration 78849 - Batch 846/963 - Train loss: 0.26669061  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.62 min) Epoch 82/300 -- Iteration 78858 - Batch 855/963 - Train loss: 0.26664030  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.62 min) Epoch 82/300 -- Iteration 78867 - Batch 864/963 - Train loss: 0.26635435  - Train acc: 0.8997 - Val loss: 0.99629629\n",
      "(16.62 min) Epoch 82/300 -- Iteration 78876 - Batch 873/963 - Train loss: 0.26649302  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.62 min) Epoch 82/300 -- Iteration 78885 - Batch 882/963 - Train loss: 0.26639051  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.63 min) Epoch 82/300 -- Iteration 78894 - Batch 891/963 - Train loss: 0.26655753  - Train acc: 0.8995 - Val loss: 0.99629629\n",
      "(16.63 min) Epoch 82/300 -- Iteration 78903 - Batch 900/963 - Train loss: 0.26626280  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.63 min) Epoch 82/300 -- Iteration 78912 - Batch 909/963 - Train loss: 0.26642201  - Train acc: 0.8996 - Val loss: 0.99629629\n",
      "(16.63 min) Epoch 82/300 -- Iteration 78921 - Batch 918/963 - Train loss: 0.26670150  - Train acc: 0.8994 - Val loss: 0.99629629\n",
      "(16.63 min) Epoch 82/300 -- Iteration 78930 - Batch 927/963 - Train loss: 0.26675369  - Train acc: 0.8995 - Val loss: 0.99629629\n",
      "(16.63 min) Epoch 82/300 -- Iteration 78939 - Batch 936/963 - Train loss: 0.26670710  - Train acc: 0.8994 - Val loss: 0.99629629\n",
      "(16.64 min) Epoch 82/300 -- Iteration 78948 - Batch 945/963 - Train loss: 0.26677279  - Train acc: 0.8993 - Val loss: 0.99629629\n",
      "(16.64 min) Epoch 82/300 -- Iteration 78957 - Batch 954/963 - Train loss: 0.26722369  - Train acc: 0.8992 - Val loss: 0.99629629\n",
      "(16.64 min) Epoch 82/300 -- Iteration 78966 - Batch 962/963 - Train loss: 0.26735228  - Train acc: 0.8991 - Val loss: 1.00397301 - Val acc: 0.5917\n",
      "(16.64 min) Epoch 83/300 -- Iteration 78975 - Batch 9/963 - Train loss: 0.25057767  - Train acc: 0.9008 - Val loss: 1.00397301\n",
      "(16.64 min) Epoch 83/300 -- Iteration 78984 - Batch 18/963 - Train loss: 0.26619886  - Train acc: 0.9005 - Val loss: 1.00397301\n",
      "(16.65 min) Epoch 83/300 -- Iteration 78993 - Batch 27/963 - Train loss: 0.27101218  - Train acc: 0.8996 - Val loss: 1.00397301\n",
      "(16.65 min) Epoch 83/300 -- Iteration 79002 - Batch 36/963 - Train loss: 0.27094298  - Train acc: 0.9003 - Val loss: 1.00397301\n",
      "(16.65 min) Epoch 83/300 -- Iteration 79011 - Batch 45/963 - Train loss: 0.27109742  - Train acc: 0.9012 - Val loss: 1.00397301\n",
      "(16.65 min) Epoch 83/300 -- Iteration 79020 - Batch 54/963 - Train loss: 0.26665989  - Train acc: 0.9028 - Val loss: 1.00397301\n",
      "(16.65 min) Epoch 83/300 -- Iteration 79029 - Batch 63/963 - Train loss: 0.26293654  - Train acc: 0.9031 - Val loss: 1.00397301\n",
      "(16.66 min) Epoch 83/300 -- Iteration 79038 - Batch 72/963 - Train loss: 0.26486952  - Train acc: 0.9023 - Val loss: 1.00397301\n",
      "(16.66 min) Epoch 83/300 -- Iteration 79047 - Batch 81/963 - Train loss: 0.26205850  - Train acc: 0.9034 - Val loss: 1.00397301\n",
      "(16.66 min) Epoch 83/300 -- Iteration 79056 - Batch 90/963 - Train loss: 0.26528057  - Train acc: 0.9016 - Val loss: 1.00397301\n",
      "(16.66 min) Epoch 83/300 -- Iteration 79065 - Batch 99/963 - Train loss: 0.26644311  - Train acc: 0.9009 - Val loss: 1.00397301\n",
      "(16.66 min) Epoch 83/300 -- Iteration 79074 - Batch 108/963 - Train loss: 0.26638987  - Train acc: 0.9009 - Val loss: 1.00397301\n",
      "(16.67 min) Epoch 83/300 -- Iteration 79083 - Batch 117/963 - Train loss: 0.26665872  - Train acc: 0.9007 - Val loss: 1.00397301\n",
      "(16.67 min) Epoch 83/300 -- Iteration 79092 - Batch 126/963 - Train loss: 0.26718462  - Train acc: 0.9002 - Val loss: 1.00397301\n",
      "(16.67 min) Epoch 83/300 -- Iteration 79101 - Batch 135/963 - Train loss: 0.26759902  - Train acc: 0.9002 - Val loss: 1.00397301\n",
      "(16.67 min) Epoch 83/300 -- Iteration 79110 - Batch 144/963 - Train loss: 0.26782588  - Train acc: 0.8998 - Val loss: 1.00397301\n",
      "(16.67 min) Epoch 83/300 -- Iteration 79119 - Batch 153/963 - Train loss: 0.26646756  - Train acc: 0.9001 - Val loss: 1.00397301\n",
      "(16.67 min) Epoch 83/300 -- Iteration 79128 - Batch 162/963 - Train loss: 0.26661523  - Train acc: 0.9001 - Val loss: 1.00397301\n",
      "(16.68 min) Epoch 83/300 -- Iteration 79137 - Batch 171/963 - Train loss: 0.26865719  - Train acc: 0.8994 - Val loss: 1.00397301\n",
      "(16.68 min) Epoch 83/300 -- Iteration 79146 - Batch 180/963 - Train loss: 0.26937776  - Train acc: 0.8993 - Val loss: 1.00397301\n",
      "(16.68 min) Epoch 83/300 -- Iteration 79155 - Batch 189/963 - Train loss: 0.26906520  - Train acc: 0.8994 - Val loss: 1.00397301\n",
      "(16.68 min) Epoch 83/300 -- Iteration 79164 - Batch 198/963 - Train loss: 0.26991815  - Train acc: 0.8987 - Val loss: 1.00397301\n",
      "(16.68 min) Epoch 83/300 -- Iteration 79173 - Batch 207/963 - Train loss: 0.26906635  - Train acc: 0.8990 - Val loss: 1.00397301\n",
      "(16.69 min) Epoch 83/300 -- Iteration 79182 - Batch 216/963 - Train loss: 0.26797259  - Train acc: 0.8995 - Val loss: 1.00397301\n",
      "(16.69 min) Epoch 83/300 -- Iteration 79191 - Batch 225/963 - Train loss: 0.26838321  - Train acc: 0.8991 - Val loss: 1.00397301\n",
      "(16.69 min) Epoch 83/300 -- Iteration 79200 - Batch 234/963 - Train loss: 0.26817615  - Train acc: 0.8996 - Val loss: 1.00397301\n",
      "(16.69 min) Epoch 83/300 -- Iteration 79209 - Batch 243/963 - Train loss: 0.26776218  - Train acc: 0.8995 - Val loss: 1.00397301\n",
      "(16.69 min) Epoch 83/300 -- Iteration 79218 - Batch 252/963 - Train loss: 0.26722173  - Train acc: 0.8995 - Val loss: 1.00397301\n",
      "(16.70 min) Epoch 83/300 -- Iteration 79227 - Batch 261/963 - Train loss: 0.26633285  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.70 min) Epoch 83/300 -- Iteration 79236 - Batch 270/963 - Train loss: 0.26603629  - Train acc: 0.9000 - Val loss: 1.00397301\n",
      "(16.70 min) Epoch 83/300 -- Iteration 79245 - Batch 279/963 - Train loss: 0.26616093  - Train acc: 0.8997 - Val loss: 1.00397301\n",
      "(16.70 min) Epoch 83/300 -- Iteration 79254 - Batch 288/963 - Train loss: 0.26563332  - Train acc: 0.8998 - Val loss: 1.00397301\n",
      "(16.70 min) Epoch 83/300 -- Iteration 79263 - Batch 297/963 - Train loss: 0.26577293  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.71 min) Epoch 83/300 -- Iteration 79272 - Batch 306/963 - Train loss: 0.26547577  - Train acc: 0.9000 - Val loss: 1.00397301\n",
      "(16.71 min) Epoch 83/300 -- Iteration 79281 - Batch 315/963 - Train loss: 0.26583717  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.71 min) Epoch 83/300 -- Iteration 79290 - Batch 324/963 - Train loss: 0.26550245  - Train acc: 0.9000 - Val loss: 1.00397301\n",
      "(16.71 min) Epoch 83/300 -- Iteration 79299 - Batch 333/963 - Train loss: 0.26551710  - Train acc: 0.9000 - Val loss: 1.00397301\n",
      "(16.71 min) Epoch 83/300 -- Iteration 79308 - Batch 342/963 - Train loss: 0.26537375  - Train acc: 0.9002 - Val loss: 1.00397301\n",
      "(16.71 min) Epoch 83/300 -- Iteration 79317 - Batch 351/963 - Train loss: 0.26582988  - Train acc: 0.9001 - Val loss: 1.00397301\n",
      "(16.72 min) Epoch 83/300 -- Iteration 79326 - Batch 360/963 - Train loss: 0.26672873  - Train acc: 0.8996 - Val loss: 1.00397301\n",
      "(16.72 min) Epoch 83/300 -- Iteration 79335 - Batch 369/963 - Train loss: 0.26668010  - Train acc: 0.8997 - Val loss: 1.00397301\n",
      "(16.72 min) Epoch 83/300 -- Iteration 79344 - Batch 378/963 - Train loss: 0.26693841  - Train acc: 0.8995 - Val loss: 1.00397301\n",
      "(16.72 min) Epoch 83/300 -- Iteration 79353 - Batch 387/963 - Train loss: 0.26696252  - Train acc: 0.8997 - Val loss: 1.00397301\n",
      "(16.72 min) Epoch 83/300 -- Iteration 79362 - Batch 396/963 - Train loss: 0.26784258  - Train acc: 0.8995 - Val loss: 1.00397301\n",
      "(16.73 min) Epoch 83/300 -- Iteration 79371 - Batch 405/963 - Train loss: 0.26813341  - Train acc: 0.8994 - Val loss: 1.00397301\n",
      "(16.73 min) Epoch 83/300 -- Iteration 79380 - Batch 414/963 - Train loss: 0.26769296  - Train acc: 0.8996 - Val loss: 1.00397301\n",
      "(16.73 min) Epoch 83/300 -- Iteration 79389 - Batch 423/963 - Train loss: 0.26765583  - Train acc: 0.8995 - Val loss: 1.00397301\n",
      "(16.73 min) Epoch 83/300 -- Iteration 79398 - Batch 432/963 - Train loss: 0.26786974  - Train acc: 0.8995 - Val loss: 1.00397301\n",
      "(16.73 min) Epoch 83/300 -- Iteration 79407 - Batch 441/963 - Train loss: 0.26749112  - Train acc: 0.8997 - Val loss: 1.00397301\n",
      "(16.74 min) Epoch 83/300 -- Iteration 79416 - Batch 450/963 - Train loss: 0.26770358  - Train acc: 0.8997 - Val loss: 1.00397301\n",
      "(16.74 min) Epoch 83/300 -- Iteration 79425 - Batch 459/963 - Train loss: 0.26743541  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.74 min) Epoch 83/300 -- Iteration 79434 - Batch 468/963 - Train loss: 0.26742294  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.74 min) Epoch 83/300 -- Iteration 79443 - Batch 477/963 - Train loss: 0.26763366  - Train acc: 0.8998 - Val loss: 1.00397301\n",
      "(16.74 min) Epoch 83/300 -- Iteration 79452 - Batch 486/963 - Train loss: 0.26762670  - Train acc: 0.9000 - Val loss: 1.00397301\n",
      "(16.74 min) Epoch 83/300 -- Iteration 79461 - Batch 495/963 - Train loss: 0.26753637  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.75 min) Epoch 83/300 -- Iteration 79470 - Batch 504/963 - Train loss: 0.26769784  - Train acc: 0.8998 - Val loss: 1.00397301\n",
      "(16.75 min) Epoch 83/300 -- Iteration 79479 - Batch 513/963 - Train loss: 0.26764078  - Train acc: 0.8998 - Val loss: 1.00397301\n",
      "(16.75 min) Epoch 83/300 -- Iteration 79488 - Batch 522/963 - Train loss: 0.26777814  - Train acc: 0.8998 - Val loss: 1.00397301\n",
      "(16.75 min) Epoch 83/300 -- Iteration 79497 - Batch 531/963 - Train loss: 0.26759819  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.75 min) Epoch 83/300 -- Iteration 79506 - Batch 540/963 - Train loss: 0.26760601  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.76 min) Epoch 83/300 -- Iteration 79515 - Batch 549/963 - Train loss: 0.26734898  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.76 min) Epoch 83/300 -- Iteration 79524 - Batch 558/963 - Train loss: 0.26669405  - Train acc: 0.9003 - Val loss: 1.00397301\n",
      "(16.76 min) Epoch 83/300 -- Iteration 79533 - Batch 567/963 - Train loss: 0.26688202  - Train acc: 0.9000 - Val loss: 1.00397301\n",
      "(16.76 min) Epoch 83/300 -- Iteration 79542 - Batch 576/963 - Train loss: 0.26716547  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.76 min) Epoch 83/300 -- Iteration 79551 - Batch 585/963 - Train loss: 0.26715359  - Train acc: 0.8997 - Val loss: 1.00397301\n",
      "(16.77 min) Epoch 83/300 -- Iteration 79560 - Batch 594/963 - Train loss: 0.26675718  - Train acc: 0.8999 - Val loss: 1.00397301\n",
      "(16.77 min) Epoch 83/300 -- Iteration 79569 - Batch 603/963 - Train loss: 0.26672526  - Train acc: 0.9000 - Val loss: 1.00397301\n",
      "(16.77 min) Epoch 83/300 -- Iteration 79578 - Batch 612/963 - Train loss: 0.26680812  - Train acc: 0.9001 - Val loss: 1.00397301\n",
      "(16.77 min) Epoch 83/300 -- Iteration 79587 - Batch 621/963 - Train loss: 0.26690109  - Train acc: 0.9000 - Val loss: 1.00397301\n",
      "(16.77 min) Epoch 83/300 -- Iteration 79596 - Batch 630/963 - Train loss: 0.26691082  - Train acc: 0.9001 - Val loss: 1.00397301\n",
      "(16.78 min) Epoch 83/300 -- Iteration 79605 - Batch 639/963 - Train loss: 0.26665621  - Train acc: 0.9002 - Val loss: 1.00397301\n",
      "(16.78 min) Epoch 83/300 -- Iteration 79614 - Batch 648/963 - Train loss: 0.26602384  - Train acc: 0.9004 - Val loss: 1.00397301\n",
      "(16.78 min) Epoch 83/300 -- Iteration 79623 - Batch 657/963 - Train loss: 0.26609503  - Train acc: 0.9004 - Val loss: 1.00397301\n",
      "(16.78 min) Epoch 83/300 -- Iteration 79632 - Batch 666/963 - Train loss: 0.26590734  - Train acc: 0.9004 - Val loss: 1.00397301\n",
      "(16.78 min) Epoch 83/300 -- Iteration 79641 - Batch 675/963 - Train loss: 0.26619021  - Train acc: 0.9004 - Val loss: 1.00397301\n",
      "(16.78 min) Epoch 83/300 -- Iteration 79650 - Batch 684/963 - Train loss: 0.26630592  - Train acc: 0.9004 - Val loss: 1.00397301\n",
      "(16.79 min) Epoch 83/300 -- Iteration 79659 - Batch 693/963 - Train loss: 0.26602443  - Train acc: 0.9006 - Val loss: 1.00397301\n",
      "(16.79 min) Epoch 83/300 -- Iteration 79668 - Batch 702/963 - Train loss: 0.26603456  - Train acc: 0.9005 - Val loss: 1.00397301\n",
      "(16.79 min) Epoch 83/300 -- Iteration 79677 - Batch 711/963 - Train loss: 0.26610735  - Train acc: 0.9006 - Val loss: 1.00397301\n",
      "(16.79 min) Epoch 83/300 -- Iteration 79686 - Batch 720/963 - Train loss: 0.26632889  - Train acc: 0.9005 - Val loss: 1.00397301\n",
      "(16.79 min) Epoch 83/300 -- Iteration 79695 - Batch 729/963 - Train loss: 0.26674573  - Train acc: 0.9003 - Val loss: 1.00397301\n",
      "(16.80 min) Epoch 83/300 -- Iteration 79704 - Batch 738/963 - Train loss: 0.26672137  - Train acc: 0.9004 - Val loss: 1.00397301\n",
      "(16.80 min) Epoch 83/300 -- Iteration 79713 - Batch 747/963 - Train loss: 0.26638586  - Train acc: 0.9005 - Val loss: 1.00397301\n",
      "(16.80 min) Epoch 83/300 -- Iteration 79722 - Batch 756/963 - Train loss: 0.26617873  - Train acc: 0.9006 - Val loss: 1.00397301\n",
      "(16.80 min) Epoch 83/300 -- Iteration 79731 - Batch 765/963 - Train loss: 0.26617982  - Train acc: 0.9006 - Val loss: 1.00397301\n",
      "(16.80 min) Epoch 83/300 -- Iteration 79740 - Batch 774/963 - Train loss: 0.26609887  - Train acc: 0.9008 - Val loss: 1.00397301\n",
      "(16.81 min) Epoch 83/300 -- Iteration 79749 - Batch 783/963 - Train loss: 0.26610433  - Train acc: 0.9008 - Val loss: 1.00397301\n",
      "(16.81 min) Epoch 83/300 -- Iteration 79758 - Batch 792/963 - Train loss: 0.26611686  - Train acc: 0.9009 - Val loss: 1.00397301\n",
      "(16.81 min) Epoch 83/300 -- Iteration 79767 - Batch 801/963 - Train loss: 0.26626748  - Train acc: 0.9008 - Val loss: 1.00397301\n",
      "(16.81 min) Epoch 83/300 -- Iteration 79776 - Batch 810/963 - Train loss: 0.26607968  - Train acc: 0.9009 - Val loss: 1.00397301\n",
      "(16.81 min) Epoch 83/300 -- Iteration 79785 - Batch 819/963 - Train loss: 0.26586979  - Train acc: 0.9010 - Val loss: 1.00397301\n",
      "(16.81 min) Epoch 83/300 -- Iteration 79794 - Batch 828/963 - Train loss: 0.26574831  - Train acc: 0.9010 - Val loss: 1.00397301\n",
      "(16.82 min) Epoch 83/300 -- Iteration 79803 - Batch 837/963 - Train loss: 0.26588830  - Train acc: 0.9009 - Val loss: 1.00397301\n",
      "(16.82 min) Epoch 83/300 -- Iteration 79812 - Batch 846/963 - Train loss: 0.26574717  - Train acc: 0.9009 - Val loss: 1.00397301\n",
      "(16.82 min) Epoch 83/300 -- Iteration 79821 - Batch 855/963 - Train loss: 0.26558627  - Train acc: 0.9010 - Val loss: 1.00397301\n",
      "(16.82 min) Epoch 83/300 -- Iteration 79830 - Batch 864/963 - Train loss: 0.26552698  - Train acc: 0.9009 - Val loss: 1.00397301\n",
      "(16.82 min) Epoch 83/300 -- Iteration 79839 - Batch 873/963 - Train loss: 0.26549326  - Train acc: 0.9010 - Val loss: 1.00397301\n",
      "(16.83 min) Epoch 83/300 -- Iteration 79848 - Batch 882/963 - Train loss: 0.26578070  - Train acc: 0.9008 - Val loss: 1.00397301\n",
      "(16.83 min) Epoch 83/300 -- Iteration 79857 - Batch 891/963 - Train loss: 0.26561031  - Train acc: 0.9009 - Val loss: 1.00397301\n",
      "(16.83 min) Epoch 83/300 -- Iteration 79866 - Batch 900/963 - Train loss: 0.26559450  - Train acc: 0.9009 - Val loss: 1.00397301\n",
      "(16.83 min) Epoch 83/300 -- Iteration 79875 - Batch 909/963 - Train loss: 0.26531748  - Train acc: 0.9010 - Val loss: 1.00397301\n",
      "(16.83 min) Epoch 83/300 -- Iteration 79884 - Batch 918/963 - Train loss: 0.26602835  - Train acc: 0.9007 - Val loss: 1.00397301\n",
      "(16.84 min) Epoch 83/300 -- Iteration 79893 - Batch 927/963 - Train loss: 0.26642448  - Train acc: 0.9004 - Val loss: 1.00397301\n",
      "(16.84 min) Epoch 83/300 -- Iteration 79902 - Batch 936/963 - Train loss: 0.26654881  - Train acc: 0.9003 - Val loss: 1.00397301\n",
      "(16.84 min) Epoch 83/300 -- Iteration 79911 - Batch 945/963 - Train loss: 0.26623722  - Train acc: 0.9005 - Val loss: 1.00397301\n",
      "(16.84 min) Epoch 83/300 -- Iteration 79920 - Batch 954/963 - Train loss: 0.26605215  - Train acc: 0.9005 - Val loss: 1.00397301\n",
      "(16.84 min) Epoch 83/300 -- Iteration 79929 - Batch 962/963 - Train loss: 0.26583137  - Train acc: 0.9006 - Val loss: 0.99068105 - Val acc: 0.5933\n",
      "(16.85 min) Epoch 84/300 -- Iteration 79938 - Batch 9/963 - Train loss: 0.25373190  - Train acc: 0.9062 - Val loss: 0.99068105\n",
      "(16.85 min) Epoch 84/300 -- Iteration 79947 - Batch 18/963 - Train loss: 0.25903683  - Train acc: 0.9038 - Val loss: 0.99068105\n",
      "(16.85 min) Epoch 84/300 -- Iteration 79956 - Batch 27/963 - Train loss: 0.24037774  - Train acc: 0.9113 - Val loss: 0.99068105\n",
      "(16.85 min) Epoch 84/300 -- Iteration 79965 - Batch 36/963 - Train loss: 0.24515482  - Train acc: 0.9077 - Val loss: 0.99068105\n",
      "(16.85 min) Epoch 84/300 -- Iteration 79974 - Batch 45/963 - Train loss: 0.25085431  - Train acc: 0.9044 - Val loss: 0.99068105\n",
      "(16.85 min) Epoch 84/300 -- Iteration 79983 - Batch 54/963 - Train loss: 0.25231512  - Train acc: 0.9054 - Val loss: 0.99068105\n",
      "(16.86 min) Epoch 84/300 -- Iteration 79992 - Batch 63/963 - Train loss: 0.25596738  - Train acc: 0.9037 - Val loss: 0.99068105\n",
      "(16.86 min) Epoch 84/300 -- Iteration 80001 - Batch 72/963 - Train loss: 0.25551158  - Train acc: 0.9045 - Val loss: 0.99068105\n",
      "(16.86 min) Epoch 84/300 -- Iteration 80010 - Batch 81/963 - Train loss: 0.25591657  - Train acc: 0.9042 - Val loss: 0.99068105\n",
      "(16.86 min) Epoch 84/300 -- Iteration 80019 - Batch 90/963 - Train loss: 0.25574122  - Train acc: 0.9038 - Val loss: 0.99068105\n",
      "(16.86 min) Epoch 84/300 -- Iteration 80028 - Batch 99/963 - Train loss: 0.25638976  - Train acc: 0.9041 - Val loss: 0.99068105\n",
      "(16.87 min) Epoch 84/300 -- Iteration 80037 - Batch 108/963 - Train loss: 0.25739493  - Train acc: 0.9036 - Val loss: 0.99068105\n",
      "(16.87 min) Epoch 84/300 -- Iteration 80046 - Batch 117/963 - Train loss: 0.26097865  - Train acc: 0.9019 - Val loss: 0.99068105\n",
      "(16.87 min) Epoch 84/300 -- Iteration 80055 - Batch 126/963 - Train loss: 0.26109684  - Train acc: 0.9021 - Val loss: 0.99068105\n",
      "(16.87 min) Epoch 84/300 -- Iteration 80064 - Batch 135/963 - Train loss: 0.26292752  - Train acc: 0.9017 - Val loss: 0.99068105\n",
      "(16.87 min) Epoch 84/300 -- Iteration 80073 - Batch 144/963 - Train loss: 0.26250403  - Train acc: 0.9012 - Val loss: 0.99068105\n",
      "(16.88 min) Epoch 84/300 -- Iteration 80082 - Batch 153/963 - Train loss: 0.26249210  - Train acc: 0.9020 - Val loss: 0.99068105\n",
      "(16.88 min) Epoch 84/300 -- Iteration 80091 - Batch 162/963 - Train loss: 0.26182218  - Train acc: 0.9024 - Val loss: 0.99068105\n",
      "(16.88 min) Epoch 84/300 -- Iteration 80100 - Batch 171/963 - Train loss: 0.26184187  - Train acc: 0.9023 - Val loss: 0.99068105\n",
      "(16.88 min) Epoch 84/300 -- Iteration 80109 - Batch 180/963 - Train loss: 0.26251462  - Train acc: 0.9018 - Val loss: 0.99068105\n",
      "(16.88 min) Epoch 84/300 -- Iteration 80118 - Batch 189/963 - Train loss: 0.26271718  - Train acc: 0.9020 - Val loss: 0.99068105\n",
      "(16.88 min) Epoch 84/300 -- Iteration 80127 - Batch 198/963 - Train loss: 0.26232530  - Train acc: 0.9022 - Val loss: 0.99068105\n",
      "(16.89 min) Epoch 84/300 -- Iteration 80136 - Batch 207/963 - Train loss: 0.26365117  - Train acc: 0.9011 - Val loss: 0.99068105\n",
      "(16.89 min) Epoch 84/300 -- Iteration 80145 - Batch 216/963 - Train loss: 0.26346715  - Train acc: 0.9011 - Val loss: 0.99068105\n",
      "(16.89 min) Epoch 84/300 -- Iteration 80154 - Batch 225/963 - Train loss: 0.26286837  - Train acc: 0.9013 - Val loss: 0.99068105\n",
      "(16.89 min) Epoch 84/300 -- Iteration 80163 - Batch 234/963 - Train loss: 0.26318253  - Train acc: 0.9013 - Val loss: 0.99068105\n",
      "(16.89 min) Epoch 84/300 -- Iteration 80172 - Batch 243/963 - Train loss: 0.26414870  - Train acc: 0.9007 - Val loss: 0.99068105\n",
      "(16.90 min) Epoch 84/300 -- Iteration 80181 - Batch 252/963 - Train loss: 0.26522083  - Train acc: 0.9006 - Val loss: 0.99068105\n",
      "(16.90 min) Epoch 84/300 -- Iteration 80190 - Batch 261/963 - Train loss: 0.26576308  - Train acc: 0.9004 - Val loss: 0.99068105\n",
      "(16.90 min) Epoch 84/300 -- Iteration 80199 - Batch 270/963 - Train loss: 0.26521835  - Train acc: 0.9006 - Val loss: 0.99068105\n",
      "(16.90 min) Epoch 84/300 -- Iteration 80208 - Batch 279/963 - Train loss: 0.26451200  - Train acc: 0.9008 - Val loss: 0.99068105\n",
      "(16.90 min) Epoch 84/300 -- Iteration 80217 - Batch 288/963 - Train loss: 0.26518239  - Train acc: 0.9004 - Val loss: 0.99068105\n",
      "(16.91 min) Epoch 84/300 -- Iteration 80226 - Batch 297/963 - Train loss: 0.26464529  - Train acc: 0.9006 - Val loss: 0.99068105\n",
      "(16.91 min) Epoch 84/300 -- Iteration 80235 - Batch 306/963 - Train loss: 0.26489074  - Train acc: 0.9007 - Val loss: 0.99068105\n",
      "(16.91 min) Epoch 84/300 -- Iteration 80244 - Batch 315/963 - Train loss: 0.26583185  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(16.91 min) Epoch 84/300 -- Iteration 80253 - Batch 324/963 - Train loss: 0.26537073  - Train acc: 0.9006 - Val loss: 0.99068105\n",
      "(16.91 min) Epoch 84/300 -- Iteration 80262 - Batch 333/963 - Train loss: 0.26563786  - Train acc: 0.9005 - Val loss: 0.99068105\n",
      "(16.92 min) Epoch 84/300 -- Iteration 80271 - Batch 342/963 - Train loss: 0.26564485  - Train acc: 0.9006 - Val loss: 0.99068105\n",
      "(16.92 min) Epoch 84/300 -- Iteration 80280 - Batch 351/963 - Train loss: 0.26580890  - Train acc: 0.9004 - Val loss: 0.99068105\n",
      "(16.92 min) Epoch 84/300 -- Iteration 80289 - Batch 360/963 - Train loss: 0.26592851  - Train acc: 0.9004 - Val loss: 0.99068105\n",
      "(16.92 min) Epoch 84/300 -- Iteration 80298 - Batch 369/963 - Train loss: 0.26614678  - Train acc: 0.9006 - Val loss: 0.99068105\n",
      "(16.92 min) Epoch 84/300 -- Iteration 80307 - Batch 378/963 - Train loss: 0.26628258  - Train acc: 0.9004 - Val loss: 0.99068105\n",
      "(16.93 min) Epoch 84/300 -- Iteration 80316 - Batch 387/963 - Train loss: 0.26639321  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(16.93 min) Epoch 84/300 -- Iteration 80325 - Batch 396/963 - Train loss: 0.26631429  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(16.93 min) Epoch 84/300 -- Iteration 80334 - Batch 405/963 - Train loss: 0.26650782  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(16.93 min) Epoch 84/300 -- Iteration 80343 - Batch 414/963 - Train loss: 0.26663700  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(16.93 min) Epoch 84/300 -- Iteration 80352 - Batch 423/963 - Train loss: 0.26697642  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(16.93 min) Epoch 84/300 -- Iteration 80361 - Batch 432/963 - Train loss: 0.26667718  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(16.94 min) Epoch 84/300 -- Iteration 80370 - Batch 441/963 - Train loss: 0.26654146  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(16.94 min) Epoch 84/300 -- Iteration 80379 - Batch 450/963 - Train loss: 0.26624927  - Train acc: 0.9005 - Val loss: 0.99068105\n",
      "(16.94 min) Epoch 84/300 -- Iteration 80388 - Batch 459/963 - Train loss: 0.26614590  - Train acc: 0.9005 - Val loss: 0.99068105\n",
      "(16.94 min) Epoch 84/300 -- Iteration 80397 - Batch 468/963 - Train loss: 0.26607303  - Train acc: 0.9004 - Val loss: 0.99068105\n",
      "(16.94 min) Epoch 84/300 -- Iteration 80406 - Batch 477/963 - Train loss: 0.26642743  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(16.95 min) Epoch 84/300 -- Iteration 80415 - Batch 486/963 - Train loss: 0.26665598  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(16.95 min) Epoch 84/300 -- Iteration 80424 - Batch 495/963 - Train loss: 0.26597439  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(16.95 min) Epoch 84/300 -- Iteration 80433 - Batch 504/963 - Train loss: 0.26602041  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(16.95 min) Epoch 84/300 -- Iteration 80442 - Batch 513/963 - Train loss: 0.26585093  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(16.95 min) Epoch 84/300 -- Iteration 80451 - Batch 522/963 - Train loss: 0.26570388  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(16.96 min) Epoch 84/300 -- Iteration 80460 - Batch 531/963 - Train loss: 0.26623681  - Train acc: 0.8999 - Val loss: 0.99068105\n",
      "(16.96 min) Epoch 84/300 -- Iteration 80469 - Batch 540/963 - Train loss: 0.26634812  - Train acc: 0.8999 - Val loss: 0.99068105\n",
      "(16.96 min) Epoch 84/300 -- Iteration 80478 - Batch 549/963 - Train loss: 0.26618271  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(16.96 min) Epoch 84/300 -- Iteration 80487 - Batch 558/963 - Train loss: 0.26601632  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(16.96 min) Epoch 84/300 -- Iteration 80496 - Batch 567/963 - Train loss: 0.26598908  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(16.97 min) Epoch 84/300 -- Iteration 80505 - Batch 576/963 - Train loss: 0.26596105  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(16.97 min) Epoch 84/300 -- Iteration 80514 - Batch 585/963 - Train loss: 0.26594138  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(16.97 min) Epoch 84/300 -- Iteration 80523 - Batch 594/963 - Train loss: 0.26603020  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(16.97 min) Epoch 84/300 -- Iteration 80532 - Batch 603/963 - Train loss: 0.26610445  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(16.97 min) Epoch 84/300 -- Iteration 80541 - Batch 612/963 - Train loss: 0.26589855  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(16.97 min) Epoch 84/300 -- Iteration 80550 - Batch 621/963 - Train loss: 0.26551284  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(16.98 min) Epoch 84/300 -- Iteration 80559 - Batch 630/963 - Train loss: 0.26574936  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(16.98 min) Epoch 84/300 -- Iteration 80568 - Batch 639/963 - Train loss: 0.26562469  - Train acc: 0.9004 - Val loss: 0.99068105\n",
      "(16.98 min) Epoch 84/300 -- Iteration 80577 - Batch 648/963 - Train loss: 0.26582565  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(16.98 min) Epoch 84/300 -- Iteration 80586 - Batch 657/963 - Train loss: 0.26573540  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(16.98 min) Epoch 84/300 -- Iteration 80595 - Batch 666/963 - Train loss: 0.26577968  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(16.99 min) Epoch 84/300 -- Iteration 80604 - Batch 675/963 - Train loss: 0.26571692  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(16.99 min) Epoch 84/300 -- Iteration 80613 - Batch 684/963 - Train loss: 0.26572656  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(16.99 min) Epoch 84/300 -- Iteration 80622 - Batch 693/963 - Train loss: 0.26589969  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(16.99 min) Epoch 84/300 -- Iteration 80631 - Batch 702/963 - Train loss: 0.26629651  - Train acc: 0.8998 - Val loss: 0.99068105\n",
      "(16.99 min) Epoch 84/300 -- Iteration 80640 - Batch 711/963 - Train loss: 0.26614096  - Train acc: 0.8999 - Val loss: 0.99068105\n",
      "(17.00 min) Epoch 84/300 -- Iteration 80649 - Batch 720/963 - Train loss: 0.26607021  - Train acc: 0.8998 - Val loss: 0.99068105\n",
      "(17.00 min) Epoch 84/300 -- Iteration 80658 - Batch 729/963 - Train loss: 0.26620625  - Train acc: 0.8998 - Val loss: 0.99068105\n",
      "(17.00 min) Epoch 84/300 -- Iteration 80667 - Batch 738/963 - Train loss: 0.26618462  - Train acc: 0.8997 - Val loss: 0.99068105\n",
      "(17.00 min) Epoch 84/300 -- Iteration 80676 - Batch 747/963 - Train loss: 0.26614460  - Train acc: 0.8998 - Val loss: 0.99068105\n",
      "(17.00 min) Epoch 84/300 -- Iteration 80685 - Batch 756/963 - Train loss: 0.26619286  - Train acc: 0.8998 - Val loss: 0.99068105\n",
      "(17.01 min) Epoch 84/300 -- Iteration 80694 - Batch 765/963 - Train loss: 0.26599873  - Train acc: 0.8999 - Val loss: 0.99068105\n",
      "(17.01 min) Epoch 84/300 -- Iteration 80703 - Batch 774/963 - Train loss: 0.26567226  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(17.01 min) Epoch 84/300 -- Iteration 80712 - Batch 783/963 - Train loss: 0.26557817  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(17.01 min) Epoch 84/300 -- Iteration 80721 - Batch 792/963 - Train loss: 0.26568652  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(17.01 min) Epoch 84/300 -- Iteration 80730 - Batch 801/963 - Train loss: 0.26594466  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(17.02 min) Epoch 84/300 -- Iteration 80739 - Batch 810/963 - Train loss: 0.26570328  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(17.02 min) Epoch 84/300 -- Iteration 80748 - Batch 819/963 - Train loss: 0.26543032  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(17.02 min) Epoch 84/300 -- Iteration 80757 - Batch 828/963 - Train loss: 0.26561186  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(17.02 min) Epoch 84/300 -- Iteration 80766 - Batch 837/963 - Train loss: 0.26553368  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(17.02 min) Epoch 84/300 -- Iteration 80775 - Batch 846/963 - Train loss: 0.26565073  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(17.02 min) Epoch 84/300 -- Iteration 80784 - Batch 855/963 - Train loss: 0.26580810  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(17.03 min) Epoch 84/300 -- Iteration 80793 - Batch 864/963 - Train loss: 0.26563420  - Train acc: 0.9001 - Val loss: 0.99068105\n",
      "(17.03 min) Epoch 84/300 -- Iteration 80802 - Batch 873/963 - Train loss: 0.26573392  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(17.03 min) Epoch 84/300 -- Iteration 80811 - Batch 882/963 - Train loss: 0.26569015  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(17.03 min) Epoch 84/300 -- Iteration 80820 - Batch 891/963 - Train loss: 0.26571315  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(17.03 min) Epoch 84/300 -- Iteration 80829 - Batch 900/963 - Train loss: 0.26597726  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(17.04 min) Epoch 84/300 -- Iteration 80838 - Batch 909/963 - Train loss: 0.26595748  - Train acc: 0.9000 - Val loss: 0.99068105\n",
      "(17.04 min) Epoch 84/300 -- Iteration 80847 - Batch 918/963 - Train loss: 0.26577998  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(17.04 min) Epoch 84/300 -- Iteration 80856 - Batch 927/963 - Train loss: 0.26568313  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(17.04 min) Epoch 84/300 -- Iteration 80865 - Batch 936/963 - Train loss: 0.26570398  - Train acc: 0.9002 - Val loss: 0.99068105\n",
      "(17.04 min) Epoch 84/300 -- Iteration 80874 - Batch 945/963 - Train loss: 0.26541339  - Train acc: 0.9003 - Val loss: 0.99068105\n",
      "(17.05 min) Epoch 84/300 -- Iteration 80883 - Batch 954/963 - Train loss: 0.26515330  - Train acc: 0.9004 - Val loss: 0.99068105\n",
      "(17.05 min) Epoch 84/300 -- Iteration 80892 - Batch 962/963 - Train loss: 0.26506017  - Train acc: 0.9003 - Val loss: 0.99161416 - Val acc: 0.5917\n",
      "(17.05 min) Epoch 85/300 -- Iteration 80901 - Batch 9/963 - Train loss: 0.24477915  - Train acc: 0.9180 - Val loss: 0.99161416\n",
      "(17.05 min) Epoch 85/300 -- Iteration 80910 - Batch 18/963 - Train loss: 0.25148438  - Train acc: 0.9112 - Val loss: 0.99161416\n",
      "(17.05 min) Epoch 85/300 -- Iteration 80919 - Batch 27/963 - Train loss: 0.25593111  - Train acc: 0.9074 - Val loss: 0.99161416\n",
      "(17.06 min) Epoch 85/300 -- Iteration 80928 - Batch 36/963 - Train loss: 0.24891338  - Train acc: 0.9094 - Val loss: 0.99161416\n",
      "(17.06 min) Epoch 85/300 -- Iteration 80937 - Batch 45/963 - Train loss: 0.24486502  - Train acc: 0.9107 - Val loss: 0.99161416\n",
      "(17.06 min) Epoch 85/300 -- Iteration 80946 - Batch 54/963 - Train loss: 0.24960905  - Train acc: 0.9075 - Val loss: 0.99161416\n",
      "(17.06 min) Epoch 85/300 -- Iteration 80955 - Batch 63/963 - Train loss: 0.25284221  - Train acc: 0.9059 - Val loss: 0.99161416\n",
      "(17.06 min) Epoch 85/300 -- Iteration 80964 - Batch 72/963 - Train loss: 0.25686607  - Train acc: 0.9040 - Val loss: 0.99161416\n",
      "(17.06 min) Epoch 85/300 -- Iteration 80973 - Batch 81/963 - Train loss: 0.25830265  - Train acc: 0.9029 - Val loss: 0.99161416\n",
      "(17.07 min) Epoch 85/300 -- Iteration 80982 - Batch 90/963 - Train loss: 0.26062289  - Train acc: 0.9022 - Val loss: 0.99161416\n",
      "(17.07 min) Epoch 85/300 -- Iteration 80991 - Batch 99/963 - Train loss: 0.26253684  - Train acc: 0.9015 - Val loss: 0.99161416\n",
      "(17.07 min) Epoch 85/300 -- Iteration 81000 - Batch 108/963 - Train loss: 0.26257288  - Train acc: 0.9012 - Val loss: 0.99161416\n",
      "(17.07 min) Epoch 85/300 -- Iteration 81009 - Batch 117/963 - Train loss: 0.26359826  - Train acc: 0.9012 - Val loss: 0.99161416\n",
      "(17.07 min) Epoch 85/300 -- Iteration 81018 - Batch 126/963 - Train loss: 0.26159333  - Train acc: 0.9020 - Val loss: 0.99161416\n",
      "(17.08 min) Epoch 85/300 -- Iteration 81027 - Batch 135/963 - Train loss: 0.26144380  - Train acc: 0.9026 - Val loss: 0.99161416\n",
      "(17.08 min) Epoch 85/300 -- Iteration 81036 - Batch 144/963 - Train loss: 0.26298047  - Train acc: 0.9020 - Val loss: 0.99161416\n",
      "(17.08 min) Epoch 85/300 -- Iteration 81045 - Batch 153/963 - Train loss: 0.26266842  - Train acc: 0.9022 - Val loss: 0.99161416\n",
      "(17.08 min) Epoch 85/300 -- Iteration 81054 - Batch 162/963 - Train loss: 0.26288985  - Train acc: 0.9020 - Val loss: 0.99161416\n",
      "(17.08 min) Epoch 85/300 -- Iteration 81063 - Batch 171/963 - Train loss: 0.26258784  - Train acc: 0.9021 - Val loss: 0.99161416\n",
      "(17.09 min) Epoch 85/300 -- Iteration 81072 - Batch 180/963 - Train loss: 0.26414995  - Train acc: 0.9016 - Val loss: 0.99161416\n",
      "(17.09 min) Epoch 85/300 -- Iteration 81081 - Batch 189/963 - Train loss: 0.26314238  - Train acc: 0.9023 - Val loss: 0.99161416\n",
      "(17.09 min) Epoch 85/300 -- Iteration 81090 - Batch 198/963 - Train loss: 0.26309141  - Train acc: 0.9019 - Val loss: 0.99161416\n",
      "(17.09 min) Epoch 85/300 -- Iteration 81099 - Batch 207/963 - Train loss: 0.26355104  - Train acc: 0.9017 - Val loss: 0.99161416\n",
      "(17.09 min) Epoch 85/300 -- Iteration 81108 - Batch 216/963 - Train loss: 0.26285962  - Train acc: 0.9023 - Val loss: 0.99161416\n",
      "(17.09 min) Epoch 85/300 -- Iteration 81117 - Batch 225/963 - Train loss: 0.26161509  - Train acc: 0.9028 - Val loss: 0.99161416\n",
      "(17.10 min) Epoch 85/300 -- Iteration 81126 - Batch 234/963 - Train loss: 0.26084911  - Train acc: 0.9031 - Val loss: 0.99161416\n",
      "(17.10 min) Epoch 85/300 -- Iteration 81135 - Batch 243/963 - Train loss: 0.26287492  - Train acc: 0.9022 - Val loss: 0.99161416\n",
      "(17.10 min) Epoch 85/300 -- Iteration 81144 - Batch 252/963 - Train loss: 0.26324712  - Train acc: 0.9021 - Val loss: 0.99161416\n",
      "(17.10 min) Epoch 85/300 -- Iteration 81153 - Batch 261/963 - Train loss: 0.26287881  - Train acc: 0.9026 - Val loss: 0.99161416\n",
      "(17.10 min) Epoch 85/300 -- Iteration 81162 - Batch 270/963 - Train loss: 0.26371422  - Train acc: 0.9022 - Val loss: 0.99161416\n",
      "(17.11 min) Epoch 85/300 -- Iteration 81171 - Batch 279/963 - Train loss: 0.26452775  - Train acc: 0.9019 - Val loss: 0.99161416\n",
      "(17.11 min) Epoch 85/300 -- Iteration 81180 - Batch 288/963 - Train loss: 0.26462977  - Train acc: 0.9020 - Val loss: 0.99161416\n",
      "(17.11 min) Epoch 85/300 -- Iteration 81189 - Batch 297/963 - Train loss: 0.26429545  - Train acc: 0.9019 - Val loss: 0.99161416\n",
      "(17.11 min) Epoch 85/300 -- Iteration 81198 - Batch 306/963 - Train loss: 0.26458560  - Train acc: 0.9014 - Val loss: 0.99161416\n",
      "(17.11 min) Epoch 85/300 -- Iteration 81207 - Batch 315/963 - Train loss: 0.26462237  - Train acc: 0.9015 - Val loss: 0.99161416\n",
      "(17.12 min) Epoch 85/300 -- Iteration 81216 - Batch 324/963 - Train loss: 0.26435103  - Train acc: 0.9016 - Val loss: 0.99161416\n",
      "(17.12 min) Epoch 85/300 -- Iteration 81225 - Batch 333/963 - Train loss: 0.26595591  - Train acc: 0.9008 - Val loss: 0.99161416\n",
      "(17.12 min) Epoch 85/300 -- Iteration 81234 - Batch 342/963 - Train loss: 0.26611410  - Train acc: 0.9006 - Val loss: 0.99161416\n",
      "(17.12 min) Epoch 85/300 -- Iteration 81243 - Batch 351/963 - Train loss: 0.26542181  - Train acc: 0.9011 - Val loss: 0.99161416\n",
      "(17.12 min) Epoch 85/300 -- Iteration 81252 - Batch 360/963 - Train loss: 0.26520879  - Train acc: 0.9011 - Val loss: 0.99161416\n",
      "(17.13 min) Epoch 85/300 -- Iteration 81261 - Batch 369/963 - Train loss: 0.26581214  - Train acc: 0.9007 - Val loss: 0.99161416\n",
      "(17.13 min) Epoch 85/300 -- Iteration 81270 - Batch 378/963 - Train loss: 0.26588161  - Train acc: 0.9007 - Val loss: 0.99161416\n",
      "(17.13 min) Epoch 85/300 -- Iteration 81279 - Batch 387/963 - Train loss: 0.26557949  - Train acc: 0.9009 - Val loss: 0.99161416\n",
      "(17.13 min) Epoch 85/300 -- Iteration 81288 - Batch 396/963 - Train loss: 0.26614972  - Train acc: 0.9004 - Val loss: 0.99161416\n",
      "(17.13 min) Epoch 85/300 -- Iteration 81297 - Batch 405/963 - Train loss: 0.26640613  - Train acc: 0.9002 - Val loss: 0.99161416\n",
      "(17.13 min) Epoch 85/300 -- Iteration 81306 - Batch 414/963 - Train loss: 0.26661812  - Train acc: 0.9001 - Val loss: 0.99161416\n",
      "(17.14 min) Epoch 85/300 -- Iteration 81315 - Batch 423/963 - Train loss: 0.26619252  - Train acc: 0.9002 - Val loss: 0.99161416\n",
      "(17.14 min) Epoch 85/300 -- Iteration 81324 - Batch 432/963 - Train loss: 0.26610808  - Train acc: 0.9003 - Val loss: 0.99161416\n",
      "(17.14 min) Epoch 85/300 -- Iteration 81333 - Batch 441/963 - Train loss: 0.26540801  - Train acc: 0.9005 - Val loss: 0.99161416\n",
      "(17.14 min) Epoch 85/300 -- Iteration 81342 - Batch 450/963 - Train loss: 0.26552613  - Train acc: 0.9004 - Val loss: 0.99161416\n",
      "(17.14 min) Epoch 85/300 -- Iteration 81351 - Batch 459/963 - Train loss: 0.26512754  - Train acc: 0.9006 - Val loss: 0.99161416\n",
      "(17.15 min) Epoch 85/300 -- Iteration 81360 - Batch 468/963 - Train loss: 0.26540687  - Train acc: 0.9003 - Val loss: 0.99161416\n",
      "(17.15 min) Epoch 85/300 -- Iteration 81369 - Batch 477/963 - Train loss: 0.26517845  - Train acc: 0.9002 - Val loss: 0.99161416\n",
      "(17.15 min) Epoch 85/300 -- Iteration 81378 - Batch 486/963 - Train loss: 0.26526156  - Train acc: 0.9003 - Val loss: 0.99161416\n",
      "(17.15 min) Epoch 85/300 -- Iteration 81387 - Batch 495/963 - Train loss: 0.26449681  - Train acc: 0.9006 - Val loss: 0.99161416\n",
      "(17.15 min) Epoch 85/300 -- Iteration 81396 - Batch 504/963 - Train loss: 0.26419829  - Train acc: 0.9008 - Val loss: 0.99161416\n",
      "(17.16 min) Epoch 85/300 -- Iteration 81405 - Batch 513/963 - Train loss: 0.26457555  - Train acc: 0.9008 - Val loss: 0.99161416\n",
      "(17.16 min) Epoch 85/300 -- Iteration 81414 - Batch 522/963 - Train loss: 0.26462617  - Train acc: 0.9008 - Val loss: 0.99161416\n",
      "(17.16 min) Epoch 85/300 -- Iteration 81423 - Batch 531/963 - Train loss: 0.26431232  - Train acc: 0.9009 - Val loss: 0.99161416\n",
      "(17.16 min) Epoch 85/300 -- Iteration 81432 - Batch 540/963 - Train loss: 0.26473502  - Train acc: 0.9006 - Val loss: 0.99161416\n",
      "(17.16 min) Epoch 85/300 -- Iteration 81441 - Batch 549/963 - Train loss: 0.26471345  - Train acc: 0.9007 - Val loss: 0.99161416\n",
      "(17.16 min) Epoch 85/300 -- Iteration 81450 - Batch 558/963 - Train loss: 0.26479898  - Train acc: 0.9005 - Val loss: 0.99161416\n",
      "(17.17 min) Epoch 85/300 -- Iteration 81459 - Batch 567/963 - Train loss: 0.26506997  - Train acc: 0.9003 - Val loss: 0.99161416\n",
      "(17.17 min) Epoch 85/300 -- Iteration 81468 - Batch 576/963 - Train loss: 0.26550810  - Train acc: 0.9001 - Val loss: 0.99161416\n",
      "(17.17 min) Epoch 85/300 -- Iteration 81477 - Batch 585/963 - Train loss: 0.26548365  - Train acc: 0.9002 - Val loss: 0.99161416\n",
      "(17.17 min) Epoch 85/300 -- Iteration 81486 - Batch 594/963 - Train loss: 0.26598001  - Train acc: 0.9000 - Val loss: 0.99161416\n",
      "(17.17 min) Epoch 85/300 -- Iteration 81495 - Batch 603/963 - Train loss: 0.26649427  - Train acc: 0.8999 - Val loss: 0.99161416\n",
      "(17.18 min) Epoch 85/300 -- Iteration 81504 - Batch 612/963 - Train loss: 0.26672727  - Train acc: 0.8998 - Val loss: 0.99161416\n",
      "(17.18 min) Epoch 85/300 -- Iteration 81513 - Batch 621/963 - Train loss: 0.26652241  - Train acc: 0.9001 - Val loss: 0.99161416\n",
      "(17.18 min) Epoch 85/300 -- Iteration 81522 - Batch 630/963 - Train loss: 0.26649519  - Train acc: 0.8999 - Val loss: 0.99161416\n",
      "(17.18 min) Epoch 85/300 -- Iteration 81531 - Batch 639/963 - Train loss: 0.26708767  - Train acc: 0.8996 - Val loss: 0.99161416\n",
      "(17.18 min) Epoch 85/300 -- Iteration 81540 - Batch 648/963 - Train loss: 0.26689266  - Train acc: 0.8997 - Val loss: 0.99161416\n",
      "(17.19 min) Epoch 85/300 -- Iteration 81549 - Batch 657/963 - Train loss: 0.26711389  - Train acc: 0.8996 - Val loss: 0.99161416\n",
      "(17.19 min) Epoch 85/300 -- Iteration 81558 - Batch 666/963 - Train loss: 0.26706358  - Train acc: 0.8996 - Val loss: 0.99161416\n",
      "(17.19 min) Epoch 85/300 -- Iteration 81567 - Batch 675/963 - Train loss: 0.26719893  - Train acc: 0.8995 - Val loss: 0.99161416\n",
      "(17.19 min) Epoch 85/300 -- Iteration 81576 - Batch 684/963 - Train loss: 0.26762935  - Train acc: 0.8992 - Val loss: 0.99161416\n",
      "(17.19 min) Epoch 85/300 -- Iteration 81585 - Batch 693/963 - Train loss: 0.26786444  - Train acc: 0.8992 - Val loss: 0.99161416\n",
      "(17.20 min) Epoch 85/300 -- Iteration 81594 - Batch 702/963 - Train loss: 0.26778625  - Train acc: 0.8992 - Val loss: 0.99161416\n",
      "(17.20 min) Epoch 85/300 -- Iteration 81603 - Batch 711/963 - Train loss: 0.26780013  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.20 min) Epoch 85/300 -- Iteration 81612 - Batch 720/963 - Train loss: 0.26759100  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.20 min) Epoch 85/300 -- Iteration 81621 - Batch 729/963 - Train loss: 0.26746257  - Train acc: 0.8992 - Val loss: 0.99161416\n",
      "(17.20 min) Epoch 85/300 -- Iteration 81630 - Batch 738/963 - Train loss: 0.26760279  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.20 min) Epoch 85/300 -- Iteration 81639 - Batch 747/963 - Train loss: 0.26775045  - Train acc: 0.8988 - Val loss: 0.99161416\n",
      "(17.21 min) Epoch 85/300 -- Iteration 81648 - Batch 756/963 - Train loss: 0.26722961  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.21 min) Epoch 85/300 -- Iteration 81657 - Batch 765/963 - Train loss: 0.26710952  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.21 min) Epoch 85/300 -- Iteration 81666 - Batch 774/963 - Train loss: 0.26728096  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.21 min) Epoch 85/300 -- Iteration 81675 - Batch 783/963 - Train loss: 0.26717488  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.21 min) Epoch 85/300 -- Iteration 81684 - Batch 792/963 - Train loss: 0.26729683  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.22 min) Epoch 85/300 -- Iteration 81693 - Batch 801/963 - Train loss: 0.26732245  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.22 min) Epoch 85/300 -- Iteration 81702 - Batch 810/963 - Train loss: 0.26733567  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.22 min) Epoch 85/300 -- Iteration 81711 - Batch 819/963 - Train loss: 0.26747131  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.22 min) Epoch 85/300 -- Iteration 81720 - Batch 828/963 - Train loss: 0.26746723  - Train acc: 0.8989 - Val loss: 0.99161416\n",
      "(17.22 min) Epoch 85/300 -- Iteration 81729 - Batch 837/963 - Train loss: 0.26751718  - Train acc: 0.8988 - Val loss: 0.99161416\n",
      "(17.23 min) Epoch 85/300 -- Iteration 81738 - Batch 846/963 - Train loss: 0.26736357  - Train acc: 0.8988 - Val loss: 0.99161416\n",
      "(17.23 min) Epoch 85/300 -- Iteration 81747 - Batch 855/963 - Train loss: 0.26733921  - Train acc: 0.8988 - Val loss: 0.99161416\n",
      "(17.23 min) Epoch 85/300 -- Iteration 81756 - Batch 864/963 - Train loss: 0.26702221  - Train acc: 0.8989 - Val loss: 0.99161416\n",
      "(17.23 min) Epoch 85/300 -- Iteration 81765 - Batch 873/963 - Train loss: 0.26674554  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.23 min) Epoch 85/300 -- Iteration 81774 - Batch 882/963 - Train loss: 0.26667244  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.23 min) Epoch 85/300 -- Iteration 81783 - Batch 891/963 - Train loss: 0.26680256  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.24 min) Epoch 85/300 -- Iteration 81792 - Batch 900/963 - Train loss: 0.26657668  - Train acc: 0.8992 - Val loss: 0.99161416\n",
      "(17.24 min) Epoch 85/300 -- Iteration 81801 - Batch 909/963 - Train loss: 0.26686331  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.24 min) Epoch 85/300 -- Iteration 81810 - Batch 918/963 - Train loss: 0.26700955  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.24 min) Epoch 85/300 -- Iteration 81819 - Batch 927/963 - Train loss: 0.26665975  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.24 min) Epoch 85/300 -- Iteration 81828 - Batch 936/963 - Train loss: 0.26672544  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.25 min) Epoch 85/300 -- Iteration 81837 - Batch 945/963 - Train loss: 0.26702203  - Train acc: 0.8990 - Val loss: 0.99161416\n",
      "(17.25 min) Epoch 85/300 -- Iteration 81846 - Batch 954/963 - Train loss: 0.26683764  - Train acc: 0.8991 - Val loss: 0.99161416\n",
      "(17.25 min) Epoch 85/300 -- Iteration 81855 - Batch 962/963 - Train loss: 0.26682926  - Train acc: 0.8991 - Val loss: 1.01005983 - Val acc: 0.5900\n",
      "(17.25 min) Epoch 86/300 -- Iteration 81864 - Batch 9/963 - Train loss: 0.26011088  - Train acc: 0.9102 - Val loss: 1.01005983\n",
      "(17.25 min) Epoch 86/300 -- Iteration 81873 - Batch 18/963 - Train loss: 0.26566394  - Train acc: 0.9042 - Val loss: 1.01005983\n",
      "(17.26 min) Epoch 86/300 -- Iteration 81882 - Batch 27/963 - Train loss: 0.26236352  - Train acc: 0.9029 - Val loss: 1.01005983\n",
      "(17.26 min) Epoch 86/300 -- Iteration 81891 - Batch 36/963 - Train loss: 0.26933563  - Train acc: 0.9014 - Val loss: 1.01005983\n",
      "(17.26 min) Epoch 86/300 -- Iteration 81900 - Batch 45/963 - Train loss: 0.27001602  - Train acc: 0.9003 - Val loss: 1.01005983\n",
      "(17.26 min) Epoch 86/300 -- Iteration 81909 - Batch 54/963 - Train loss: 0.26866194  - Train acc: 0.9010 - Val loss: 1.01005983\n",
      "(17.26 min) Epoch 86/300 -- Iteration 81918 - Batch 63/963 - Train loss: 0.27413851  - Train acc: 0.8989 - Val loss: 1.01005983\n",
      "(17.27 min) Epoch 86/300 -- Iteration 81927 - Batch 72/963 - Train loss: 0.27532891  - Train acc: 0.8980 - Val loss: 1.01005983\n",
      "(17.27 min) Epoch 86/300 -- Iteration 81936 - Batch 81/963 - Train loss: 0.27598151  - Train acc: 0.8980 - Val loss: 1.01005983\n",
      "(17.27 min) Epoch 86/300 -- Iteration 81945 - Batch 90/963 - Train loss: 0.27538849  - Train acc: 0.8981 - Val loss: 1.01005983\n",
      "(17.27 min) Epoch 86/300 -- Iteration 81954 - Batch 99/963 - Train loss: 0.27554889  - Train acc: 0.8980 - Val loss: 1.01005983\n",
      "(17.27 min) Epoch 86/300 -- Iteration 81963 - Batch 108/963 - Train loss: 0.27325606  - Train acc: 0.8987 - Val loss: 1.01005983\n",
      "(17.28 min) Epoch 86/300 -- Iteration 81972 - Batch 117/963 - Train loss: 0.27389356  - Train acc: 0.8980 - Val loss: 1.01005983\n",
      "(17.28 min) Epoch 86/300 -- Iteration 81981 - Batch 126/963 - Train loss: 0.27214911  - Train acc: 0.8989 - Val loss: 1.01005983\n",
      "(17.28 min) Epoch 86/300 -- Iteration 81990 - Batch 135/963 - Train loss: 0.27322216  - Train acc: 0.8982 - Val loss: 1.01005983\n",
      "(17.28 min) Epoch 86/300 -- Iteration 81999 - Batch 144/963 - Train loss: 0.27227052  - Train acc: 0.8986 - Val loss: 1.01005983\n",
      "(17.28 min) Epoch 86/300 -- Iteration 82008 - Batch 153/963 - Train loss: 0.26912152  - Train acc: 0.9001 - Val loss: 1.01005983\n",
      "(17.28 min) Epoch 86/300 -- Iteration 82017 - Batch 162/963 - Train loss: 0.26963709  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.29 min) Epoch 86/300 -- Iteration 82026 - Batch 171/963 - Train loss: 0.26960269  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.29 min) Epoch 86/300 -- Iteration 82035 - Batch 180/963 - Train loss: 0.27018507  - Train acc: 0.8986 - Val loss: 1.01005983\n",
      "(17.29 min) Epoch 86/300 -- Iteration 82044 - Batch 189/963 - Train loss: 0.26936491  - Train acc: 0.8988 - Val loss: 1.01005983\n",
      "(17.29 min) Epoch 86/300 -- Iteration 82053 - Batch 198/963 - Train loss: 0.26852342  - Train acc: 0.8989 - Val loss: 1.01005983\n",
      "(17.29 min) Epoch 86/300 -- Iteration 82062 - Batch 207/963 - Train loss: 0.26873870  - Train acc: 0.8987 - Val loss: 1.01005983\n",
      "(17.30 min) Epoch 86/300 -- Iteration 82071 - Batch 216/963 - Train loss: 0.26943945  - Train acc: 0.8987 - Val loss: 1.01005983\n",
      "(17.30 min) Epoch 86/300 -- Iteration 82080 - Batch 225/963 - Train loss: 0.26912433  - Train acc: 0.8987 - Val loss: 1.01005983\n",
      "(17.30 min) Epoch 86/300 -- Iteration 82089 - Batch 234/963 - Train loss: 0.26797346  - Train acc: 0.8992 - Val loss: 1.01005983\n",
      "(17.30 min) Epoch 86/300 -- Iteration 82098 - Batch 243/963 - Train loss: 0.26654718  - Train acc: 0.8998 - Val loss: 1.01005983\n",
      "(17.30 min) Epoch 86/300 -- Iteration 82107 - Batch 252/963 - Train loss: 0.26632986  - Train acc: 0.8998 - Val loss: 1.01005983\n",
      "(17.31 min) Epoch 86/300 -- Iteration 82116 - Batch 261/963 - Train loss: 0.26564633  - Train acc: 0.9000 - Val loss: 1.01005983\n",
      "(17.31 min) Epoch 86/300 -- Iteration 82125 - Batch 270/963 - Train loss: 0.26516818  - Train acc: 0.9003 - Val loss: 1.01005983\n",
      "(17.31 min) Epoch 86/300 -- Iteration 82134 - Batch 279/963 - Train loss: 0.26604538  - Train acc: 0.8999 - Val loss: 1.01005983\n",
      "(17.31 min) Epoch 86/300 -- Iteration 82143 - Batch 288/963 - Train loss: 0.26515497  - Train acc: 0.9001 - Val loss: 1.01005983\n",
      "(17.31 min) Epoch 86/300 -- Iteration 82152 - Batch 297/963 - Train loss: 0.26500863  - Train acc: 0.9002 - Val loss: 1.01005983\n",
      "(17.31 min) Epoch 86/300 -- Iteration 82161 - Batch 306/963 - Train loss: 0.26499918  - Train acc: 0.9001 - Val loss: 1.01005983\n",
      "(17.32 min) Epoch 86/300 -- Iteration 82170 - Batch 315/963 - Train loss: 0.26534731  - Train acc: 0.9002 - Val loss: 1.01005983\n",
      "(17.32 min) Epoch 86/300 -- Iteration 82179 - Batch 324/963 - Train loss: 0.26545425  - Train acc: 0.9000 - Val loss: 1.01005983\n",
      "(17.32 min) Epoch 86/300 -- Iteration 82188 - Batch 333/963 - Train loss: 0.26537853  - Train acc: 0.8997 - Val loss: 1.01005983\n",
      "(17.32 min) Epoch 86/300 -- Iteration 82197 - Batch 342/963 - Train loss: 0.26548567  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.32 min) Epoch 86/300 -- Iteration 82206 - Batch 351/963 - Train loss: 0.26574905  - Train acc: 0.8993 - Val loss: 1.01005983\n",
      "(17.33 min) Epoch 86/300 -- Iteration 82215 - Batch 360/963 - Train loss: 0.26577133  - Train acc: 0.8992 - Val loss: 1.01005983\n",
      "(17.33 min) Epoch 86/300 -- Iteration 82224 - Batch 369/963 - Train loss: 0.26624559  - Train acc: 0.8990 - Val loss: 1.01005983\n",
      "(17.33 min) Epoch 86/300 -- Iteration 82233 - Batch 378/963 - Train loss: 0.26576810  - Train acc: 0.8993 - Val loss: 1.01005983\n",
      "(17.33 min) Epoch 86/300 -- Iteration 82242 - Batch 387/963 - Train loss: 0.26638596  - Train acc: 0.8990 - Val loss: 1.01005983\n",
      "(17.33 min) Epoch 86/300 -- Iteration 82251 - Batch 396/963 - Train loss: 0.26540721  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.34 min) Epoch 86/300 -- Iteration 82260 - Batch 405/963 - Train loss: 0.26508387  - Train acc: 0.8997 - Val loss: 1.01005983\n",
      "(17.34 min) Epoch 86/300 -- Iteration 82269 - Batch 414/963 - Train loss: 0.26462313  - Train acc: 0.8999 - Val loss: 1.01005983\n",
      "(17.34 min) Epoch 86/300 -- Iteration 82278 - Batch 423/963 - Train loss: 0.26507355  - Train acc: 0.8997 - Val loss: 1.01005983\n",
      "(17.34 min) Epoch 86/300 -- Iteration 82287 - Batch 432/963 - Train loss: 0.26525591  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.34 min) Epoch 86/300 -- Iteration 82296 - Batch 441/963 - Train loss: 0.26546278  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.34 min) Epoch 86/300 -- Iteration 82305 - Batch 450/963 - Train loss: 0.26528568  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.35 min) Epoch 86/300 -- Iteration 82314 - Batch 459/963 - Train loss: 0.26574482  - Train acc: 0.8992 - Val loss: 1.01005983\n",
      "(17.35 min) Epoch 86/300 -- Iteration 82323 - Batch 468/963 - Train loss: 0.26519406  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.35 min) Epoch 86/300 -- Iteration 82332 - Batch 477/963 - Train loss: 0.26511269  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.35 min) Epoch 86/300 -- Iteration 82341 - Batch 486/963 - Train loss: 0.26496538  - Train acc: 0.8998 - Val loss: 1.01005983\n",
      "(17.35 min) Epoch 86/300 -- Iteration 82350 - Batch 495/963 - Train loss: 0.26521162  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.36 min) Epoch 86/300 -- Iteration 82359 - Batch 504/963 - Train loss: 0.26536593  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.36 min) Epoch 86/300 -- Iteration 82368 - Batch 513/963 - Train loss: 0.26534448  - Train acc: 0.8993 - Val loss: 1.01005983\n",
      "(17.36 min) Epoch 86/300 -- Iteration 82377 - Batch 522/963 - Train loss: 0.26507402  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.36 min) Epoch 86/300 -- Iteration 82386 - Batch 531/963 - Train loss: 0.26476174  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.36 min) Epoch 86/300 -- Iteration 82395 - Batch 540/963 - Train loss: 0.26469873  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.37 min) Epoch 86/300 -- Iteration 82404 - Batch 549/963 - Train loss: 0.26485218  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.37 min) Epoch 86/300 -- Iteration 82413 - Batch 558/963 - Train loss: 0.26574925  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.37 min) Epoch 86/300 -- Iteration 82422 - Batch 567/963 - Train loss: 0.26608669  - Train acc: 0.8991 - Val loss: 1.01005983\n",
      "(17.37 min) Epoch 86/300 -- Iteration 82431 - Batch 576/963 - Train loss: 0.26597555  - Train acc: 0.8992 - Val loss: 1.01005983\n",
      "(17.37 min) Epoch 86/300 -- Iteration 82440 - Batch 585/963 - Train loss: 0.26549841  - Train acc: 0.8993 - Val loss: 1.01005983\n",
      "(17.37 min) Epoch 86/300 -- Iteration 82449 - Batch 594/963 - Train loss: 0.26554526  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.38 min) Epoch 86/300 -- Iteration 82458 - Batch 603/963 - Train loss: 0.26532677  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.38 min) Epoch 86/300 -- Iteration 82467 - Batch 612/963 - Train loss: 0.26509724  - Train acc: 0.8997 - Val loss: 1.01005983\n",
      "(17.38 min) Epoch 86/300 -- Iteration 82476 - Batch 621/963 - Train loss: 0.26498032  - Train acc: 0.8998 - Val loss: 1.01005983\n",
      "(17.38 min) Epoch 86/300 -- Iteration 82485 - Batch 630/963 - Train loss: 0.26502529  - Train acc: 0.8998 - Val loss: 1.01005983\n",
      "(17.38 min) Epoch 86/300 -- Iteration 82494 - Batch 639/963 - Train loss: 0.26526206  - Train acc: 0.8997 - Val loss: 1.01005983\n",
      "(17.39 min) Epoch 86/300 -- Iteration 82503 - Batch 648/963 - Train loss: 0.26540506  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.39 min) Epoch 86/300 -- Iteration 82512 - Batch 657/963 - Train loss: 0.26531498  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.39 min) Epoch 86/300 -- Iteration 82521 - Batch 666/963 - Train loss: 0.26541396  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.39 min) Epoch 86/300 -- Iteration 82530 - Batch 675/963 - Train loss: 0.26565777  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.39 min) Epoch 86/300 -- Iteration 82539 - Batch 684/963 - Train loss: 0.26558316  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.40 min) Epoch 86/300 -- Iteration 82548 - Batch 693/963 - Train loss: 0.26539840  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.40 min) Epoch 86/300 -- Iteration 82557 - Batch 702/963 - Train loss: 0.26588325  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.40 min) Epoch 86/300 -- Iteration 82566 - Batch 711/963 - Train loss: 0.26588811  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.40 min) Epoch 86/300 -- Iteration 82575 - Batch 720/963 - Train loss: 0.26564922  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.40 min) Epoch 86/300 -- Iteration 82584 - Batch 729/963 - Train loss: 0.26517724  - Train acc: 0.8998 - Val loss: 1.01005983\n",
      "(17.41 min) Epoch 86/300 -- Iteration 82593 - Batch 738/963 - Train loss: 0.26525307  - Train acc: 0.8998 - Val loss: 1.01005983\n",
      "(17.41 min) Epoch 86/300 -- Iteration 82602 - Batch 747/963 - Train loss: 0.26538268  - Train acc: 0.8997 - Val loss: 1.01005983\n",
      "(17.41 min) Epoch 86/300 -- Iteration 82611 - Batch 756/963 - Train loss: 0.26565522  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.41 min) Epoch 86/300 -- Iteration 82620 - Batch 765/963 - Train loss: 0.26572520  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.41 min) Epoch 86/300 -- Iteration 82629 - Batch 774/963 - Train loss: 0.26606799  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.41 min) Epoch 86/300 -- Iteration 82638 - Batch 783/963 - Train loss: 0.26589342  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.42 min) Epoch 86/300 -- Iteration 82647 - Batch 792/963 - Train loss: 0.26603594  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.42 min) Epoch 86/300 -- Iteration 82656 - Batch 801/963 - Train loss: 0.26581508  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.42 min) Epoch 86/300 -- Iteration 82665 - Batch 810/963 - Train loss: 0.26622636  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.42 min) Epoch 86/300 -- Iteration 82674 - Batch 819/963 - Train loss: 0.26625352  - Train acc: 0.8996 - Val loss: 1.01005983\n",
      "(17.42 min) Epoch 86/300 -- Iteration 82683 - Batch 828/963 - Train loss: 0.26649209  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.43 min) Epoch 86/300 -- Iteration 82692 - Batch 837/963 - Train loss: 0.26637435  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.43 min) Epoch 86/300 -- Iteration 82701 - Batch 846/963 - Train loss: 0.26621442  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.43 min) Epoch 86/300 -- Iteration 82710 - Batch 855/963 - Train loss: 0.26625840  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.43 min) Epoch 86/300 -- Iteration 82719 - Batch 864/963 - Train loss: 0.26617087  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.43 min) Epoch 86/300 -- Iteration 82728 - Batch 873/963 - Train loss: 0.26580840  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.44 min) Epoch 86/300 -- Iteration 82737 - Batch 882/963 - Train loss: 0.26595796  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.44 min) Epoch 86/300 -- Iteration 82746 - Batch 891/963 - Train loss: 0.26602801  - Train acc: 0.8993 - Val loss: 1.01005983\n",
      "(17.44 min) Epoch 86/300 -- Iteration 82755 - Batch 900/963 - Train loss: 0.26566055  - Train acc: 0.8995 - Val loss: 1.01005983\n",
      "(17.44 min) Epoch 86/300 -- Iteration 82764 - Batch 909/963 - Train loss: 0.26562627  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.44 min) Epoch 86/300 -- Iteration 82773 - Batch 918/963 - Train loss: 0.26575562  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.44 min) Epoch 86/300 -- Iteration 82782 - Batch 927/963 - Train loss: 0.26570340  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.45 min) Epoch 86/300 -- Iteration 82791 - Batch 936/963 - Train loss: 0.26568306  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.45 min) Epoch 86/300 -- Iteration 82800 - Batch 945/963 - Train loss: 0.26564912  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.45 min) Epoch 86/300 -- Iteration 82809 - Batch 954/963 - Train loss: 0.26547247  - Train acc: 0.8994 - Val loss: 1.01005983\n",
      "(17.45 min) Epoch 86/300 -- Iteration 82818 - Batch 962/963 - Train loss: 0.26554220  - Train acc: 0.8994 - Val loss: 1.01788461 - Val acc: 0.5850\n",
      "(17.46 min) Epoch 87/300 -- Iteration 82827 - Batch 9/963 - Train loss: 0.28946375  - Train acc: 0.8867 - Val loss: 1.01788461\n",
      "(17.46 min) Epoch 87/300 -- Iteration 82836 - Batch 18/963 - Train loss: 0.27682083  - Train acc: 0.8931 - Val loss: 1.01788461\n",
      "(17.46 min) Epoch 87/300 -- Iteration 82845 - Batch 27/963 - Train loss: 0.27874534  - Train acc: 0.8945 - Val loss: 1.01788461\n",
      "(17.46 min) Epoch 87/300 -- Iteration 82854 - Batch 36/963 - Train loss: 0.27259036  - Train acc: 0.8991 - Val loss: 1.01788461\n",
      "(17.46 min) Epoch 87/300 -- Iteration 82863 - Batch 45/963 - Train loss: 0.26604237  - Train acc: 0.9012 - Val loss: 1.01788461\n",
      "(17.46 min) Epoch 87/300 -- Iteration 82872 - Batch 54/963 - Train loss: 0.26204192  - Train acc: 0.9040 - Val loss: 1.01788461\n",
      "(17.47 min) Epoch 87/300 -- Iteration 82881 - Batch 63/963 - Train loss: 0.26401191  - Train acc: 0.9019 - Val loss: 1.01788461\n",
      "(17.47 min) Epoch 87/300 -- Iteration 82890 - Batch 72/963 - Train loss: 0.26370566  - Train acc: 0.9019 - Val loss: 1.01788461\n",
      "(17.47 min) Epoch 87/300 -- Iteration 82899 - Batch 81/963 - Train loss: 0.26144039  - Train acc: 0.9033 - Val loss: 1.01788461\n",
      "(17.47 min) Epoch 87/300 -- Iteration 82908 - Batch 90/963 - Train loss: 0.26342478  - Train acc: 0.9025 - Val loss: 1.01788461\n",
      "(17.47 min) Epoch 87/300 -- Iteration 82917 - Batch 99/963 - Train loss: 0.26131389  - Train acc: 0.9024 - Val loss: 1.01788461\n",
      "(17.48 min) Epoch 87/300 -- Iteration 82926 - Batch 108/963 - Train loss: 0.26242037  - Train acc: 0.9015 - Val loss: 1.01788461\n",
      "(17.48 min) Epoch 87/300 -- Iteration 82935 - Batch 117/963 - Train loss: 0.26423644  - Train acc: 0.9004 - Val loss: 1.01788461\n",
      "(17.48 min) Epoch 87/300 -- Iteration 82944 - Batch 126/963 - Train loss: 0.26520742  - Train acc: 0.9003 - Val loss: 1.01788461\n",
      "(17.48 min) Epoch 87/300 -- Iteration 82953 - Batch 135/963 - Train loss: 0.26471213  - Train acc: 0.9006 - Val loss: 1.01788461\n",
      "(17.48 min) Epoch 87/300 -- Iteration 82962 - Batch 144/963 - Train loss: 0.26539390  - Train acc: 0.9004 - Val loss: 1.01788461\n",
      "(17.49 min) Epoch 87/300 -- Iteration 82971 - Batch 153/963 - Train loss: 0.26611322  - Train acc: 0.9002 - Val loss: 1.01788461\n",
      "(17.49 min) Epoch 87/300 -- Iteration 82980 - Batch 162/963 - Train loss: 0.26692462  - Train acc: 0.8996 - Val loss: 1.01788461\n",
      "(17.49 min) Epoch 87/300 -- Iteration 82989 - Batch 171/963 - Train loss: 0.26747315  - Train acc: 0.8990 - Val loss: 1.01788461\n",
      "(17.49 min) Epoch 87/300 -- Iteration 82998 - Batch 180/963 - Train loss: 0.26688308  - Train acc: 0.8987 - Val loss: 1.01788461\n",
      "(17.49 min) Epoch 87/300 -- Iteration 83007 - Batch 189/963 - Train loss: 0.26752726  - Train acc: 0.8983 - Val loss: 1.01788461\n",
      "(17.50 min) Epoch 87/300 -- Iteration 83016 - Batch 198/963 - Train loss: 0.26791268  - Train acc: 0.8982 - Val loss: 1.01788461\n",
      "(17.50 min) Epoch 87/300 -- Iteration 83025 - Batch 207/963 - Train loss: 0.26734199  - Train acc: 0.8986 - Val loss: 1.01788461\n",
      "(17.50 min) Epoch 87/300 -- Iteration 83034 - Batch 216/963 - Train loss: 0.26609432  - Train acc: 0.8991 - Val loss: 1.01788461\n",
      "(17.50 min) Epoch 87/300 -- Iteration 83043 - Batch 225/963 - Train loss: 0.26616299  - Train acc: 0.8993 - Val loss: 1.01788461\n",
      "(17.50 min) Epoch 87/300 -- Iteration 83052 - Batch 234/963 - Train loss: 0.26637167  - Train acc: 0.8991 - Val loss: 1.01788461\n",
      "(17.50 min) Epoch 87/300 -- Iteration 83061 - Batch 243/963 - Train loss: 0.26703495  - Train acc: 0.8988 - Val loss: 1.01788461\n",
      "(17.51 min) Epoch 87/300 -- Iteration 83070 - Batch 252/963 - Train loss: 0.26715073  - Train acc: 0.8991 - Val loss: 1.01788461\n",
      "(17.51 min) Epoch 87/300 -- Iteration 83079 - Batch 261/963 - Train loss: 0.26689516  - Train acc: 0.8993 - Val loss: 1.01788461\n",
      "(17.51 min) Epoch 87/300 -- Iteration 83088 - Batch 270/963 - Train loss: 0.26690381  - Train acc: 0.8990 - Val loss: 1.01788461\n",
      "(17.51 min) Epoch 87/300 -- Iteration 83097 - Batch 279/963 - Train loss: 0.26678013  - Train acc: 0.8990 - Val loss: 1.01788461\n",
      "(17.51 min) Epoch 87/300 -- Iteration 83106 - Batch 288/963 - Train loss: 0.26687521  - Train acc: 0.8991 - Val loss: 1.01788461\n",
      "(17.52 min) Epoch 87/300 -- Iteration 83115 - Batch 297/963 - Train loss: 0.26624002  - Train acc: 0.8994 - Val loss: 1.01788461\n",
      "(17.52 min) Epoch 87/300 -- Iteration 83124 - Batch 306/963 - Train loss: 0.26658548  - Train acc: 0.8995 - Val loss: 1.01788461\n",
      "(17.52 min) Epoch 87/300 -- Iteration 83133 - Batch 315/963 - Train loss: 0.26714363  - Train acc: 0.8990 - Val loss: 1.01788461\n",
      "(17.52 min) Epoch 87/300 -- Iteration 83142 - Batch 324/963 - Train loss: 0.26776579  - Train acc: 0.8987 - Val loss: 1.01788461\n",
      "(17.52 min) Epoch 87/300 -- Iteration 83151 - Batch 333/963 - Train loss: 0.26778949  - Train acc: 0.8988 - Val loss: 1.01788461\n",
      "(17.53 min) Epoch 87/300 -- Iteration 83160 - Batch 342/963 - Train loss: 0.26770719  - Train acc: 0.8986 - Val loss: 1.01788461\n",
      "(17.53 min) Epoch 87/300 -- Iteration 83169 - Batch 351/963 - Train loss: 0.26858820  - Train acc: 0.8986 - Val loss: 1.01788461\n",
      "(17.53 min) Epoch 87/300 -- Iteration 83178 - Batch 360/963 - Train loss: 0.26847608  - Train acc: 0.8988 - Val loss: 1.01788461\n",
      "(17.53 min) Epoch 87/300 -- Iteration 83187 - Batch 369/963 - Train loss: 0.26851941  - Train acc: 0.8985 - Val loss: 1.01788461\n",
      "(17.53 min) Epoch 87/300 -- Iteration 83196 - Batch 378/963 - Train loss: 0.26910752  - Train acc: 0.8982 - Val loss: 1.01788461\n",
      "(17.54 min) Epoch 87/300 -- Iteration 83205 - Batch 387/963 - Train loss: 0.26912649  - Train acc: 0.8981 - Val loss: 1.01788461\n",
      "(17.54 min) Epoch 87/300 -- Iteration 83214 - Batch 396/963 - Train loss: 0.26885714  - Train acc: 0.8982 - Val loss: 1.01788461\n",
      "(17.54 min) Epoch 87/300 -- Iteration 83223 - Batch 405/963 - Train loss: 0.26888287  - Train acc: 0.8982 - Val loss: 1.01788461\n",
      "(17.54 min) Epoch 87/300 -- Iteration 83232 - Batch 414/963 - Train loss: 0.26909943  - Train acc: 0.8983 - Val loss: 1.01788461\n",
      "(17.54 min) Epoch 87/300 -- Iteration 83241 - Batch 423/963 - Train loss: 0.26826764  - Train acc: 0.8987 - Val loss: 1.01788461\n",
      "(17.54 min) Epoch 87/300 -- Iteration 83250 - Batch 432/963 - Train loss: 0.26782944  - Train acc: 0.8987 - Val loss: 1.01788461\n",
      "(17.55 min) Epoch 87/300 -- Iteration 83259 - Batch 441/963 - Train loss: 0.26734425  - Train acc: 0.8989 - Val loss: 1.01788461\n",
      "(17.55 min) Epoch 87/300 -- Iteration 83268 - Batch 450/963 - Train loss: 0.26645116  - Train acc: 0.8993 - Val loss: 1.01788461\n",
      "(17.55 min) Epoch 87/300 -- Iteration 83277 - Batch 459/963 - Train loss: 0.26659699  - Train acc: 0.8993 - Val loss: 1.01788461\n",
      "(17.55 min) Epoch 87/300 -- Iteration 83286 - Batch 468/963 - Train loss: 0.26576125  - Train acc: 0.8995 - Val loss: 1.01788461\n",
      "(17.56 min) Epoch 87/300 -- Iteration 83295 - Batch 477/963 - Train loss: 0.26618879  - Train acc: 0.8993 - Val loss: 1.01788461\n",
      "(17.56 min) Epoch 87/300 -- Iteration 83304 - Batch 486/963 - Train loss: 0.26624530  - Train acc: 0.8993 - Val loss: 1.01788461\n",
      "(17.56 min) Epoch 87/300 -- Iteration 83313 - Batch 495/963 - Train loss: 0.26588715  - Train acc: 0.8994 - Val loss: 1.01788461\n",
      "(17.56 min) Epoch 87/300 -- Iteration 83322 - Batch 504/963 - Train loss: 0.26604967  - Train acc: 0.8996 - Val loss: 1.01788461\n",
      "(17.56 min) Epoch 87/300 -- Iteration 83331 - Batch 513/963 - Train loss: 0.26658233  - Train acc: 0.8993 - Val loss: 1.01788461\n",
      "(17.57 min) Epoch 87/300 -- Iteration 83340 - Batch 522/963 - Train loss: 0.26607863  - Train acc: 0.8995 - Val loss: 1.01788461\n",
      "(17.57 min) Epoch 87/300 -- Iteration 83349 - Batch 531/963 - Train loss: 0.26645233  - Train acc: 0.8994 - Val loss: 1.01788461\n",
      "(17.57 min) Epoch 87/300 -- Iteration 83358 - Batch 540/963 - Train loss: 0.26633029  - Train acc: 0.8994 - Val loss: 1.01788461\n",
      "(17.57 min) Epoch 87/300 -- Iteration 83367 - Batch 549/963 - Train loss: 0.26676506  - Train acc: 0.8994 - Val loss: 1.01788461\n",
      "(17.57 min) Epoch 87/300 -- Iteration 83376 - Batch 558/963 - Train loss: 0.26682253  - Train acc: 0.8993 - Val loss: 1.01788461\n",
      "(17.58 min) Epoch 87/300 -- Iteration 83385 - Batch 567/963 - Train loss: 0.26717839  - Train acc: 0.8992 - Val loss: 1.01788461\n",
      "(17.58 min) Epoch 87/300 -- Iteration 83394 - Batch 576/963 - Train loss: 0.26710306  - Train acc: 0.8992 - Val loss: 1.01788461\n",
      "(17.58 min) Epoch 87/300 -- Iteration 83403 - Batch 585/963 - Train loss: 0.26707873  - Train acc: 0.8992 - Val loss: 1.01788461\n",
      "(17.58 min) Epoch 87/300 -- Iteration 83412 - Batch 594/963 - Train loss: 0.26661160  - Train acc: 0.8994 - Val loss: 1.01788461\n",
      "(17.58 min) Epoch 87/300 -- Iteration 83421 - Batch 603/963 - Train loss: 0.26676871  - Train acc: 0.8993 - Val loss: 1.01788461\n",
      "(17.59 min) Epoch 87/300 -- Iteration 83430 - Batch 612/963 - Train loss: 0.26662317  - Train acc: 0.8994 - Val loss: 1.01788461\n",
      "(17.59 min) Epoch 87/300 -- Iteration 83439 - Batch 621/963 - Train loss: 0.26637153  - Train acc: 0.8995 - Val loss: 1.01788461\n",
      "(17.59 min) Epoch 87/300 -- Iteration 83448 - Batch 630/963 - Train loss: 0.26636794  - Train acc: 0.8994 - Val loss: 1.01788461\n",
      "(17.59 min) Epoch 87/300 -- Iteration 83457 - Batch 639/963 - Train loss: 0.26600156  - Train acc: 0.8996 - Val loss: 1.01788461\n",
      "(17.59 min) Epoch 87/300 -- Iteration 83466 - Batch 648/963 - Train loss: 0.26602353  - Train acc: 0.8997 - Val loss: 1.01788461\n",
      "(17.60 min) Epoch 87/300 -- Iteration 83475 - Batch 657/963 - Train loss: 0.26606786  - Train acc: 0.8996 - Val loss: 1.01788461\n",
      "(17.60 min) Epoch 87/300 -- Iteration 83484 - Batch 666/963 - Train loss: 0.26594775  - Train acc: 0.8996 - Val loss: 1.01788461\n",
      "(17.60 min) Epoch 87/300 -- Iteration 83493 - Batch 675/963 - Train loss: 0.26564349  - Train acc: 0.8997 - Val loss: 1.01788461\n",
      "(17.60 min) Epoch 87/300 -- Iteration 83502 - Batch 684/963 - Train loss: 0.26578801  - Train acc: 0.8996 - Val loss: 1.01788461\n",
      "(17.60 min) Epoch 87/300 -- Iteration 83511 - Batch 693/963 - Train loss: 0.26561178  - Train acc: 0.8997 - Val loss: 1.01788461\n",
      "(17.60 min) Epoch 87/300 -- Iteration 83520 - Batch 702/963 - Train loss: 0.26534157  - Train acc: 0.8998 - Val loss: 1.01788461\n",
      "(17.61 min) Epoch 87/300 -- Iteration 83529 - Batch 711/963 - Train loss: 0.26537026  - Train acc: 0.8998 - Val loss: 1.01788461\n",
      "(17.61 min) Epoch 87/300 -- Iteration 83538 - Batch 720/963 - Train loss: 0.26576004  - Train acc: 0.8996 - Val loss: 1.01788461\n",
      "(17.61 min) Epoch 87/300 -- Iteration 83547 - Batch 729/963 - Train loss: 0.26555632  - Train acc: 0.8997 - Val loss: 1.01788461\n",
      "(17.61 min) Epoch 87/300 -- Iteration 83556 - Batch 738/963 - Train loss: 0.26565213  - Train acc: 0.8997 - Val loss: 1.01788461\n",
      "(17.61 min) Epoch 87/300 -- Iteration 83565 - Batch 747/963 - Train loss: 0.26570970  - Train acc: 0.8996 - Val loss: 1.01788461\n",
      "(17.62 min) Epoch 87/300 -- Iteration 83574 - Batch 756/963 - Train loss: 0.26547255  - Train acc: 0.8997 - Val loss: 1.01788461\n",
      "(17.62 min) Epoch 87/300 -- Iteration 83583 - Batch 765/963 - Train loss: 0.26542584  - Train acc: 0.8998 - Val loss: 1.01788461\n",
      "(17.62 min) Epoch 87/300 -- Iteration 83592 - Batch 774/963 - Train loss: 0.26551775  - Train acc: 0.8998 - Val loss: 1.01788461\n",
      "(17.62 min) Epoch 87/300 -- Iteration 83601 - Batch 783/963 - Train loss: 0.26539745  - Train acc: 0.8999 - Val loss: 1.01788461\n",
      "(17.62 min) Epoch 87/300 -- Iteration 83610 - Batch 792/963 - Train loss: 0.26544190  - Train acc: 0.8999 - Val loss: 1.01788461\n",
      "(17.63 min) Epoch 87/300 -- Iteration 83619 - Batch 801/963 - Train loss: 0.26535851  - Train acc: 0.9000 - Val loss: 1.01788461\n",
      "(17.63 min) Epoch 87/300 -- Iteration 83628 - Batch 810/963 - Train loss: 0.26536477  - Train acc: 0.9000 - Val loss: 1.01788461\n",
      "(17.63 min) Epoch 87/300 -- Iteration 83637 - Batch 819/963 - Train loss: 0.26536269  - Train acc: 0.9000 - Val loss: 1.01788461\n",
      "(17.63 min) Epoch 87/300 -- Iteration 83646 - Batch 828/963 - Train loss: 0.26544813  - Train acc: 0.9000 - Val loss: 1.01788461\n",
      "(17.63 min) Epoch 87/300 -- Iteration 83655 - Batch 837/963 - Train loss: 0.26516819  - Train acc: 0.9001 - Val loss: 1.01788461\n",
      "(17.64 min) Epoch 87/300 -- Iteration 83664 - Batch 846/963 - Train loss: 0.26502989  - Train acc: 0.9001 - Val loss: 1.01788461\n",
      "(17.64 min) Epoch 87/300 -- Iteration 83673 - Batch 855/963 - Train loss: 0.26510037  - Train acc: 0.9000 - Val loss: 1.01788461\n",
      "(17.64 min) Epoch 87/300 -- Iteration 83682 - Batch 864/963 - Train loss: 0.26507424  - Train acc: 0.9000 - Val loss: 1.01788461\n",
      "(17.64 min) Epoch 87/300 -- Iteration 83691 - Batch 873/963 - Train loss: 0.26545856  - Train acc: 0.8999 - Val loss: 1.01788461\n",
      "(17.64 min) Epoch 87/300 -- Iteration 83700 - Batch 882/963 - Train loss: 0.26550919  - Train acc: 0.8998 - Val loss: 1.01788461\n",
      "(17.65 min) Epoch 87/300 -- Iteration 83709 - Batch 891/963 - Train loss: 0.26552950  - Train acc: 0.8998 - Val loss: 1.01788461\n",
      "(17.65 min) Epoch 87/300 -- Iteration 83718 - Batch 900/963 - Train loss: 0.26532572  - Train acc: 0.9000 - Val loss: 1.01788461\n",
      "(17.65 min) Epoch 87/300 -- Iteration 83727 - Batch 909/963 - Train loss: 0.26533424  - Train acc: 0.9000 - Val loss: 1.01788461\n",
      "(17.65 min) Epoch 87/300 -- Iteration 83736 - Batch 918/963 - Train loss: 0.26520209  - Train acc: 0.9001 - Val loss: 1.01788461\n",
      "(17.65 min) Epoch 87/300 -- Iteration 83745 - Batch 927/963 - Train loss: 0.26513009  - Train acc: 0.9001 - Val loss: 1.01788461\n",
      "(17.66 min) Epoch 87/300 -- Iteration 83754 - Batch 936/963 - Train loss: 0.26506505  - Train acc: 0.9001 - Val loss: 1.01788461\n",
      "(17.66 min) Epoch 87/300 -- Iteration 83763 - Batch 945/963 - Train loss: 0.26491143  - Train acc: 0.9001 - Val loss: 1.01788461\n",
      "(17.66 min) Epoch 87/300 -- Iteration 83772 - Batch 954/963 - Train loss: 0.26498058  - Train acc: 0.9000 - Val loss: 1.01788461\n",
      "(17.66 min) Epoch 87/300 -- Iteration 83781 - Batch 962/963 - Train loss: 0.26481062  - Train acc: 0.9000 - Val loss: 1.02926636 - Val acc: 0.5800\n",
      "(17.66 min) Epoch 88/300 -- Iteration 83790 - Batch 9/963 - Train loss: 0.27299278  - Train acc: 0.8977 - Val loss: 1.02926636\n",
      "(17.67 min) Epoch 88/300 -- Iteration 83799 - Batch 18/963 - Train loss: 0.26937279  - Train acc: 0.8943 - Val loss: 1.02926636\n",
      "(17.67 min) Epoch 88/300 -- Iteration 83808 - Batch 27/963 - Train loss: 0.26230654  - Train acc: 0.8976 - Val loss: 1.02926636\n",
      "(17.67 min) Epoch 88/300 -- Iteration 83817 - Batch 36/963 - Train loss: 0.25461282  - Train acc: 0.9014 - Val loss: 1.02926636\n",
      "(17.67 min) Epoch 88/300 -- Iteration 83826 - Batch 45/963 - Train loss: 0.25566920  - Train acc: 0.9018 - Val loss: 1.02926636\n",
      "(17.67 min) Epoch 88/300 -- Iteration 83835 - Batch 54/963 - Train loss: 0.26052036  - Train acc: 0.9003 - Val loss: 1.02926636\n",
      "(17.68 min) Epoch 88/300 -- Iteration 83844 - Batch 63/963 - Train loss: 0.26302411  - Train acc: 0.8997 - Val loss: 1.02926636\n",
      "(17.68 min) Epoch 88/300 -- Iteration 83853 - Batch 72/963 - Train loss: 0.26513456  - Train acc: 0.8983 - Val loss: 1.02926636\n",
      "(17.68 min) Epoch 88/300 -- Iteration 83862 - Batch 81/963 - Train loss: 0.26701484  - Train acc: 0.8979 - Val loss: 1.02926636\n",
      "(17.68 min) Epoch 88/300 -- Iteration 83871 - Batch 90/963 - Train loss: 0.26465916  - Train acc: 0.8984 - Val loss: 1.02926636\n",
      "(17.68 min) Epoch 88/300 -- Iteration 83880 - Batch 99/963 - Train loss: 0.26506422  - Train acc: 0.8984 - Val loss: 1.02926636\n",
      "(17.69 min) Epoch 88/300 -- Iteration 83889 - Batch 108/963 - Train loss: 0.26439646  - Train acc: 0.8992 - Val loss: 1.02926636\n",
      "(17.69 min) Epoch 88/300 -- Iteration 83898 - Batch 117/963 - Train loss: 0.26362654  - Train acc: 0.8998 - Val loss: 1.02926636\n",
      "(17.69 min) Epoch 88/300 -- Iteration 83907 - Batch 126/963 - Train loss: 0.26514719  - Train acc: 0.8995 - Val loss: 1.02926636\n",
      "(17.69 min) Epoch 88/300 -- Iteration 83916 - Batch 135/963 - Train loss: 0.26331758  - Train acc: 0.9003 - Val loss: 1.02926636\n",
      "(17.69 min) Epoch 88/300 -- Iteration 83925 - Batch 144/963 - Train loss: 0.26218287  - Train acc: 0.9007 - Val loss: 1.02926636\n",
      "(17.69 min) Epoch 88/300 -- Iteration 83934 - Batch 153/963 - Train loss: 0.26141611  - Train acc: 0.9009 - Val loss: 1.02926636\n",
      "(17.70 min) Epoch 88/300 -- Iteration 83943 - Batch 162/963 - Train loss: 0.26211226  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.70 min) Epoch 88/300 -- Iteration 83952 - Batch 171/963 - Train loss: 0.26161934  - Train acc: 0.9008 - Val loss: 1.02926636\n",
      "(17.70 min) Epoch 88/300 -- Iteration 83961 - Batch 180/963 - Train loss: 0.26224136  - Train acc: 0.9002 - Val loss: 1.02926636\n",
      "(17.70 min) Epoch 88/300 -- Iteration 83970 - Batch 189/963 - Train loss: 0.26241106  - Train acc: 0.8998 - Val loss: 1.02926636\n",
      "(17.70 min) Epoch 88/300 -- Iteration 83979 - Batch 198/963 - Train loss: 0.26210312  - Train acc: 0.9001 - Val loss: 1.02926636\n",
      "(17.71 min) Epoch 88/300 -- Iteration 83988 - Batch 207/963 - Train loss: 0.26356229  - Train acc: 0.8993 - Val loss: 1.02926636\n",
      "(17.71 min) Epoch 88/300 -- Iteration 83997 - Batch 216/963 - Train loss: 0.26377605  - Train acc: 0.8992 - Val loss: 1.02926636\n",
      "(17.71 min) Epoch 88/300 -- Iteration 84006 - Batch 225/963 - Train loss: 0.26461437  - Train acc: 0.8988 - Val loss: 1.02926636\n",
      "(17.71 min) Epoch 88/300 -- Iteration 84015 - Batch 234/963 - Train loss: 0.26517801  - Train acc: 0.8984 - Val loss: 1.02926636\n",
      "(17.71 min) Epoch 88/300 -- Iteration 84024 - Batch 243/963 - Train loss: 0.26591141  - Train acc: 0.8979 - Val loss: 1.02926636\n",
      "(17.72 min) Epoch 88/300 -- Iteration 84033 - Batch 252/963 - Train loss: 0.26723321  - Train acc: 0.8974 - Val loss: 1.02926636\n",
      "(17.72 min) Epoch 88/300 -- Iteration 84042 - Batch 261/963 - Train loss: 0.26708617  - Train acc: 0.8977 - Val loss: 1.02926636\n",
      "(17.72 min) Epoch 88/300 -- Iteration 84051 - Batch 270/963 - Train loss: 0.26603874  - Train acc: 0.8983 - Val loss: 1.02926636\n",
      "(17.72 min) Epoch 88/300 -- Iteration 84060 - Batch 279/963 - Train loss: 0.26656427  - Train acc: 0.8979 - Val loss: 1.02926636\n",
      "(17.72 min) Epoch 88/300 -- Iteration 84069 - Batch 288/963 - Train loss: 0.26596766  - Train acc: 0.8983 - Val loss: 1.02926636\n",
      "(17.72 min) Epoch 88/300 -- Iteration 84078 - Batch 297/963 - Train loss: 0.26701312  - Train acc: 0.8978 - Val loss: 1.02926636\n",
      "(17.73 min) Epoch 88/300 -- Iteration 84087 - Batch 306/963 - Train loss: 0.26663472  - Train acc: 0.8980 - Val loss: 1.02926636\n",
      "(17.73 min) Epoch 88/300 -- Iteration 84096 - Batch 315/963 - Train loss: 0.26755378  - Train acc: 0.8977 - Val loss: 1.02926636\n",
      "(17.73 min) Epoch 88/300 -- Iteration 84105 - Batch 324/963 - Train loss: 0.26740513  - Train acc: 0.8977 - Val loss: 1.02926636\n",
      "(17.73 min) Epoch 88/300 -- Iteration 84114 - Batch 333/963 - Train loss: 0.26743699  - Train acc: 0.8976 - Val loss: 1.02926636\n",
      "(17.73 min) Epoch 88/300 -- Iteration 84123 - Batch 342/963 - Train loss: 0.26696521  - Train acc: 0.8978 - Val loss: 1.02926636\n",
      "(17.74 min) Epoch 88/300 -- Iteration 84132 - Batch 351/963 - Train loss: 0.26653854  - Train acc: 0.8980 - Val loss: 1.02926636\n",
      "(17.74 min) Epoch 88/300 -- Iteration 84141 - Batch 360/963 - Train loss: 0.26606624  - Train acc: 0.8981 - Val loss: 1.02926636\n",
      "(17.74 min) Epoch 88/300 -- Iteration 84150 - Batch 369/963 - Train loss: 0.26632305  - Train acc: 0.8982 - Val loss: 1.02926636\n",
      "(17.74 min) Epoch 88/300 -- Iteration 84159 - Batch 378/963 - Train loss: 0.26674278  - Train acc: 0.8981 - Val loss: 1.02926636\n",
      "(17.74 min) Epoch 88/300 -- Iteration 84168 - Batch 387/963 - Train loss: 0.26644701  - Train acc: 0.8983 - Val loss: 1.02926636\n",
      "(17.75 min) Epoch 88/300 -- Iteration 84177 - Batch 396/963 - Train loss: 0.26584514  - Train acc: 0.8986 - Val loss: 1.02926636\n",
      "(17.75 min) Epoch 88/300 -- Iteration 84186 - Batch 405/963 - Train loss: 0.26601325  - Train acc: 0.8986 - Val loss: 1.02926636\n",
      "(17.75 min) Epoch 88/300 -- Iteration 84195 - Batch 414/963 - Train loss: 0.26581344  - Train acc: 0.8988 - Val loss: 1.02926636\n",
      "(17.75 min) Epoch 88/300 -- Iteration 84204 - Batch 423/963 - Train loss: 0.26626932  - Train acc: 0.8987 - Val loss: 1.02926636\n",
      "(17.75 min) Epoch 88/300 -- Iteration 84213 - Batch 432/963 - Train loss: 0.26617592  - Train acc: 0.8989 - Val loss: 1.02926636\n",
      "(17.75 min) Epoch 88/300 -- Iteration 84222 - Batch 441/963 - Train loss: 0.26589014  - Train acc: 0.8992 - Val loss: 1.02926636\n",
      "(17.76 min) Epoch 88/300 -- Iteration 84231 - Batch 450/963 - Train loss: 0.26567054  - Train acc: 0.8993 - Val loss: 1.02926636\n",
      "(17.76 min) Epoch 88/300 -- Iteration 84240 - Batch 459/963 - Train loss: 0.26527167  - Train acc: 0.8994 - Val loss: 1.02926636\n",
      "(17.76 min) Epoch 88/300 -- Iteration 84249 - Batch 468/963 - Train loss: 0.26528599  - Train acc: 0.8996 - Val loss: 1.02926636\n",
      "(17.76 min) Epoch 88/300 -- Iteration 84258 - Batch 477/963 - Train loss: 0.26564148  - Train acc: 0.8996 - Val loss: 1.02926636\n",
      "(17.76 min) Epoch 88/300 -- Iteration 84267 - Batch 486/963 - Train loss: 0.26603807  - Train acc: 0.8994 - Val loss: 1.02926636\n",
      "(17.77 min) Epoch 88/300 -- Iteration 84276 - Batch 495/963 - Train loss: 0.26586656  - Train acc: 0.8994 - Val loss: 1.02926636\n",
      "(17.77 min) Epoch 88/300 -- Iteration 84285 - Batch 504/963 - Train loss: 0.26607400  - Train acc: 0.8994 - Val loss: 1.02926636\n",
      "(17.77 min) Epoch 88/300 -- Iteration 84294 - Batch 513/963 - Train loss: 0.26561617  - Train acc: 0.8996 - Val loss: 1.02926636\n",
      "(17.77 min) Epoch 88/300 -- Iteration 84303 - Batch 522/963 - Train loss: 0.26562640  - Train acc: 0.8995 - Val loss: 1.02926636\n",
      "(17.77 min) Epoch 88/300 -- Iteration 84312 - Batch 531/963 - Train loss: 0.26530882  - Train acc: 0.8997 - Val loss: 1.02926636\n",
      "(17.78 min) Epoch 88/300 -- Iteration 84321 - Batch 540/963 - Train loss: 0.26575194  - Train acc: 0.8996 - Val loss: 1.02926636\n",
      "(17.78 min) Epoch 88/300 -- Iteration 84330 - Batch 549/963 - Train loss: 0.26578435  - Train acc: 0.8996 - Val loss: 1.02926636\n",
      "(17.78 min) Epoch 88/300 -- Iteration 84339 - Batch 558/963 - Train loss: 0.26573111  - Train acc: 0.8997 - Val loss: 1.02926636\n",
      "(17.78 min) Epoch 88/300 -- Iteration 84348 - Batch 567/963 - Train loss: 0.26533984  - Train acc: 0.8998 - Val loss: 1.02926636\n",
      "(17.78 min) Epoch 88/300 -- Iteration 84357 - Batch 576/963 - Train loss: 0.26500274  - Train acc: 0.8998 - Val loss: 1.02926636\n",
      "(17.78 min) Epoch 88/300 -- Iteration 84366 - Batch 585/963 - Train loss: 0.26480503  - Train acc: 0.8999 - Val loss: 1.02926636\n",
      "(17.79 min) Epoch 88/300 -- Iteration 84375 - Batch 594/963 - Train loss: 0.26442382  - Train acc: 0.9002 - Val loss: 1.02926636\n",
      "(17.79 min) Epoch 88/300 -- Iteration 84384 - Batch 603/963 - Train loss: 0.26391240  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.79 min) Epoch 88/300 -- Iteration 84393 - Batch 612/963 - Train loss: 0.26351765  - Train acc: 0.9008 - Val loss: 1.02926636\n",
      "(17.79 min) Epoch 88/300 -- Iteration 84402 - Batch 621/963 - Train loss: 0.26337112  - Train acc: 0.9008 - Val loss: 1.02926636\n",
      "(17.79 min) Epoch 88/300 -- Iteration 84411 - Batch 630/963 - Train loss: 0.26355749  - Train acc: 0.9008 - Val loss: 1.02926636\n",
      "(17.80 min) Epoch 88/300 -- Iteration 84420 - Batch 639/963 - Train loss: 0.26343244  - Train acc: 0.9008 - Val loss: 1.02926636\n",
      "(17.80 min) Epoch 88/300 -- Iteration 84429 - Batch 648/963 - Train loss: 0.26390639  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.80 min) Epoch 88/300 -- Iteration 84438 - Batch 657/963 - Train loss: 0.26381613  - Train acc: 0.9007 - Val loss: 1.02926636\n",
      "(17.80 min) Epoch 88/300 -- Iteration 84447 - Batch 666/963 - Train loss: 0.26371863  - Train acc: 0.9007 - Val loss: 1.02926636\n",
      "(17.80 min) Epoch 88/300 -- Iteration 84456 - Batch 675/963 - Train loss: 0.26348423  - Train acc: 0.9007 - Val loss: 1.02926636\n",
      "(17.81 min) Epoch 88/300 -- Iteration 84465 - Batch 684/963 - Train loss: 0.26358522  - Train acc: 0.9007 - Val loss: 1.02926636\n",
      "(17.81 min) Epoch 88/300 -- Iteration 84474 - Batch 693/963 - Train loss: 0.26353089  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.81 min) Epoch 88/300 -- Iteration 84483 - Batch 702/963 - Train loss: 0.26371860  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.81 min) Epoch 88/300 -- Iteration 84492 - Batch 711/963 - Train loss: 0.26342988  - Train acc: 0.9007 - Val loss: 1.02926636\n",
      "(17.81 min) Epoch 88/300 -- Iteration 84501 - Batch 720/963 - Train loss: 0.26361802  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.82 min) Epoch 88/300 -- Iteration 84510 - Batch 729/963 - Train loss: 0.26374129  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.82 min) Epoch 88/300 -- Iteration 84519 - Batch 738/963 - Train loss: 0.26378386  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.82 min) Epoch 88/300 -- Iteration 84528 - Batch 747/963 - Train loss: 0.26347283  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.82 min) Epoch 88/300 -- Iteration 84537 - Batch 756/963 - Train loss: 0.26326705  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.82 min) Epoch 88/300 -- Iteration 84546 - Batch 765/963 - Train loss: 0.26309280  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.82 min) Epoch 88/300 -- Iteration 84555 - Batch 774/963 - Train loss: 0.26328784  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.83 min) Epoch 88/300 -- Iteration 84564 - Batch 783/963 - Train loss: 0.26346467  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.83 min) Epoch 88/300 -- Iteration 84573 - Batch 792/963 - Train loss: 0.26354585  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.83 min) Epoch 88/300 -- Iteration 84582 - Batch 801/963 - Train loss: 0.26338248  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.83 min) Epoch 88/300 -- Iteration 84591 - Batch 810/963 - Train loss: 0.26372294  - Train acc: 0.9004 - Val loss: 1.02926636\n",
      "(17.83 min) Epoch 88/300 -- Iteration 84600 - Batch 819/963 - Train loss: 0.26336548  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.84 min) Epoch 88/300 -- Iteration 84609 - Batch 828/963 - Train loss: 0.26350288  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.84 min) Epoch 88/300 -- Iteration 84618 - Batch 837/963 - Train loss: 0.26314724  - Train acc: 0.9007 - Val loss: 1.02926636\n",
      "(17.84 min) Epoch 88/300 -- Iteration 84627 - Batch 846/963 - Train loss: 0.26336296  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.84 min) Epoch 88/300 -- Iteration 84636 - Batch 855/963 - Train loss: 0.26329484  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.84 min) Epoch 88/300 -- Iteration 84645 - Batch 864/963 - Train loss: 0.26343435  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.85 min) Epoch 88/300 -- Iteration 84654 - Batch 873/963 - Train loss: 0.26329173  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.85 min) Epoch 88/300 -- Iteration 84663 - Batch 882/963 - Train loss: 0.26340153  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.85 min) Epoch 88/300 -- Iteration 84672 - Batch 891/963 - Train loss: 0.26329462  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.85 min) Epoch 88/300 -- Iteration 84681 - Batch 900/963 - Train loss: 0.26339189  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.85 min) Epoch 88/300 -- Iteration 84690 - Batch 909/963 - Train loss: 0.26312299  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.86 min) Epoch 88/300 -- Iteration 84699 - Batch 918/963 - Train loss: 0.26327407  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.86 min) Epoch 88/300 -- Iteration 84708 - Batch 927/963 - Train loss: 0.26308030  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.86 min) Epoch 88/300 -- Iteration 84717 - Batch 936/963 - Train loss: 0.26331263  - Train acc: 0.9005 - Val loss: 1.02926636\n",
      "(17.86 min) Epoch 88/300 -- Iteration 84726 - Batch 945/963 - Train loss: 0.26321239  - Train acc: 0.9006 - Val loss: 1.02926636\n",
      "(17.86 min) Epoch 88/300 -- Iteration 84735 - Batch 954/963 - Train loss: 0.26309171  - Train acc: 0.9007 - Val loss: 1.02926636\n",
      "(17.86 min) Epoch 88/300 -- Iteration 84744 - Batch 962/963 - Train loss: 0.26306586  - Train acc: 0.9006 - Val loss: 1.00792420 - Val acc: 0.5883\n",
      "(17.87 min) Epoch 89/300 -- Iteration 84753 - Batch 9/963 - Train loss: 0.25070109  - Train acc: 0.9094 - Val loss: 1.00792420\n",
      "(17.87 min) Epoch 89/300 -- Iteration 84762 - Batch 18/963 - Train loss: 0.26552917  - Train acc: 0.9009 - Val loss: 1.00792420\n",
      "(17.87 min) Epoch 89/300 -- Iteration 84771 - Batch 27/963 - Train loss: 0.26752906  - Train acc: 0.9015 - Val loss: 1.00792420\n",
      "(17.87 min) Epoch 89/300 -- Iteration 84780 - Batch 36/963 - Train loss: 0.26695972  - Train acc: 0.9012 - Val loss: 1.00792420\n",
      "(17.87 min) Epoch 89/300 -- Iteration 84789 - Batch 45/963 - Train loss: 0.27170821  - Train acc: 0.9005 - Val loss: 1.00792420\n",
      "(17.88 min) Epoch 89/300 -- Iteration 84798 - Batch 54/963 - Train loss: 0.26794123  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(17.88 min) Epoch 89/300 -- Iteration 84807 - Batch 63/963 - Train loss: 0.26293165  - Train acc: 0.9049 - Val loss: 1.00792420\n",
      "(17.88 min) Epoch 89/300 -- Iteration 84816 - Batch 72/963 - Train loss: 0.26388964  - Train acc: 0.9038 - Val loss: 1.00792420\n",
      "(17.88 min) Epoch 89/300 -- Iteration 84825 - Batch 81/963 - Train loss: 0.26490862  - Train acc: 0.9041 - Val loss: 1.00792420\n",
      "(17.88 min) Epoch 89/300 -- Iteration 84834 - Batch 90/963 - Train loss: 0.26521979  - Train acc: 0.9038 - Val loss: 1.00792420\n",
      "(17.89 min) Epoch 89/300 -- Iteration 84843 - Batch 99/963 - Train loss: 0.26743444  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(17.89 min) Epoch 89/300 -- Iteration 84852 - Batch 108/963 - Train loss: 0.26621411  - Train acc: 0.9027 - Val loss: 1.00792420\n",
      "(17.89 min) Epoch 89/300 -- Iteration 84861 - Batch 117/963 - Train loss: 0.26548995  - Train acc: 0.9029 - Val loss: 1.00792420\n",
      "(17.89 min) Epoch 89/300 -- Iteration 84870 - Batch 126/963 - Train loss: 0.26466042  - Train acc: 0.9028 - Val loss: 1.00792420\n",
      "(17.89 min) Epoch 89/300 -- Iteration 84879 - Batch 135/963 - Train loss: 0.26469706  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(17.90 min) Epoch 89/300 -- Iteration 84888 - Batch 144/963 - Train loss: 0.26500944  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(17.90 min) Epoch 89/300 -- Iteration 84897 - Batch 153/963 - Train loss: 0.26573094  - Train acc: 0.9021 - Val loss: 1.00792420\n",
      "(17.90 min) Epoch 89/300 -- Iteration 84906 - Batch 162/963 - Train loss: 0.26302335  - Train acc: 0.9028 - Val loss: 1.00792420\n",
      "(17.90 min) Epoch 89/300 -- Iteration 84915 - Batch 171/963 - Train loss: 0.26338118  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(17.90 min) Epoch 89/300 -- Iteration 84924 - Batch 180/963 - Train loss: 0.26389562  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(17.90 min) Epoch 89/300 -- Iteration 84933 - Batch 189/963 - Train loss: 0.26281183  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(17.91 min) Epoch 89/300 -- Iteration 84942 - Batch 198/963 - Train loss: 0.26340952  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(17.91 min) Epoch 89/300 -- Iteration 84951 - Batch 207/963 - Train loss: 0.26416990  - Train acc: 0.9017 - Val loss: 1.00792420\n",
      "(17.91 min) Epoch 89/300 -- Iteration 84960 - Batch 216/963 - Train loss: 0.26371745  - Train acc: 0.9018 - Val loss: 1.00792420\n",
      "(17.91 min) Epoch 89/300 -- Iteration 84969 - Batch 225/963 - Train loss: 0.26262782  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(17.91 min) Epoch 89/300 -- Iteration 84978 - Batch 234/963 - Train loss: 0.26239119  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(17.92 min) Epoch 89/300 -- Iteration 84987 - Batch 243/963 - Train loss: 0.26086294  - Train acc: 0.9031 - Val loss: 1.00792420\n",
      "(17.92 min) Epoch 89/300 -- Iteration 84996 - Batch 252/963 - Train loss: 0.26024954  - Train acc: 0.9033 - Val loss: 1.00792420\n",
      "(17.92 min) Epoch 89/300 -- Iteration 85005 - Batch 261/963 - Train loss: 0.26103543  - Train acc: 0.9030 - Val loss: 1.00792420\n",
      "(17.92 min) Epoch 89/300 -- Iteration 85014 - Batch 270/963 - Train loss: 0.26136488  - Train acc: 0.9027 - Val loss: 1.00792420\n",
      "(17.92 min) Epoch 89/300 -- Iteration 85023 - Batch 279/963 - Train loss: 0.26105599  - Train acc: 0.9027 - Val loss: 1.00792420\n",
      "(17.93 min) Epoch 89/300 -- Iteration 85032 - Batch 288/963 - Train loss: 0.26016284  - Train acc: 0.9032 - Val loss: 1.00792420\n",
      "(17.93 min) Epoch 89/300 -- Iteration 85041 - Batch 297/963 - Train loss: 0.26017694  - Train acc: 0.9031 - Val loss: 1.00792420\n",
      "(17.93 min) Epoch 89/300 -- Iteration 85050 - Batch 306/963 - Train loss: 0.26104247  - Train acc: 0.9030 - Val loss: 1.00792420\n",
      "(17.93 min) Epoch 89/300 -- Iteration 85059 - Batch 315/963 - Train loss: 0.26058664  - Train acc: 0.9029 - Val loss: 1.00792420\n",
      "(17.93 min) Epoch 89/300 -- Iteration 85068 - Batch 324/963 - Train loss: 0.26131244  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(17.93 min) Epoch 89/300 -- Iteration 85077 - Batch 333/963 - Train loss: 0.26157763  - Train acc: 0.9021 - Val loss: 1.00792420\n",
      "(17.94 min) Epoch 89/300 -- Iteration 85086 - Batch 342/963 - Train loss: 0.26102892  - Train acc: 0.9020 - Val loss: 1.00792420\n",
      "(17.94 min) Epoch 89/300 -- Iteration 85095 - Batch 351/963 - Train loss: 0.26102144  - Train acc: 0.9021 - Val loss: 1.00792420\n",
      "(17.94 min) Epoch 89/300 -- Iteration 85104 - Batch 360/963 - Train loss: 0.26120958  - Train acc: 0.9019 - Val loss: 1.00792420\n",
      "(17.94 min) Epoch 89/300 -- Iteration 85113 - Batch 369/963 - Train loss: 0.26142699  - Train acc: 0.9018 - Val loss: 1.00792420\n",
      "(17.94 min) Epoch 89/300 -- Iteration 85122 - Batch 378/963 - Train loss: 0.26099821  - Train acc: 0.9019 - Val loss: 1.00792420\n",
      "(17.95 min) Epoch 89/300 -- Iteration 85131 - Batch 387/963 - Train loss: 0.26102737  - Train acc: 0.9017 - Val loss: 1.00792420\n",
      "(17.95 min) Epoch 89/300 -- Iteration 85140 - Batch 396/963 - Train loss: 0.26066161  - Train acc: 0.9019 - Val loss: 1.00792420\n",
      "(17.95 min) Epoch 89/300 -- Iteration 85149 - Batch 405/963 - Train loss: 0.26092784  - Train acc: 0.9017 - Val loss: 1.00792420\n",
      "(17.95 min) Epoch 89/300 -- Iteration 85158 - Batch 414/963 - Train loss: 0.26082966  - Train acc: 0.9021 - Val loss: 1.00792420\n",
      "(17.95 min) Epoch 89/300 -- Iteration 85167 - Batch 423/963 - Train loss: 0.26053792  - Train acc: 0.9022 - Val loss: 1.00792420\n",
      "(17.96 min) Epoch 89/300 -- Iteration 85176 - Batch 432/963 - Train loss: 0.26037066  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(17.96 min) Epoch 89/300 -- Iteration 85185 - Batch 441/963 - Train loss: 0.26044991  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(17.96 min) Epoch 89/300 -- Iteration 85194 - Batch 450/963 - Train loss: 0.26041604  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(17.96 min) Epoch 89/300 -- Iteration 85203 - Batch 459/963 - Train loss: 0.26069307  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(17.96 min) Epoch 89/300 -- Iteration 85212 - Batch 468/963 - Train loss: 0.26043546  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(17.97 min) Epoch 89/300 -- Iteration 85221 - Batch 477/963 - Train loss: 0.26046465  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(17.97 min) Epoch 89/300 -- Iteration 85230 - Batch 486/963 - Train loss: 0.26072179  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(17.97 min) Epoch 89/300 -- Iteration 85239 - Batch 495/963 - Train loss: 0.26070486  - Train acc: 0.9022 - Val loss: 1.00792420\n",
      "(17.97 min) Epoch 89/300 -- Iteration 85248 - Batch 504/963 - Train loss: 0.26134475  - Train acc: 0.9020 - Val loss: 1.00792420\n",
      "(17.97 min) Epoch 89/300 -- Iteration 85257 - Batch 513/963 - Train loss: 0.26141183  - Train acc: 0.9022 - Val loss: 1.00792420\n",
      "(17.97 min) Epoch 89/300 -- Iteration 85266 - Batch 522/963 - Train loss: 0.26119363  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(17.98 min) Epoch 89/300 -- Iteration 85275 - Batch 531/963 - Train loss: 0.26065913  - Train acc: 0.9025 - Val loss: 1.00792420\n",
      "(17.98 min) Epoch 89/300 -- Iteration 85284 - Batch 540/963 - Train loss: 0.26047466  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(17.98 min) Epoch 89/300 -- Iteration 85293 - Batch 549/963 - Train loss: 0.26073157  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(17.98 min) Epoch 89/300 -- Iteration 85302 - Batch 558/963 - Train loss: 0.26077885  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(17.98 min) Epoch 89/300 -- Iteration 85311 - Batch 567/963 - Train loss: 0.26055480  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(17.99 min) Epoch 89/300 -- Iteration 85320 - Batch 576/963 - Train loss: 0.26037872  - Train acc: 0.9025 - Val loss: 1.00792420\n",
      "(17.99 min) Epoch 89/300 -- Iteration 85329 - Batch 585/963 - Train loss: 0.25990224  - Train acc: 0.9027 - Val loss: 1.00792420\n",
      "(17.99 min) Epoch 89/300 -- Iteration 85338 - Batch 594/963 - Train loss: 0.25991450  - Train acc: 0.9027 - Val loss: 1.00792420\n",
      "(17.99 min) Epoch 89/300 -- Iteration 85347 - Batch 603/963 - Train loss: 0.25992821  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(17.99 min) Epoch 89/300 -- Iteration 85356 - Batch 612/963 - Train loss: 0.26068008  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(18.00 min) Epoch 89/300 -- Iteration 85365 - Batch 621/963 - Train loss: 0.26092454  - Train acc: 0.9022 - Val loss: 1.00792420\n",
      "(18.00 min) Epoch 89/300 -- Iteration 85374 - Batch 630/963 - Train loss: 0.26049825  - Train acc: 0.9022 - Val loss: 1.00792420\n",
      "(18.00 min) Epoch 89/300 -- Iteration 85383 - Batch 639/963 - Train loss: 0.26023962  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(18.00 min) Epoch 89/300 -- Iteration 85392 - Batch 648/963 - Train loss: 0.26056661  - Train acc: 0.9020 - Val loss: 1.00792420\n",
      "(18.00 min) Epoch 89/300 -- Iteration 85401 - Batch 657/963 - Train loss: 0.26051443  - Train acc: 0.9020 - Val loss: 1.00792420\n",
      "(18.01 min) Epoch 89/300 -- Iteration 85410 - Batch 666/963 - Train loss: 0.26031032  - Train acc: 0.9022 - Val loss: 1.00792420\n",
      "(18.01 min) Epoch 89/300 -- Iteration 85419 - Batch 675/963 - Train loss: 0.26031174  - Train acc: 0.9022 - Val loss: 1.00792420\n",
      "(18.01 min) Epoch 89/300 -- Iteration 85428 - Batch 684/963 - Train loss: 0.26023140  - Train acc: 0.9022 - Val loss: 1.00792420\n",
      "(18.01 min) Epoch 89/300 -- Iteration 85437 - Batch 693/963 - Train loss: 0.26006461  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(18.01 min) Epoch 89/300 -- Iteration 85446 - Batch 702/963 - Train loss: 0.26017934  - Train acc: 0.9021 - Val loss: 1.00792420\n",
      "(18.01 min) Epoch 89/300 -- Iteration 85455 - Batch 711/963 - Train loss: 0.26015671  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(18.02 min) Epoch 89/300 -- Iteration 85464 - Batch 720/963 - Train loss: 0.25994793  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(18.02 min) Epoch 89/300 -- Iteration 85473 - Batch 729/963 - Train loss: 0.25987196  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(18.02 min) Epoch 89/300 -- Iteration 85482 - Batch 738/963 - Train loss: 0.25985976  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(18.02 min) Epoch 89/300 -- Iteration 85491 - Batch 747/963 - Train loss: 0.25951077  - Train acc: 0.9025 - Val loss: 1.00792420\n",
      "(18.02 min) Epoch 89/300 -- Iteration 85500 - Batch 756/963 - Train loss: 0.25925977  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(18.03 min) Epoch 89/300 -- Iteration 85509 - Batch 765/963 - Train loss: 0.25901328  - Train acc: 0.9026 - Val loss: 1.00792420\n",
      "(18.03 min) Epoch 89/300 -- Iteration 85518 - Batch 774/963 - Train loss: 0.25927032  - Train acc: 0.9025 - Val loss: 1.00792420\n",
      "(18.03 min) Epoch 89/300 -- Iteration 85527 - Batch 783/963 - Train loss: 0.25929852  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(18.03 min) Epoch 89/300 -- Iteration 85536 - Batch 792/963 - Train loss: 0.25925918  - Train acc: 0.9025 - Val loss: 1.00792420\n",
      "(18.03 min) Epoch 89/300 -- Iteration 85545 - Batch 801/963 - Train loss: 0.25961986  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(18.03 min) Epoch 89/300 -- Iteration 85554 - Batch 810/963 - Train loss: 0.25972890  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(18.04 min) Epoch 89/300 -- Iteration 85563 - Batch 819/963 - Train loss: 0.25964369  - Train acc: 0.9025 - Val loss: 1.00792420\n",
      "(18.04 min) Epoch 89/300 -- Iteration 85572 - Batch 828/963 - Train loss: 0.25987417  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(18.04 min) Epoch 89/300 -- Iteration 85581 - Batch 837/963 - Train loss: 0.25982099  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(18.04 min) Epoch 89/300 -- Iteration 85590 - Batch 846/963 - Train loss: 0.25972143  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(18.04 min) Epoch 89/300 -- Iteration 85599 - Batch 855/963 - Train loss: 0.25984783  - Train acc: 0.9024 - Val loss: 1.00792420\n",
      "(18.05 min) Epoch 89/300 -- Iteration 85608 - Batch 864/963 - Train loss: 0.25995699  - Train acc: 0.9023 - Val loss: 1.00792420\n",
      "(18.05 min) Epoch 89/300 -- Iteration 85617 - Batch 873/963 - Train loss: 0.26002685  - Train acc: 0.9022 - Val loss: 1.00792420\n",
      "(18.05 min) Epoch 89/300 -- Iteration 85626 - Batch 882/963 - Train loss: 0.26025694  - Train acc: 0.9021 - Val loss: 1.00792420\n",
      "(18.05 min) Epoch 89/300 -- Iteration 85635 - Batch 891/963 - Train loss: 0.26035299  - Train acc: 0.9021 - Val loss: 1.00792420\n",
      "(18.05 min) Epoch 89/300 -- Iteration 85644 - Batch 900/963 - Train loss: 0.26024026  - Train acc: 0.9021 - Val loss: 1.00792420\n",
      "(18.06 min) Epoch 89/300 -- Iteration 85653 - Batch 909/963 - Train loss: 0.26028561  - Train acc: 0.9020 - Val loss: 1.00792420\n",
      "(18.06 min) Epoch 89/300 -- Iteration 85662 - Batch 918/963 - Train loss: 0.26037335  - Train acc: 0.9021 - Val loss: 1.00792420\n",
      "(18.06 min) Epoch 89/300 -- Iteration 85671 - Batch 927/963 - Train loss: 0.26035337  - Train acc: 0.9020 - Val loss: 1.00792420\n",
      "(18.06 min) Epoch 89/300 -- Iteration 85680 - Batch 936/963 - Train loss: 0.26043097  - Train acc: 0.9019 - Val loss: 1.00792420\n",
      "(18.06 min) Epoch 89/300 -- Iteration 85689 - Batch 945/963 - Train loss: 0.26043251  - Train acc: 0.9020 - Val loss: 1.00792420\n",
      "(18.07 min) Epoch 89/300 -- Iteration 85698 - Batch 954/963 - Train loss: 0.26063641  - Train acc: 0.9019 - Val loss: 1.00792420\n",
      "(18.07 min) Epoch 89/300 -- Iteration 85707 - Batch 962/963 - Train loss: 0.26088872  - Train acc: 0.9018 - Val loss: 1.04518545 - Val acc: 0.5717\n",
      "(18.07 min) Epoch 90/300 -- Iteration 85716 - Batch 9/963 - Train loss: 0.23472799  - Train acc: 0.9156 - Val loss: 1.04518545\n",
      "(18.07 min) Epoch 90/300 -- Iteration 85725 - Batch 18/963 - Train loss: 0.24637366  - Train acc: 0.9079 - Val loss: 1.04518545\n",
      "(18.07 min) Epoch 90/300 -- Iteration 85734 - Batch 27/963 - Train loss: 0.24562937  - Train acc: 0.9079 - Val loss: 1.04518545\n",
      "(18.08 min) Epoch 90/300 -- Iteration 85743 - Batch 36/963 - Train loss: 0.25402076  - Train acc: 0.9075 - Val loss: 1.04518545\n",
      "(18.08 min) Epoch 90/300 -- Iteration 85752 - Batch 45/963 - Train loss: 0.25702404  - Train acc: 0.9049 - Val loss: 1.04518545\n",
      "(18.08 min) Epoch 90/300 -- Iteration 85761 - Batch 54/963 - Train loss: 0.25945840  - Train acc: 0.9036 - Val loss: 1.04518545\n",
      "(18.08 min) Epoch 90/300 -- Iteration 85770 - Batch 63/963 - Train loss: 0.26036568  - Train acc: 0.9023 - Val loss: 1.04518545\n",
      "(18.08 min) Epoch 90/300 -- Iteration 85779 - Batch 72/963 - Train loss: 0.25889795  - Train acc: 0.9035 - Val loss: 1.04518545\n",
      "(18.09 min) Epoch 90/300 -- Iteration 85788 - Batch 81/963 - Train loss: 0.26163675  - Train acc: 0.9019 - Val loss: 1.04518545\n",
      "(18.09 min) Epoch 90/300 -- Iteration 85797 - Batch 90/963 - Train loss: 0.26082979  - Train acc: 0.9020 - Val loss: 1.04518545\n",
      "(18.09 min) Epoch 90/300 -- Iteration 85806 - Batch 99/963 - Train loss: 0.26131523  - Train acc: 0.9022 - Val loss: 1.04518545\n",
      "(18.09 min) Epoch 90/300 -- Iteration 85815 - Batch 108/963 - Train loss: 0.26185679  - Train acc: 0.9017 - Val loss: 1.04518545\n",
      "(18.09 min) Epoch 90/300 -- Iteration 85824 - Batch 117/963 - Train loss: 0.25997295  - Train acc: 0.9021 - Val loss: 1.04518545\n",
      "(18.09 min) Epoch 90/300 -- Iteration 85833 - Batch 126/963 - Train loss: 0.25822694  - Train acc: 0.9032 - Val loss: 1.04518545\n",
      "(18.10 min) Epoch 90/300 -- Iteration 85842 - Batch 135/963 - Train loss: 0.25860049  - Train acc: 0.9034 - Val loss: 1.04518545\n",
      "(18.10 min) Epoch 90/300 -- Iteration 85851 - Batch 144/963 - Train loss: 0.25732678  - Train acc: 0.9039 - Val loss: 1.04518545\n",
      "(18.10 min) Epoch 90/300 -- Iteration 85860 - Batch 153/963 - Train loss: 0.25856538  - Train acc: 0.9037 - Val loss: 1.04518545\n",
      "(18.10 min) Epoch 90/300 -- Iteration 85869 - Batch 162/963 - Train loss: 0.25913826  - Train acc: 0.9036 - Val loss: 1.04518545\n",
      "(18.10 min) Epoch 90/300 -- Iteration 85878 - Batch 171/963 - Train loss: 0.25874726  - Train acc: 0.9038 - Val loss: 1.04518545\n",
      "(18.11 min) Epoch 90/300 -- Iteration 85887 - Batch 180/963 - Train loss: 0.25990676  - Train acc: 0.9029 - Val loss: 1.04518545\n",
      "(18.11 min) Epoch 90/300 -- Iteration 85896 - Batch 189/963 - Train loss: 0.26045701  - Train acc: 0.9022 - Val loss: 1.04518545\n",
      "(18.11 min) Epoch 90/300 -- Iteration 85905 - Batch 198/963 - Train loss: 0.26012381  - Train acc: 0.9021 - Val loss: 1.04518545\n",
      "(18.11 min) Epoch 90/300 -- Iteration 85914 - Batch 207/963 - Train loss: 0.25991361  - Train acc: 0.9026 - Val loss: 1.04518545\n",
      "(18.11 min) Epoch 90/300 -- Iteration 85923 - Batch 216/963 - Train loss: 0.26102390  - Train acc: 0.9021 - Val loss: 1.04518545\n",
      "(18.12 min) Epoch 90/300 -- Iteration 85932 - Batch 225/963 - Train loss: 0.26027479  - Train acc: 0.9021 - Val loss: 1.04518545\n",
      "(18.12 min) Epoch 90/300 -- Iteration 85941 - Batch 234/963 - Train loss: 0.25972903  - Train acc: 0.9024 - Val loss: 1.04518545\n",
      "(18.12 min) Epoch 90/300 -- Iteration 85950 - Batch 243/963 - Train loss: 0.25992105  - Train acc: 0.9021 - Val loss: 1.04518545\n",
      "(18.12 min) Epoch 90/300 -- Iteration 85959 - Batch 252/963 - Train loss: 0.26061250  - Train acc: 0.9018 - Val loss: 1.04518545\n",
      "(18.12 min) Epoch 90/300 -- Iteration 85968 - Batch 261/963 - Train loss: 0.26054839  - Train acc: 0.9019 - Val loss: 1.04518545\n",
      "(18.12 min) Epoch 90/300 -- Iteration 85977 - Batch 270/963 - Train loss: 0.26128512  - Train acc: 0.9015 - Val loss: 1.04518545\n",
      "(18.13 min) Epoch 90/300 -- Iteration 85986 - Batch 279/963 - Train loss: 0.26136985  - Train acc: 0.9016 - Val loss: 1.04518545\n",
      "(18.13 min) Epoch 90/300 -- Iteration 85995 - Batch 288/963 - Train loss: 0.26314304  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.13 min) Epoch 90/300 -- Iteration 86004 - Batch 297/963 - Train loss: 0.26255993  - Train acc: 0.9012 - Val loss: 1.04518545\n",
      "(18.13 min) Epoch 90/300 -- Iteration 86013 - Batch 306/963 - Train loss: 0.26234862  - Train acc: 0.9013 - Val loss: 1.04518545\n",
      "(18.13 min) Epoch 90/300 -- Iteration 86022 - Batch 315/963 - Train loss: 0.26249952  - Train acc: 0.9013 - Val loss: 1.04518545\n",
      "(18.14 min) Epoch 90/300 -- Iteration 86031 - Batch 324/963 - Train loss: 0.26230436  - Train acc: 0.9012 - Val loss: 1.04518545\n",
      "(18.14 min) Epoch 90/300 -- Iteration 86040 - Batch 333/963 - Train loss: 0.26224654  - Train acc: 0.9012 - Val loss: 1.04518545\n",
      "(18.14 min) Epoch 90/300 -- Iteration 86049 - Batch 342/963 - Train loss: 0.26242023  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.14 min) Epoch 90/300 -- Iteration 86058 - Batch 351/963 - Train loss: 0.26261731  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.14 min) Epoch 90/300 -- Iteration 86067 - Batch 360/963 - Train loss: 0.26326654  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.14 min) Epoch 90/300 -- Iteration 86076 - Batch 369/963 - Train loss: 0.26400790  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.15 min) Epoch 90/300 -- Iteration 86085 - Batch 378/963 - Train loss: 0.26465109  - Train acc: 0.9006 - Val loss: 1.04518545\n",
      "(18.15 min) Epoch 90/300 -- Iteration 86094 - Batch 387/963 - Train loss: 0.26417640  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.15 min) Epoch 90/300 -- Iteration 86103 - Batch 396/963 - Train loss: 0.26350315  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.15 min) Epoch 90/300 -- Iteration 86112 - Batch 405/963 - Train loss: 0.26327034  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.15 min) Epoch 90/300 -- Iteration 86121 - Batch 414/963 - Train loss: 0.26405596  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.16 min) Epoch 90/300 -- Iteration 86130 - Batch 423/963 - Train loss: 0.26401476  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.16 min) Epoch 90/300 -- Iteration 86139 - Batch 432/963 - Train loss: 0.26426246  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.16 min) Epoch 90/300 -- Iteration 86148 - Batch 441/963 - Train loss: 0.26442717  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.16 min) Epoch 90/300 -- Iteration 86157 - Batch 450/963 - Train loss: 0.26442404  - Train acc: 0.9006 - Val loss: 1.04518545\n",
      "(18.17 min) Epoch 90/300 -- Iteration 86166 - Batch 459/963 - Train loss: 0.26445951  - Train acc: 0.9006 - Val loss: 1.04518545\n",
      "(18.17 min) Epoch 90/300 -- Iteration 86175 - Batch 468/963 - Train loss: 0.26454031  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.17 min) Epoch 90/300 -- Iteration 86184 - Batch 477/963 - Train loss: 0.26445468  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.17 min) Epoch 90/300 -- Iteration 86193 - Batch 486/963 - Train loss: 0.26440590  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.17 min) Epoch 90/300 -- Iteration 86202 - Batch 495/963 - Train loss: 0.26433121  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.17 min) Epoch 90/300 -- Iteration 86211 - Batch 504/963 - Train loss: 0.26445460  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.18 min) Epoch 90/300 -- Iteration 86220 - Batch 513/963 - Train loss: 0.26451052  - Train acc: 0.9006 - Val loss: 1.04518545\n",
      "(18.18 min) Epoch 90/300 -- Iteration 86229 - Batch 522/963 - Train loss: 0.26464075  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.18 min) Epoch 90/300 -- Iteration 86238 - Batch 531/963 - Train loss: 0.26459698  - Train acc: 0.9005 - Val loss: 1.04518545\n",
      "(18.18 min) Epoch 90/300 -- Iteration 86247 - Batch 540/963 - Train loss: 0.26419399  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.18 min) Epoch 90/300 -- Iteration 86256 - Batch 549/963 - Train loss: 0.26431600  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.19 min) Epoch 90/300 -- Iteration 86265 - Batch 558/963 - Train loss: 0.26395768  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.19 min) Epoch 90/300 -- Iteration 86274 - Batch 567/963 - Train loss: 0.26371340  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.19 min) Epoch 90/300 -- Iteration 86283 - Batch 576/963 - Train loss: 0.26358469  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.19 min) Epoch 90/300 -- Iteration 86292 - Batch 585/963 - Train loss: 0.26364133  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.19 min) Epoch 90/300 -- Iteration 86301 - Batch 594/963 - Train loss: 0.26379069  - Train acc: 0.9006 - Val loss: 1.04518545\n",
      "(18.20 min) Epoch 90/300 -- Iteration 86310 - Batch 603/963 - Train loss: 0.26345423  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.20 min) Epoch 90/300 -- Iteration 86319 - Batch 612/963 - Train loss: 0.26326456  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.20 min) Epoch 90/300 -- Iteration 86328 - Batch 621/963 - Train loss: 0.26340214  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.20 min) Epoch 90/300 -- Iteration 86337 - Batch 630/963 - Train loss: 0.26331313  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.20 min) Epoch 90/300 -- Iteration 86346 - Batch 639/963 - Train loss: 0.26325513  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.21 min) Epoch 90/300 -- Iteration 86355 - Batch 648/963 - Train loss: 0.26334675  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.21 min) Epoch 90/300 -- Iteration 86364 - Batch 657/963 - Train loss: 0.26354671  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.21 min) Epoch 90/300 -- Iteration 86373 - Batch 666/963 - Train loss: 0.26319196  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.21 min) Epoch 90/300 -- Iteration 86382 - Batch 675/963 - Train loss: 0.26340825  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.21 min) Epoch 90/300 -- Iteration 86391 - Batch 684/963 - Train loss: 0.26329849  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.22 min) Epoch 90/300 -- Iteration 86400 - Batch 693/963 - Train loss: 0.26312444  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.22 min) Epoch 90/300 -- Iteration 86409 - Batch 702/963 - Train loss: 0.26363887  - Train acc: 0.9005 - Val loss: 1.04518545\n",
      "(18.22 min) Epoch 90/300 -- Iteration 86418 - Batch 711/963 - Train loss: 0.26329387  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.22 min) Epoch 90/300 -- Iteration 86427 - Batch 720/963 - Train loss: 0.26295403  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.22 min) Epoch 90/300 -- Iteration 86436 - Batch 729/963 - Train loss: 0.26294238  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.22 min) Epoch 90/300 -- Iteration 86445 - Batch 738/963 - Train loss: 0.26255507  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.23 min) Epoch 90/300 -- Iteration 86454 - Batch 747/963 - Train loss: 0.26230835  - Train acc: 0.9010 - Val loss: 1.04518545\n",
      "(18.23 min) Epoch 90/300 -- Iteration 86463 - Batch 756/963 - Train loss: 0.26213713  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.23 min) Epoch 90/300 -- Iteration 86472 - Batch 765/963 - Train loss: 0.26204798  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.23 min) Epoch 90/300 -- Iteration 86481 - Batch 774/963 - Train loss: 0.26194474  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.23 min) Epoch 90/300 -- Iteration 86490 - Batch 783/963 - Train loss: 0.26193092  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.24 min) Epoch 90/300 -- Iteration 86499 - Batch 792/963 - Train loss: 0.26211396  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.24 min) Epoch 90/300 -- Iteration 86508 - Batch 801/963 - Train loss: 0.26223273  - Train acc: 0.9010 - Val loss: 1.04518545\n",
      "(18.24 min) Epoch 90/300 -- Iteration 86517 - Batch 810/963 - Train loss: 0.26243643  - Train acc: 0.9010 - Val loss: 1.04518545\n",
      "(18.24 min) Epoch 90/300 -- Iteration 86526 - Batch 819/963 - Train loss: 0.26219862  - Train acc: 0.9012 - Val loss: 1.04518545\n",
      "(18.24 min) Epoch 90/300 -- Iteration 86535 - Batch 828/963 - Train loss: 0.26225740  - Train acc: 0.9012 - Val loss: 1.04518545\n",
      "(18.25 min) Epoch 90/300 -- Iteration 86544 - Batch 837/963 - Train loss: 0.26235128  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.25 min) Epoch 90/300 -- Iteration 86553 - Batch 846/963 - Train loss: 0.26226490  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.25 min) Epoch 90/300 -- Iteration 86562 - Batch 855/963 - Train loss: 0.26223011  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.25 min) Epoch 90/300 -- Iteration 86571 - Batch 864/963 - Train loss: 0.26232305  - Train acc: 0.9011 - Val loss: 1.04518545\n",
      "(18.25 min) Epoch 90/300 -- Iteration 86580 - Batch 873/963 - Train loss: 0.26243330  - Train acc: 0.9010 - Val loss: 1.04518545\n",
      "(18.26 min) Epoch 90/300 -- Iteration 86589 - Batch 882/963 - Train loss: 0.26249453  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.26 min) Epoch 90/300 -- Iteration 86598 - Batch 891/963 - Train loss: 0.26216254  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.26 min) Epoch 90/300 -- Iteration 86607 - Batch 900/963 - Train loss: 0.26212321  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.26 min) Epoch 90/300 -- Iteration 86616 - Batch 909/963 - Train loss: 0.26187663  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.26 min) Epoch 90/300 -- Iteration 86625 - Batch 918/963 - Train loss: 0.26167985  - Train acc: 0.9009 - Val loss: 1.04518545\n",
      "(18.27 min) Epoch 90/300 -- Iteration 86634 - Batch 927/963 - Train loss: 0.26193285  - Train acc: 0.9008 - Val loss: 1.04518545\n",
      "(18.27 min) Epoch 90/300 -- Iteration 86643 - Batch 936/963 - Train loss: 0.26219554  - Train acc: 0.9007 - Val loss: 1.04518545\n",
      "(18.27 min) Epoch 90/300 -- Iteration 86652 - Batch 945/963 - Train loss: 0.26243072  - Train acc: 0.9005 - Val loss: 1.04518545\n",
      "(18.27 min) Epoch 90/300 -- Iteration 86661 - Batch 954/963 - Train loss: 0.26244836  - Train acc: 0.9005 - Val loss: 1.04518545\n",
      "(18.27 min) Epoch 90/300 -- Iteration 86670 - Batch 962/963 - Train loss: 0.26247826  - Train acc: 0.9005 - Val loss: 1.02641189 - Val acc: 0.5767\n",
      "(18.28 min) Epoch 91/300 -- Iteration 86679 - Batch 9/963 - Train loss: 0.26140511  - Train acc: 0.8984 - Val loss: 1.02641189\n",
      "(18.28 min) Epoch 91/300 -- Iteration 86688 - Batch 18/963 - Train loss: 0.24958363  - Train acc: 0.9075 - Val loss: 1.02641189\n",
      "(18.28 min) Epoch 91/300 -- Iteration 86697 - Batch 27/963 - Train loss: 0.24576234  - Train acc: 0.9088 - Val loss: 1.02641189\n",
      "(18.28 min) Epoch 91/300 -- Iteration 86706 - Batch 36/963 - Train loss: 0.25142467  - Train acc: 0.9065 - Val loss: 1.02641189\n",
      "(18.28 min) Epoch 91/300 -- Iteration 86715 - Batch 45/963 - Train loss: 0.25918273  - Train acc: 0.9020 - Val loss: 1.02641189\n",
      "(18.28 min) Epoch 91/300 -- Iteration 86724 - Batch 54/963 - Train loss: 0.25492842  - Train acc: 0.9045 - Val loss: 1.02641189\n",
      "(18.29 min) Epoch 91/300 -- Iteration 86733 - Batch 63/963 - Train loss: 0.25819983  - Train acc: 0.9025 - Val loss: 1.02641189\n",
      "(18.29 min) Epoch 91/300 -- Iteration 86742 - Batch 72/963 - Train loss: 0.25499631  - Train acc: 0.9039 - Val loss: 1.02641189\n",
      "(18.29 min) Epoch 91/300 -- Iteration 86751 - Batch 81/963 - Train loss: 0.25582827  - Train acc: 0.9038 - Val loss: 1.02641189\n",
      "(18.29 min) Epoch 91/300 -- Iteration 86760 - Batch 90/963 - Train loss: 0.25373448  - Train acc: 0.9044 - Val loss: 1.02641189\n",
      "(18.29 min) Epoch 91/300 -- Iteration 86769 - Batch 99/963 - Train loss: 0.25518838  - Train acc: 0.9040 - Val loss: 1.02641189\n",
      "(18.30 min) Epoch 91/300 -- Iteration 86778 - Batch 108/963 - Train loss: 0.25257357  - Train acc: 0.9050 - Val loss: 1.02641189\n",
      "(18.30 min) Epoch 91/300 -- Iteration 86787 - Batch 117/963 - Train loss: 0.25356765  - Train acc: 0.9048 - Val loss: 1.02641189\n",
      "(18.30 min) Epoch 91/300 -- Iteration 86796 - Batch 126/963 - Train loss: 0.25259284  - Train acc: 0.9057 - Val loss: 1.02641189\n",
      "(18.30 min) Epoch 91/300 -- Iteration 86805 - Batch 135/963 - Train loss: 0.25494047  - Train acc: 0.9046 - Val loss: 1.02641189\n",
      "(18.30 min) Epoch 91/300 -- Iteration 86814 - Batch 144/963 - Train loss: 0.25477024  - Train acc: 0.9046 - Val loss: 1.02641189\n",
      "(18.31 min) Epoch 91/300 -- Iteration 86823 - Batch 153/963 - Train loss: 0.25647330  - Train acc: 0.9038 - Val loss: 1.02641189\n",
      "(18.31 min) Epoch 91/300 -- Iteration 86832 - Batch 162/963 - Train loss: 0.25649737  - Train acc: 0.9037 - Val loss: 1.02641189\n",
      "(18.31 min) Epoch 91/300 -- Iteration 86841 - Batch 171/963 - Train loss: 0.25776040  - Train acc: 0.9031 - Val loss: 1.02641189\n",
      "(18.31 min) Epoch 91/300 -- Iteration 86850 - Batch 180/963 - Train loss: 0.25856928  - Train acc: 0.9027 - Val loss: 1.02641189\n",
      "(18.31 min) Epoch 91/300 -- Iteration 86859 - Batch 189/963 - Train loss: 0.25910889  - Train acc: 0.9028 - Val loss: 1.02641189\n",
      "(18.31 min) Epoch 91/300 -- Iteration 86868 - Batch 198/963 - Train loss: 0.25894709  - Train acc: 0.9031 - Val loss: 1.02641189\n",
      "(18.32 min) Epoch 91/300 -- Iteration 86877 - Batch 207/963 - Train loss: 0.25942399  - Train acc: 0.9029 - Val loss: 1.02641189\n",
      "(18.32 min) Epoch 91/300 -- Iteration 86886 - Batch 216/963 - Train loss: 0.26042037  - Train acc: 0.9026 - Val loss: 1.02641189\n",
      "(18.32 min) Epoch 91/300 -- Iteration 86895 - Batch 225/963 - Train loss: 0.26121719  - Train acc: 0.9022 - Val loss: 1.02641189\n",
      "(18.32 min) Epoch 91/300 -- Iteration 86904 - Batch 234/963 - Train loss: 0.26219301  - Train acc: 0.9018 - Val loss: 1.02641189\n",
      "(18.32 min) Epoch 91/300 -- Iteration 86913 - Batch 243/963 - Train loss: 0.26274056  - Train acc: 0.9016 - Val loss: 1.02641189\n",
      "(18.33 min) Epoch 91/300 -- Iteration 86922 - Batch 252/963 - Train loss: 0.26213285  - Train acc: 0.9016 - Val loss: 1.02641189\n",
      "(18.33 min) Epoch 91/300 -- Iteration 86931 - Batch 261/963 - Train loss: 0.26182112  - Train acc: 0.9018 - Val loss: 1.02641189\n",
      "(18.33 min) Epoch 91/300 -- Iteration 86940 - Batch 270/963 - Train loss: 0.26233382  - Train acc: 0.9016 - Val loss: 1.02641189\n",
      "(18.33 min) Epoch 91/300 -- Iteration 86949 - Batch 279/963 - Train loss: 0.26230047  - Train acc: 0.9017 - Val loss: 1.02641189\n",
      "(18.33 min) Epoch 91/300 -- Iteration 86958 - Batch 288/963 - Train loss: 0.26210974  - Train acc: 0.9016 - Val loss: 1.02641189\n",
      "(18.34 min) Epoch 91/300 -- Iteration 86967 - Batch 297/963 - Train loss: 0.26277008  - Train acc: 0.9015 - Val loss: 1.02641189\n",
      "(18.34 min) Epoch 91/300 -- Iteration 86976 - Batch 306/963 - Train loss: 0.26360764  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.34 min) Epoch 91/300 -- Iteration 86985 - Batch 315/963 - Train loss: 0.26391508  - Train acc: 0.9007 - Val loss: 1.02641189\n",
      "(18.34 min) Epoch 91/300 -- Iteration 86994 - Batch 324/963 - Train loss: 0.26355462  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.34 min) Epoch 91/300 -- Iteration 87003 - Batch 333/963 - Train loss: 0.26339979  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.34 min) Epoch 91/300 -- Iteration 87012 - Batch 342/963 - Train loss: 0.26362119  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.35 min) Epoch 91/300 -- Iteration 87021 - Batch 351/963 - Train loss: 0.26378466  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.35 min) Epoch 91/300 -- Iteration 87030 - Batch 360/963 - Train loss: 0.26383651  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.35 min) Epoch 91/300 -- Iteration 87039 - Batch 369/963 - Train loss: 0.26360482  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.35 min) Epoch 91/300 -- Iteration 87048 - Batch 378/963 - Train loss: 0.26344253  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.35 min) Epoch 91/300 -- Iteration 87057 - Batch 387/963 - Train loss: 0.26381661  - Train acc: 0.9008 - Val loss: 1.02641189\n",
      "(18.36 min) Epoch 91/300 -- Iteration 87066 - Batch 396/963 - Train loss: 0.26454166  - Train acc: 0.9003 - Val loss: 1.02641189\n",
      "(18.36 min) Epoch 91/300 -- Iteration 87075 - Batch 405/963 - Train loss: 0.26501932  - Train acc: 0.9001 - Val loss: 1.02641189\n",
      "(18.36 min) Epoch 91/300 -- Iteration 87084 - Batch 414/963 - Train loss: 0.26515041  - Train acc: 0.8999 - Val loss: 1.02641189\n",
      "(18.36 min) Epoch 91/300 -- Iteration 87093 - Batch 423/963 - Train loss: 0.26529203  - Train acc: 0.8999 - Val loss: 1.02641189\n",
      "(18.36 min) Epoch 91/300 -- Iteration 87102 - Batch 432/963 - Train loss: 0.26546783  - Train acc: 0.8996 - Val loss: 1.02641189\n",
      "(18.37 min) Epoch 91/300 -- Iteration 87111 - Batch 441/963 - Train loss: 0.26498565  - Train acc: 0.8998 - Val loss: 1.02641189\n",
      "(18.37 min) Epoch 91/300 -- Iteration 87120 - Batch 450/963 - Train loss: 0.26469756  - Train acc: 0.8999 - Val loss: 1.02641189\n",
      "(18.37 min) Epoch 91/300 -- Iteration 87129 - Batch 459/963 - Train loss: 0.26473829  - Train acc: 0.9001 - Val loss: 1.02641189\n",
      "(18.37 min) Epoch 91/300 -- Iteration 87138 - Batch 468/963 - Train loss: 0.26454389  - Train acc: 0.9003 - Val loss: 1.02641189\n",
      "(18.37 min) Epoch 91/300 -- Iteration 87147 - Batch 477/963 - Train loss: 0.26449571  - Train acc: 0.9003 - Val loss: 1.02641189\n",
      "(18.38 min) Epoch 91/300 -- Iteration 87156 - Batch 486/963 - Train loss: 0.26419083  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.38 min) Epoch 91/300 -- Iteration 87165 - Batch 495/963 - Train loss: 0.26452611  - Train acc: 0.9004 - Val loss: 1.02641189\n",
      "(18.38 min) Epoch 91/300 -- Iteration 87174 - Batch 504/963 - Train loss: 0.26411280  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.38 min) Epoch 91/300 -- Iteration 87183 - Batch 513/963 - Train loss: 0.26391799  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.38 min) Epoch 91/300 -- Iteration 87192 - Batch 522/963 - Train loss: 0.26394159  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.38 min) Epoch 91/300 -- Iteration 87201 - Batch 531/963 - Train loss: 0.26351086  - Train acc: 0.9007 - Val loss: 1.02641189\n",
      "(18.39 min) Epoch 91/300 -- Iteration 87210 - Batch 540/963 - Train loss: 0.26361355  - Train acc: 0.9006 - Val loss: 1.02641189\n",
      "(18.39 min) Epoch 91/300 -- Iteration 87219 - Batch 549/963 - Train loss: 0.26408134  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.39 min) Epoch 91/300 -- Iteration 87228 - Batch 558/963 - Train loss: 0.26409173  - Train acc: 0.9004 - Val loss: 1.02641189\n",
      "(18.39 min) Epoch 91/300 -- Iteration 87237 - Batch 567/963 - Train loss: 0.26384596  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.39 min) Epoch 91/300 -- Iteration 87246 - Batch 576/963 - Train loss: 0.26389061  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.40 min) Epoch 91/300 -- Iteration 87255 - Batch 585/963 - Train loss: 0.26366294  - Train acc: 0.9006 - Val loss: 1.02641189\n",
      "(18.40 min) Epoch 91/300 -- Iteration 87264 - Batch 594/963 - Train loss: 0.26373904  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.40 min) Epoch 91/300 -- Iteration 87273 - Batch 603/963 - Train loss: 0.26374565  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.40 min) Epoch 91/300 -- Iteration 87282 - Batch 612/963 - Train loss: 0.26364374  - Train acc: 0.9006 - Val loss: 1.02641189\n",
      "(18.40 min) Epoch 91/300 -- Iteration 87291 - Batch 621/963 - Train loss: 0.26335662  - Train acc: 0.9008 - Val loss: 1.02641189\n",
      "(18.41 min) Epoch 91/300 -- Iteration 87300 - Batch 630/963 - Train loss: 0.26322011  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.41 min) Epoch 91/300 -- Iteration 87309 - Batch 639/963 - Train loss: 0.26312427  - Train acc: 0.9011 - Val loss: 1.02641189\n",
      "(18.41 min) Epoch 91/300 -- Iteration 87318 - Batch 648/963 - Train loss: 0.26271314  - Train acc: 0.9013 - Val loss: 1.02641189\n",
      "(18.41 min) Epoch 91/300 -- Iteration 87327 - Batch 657/963 - Train loss: 0.26300298  - Train acc: 0.9011 - Val loss: 1.02641189\n",
      "(18.41 min) Epoch 91/300 -- Iteration 87336 - Batch 666/963 - Train loss: 0.26311716  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.41 min) Epoch 91/300 -- Iteration 87345 - Batch 675/963 - Train loss: 0.26295992  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.42 min) Epoch 91/300 -- Iteration 87354 - Batch 684/963 - Train loss: 0.26281455  - Train acc: 0.9011 - Val loss: 1.02641189\n",
      "(18.42 min) Epoch 91/300 -- Iteration 87363 - Batch 693/963 - Train loss: 0.26290872  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.42 min) Epoch 91/300 -- Iteration 87372 - Batch 702/963 - Train loss: 0.26292537  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.42 min) Epoch 91/300 -- Iteration 87381 - Batch 711/963 - Train loss: 0.26317021  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.42 min) Epoch 91/300 -- Iteration 87390 - Batch 720/963 - Train loss: 0.26335675  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.43 min) Epoch 91/300 -- Iteration 87399 - Batch 729/963 - Train loss: 0.26360713  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.43 min) Epoch 91/300 -- Iteration 87408 - Batch 738/963 - Train loss: 0.26341929  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.43 min) Epoch 91/300 -- Iteration 87417 - Batch 747/963 - Train loss: 0.26354211  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.43 min) Epoch 91/300 -- Iteration 87426 - Batch 756/963 - Train loss: 0.26338077  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.43 min) Epoch 91/300 -- Iteration 87435 - Batch 765/963 - Train loss: 0.26383618  - Train acc: 0.9008 - Val loss: 1.02641189\n",
      "(18.44 min) Epoch 91/300 -- Iteration 87444 - Batch 774/963 - Train loss: 0.26374227  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.44 min) Epoch 91/300 -- Iteration 87453 - Batch 783/963 - Train loss: 0.26352908  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.44 min) Epoch 91/300 -- Iteration 87462 - Batch 792/963 - Train loss: 0.26345515  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.44 min) Epoch 91/300 -- Iteration 87471 - Batch 801/963 - Train loss: 0.26359155  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.44 min) Epoch 91/300 -- Iteration 87480 - Batch 810/963 - Train loss: 0.26311467  - Train acc: 0.9011 - Val loss: 1.02641189\n",
      "(18.44 min) Epoch 91/300 -- Iteration 87489 - Batch 819/963 - Train loss: 0.26360418  - Train acc: 0.9008 - Val loss: 1.02641189\n",
      "(18.45 min) Epoch 91/300 -- Iteration 87498 - Batch 828/963 - Train loss: 0.26319994  - Train acc: 0.9010 - Val loss: 1.02641189\n",
      "(18.45 min) Epoch 91/300 -- Iteration 87507 - Batch 837/963 - Train loss: 0.26323401  - Train acc: 0.9009 - Val loss: 1.02641189\n",
      "(18.45 min) Epoch 91/300 -- Iteration 87516 - Batch 846/963 - Train loss: 0.26363041  - Train acc: 0.9007 - Val loss: 1.02641189\n",
      "(18.45 min) Epoch 91/300 -- Iteration 87525 - Batch 855/963 - Train loss: 0.26371079  - Train acc: 0.9007 - Val loss: 1.02641189\n",
      "(18.45 min) Epoch 91/300 -- Iteration 87534 - Batch 864/963 - Train loss: 0.26411462  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.46 min) Epoch 91/300 -- Iteration 87543 - Batch 873/963 - Train loss: 0.26424993  - Train acc: 0.9004 - Val loss: 1.02641189\n",
      "(18.46 min) Epoch 91/300 -- Iteration 87552 - Batch 882/963 - Train loss: 0.26402764  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.46 min) Epoch 91/300 -- Iteration 87561 - Batch 891/963 - Train loss: 0.26409474  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.46 min) Epoch 91/300 -- Iteration 87570 - Batch 900/963 - Train loss: 0.26404393  - Train acc: 0.9005 - Val loss: 1.02641189\n",
      "(18.46 min) Epoch 91/300 -- Iteration 87579 - Batch 909/963 - Train loss: 0.26402631  - Train acc: 0.9006 - Val loss: 1.02641189\n",
      "(18.47 min) Epoch 91/300 -- Iteration 87588 - Batch 918/963 - Train loss: 0.26390695  - Train acc: 0.9007 - Val loss: 1.02641189\n",
      "(18.47 min) Epoch 91/300 -- Iteration 87597 - Batch 927/963 - Train loss: 0.26395411  - Train acc: 0.9008 - Val loss: 1.02641189\n",
      "(18.47 min) Epoch 91/300 -- Iteration 87606 - Batch 936/963 - Train loss: 0.26407748  - Train acc: 0.9007 - Val loss: 1.02641189\n",
      "(18.47 min) Epoch 91/300 -- Iteration 87615 - Batch 945/963 - Train loss: 0.26376839  - Train acc: 0.9007 - Val loss: 1.02641189\n",
      "(18.47 min) Epoch 91/300 -- Iteration 87624 - Batch 954/963 - Train loss: 0.26388865  - Train acc: 0.9007 - Val loss: 1.02641189\n",
      "(18.48 min) Epoch 91/300 -- Iteration 87633 - Batch 962/963 - Train loss: 0.26368555  - Train acc: 0.9009 - Val loss: 1.02101052 - Val acc: 0.5783\n",
      "(18.48 min) Epoch 92/300 -- Iteration 87642 - Batch 9/963 - Train loss: 0.25518912  - Train acc: 0.9023 - Val loss: 1.02101052\n",
      "(18.48 min) Epoch 92/300 -- Iteration 87651 - Batch 18/963 - Train loss: 0.25430618  - Train acc: 0.9050 - Val loss: 1.02101052\n",
      "(18.48 min) Epoch 92/300 -- Iteration 87660 - Batch 27/963 - Train loss: 0.26182740  - Train acc: 0.9015 - Val loss: 1.02101052\n",
      "(18.48 min) Epoch 92/300 -- Iteration 87669 - Batch 36/963 - Train loss: 0.26888659  - Train acc: 0.8978 - Val loss: 1.02101052\n",
      "(18.49 min) Epoch 92/300 -- Iteration 87678 - Batch 45/963 - Train loss: 0.26945895  - Train acc: 0.8991 - Val loss: 1.02101052\n",
      "(18.49 min) Epoch 92/300 -- Iteration 87687 - Batch 54/963 - Train loss: 0.26670402  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.49 min) Epoch 92/300 -- Iteration 87696 - Batch 63/963 - Train loss: 0.26896320  - Train acc: 0.8997 - Val loss: 1.02101052\n",
      "(18.49 min) Epoch 92/300 -- Iteration 87705 - Batch 72/963 - Train loss: 0.26592812  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.49 min) Epoch 92/300 -- Iteration 87714 - Batch 81/963 - Train loss: 0.26635171  - Train acc: 0.8994 - Val loss: 1.02101052\n",
      "(18.50 min) Epoch 92/300 -- Iteration 87723 - Batch 90/963 - Train loss: 0.26394544  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.50 min) Epoch 92/300 -- Iteration 87732 - Batch 99/963 - Train loss: 0.26524147  - Train acc: 0.9011 - Val loss: 1.02101052\n",
      "(18.50 min) Epoch 92/300 -- Iteration 87741 - Batch 108/963 - Train loss: 0.26643589  - Train acc: 0.8996 - Val loss: 1.02101052\n",
      "(18.50 min) Epoch 92/300 -- Iteration 87750 - Batch 117/963 - Train loss: 0.26701256  - Train acc: 0.8999 - Val loss: 1.02101052\n",
      "(18.50 min) Epoch 92/300 -- Iteration 87759 - Batch 126/963 - Train loss: 0.26899205  - Train acc: 0.8991 - Val loss: 1.02101052\n",
      "(18.50 min) Epoch 92/300 -- Iteration 87768 - Batch 135/963 - Train loss: 0.26736223  - Train acc: 0.8999 - Val loss: 1.02101052\n",
      "(18.51 min) Epoch 92/300 -- Iteration 87777 - Batch 144/963 - Train loss: 0.26852333  - Train acc: 0.8994 - Val loss: 1.02101052\n",
      "(18.51 min) Epoch 92/300 -- Iteration 87786 - Batch 153/963 - Train loss: 0.26869020  - Train acc: 0.8988 - Val loss: 1.02101052\n",
      "(18.51 min) Epoch 92/300 -- Iteration 87795 - Batch 162/963 - Train loss: 0.26723106  - Train acc: 0.8994 - Val loss: 1.02101052\n",
      "(18.51 min) Epoch 92/300 -- Iteration 87804 - Batch 171/963 - Train loss: 0.26676502  - Train acc: 0.8990 - Val loss: 1.02101052\n",
      "(18.51 min) Epoch 92/300 -- Iteration 87813 - Batch 180/963 - Train loss: 0.26830645  - Train acc: 0.8985 - Val loss: 1.02101052\n",
      "(18.52 min) Epoch 92/300 -- Iteration 87822 - Batch 189/963 - Train loss: 0.26745184  - Train acc: 0.8988 - Val loss: 1.02101052\n",
      "(18.52 min) Epoch 92/300 -- Iteration 87831 - Batch 198/963 - Train loss: 0.26684691  - Train acc: 0.8991 - Val loss: 1.02101052\n",
      "(18.52 min) Epoch 92/300 -- Iteration 87840 - Batch 207/963 - Train loss: 0.26652885  - Train acc: 0.8992 - Val loss: 1.02101052\n",
      "(18.52 min) Epoch 92/300 -- Iteration 87849 - Batch 216/963 - Train loss: 0.26661157  - Train acc: 0.8989 - Val loss: 1.02101052\n",
      "(18.52 min) Epoch 92/300 -- Iteration 87858 - Batch 225/963 - Train loss: 0.26577975  - Train acc: 0.8992 - Val loss: 1.02101052\n",
      "(18.53 min) Epoch 92/300 -- Iteration 87867 - Batch 234/963 - Train loss: 0.26659456  - Train acc: 0.8985 - Val loss: 1.02101052\n",
      "(18.53 min) Epoch 92/300 -- Iteration 87876 - Batch 243/963 - Train loss: 0.26629409  - Train acc: 0.8986 - Val loss: 1.02101052\n",
      "(18.53 min) Epoch 92/300 -- Iteration 87885 - Batch 252/963 - Train loss: 0.26613149  - Train acc: 0.8987 - Val loss: 1.02101052\n",
      "(18.53 min) Epoch 92/300 -- Iteration 87894 - Batch 261/963 - Train loss: 0.26612467  - Train acc: 0.8985 - Val loss: 1.02101052\n",
      "(18.53 min) Epoch 92/300 -- Iteration 87903 - Batch 270/963 - Train loss: 0.26677183  - Train acc: 0.8982 - Val loss: 1.02101052\n",
      "(18.54 min) Epoch 92/300 -- Iteration 87912 - Batch 279/963 - Train loss: 0.26726359  - Train acc: 0.8984 - Val loss: 1.02101052\n",
      "(18.54 min) Epoch 92/300 -- Iteration 87921 - Batch 288/963 - Train loss: 0.26800276  - Train acc: 0.8981 - Val loss: 1.02101052\n",
      "(18.54 min) Epoch 92/300 -- Iteration 87930 - Batch 297/963 - Train loss: 0.26709049  - Train acc: 0.8986 - Val loss: 1.02101052\n",
      "(18.54 min) Epoch 92/300 -- Iteration 87939 - Batch 306/963 - Train loss: 0.26585814  - Train acc: 0.8993 - Val loss: 1.02101052\n",
      "(18.54 min) Epoch 92/300 -- Iteration 87948 - Batch 315/963 - Train loss: 0.26610737  - Train acc: 0.8990 - Val loss: 1.02101052\n",
      "(18.55 min) Epoch 92/300 -- Iteration 87957 - Batch 324/963 - Train loss: 0.26532043  - Train acc: 0.8993 - Val loss: 1.02101052\n",
      "(18.55 min) Epoch 92/300 -- Iteration 87966 - Batch 333/963 - Train loss: 0.26442864  - Train acc: 0.8998 - Val loss: 1.02101052\n",
      "(18.55 min) Epoch 92/300 -- Iteration 87975 - Batch 342/963 - Train loss: 0.26369359  - Train acc: 0.9000 - Val loss: 1.02101052\n",
      "(18.55 min) Epoch 92/300 -- Iteration 87984 - Batch 351/963 - Train loss: 0.26279583  - Train acc: 0.9004 - Val loss: 1.02101052\n",
      "(18.55 min) Epoch 92/300 -- Iteration 87993 - Batch 360/963 - Train loss: 0.26325436  - Train acc: 0.9003 - Val loss: 1.02101052\n",
      "(18.56 min) Epoch 92/300 -- Iteration 88002 - Batch 369/963 - Train loss: 0.26362591  - Train acc: 0.9001 - Val loss: 1.02101052\n",
      "(18.56 min) Epoch 92/300 -- Iteration 88011 - Batch 378/963 - Train loss: 0.26397205  - Train acc: 0.8999 - Val loss: 1.02101052\n",
      "(18.56 min) Epoch 92/300 -- Iteration 88020 - Batch 387/963 - Train loss: 0.26361112  - Train acc: 0.9001 - Val loss: 1.02101052\n",
      "(18.56 min) Epoch 92/300 -- Iteration 88029 - Batch 396/963 - Train loss: 0.26323360  - Train acc: 0.9003 - Val loss: 1.02101052\n",
      "(18.56 min) Epoch 92/300 -- Iteration 88038 - Batch 405/963 - Train loss: 0.26309287  - Train acc: 0.9004 - Val loss: 1.02101052\n",
      "(18.57 min) Epoch 92/300 -- Iteration 88047 - Batch 414/963 - Train loss: 0.26263317  - Train acc: 0.9007 - Val loss: 1.02101052\n",
      "(18.57 min) Epoch 92/300 -- Iteration 88056 - Batch 423/963 - Train loss: 0.26250603  - Train acc: 0.9007 - Val loss: 1.02101052\n",
      "(18.57 min) Epoch 92/300 -- Iteration 88065 - Batch 432/963 - Train loss: 0.26279889  - Train acc: 0.9005 - Val loss: 1.02101052\n",
      "(18.57 min) Epoch 92/300 -- Iteration 88074 - Batch 441/963 - Train loss: 0.26295212  - Train acc: 0.9005 - Val loss: 1.02101052\n",
      "(18.57 min) Epoch 92/300 -- Iteration 88083 - Batch 450/963 - Train loss: 0.26231321  - Train acc: 0.9007 - Val loss: 1.02101052\n",
      "(18.57 min) Epoch 92/300 -- Iteration 88092 - Batch 459/963 - Train loss: 0.26212682  - Train acc: 0.9007 - Val loss: 1.02101052\n",
      "(18.58 min) Epoch 92/300 -- Iteration 88101 - Batch 468/963 - Train loss: 0.26212643  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.58 min) Epoch 92/300 -- Iteration 88110 - Batch 477/963 - Train loss: 0.26221596  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.58 min) Epoch 92/300 -- Iteration 88119 - Batch 486/963 - Train loss: 0.26265778  - Train acc: 0.9007 - Val loss: 1.02101052\n",
      "(18.58 min) Epoch 92/300 -- Iteration 88128 - Batch 495/963 - Train loss: 0.26254908  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.58 min) Epoch 92/300 -- Iteration 88137 - Batch 504/963 - Train loss: 0.26224057  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.59 min) Epoch 92/300 -- Iteration 88146 - Batch 513/963 - Train loss: 0.26245286  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.59 min) Epoch 92/300 -- Iteration 88155 - Batch 522/963 - Train loss: 0.26249396  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.59 min) Epoch 92/300 -- Iteration 88164 - Batch 531/963 - Train loss: 0.26245964  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.59 min) Epoch 92/300 -- Iteration 88173 - Batch 540/963 - Train loss: 0.26228033  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.59 min) Epoch 92/300 -- Iteration 88182 - Batch 549/963 - Train loss: 0.26239738  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.60 min) Epoch 92/300 -- Iteration 88191 - Batch 558/963 - Train loss: 0.26261762  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.60 min) Epoch 92/300 -- Iteration 88200 - Batch 567/963 - Train loss: 0.26227210  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.60 min) Epoch 92/300 -- Iteration 88209 - Batch 576/963 - Train loss: 0.26231612  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.60 min) Epoch 92/300 -- Iteration 88218 - Batch 585/963 - Train loss: 0.26239906  - Train acc: 0.9006 - Val loss: 1.02101052\n",
      "(18.60 min) Epoch 92/300 -- Iteration 88227 - Batch 594/963 - Train loss: 0.26210424  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.61 min) Epoch 92/300 -- Iteration 88236 - Batch 603/963 - Train loss: 0.26167358  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.61 min) Epoch 92/300 -- Iteration 88245 - Batch 612/963 - Train loss: 0.26162802  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.61 min) Epoch 92/300 -- Iteration 88254 - Batch 621/963 - Train loss: 0.26201539  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.61 min) Epoch 92/300 -- Iteration 88263 - Batch 630/963 - Train loss: 0.26180597  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.61 min) Epoch 92/300 -- Iteration 88272 - Batch 639/963 - Train loss: 0.26204617  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.62 min) Epoch 92/300 -- Iteration 88281 - Batch 648/963 - Train loss: 0.26172519  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.62 min) Epoch 92/300 -- Iteration 88290 - Batch 657/963 - Train loss: 0.26192790  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.62 min) Epoch 92/300 -- Iteration 88299 - Batch 666/963 - Train loss: 0.26197953  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.62 min) Epoch 92/300 -- Iteration 88308 - Batch 675/963 - Train loss: 0.26205921  - Train acc: 0.9008 - Val loss: 1.02101052\n",
      "(18.62 min) Epoch 92/300 -- Iteration 88317 - Batch 684/963 - Train loss: 0.26189837  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.62 min) Epoch 92/300 -- Iteration 88326 - Batch 693/963 - Train loss: 0.26182661  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.63 min) Epoch 92/300 -- Iteration 88335 - Batch 702/963 - Train loss: 0.26198786  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.63 min) Epoch 92/300 -- Iteration 88344 - Batch 711/963 - Train loss: 0.26178983  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.63 min) Epoch 92/300 -- Iteration 88353 - Batch 720/963 - Train loss: 0.26226085  - Train acc: 0.9007 - Val loss: 1.02101052\n",
      "(18.63 min) Epoch 92/300 -- Iteration 88362 - Batch 729/963 - Train loss: 0.26200927  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.63 min) Epoch 92/300 -- Iteration 88371 - Batch 738/963 - Train loss: 0.26182948  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.64 min) Epoch 92/300 -- Iteration 88380 - Batch 747/963 - Train loss: 0.26176599  - Train acc: 0.9011 - Val loss: 1.02101052\n",
      "(18.64 min) Epoch 92/300 -- Iteration 88389 - Batch 756/963 - Train loss: 0.26225972  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.64 min) Epoch 92/300 -- Iteration 88398 - Batch 765/963 - Train loss: 0.26223078  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.64 min) Epoch 92/300 -- Iteration 88407 - Batch 774/963 - Train loss: 0.26222535  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.64 min) Epoch 92/300 -- Iteration 88416 - Batch 783/963 - Train loss: 0.26198095  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.65 min) Epoch 92/300 -- Iteration 88425 - Batch 792/963 - Train loss: 0.26196295  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.65 min) Epoch 92/300 -- Iteration 88434 - Batch 801/963 - Train loss: 0.26196992  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.65 min) Epoch 92/300 -- Iteration 88443 - Batch 810/963 - Train loss: 0.26202755  - Train acc: 0.9011 - Val loss: 1.02101052\n",
      "(18.65 min) Epoch 92/300 -- Iteration 88452 - Batch 819/963 - Train loss: 0.26198871  - Train acc: 0.9012 - Val loss: 1.02101052\n",
      "(18.65 min) Epoch 92/300 -- Iteration 88461 - Batch 828/963 - Train loss: 0.26199854  - Train acc: 0.9012 - Val loss: 1.02101052\n",
      "(18.65 min) Epoch 92/300 -- Iteration 88470 - Batch 837/963 - Train loss: 0.26196917  - Train acc: 0.9013 - Val loss: 1.02101052\n",
      "(18.66 min) Epoch 92/300 -- Iteration 88479 - Batch 846/963 - Train loss: 0.26214320  - Train acc: 0.9011 - Val loss: 1.02101052\n",
      "(18.66 min) Epoch 92/300 -- Iteration 88488 - Batch 855/963 - Train loss: 0.26244739  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.66 min) Epoch 92/300 -- Iteration 88497 - Batch 864/963 - Train loss: 0.26240071  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.66 min) Epoch 92/300 -- Iteration 88506 - Batch 873/963 - Train loss: 0.26217744  - Train acc: 0.9011 - Val loss: 1.02101052\n",
      "(18.66 min) Epoch 92/300 -- Iteration 88515 - Batch 882/963 - Train loss: 0.26226546  - Train acc: 0.9011 - Val loss: 1.02101052\n",
      "(18.67 min) Epoch 92/300 -- Iteration 88524 - Batch 891/963 - Train loss: 0.26237649  - Train acc: 0.9009 - Val loss: 1.02101052\n",
      "(18.67 min) Epoch 92/300 -- Iteration 88533 - Batch 900/963 - Train loss: 0.26197143  - Train acc: 0.9010 - Val loss: 1.02101052\n",
      "(18.67 min) Epoch 92/300 -- Iteration 88542 - Batch 909/963 - Train loss: 0.26171508  - Train acc: 0.9011 - Val loss: 1.02101052\n",
      "(18.67 min) Epoch 92/300 -- Iteration 88551 - Batch 918/963 - Train loss: 0.26154606  - Train acc: 0.9013 - Val loss: 1.02101052\n",
      "(18.67 min) Epoch 92/300 -- Iteration 88560 - Batch 927/963 - Train loss: 0.26155984  - Train acc: 0.9012 - Val loss: 1.02101052\n",
      "(18.68 min) Epoch 92/300 -- Iteration 88569 - Batch 936/963 - Train loss: 0.26138935  - Train acc: 0.9014 - Val loss: 1.02101052\n",
      "(18.68 min) Epoch 92/300 -- Iteration 88578 - Batch 945/963 - Train loss: 0.26135896  - Train acc: 0.9015 - Val loss: 1.02101052\n",
      "(18.68 min) Epoch 92/300 -- Iteration 88587 - Batch 954/963 - Train loss: 0.26140802  - Train acc: 0.9014 - Val loss: 1.02101052\n",
      "(18.68 min) Epoch 92/300 -- Iteration 88596 - Batch 962/963 - Train loss: 0.26159006  - Train acc: 0.9014 - Val loss: 1.00055468 - Val acc: 0.5817\n",
      "(18.68 min) Epoch 93/300 -- Iteration 88605 - Batch 9/963 - Train loss: 0.25555155  - Train acc: 0.9062 - Val loss: 1.00055468\n",
      "(18.69 min) Epoch 93/300 -- Iteration 88614 - Batch 18/963 - Train loss: 0.27433821  - Train acc: 0.8964 - Val loss: 1.00055468\n",
      "(18.69 min) Epoch 93/300 -- Iteration 88623 - Batch 27/963 - Train loss: 0.26020201  - Train acc: 0.9051 - Val loss: 1.00055468\n",
      "(18.69 min) Epoch 93/300 -- Iteration 88632 - Batch 36/963 - Train loss: 0.26410371  - Train acc: 0.9027 - Val loss: 1.00055468\n",
      "(18.69 min) Epoch 93/300 -- Iteration 88641 - Batch 45/963 - Train loss: 0.25782512  - Train acc: 0.9032 - Val loss: 1.00055468\n",
      "(18.69 min) Epoch 93/300 -- Iteration 88650 - Batch 54/963 - Train loss: 0.25997956  - Train acc: 0.9026 - Val loss: 1.00055468\n",
      "(18.69 min) Epoch 93/300 -- Iteration 88659 - Batch 63/963 - Train loss: 0.25940761  - Train acc: 0.9030 - Val loss: 1.00055468\n",
      "(18.70 min) Epoch 93/300 -- Iteration 88668 - Batch 72/963 - Train loss: 0.25697393  - Train acc: 0.9043 - Val loss: 1.00055468\n",
      "(18.70 min) Epoch 93/300 -- Iteration 88677 - Batch 81/963 - Train loss: 0.26165469  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.70 min) Epoch 93/300 -- Iteration 88686 - Batch 90/963 - Train loss: 0.25961993  - Train acc: 0.9034 - Val loss: 1.00055468\n",
      "(18.70 min) Epoch 93/300 -- Iteration 88695 - Batch 99/963 - Train loss: 0.26153747  - Train acc: 0.9028 - Val loss: 1.00055468\n",
      "(18.70 min) Epoch 93/300 -- Iteration 88704 - Batch 108/963 - Train loss: 0.26333984  - Train acc: 0.9022 - Val loss: 1.00055468\n",
      "(18.71 min) Epoch 93/300 -- Iteration 88713 - Batch 117/963 - Train loss: 0.26114787  - Train acc: 0.9032 - Val loss: 1.00055468\n",
      "(18.71 min) Epoch 93/300 -- Iteration 88722 - Batch 126/963 - Train loss: 0.26068517  - Train acc: 0.9034 - Val loss: 1.00055468\n",
      "(18.71 min) Epoch 93/300 -- Iteration 88731 - Batch 135/963 - Train loss: 0.25977164  - Train acc: 0.9037 - Val loss: 1.00055468\n",
      "(18.71 min) Epoch 93/300 -- Iteration 88740 - Batch 144/963 - Train loss: 0.25920024  - Train acc: 0.9039 - Val loss: 1.00055468\n",
      "(18.71 min) Epoch 93/300 -- Iteration 88749 - Batch 153/963 - Train loss: 0.26066748  - Train acc: 0.9032 - Val loss: 1.00055468\n",
      "(18.72 min) Epoch 93/300 -- Iteration 88758 - Batch 162/963 - Train loss: 0.26236988  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.72 min) Epoch 93/300 -- Iteration 88767 - Batch 171/963 - Train loss: 0.26244568  - Train acc: 0.9020 - Val loss: 1.00055468\n",
      "(18.72 min) Epoch 93/300 -- Iteration 88776 - Batch 180/963 - Train loss: 0.26168329  - Train acc: 0.9021 - Val loss: 1.00055468\n",
      "(18.72 min) Epoch 93/300 -- Iteration 88785 - Batch 189/963 - Train loss: 0.26157054  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.72 min) Epoch 93/300 -- Iteration 88794 - Batch 198/963 - Train loss: 0.26150682  - Train acc: 0.9022 - Val loss: 1.00055468\n",
      "(18.72 min) Epoch 93/300 -- Iteration 88803 - Batch 207/963 - Train loss: 0.26313890  - Train acc: 0.9014 - Val loss: 1.00055468\n",
      "(18.73 min) Epoch 93/300 -- Iteration 88812 - Batch 216/963 - Train loss: 0.26284354  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.73 min) Epoch 93/300 -- Iteration 88821 - Batch 225/963 - Train loss: 0.26182818  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.73 min) Epoch 93/300 -- Iteration 88830 - Batch 234/963 - Train loss: 0.26180296  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.73 min) Epoch 93/300 -- Iteration 88839 - Batch 243/963 - Train loss: 0.26196533  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.73 min) Epoch 93/300 -- Iteration 88848 - Batch 252/963 - Train loss: 0.26232121  - Train acc: 0.9020 - Val loss: 1.00055468\n",
      "(18.74 min) Epoch 93/300 -- Iteration 88857 - Batch 261/963 - Train loss: 0.26253198  - Train acc: 0.9015 - Val loss: 1.00055468\n",
      "(18.74 min) Epoch 93/300 -- Iteration 88866 - Batch 270/963 - Train loss: 0.26171217  - Train acc: 0.9019 - Val loss: 1.00055468\n",
      "(18.74 min) Epoch 93/300 -- Iteration 88875 - Batch 279/963 - Train loss: 0.26146867  - Train acc: 0.9020 - Val loss: 1.00055468\n",
      "(18.74 min) Epoch 93/300 -- Iteration 88884 - Batch 288/963 - Train loss: 0.26143565  - Train acc: 0.9021 - Val loss: 1.00055468\n",
      "(18.74 min) Epoch 93/300 -- Iteration 88893 - Batch 297/963 - Train loss: 0.26292618  - Train acc: 0.9017 - Val loss: 1.00055468\n",
      "(18.75 min) Epoch 93/300 -- Iteration 88902 - Batch 306/963 - Train loss: 0.26254705  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.75 min) Epoch 93/300 -- Iteration 88911 - Batch 315/963 - Train loss: 0.26227577  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.75 min) Epoch 93/300 -- Iteration 88920 - Batch 324/963 - Train loss: 0.26241209  - Train acc: 0.9013 - Val loss: 1.00055468\n",
      "(18.75 min) Epoch 93/300 -- Iteration 88929 - Batch 333/963 - Train loss: 0.26274827  - Train acc: 0.9011 - Val loss: 1.00055468\n",
      "(18.75 min) Epoch 93/300 -- Iteration 88938 - Batch 342/963 - Train loss: 0.26278481  - Train acc: 0.9011 - Val loss: 1.00055468\n",
      "(18.76 min) Epoch 93/300 -- Iteration 88947 - Batch 351/963 - Train loss: 0.26191149  - Train acc: 0.9014 - Val loss: 1.00055468\n",
      "(18.76 min) Epoch 93/300 -- Iteration 88956 - Batch 360/963 - Train loss: 0.26132783  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.76 min) Epoch 93/300 -- Iteration 88965 - Batch 369/963 - Train loss: 0.26091843  - Train acc: 0.9018 - Val loss: 1.00055468\n",
      "(18.76 min) Epoch 93/300 -- Iteration 88974 - Batch 378/963 - Train loss: 0.26183730  - Train acc: 0.9015 - Val loss: 1.00055468\n",
      "(18.76 min) Epoch 93/300 -- Iteration 88983 - Batch 387/963 - Train loss: 0.26167257  - Train acc: 0.9017 - Val loss: 1.00055468\n",
      "(18.77 min) Epoch 93/300 -- Iteration 88992 - Batch 396/963 - Train loss: 0.26124379  - Train acc: 0.9019 - Val loss: 1.00055468\n",
      "(18.77 min) Epoch 93/300 -- Iteration 89001 - Batch 405/963 - Train loss: 0.26194853  - Train acc: 0.9015 - Val loss: 1.00055468\n",
      "(18.77 min) Epoch 93/300 -- Iteration 89010 - Batch 414/963 - Train loss: 0.26213689  - Train acc: 0.9015 - Val loss: 1.00055468\n",
      "(18.77 min) Epoch 93/300 -- Iteration 89019 - Batch 423/963 - Train loss: 0.26221080  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.77 min) Epoch 93/300 -- Iteration 89028 - Batch 432/963 - Train loss: 0.26257043  - Train acc: 0.9015 - Val loss: 1.00055468\n",
      "(18.77 min) Epoch 93/300 -- Iteration 89037 - Batch 441/963 - Train loss: 0.26254351  - Train acc: 0.9014 - Val loss: 1.00055468\n",
      "(18.78 min) Epoch 93/300 -- Iteration 89046 - Batch 450/963 - Train loss: 0.26298216  - Train acc: 0.9012 - Val loss: 1.00055468\n",
      "(18.78 min) Epoch 93/300 -- Iteration 89055 - Batch 459/963 - Train loss: 0.26316278  - Train acc: 0.9014 - Val loss: 1.00055468\n",
      "(18.78 min) Epoch 93/300 -- Iteration 89064 - Batch 468/963 - Train loss: 0.26300718  - Train acc: 0.9015 - Val loss: 1.00055468\n",
      "(18.78 min) Epoch 93/300 -- Iteration 89073 - Batch 477/963 - Train loss: 0.26301519  - Train acc: 0.9014 - Val loss: 1.00055468\n",
      "(18.78 min) Epoch 93/300 -- Iteration 89082 - Batch 486/963 - Train loss: 0.26316764  - Train acc: 0.9014 - Val loss: 1.00055468\n",
      "(18.79 min) Epoch 93/300 -- Iteration 89091 - Batch 495/963 - Train loss: 0.26288754  - Train acc: 0.9018 - Val loss: 1.00055468\n",
      "(18.79 min) Epoch 93/300 -- Iteration 89100 - Batch 504/963 - Train loss: 0.26339136  - Train acc: 0.9015 - Val loss: 1.00055468\n",
      "(18.79 min) Epoch 93/300 -- Iteration 89109 - Batch 513/963 - Train loss: 0.26284974  - Train acc: 0.9018 - Val loss: 1.00055468\n",
      "(18.79 min) Epoch 93/300 -- Iteration 89118 - Batch 522/963 - Train loss: 0.26258215  - Train acc: 0.9019 - Val loss: 1.00055468\n",
      "(18.79 min) Epoch 93/300 -- Iteration 89127 - Batch 531/963 - Train loss: 0.26212866  - Train acc: 0.9022 - Val loss: 1.00055468\n",
      "(18.80 min) Epoch 93/300 -- Iteration 89136 - Batch 540/963 - Train loss: 0.26249213  - Train acc: 0.9020 - Val loss: 1.00055468\n",
      "(18.80 min) Epoch 93/300 -- Iteration 89145 - Batch 549/963 - Train loss: 0.26251778  - Train acc: 0.9019 - Val loss: 1.00055468\n",
      "(18.80 min) Epoch 93/300 -- Iteration 89154 - Batch 558/963 - Train loss: 0.26250820  - Train acc: 0.9019 - Val loss: 1.00055468\n",
      "(18.80 min) Epoch 93/300 -- Iteration 89163 - Batch 567/963 - Train loss: 0.26232782  - Train acc: 0.9020 - Val loss: 1.00055468\n",
      "(18.80 min) Epoch 93/300 -- Iteration 89172 - Batch 576/963 - Train loss: 0.26248934  - Train acc: 0.9019 - Val loss: 1.00055468\n",
      "(18.80 min) Epoch 93/300 -- Iteration 89181 - Batch 585/963 - Train loss: 0.26198946  - Train acc: 0.9021 - Val loss: 1.00055468\n",
      "(18.81 min) Epoch 93/300 -- Iteration 89190 - Batch 594/963 - Train loss: 0.26151384  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.81 min) Epoch 93/300 -- Iteration 89199 - Batch 603/963 - Train loss: 0.26157553  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.81 min) Epoch 93/300 -- Iteration 89208 - Batch 612/963 - Train loss: 0.26141160  - Train acc: 0.9024 - Val loss: 1.00055468\n",
      "(18.81 min) Epoch 93/300 -- Iteration 89217 - Batch 621/963 - Train loss: 0.26106268  - Train acc: 0.9025 - Val loss: 1.00055468\n",
      "(18.81 min) Epoch 93/300 -- Iteration 89226 - Batch 630/963 - Train loss: 0.26123757  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.82 min) Epoch 93/300 -- Iteration 89235 - Batch 639/963 - Train loss: 0.26065003  - Train acc: 0.9026 - Val loss: 1.00055468\n",
      "(18.82 min) Epoch 93/300 -- Iteration 89244 - Batch 648/963 - Train loss: 0.26034943  - Train acc: 0.9027 - Val loss: 1.00055468\n",
      "(18.82 min) Epoch 93/300 -- Iteration 89253 - Batch 657/963 - Train loss: 0.26076802  - Train acc: 0.9025 - Val loss: 1.00055468\n",
      "(18.82 min) Epoch 93/300 -- Iteration 89262 - Batch 666/963 - Train loss: 0.26077145  - Train acc: 0.9025 - Val loss: 1.00055468\n",
      "(18.82 min) Epoch 93/300 -- Iteration 89271 - Batch 675/963 - Train loss: 0.26056835  - Train acc: 0.9026 - Val loss: 1.00055468\n",
      "(18.83 min) Epoch 93/300 -- Iteration 89280 - Batch 684/963 - Train loss: 0.26078451  - Train acc: 0.9024 - Val loss: 1.00055468\n",
      "(18.83 min) Epoch 93/300 -- Iteration 89289 - Batch 693/963 - Train loss: 0.26083782  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.83 min) Epoch 93/300 -- Iteration 89298 - Batch 702/963 - Train loss: 0.26095309  - Train acc: 0.9022 - Val loss: 1.00055468\n",
      "(18.83 min) Epoch 93/300 -- Iteration 89307 - Batch 711/963 - Train loss: 0.26082790  - Train acc: 0.9022 - Val loss: 1.00055468\n",
      "(18.83 min) Epoch 93/300 -- Iteration 89316 - Batch 720/963 - Train loss: 0.26064311  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.83 min) Epoch 93/300 -- Iteration 89325 - Batch 729/963 - Train loss: 0.26050370  - Train acc: 0.9024 - Val loss: 1.00055468\n",
      "(18.84 min) Epoch 93/300 -- Iteration 89334 - Batch 738/963 - Train loss: 0.26066764  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.84 min) Epoch 93/300 -- Iteration 89343 - Batch 747/963 - Train loss: 0.26068804  - Train acc: 0.9023 - Val loss: 1.00055468\n",
      "(18.84 min) Epoch 93/300 -- Iteration 89352 - Batch 756/963 - Train loss: 0.26070692  - Train acc: 0.9024 - Val loss: 1.00055468\n",
      "(18.84 min) Epoch 93/300 -- Iteration 89361 - Batch 765/963 - Train loss: 0.26076227  - Train acc: 0.9024 - Val loss: 1.00055468\n",
      "(18.84 min) Epoch 93/300 -- Iteration 89370 - Batch 774/963 - Train loss: 0.26044724  - Train acc: 0.9025 - Val loss: 1.00055468\n",
      "(18.85 min) Epoch 93/300 -- Iteration 89379 - Batch 783/963 - Train loss: 0.26023026  - Train acc: 0.9026 - Val loss: 1.00055468\n",
      "(18.85 min) Epoch 93/300 -- Iteration 89388 - Batch 792/963 - Train loss: 0.26033845  - Train acc: 0.9026 - Val loss: 1.00055468\n",
      "(18.85 min) Epoch 93/300 -- Iteration 89397 - Batch 801/963 - Train loss: 0.26042709  - Train acc: 0.9026 - Val loss: 1.00055468\n",
      "(18.85 min) Epoch 93/300 -- Iteration 89406 - Batch 810/963 - Train loss: 0.26058701  - Train acc: 0.9025 - Val loss: 1.00055468\n",
      "(18.85 min) Epoch 93/300 -- Iteration 89415 - Batch 819/963 - Train loss: 0.26043458  - Train acc: 0.9026 - Val loss: 1.00055468\n",
      "(18.86 min) Epoch 93/300 -- Iteration 89424 - Batch 828/963 - Train loss: 0.26054509  - Train acc: 0.9025 - Val loss: 1.00055468\n",
      "(18.86 min) Epoch 93/300 -- Iteration 89433 - Batch 837/963 - Train loss: 0.26034349  - Train acc: 0.9026 - Val loss: 1.00055468\n",
      "(18.86 min) Epoch 93/300 -- Iteration 89442 - Batch 846/963 - Train loss: 0.26080833  - Train acc: 0.9024 - Val loss: 1.00055468\n",
      "(18.86 min) Epoch 93/300 -- Iteration 89451 - Batch 855/963 - Train loss: 0.26097758  - Train acc: 0.9022 - Val loss: 1.00055468\n",
      "(18.86 min) Epoch 93/300 -- Iteration 89460 - Batch 864/963 - Train loss: 0.26157711  - Train acc: 0.9020 - Val loss: 1.00055468\n",
      "(18.87 min) Epoch 93/300 -- Iteration 89469 - Batch 873/963 - Train loss: 0.26171439  - Train acc: 0.9018 - Val loss: 1.00055468\n",
      "(18.87 min) Epoch 93/300 -- Iteration 89478 - Batch 882/963 - Train loss: 0.26165817  - Train acc: 0.9018 - Val loss: 1.00055468\n",
      "(18.87 min) Epoch 93/300 -- Iteration 89487 - Batch 891/963 - Train loss: 0.26179611  - Train acc: 0.9017 - Val loss: 1.00055468\n",
      "(18.87 min) Epoch 93/300 -- Iteration 89496 - Batch 900/963 - Train loss: 0.26170933  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.87 min) Epoch 93/300 -- Iteration 89505 - Batch 909/963 - Train loss: 0.26190429  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.87 min) Epoch 93/300 -- Iteration 89514 - Batch 918/963 - Train loss: 0.26173991  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.88 min) Epoch 93/300 -- Iteration 89523 - Batch 927/963 - Train loss: 0.26175982  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.88 min) Epoch 93/300 -- Iteration 89532 - Batch 936/963 - Train loss: 0.26183734  - Train acc: 0.9016 - Val loss: 1.00055468\n",
      "(18.88 min) Epoch 93/300 -- Iteration 89541 - Batch 945/963 - Train loss: 0.26218920  - Train acc: 0.9015 - Val loss: 1.00055468\n",
      "(18.88 min) Epoch 93/300 -- Iteration 89550 - Batch 954/963 - Train loss: 0.26204894  - Train acc: 0.9015 - Val loss: 1.00055468\n",
      "(18.88 min) Epoch 93/300 -- Iteration 89559 - Batch 962/963 - Train loss: 0.26193390  - Train acc: 0.9016 - Val loss: 1.01989067 - Val acc: 0.5817\n",
      "(18.89 min) Epoch 94/300 -- Iteration 89568 - Batch 9/963 - Train loss: 0.26510636  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(18.89 min) Epoch 94/300 -- Iteration 89577 - Batch 18/963 - Train loss: 0.25145048  - Train acc: 0.9100 - Val loss: 1.01989067\n",
      "(18.89 min) Epoch 94/300 -- Iteration 89586 - Batch 27/963 - Train loss: 0.26227823  - Train acc: 0.9043 - Val loss: 1.01989067\n",
      "(18.89 min) Epoch 94/300 -- Iteration 89595 - Batch 36/963 - Train loss: 0.26484930  - Train acc: 0.9043 - Val loss: 1.01989067\n",
      "(18.89 min) Epoch 94/300 -- Iteration 89604 - Batch 45/963 - Train loss: 0.26081131  - Train acc: 0.9051 - Val loss: 1.01989067\n",
      "(18.90 min) Epoch 94/300 -- Iteration 89613 - Batch 54/963 - Train loss: 0.26395512  - Train acc: 0.9028 - Val loss: 1.01989067\n",
      "(18.90 min) Epoch 94/300 -- Iteration 89622 - Batch 63/963 - Train loss: 0.26669059  - Train acc: 0.9012 - Val loss: 1.01989067\n",
      "(18.90 min) Epoch 94/300 -- Iteration 89631 - Batch 72/963 - Train loss: 0.26642022  - Train acc: 0.9014 - Val loss: 1.01989067\n",
      "(18.90 min) Epoch 94/300 -- Iteration 89640 - Batch 81/963 - Train loss: 0.26397902  - Train acc: 0.9026 - Val loss: 1.01989067\n",
      "(18.90 min) Epoch 94/300 -- Iteration 89649 - Batch 90/963 - Train loss: 0.26410178  - Train acc: 0.9027 - Val loss: 1.01989067\n",
      "(18.90 min) Epoch 94/300 -- Iteration 89658 - Batch 99/963 - Train loss: 0.26029901  - Train acc: 0.9040 - Val loss: 1.01989067\n",
      "(18.91 min) Epoch 94/300 -- Iteration 89667 - Batch 108/963 - Train loss: 0.26176717  - Train acc: 0.9031 - Val loss: 1.01989067\n",
      "(18.91 min) Epoch 94/300 -- Iteration 89676 - Batch 117/963 - Train loss: 0.26380610  - Train acc: 0.9017 - Val loss: 1.01989067\n",
      "(18.91 min) Epoch 94/300 -- Iteration 89685 - Batch 126/963 - Train loss: 0.26174799  - Train acc: 0.9027 - Val loss: 1.01989067\n",
      "(18.91 min) Epoch 94/300 -- Iteration 89694 - Batch 135/963 - Train loss: 0.26014655  - Train acc: 0.9034 - Val loss: 1.01989067\n",
      "(18.92 min) Epoch 94/300 -- Iteration 89703 - Batch 144/963 - Train loss: 0.25987189  - Train acc: 0.9032 - Val loss: 1.01989067\n",
      "(18.92 min) Epoch 94/300 -- Iteration 89712 - Batch 153/963 - Train loss: 0.25933841  - Train acc: 0.9034 - Val loss: 1.01989067\n",
      "(18.92 min) Epoch 94/300 -- Iteration 89721 - Batch 162/963 - Train loss: 0.26192867  - Train acc: 0.9026 - Val loss: 1.01989067\n",
      "(18.92 min) Epoch 94/300 -- Iteration 89730 - Batch 171/963 - Train loss: 0.26145477  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(18.92 min) Epoch 94/300 -- Iteration 89739 - Batch 180/963 - Train loss: 0.26151177  - Train acc: 0.9025 - Val loss: 1.01989067\n",
      "(18.92 min) Epoch 94/300 -- Iteration 89748 - Batch 189/963 - Train loss: 0.26146277  - Train acc: 0.9021 - Val loss: 1.01989067\n",
      "(18.93 min) Epoch 94/300 -- Iteration 89757 - Batch 198/963 - Train loss: 0.26098533  - Train acc: 0.9021 - Val loss: 1.01989067\n",
      "(18.93 min) Epoch 94/300 -- Iteration 89766 - Batch 207/963 - Train loss: 0.25995022  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(18.93 min) Epoch 94/300 -- Iteration 89775 - Batch 216/963 - Train loss: 0.26030839  - Train acc: 0.9019 - Val loss: 1.01989067\n",
      "(18.93 min) Epoch 94/300 -- Iteration 89784 - Batch 225/963 - Train loss: 0.26006805  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(18.93 min) Epoch 94/300 -- Iteration 89793 - Batch 234/963 - Train loss: 0.25977952  - Train acc: 0.9021 - Val loss: 1.01989067\n",
      "(18.94 min) Epoch 94/300 -- Iteration 89802 - Batch 243/963 - Train loss: 0.25882648  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(18.94 min) Epoch 94/300 -- Iteration 89811 - Batch 252/963 - Train loss: 0.25966946  - Train acc: 0.9018 - Val loss: 1.01989067\n",
      "(18.94 min) Epoch 94/300 -- Iteration 89820 - Batch 261/963 - Train loss: 0.26041612  - Train acc: 0.9015 - Val loss: 1.01989067\n",
      "(18.94 min) Epoch 94/300 -- Iteration 89829 - Batch 270/963 - Train loss: 0.26105708  - Train acc: 0.9011 - Val loss: 1.01989067\n",
      "(18.94 min) Epoch 94/300 -- Iteration 89838 - Batch 279/963 - Train loss: 0.26134186  - Train acc: 0.9009 - Val loss: 1.01989067\n",
      "(18.95 min) Epoch 94/300 -- Iteration 89847 - Batch 288/963 - Train loss: 0.26144333  - Train acc: 0.9008 - Val loss: 1.01989067\n",
      "(18.95 min) Epoch 94/300 -- Iteration 89856 - Batch 297/963 - Train loss: 0.26093796  - Train acc: 0.9012 - Val loss: 1.01989067\n",
      "(18.95 min) Epoch 94/300 -- Iteration 89865 - Batch 306/963 - Train loss: 0.26161311  - Train acc: 0.9007 - Val loss: 1.01989067\n",
      "(18.95 min) Epoch 94/300 -- Iteration 89874 - Batch 315/963 - Train loss: 0.26199155  - Train acc: 0.9002 - Val loss: 1.01989067\n",
      "(18.95 min) Epoch 94/300 -- Iteration 89883 - Batch 324/963 - Train loss: 0.26159049  - Train acc: 0.9006 - Val loss: 1.01989067\n",
      "(18.96 min) Epoch 94/300 -- Iteration 89892 - Batch 333/963 - Train loss: 0.26096205  - Train acc: 0.9007 - Val loss: 1.01989067\n",
      "(18.96 min) Epoch 94/300 -- Iteration 89901 - Batch 342/963 - Train loss: 0.26088357  - Train acc: 0.9010 - Val loss: 1.01989067\n",
      "(18.96 min) Epoch 94/300 -- Iteration 89910 - Batch 351/963 - Train loss: 0.26133568  - Train acc: 0.9006 - Val loss: 1.01989067\n",
      "(18.96 min) Epoch 94/300 -- Iteration 89919 - Batch 360/963 - Train loss: 0.26187995  - Train acc: 0.9006 - Val loss: 1.01989067\n",
      "(18.96 min) Epoch 94/300 -- Iteration 89928 - Batch 369/963 - Train loss: 0.26197547  - Train acc: 0.9005 - Val loss: 1.01989067\n",
      "(18.97 min) Epoch 94/300 -- Iteration 89937 - Batch 378/963 - Train loss: 0.26194126  - Train acc: 0.9006 - Val loss: 1.01989067\n",
      "(18.97 min) Epoch 94/300 -- Iteration 89946 - Batch 387/963 - Train loss: 0.26181239  - Train acc: 0.9006 - Val loss: 1.01989067\n",
      "(18.97 min) Epoch 94/300 -- Iteration 89955 - Batch 396/963 - Train loss: 0.26195782  - Train acc: 0.9005 - Val loss: 1.01989067\n",
      "(18.97 min) Epoch 94/300 -- Iteration 89964 - Batch 405/963 - Train loss: 0.26188448  - Train acc: 0.9005 - Val loss: 1.01989067\n",
      "(18.97 min) Epoch 94/300 -- Iteration 89973 - Batch 414/963 - Train loss: 0.26226176  - Train acc: 0.9002 - Val loss: 1.01989067\n",
      "(18.98 min) Epoch 94/300 -- Iteration 89982 - Batch 423/963 - Train loss: 0.26249559  - Train acc: 0.9001 - Val loss: 1.01989067\n",
      "(18.98 min) Epoch 94/300 -- Iteration 89991 - Batch 432/963 - Train loss: 0.26264006  - Train acc: 0.9001 - Val loss: 1.01989067\n",
      "(18.98 min) Epoch 94/300 -- Iteration 90000 - Batch 441/963 - Train loss: 0.26227183  - Train acc: 0.9004 - Val loss: 1.01989067\n",
      "(18.98 min) Epoch 94/300 -- Iteration 90009 - Batch 450/963 - Train loss: 0.26199372  - Train acc: 0.9005 - Val loss: 1.01989067\n",
      "(18.98 min) Epoch 94/300 -- Iteration 90018 - Batch 459/963 - Train loss: 0.26178288  - Train acc: 0.9007 - Val loss: 1.01989067\n",
      "(18.99 min) Epoch 94/300 -- Iteration 90027 - Batch 468/963 - Train loss: 0.26169232  - Train acc: 0.9010 - Val loss: 1.01989067\n",
      "(18.99 min) Epoch 94/300 -- Iteration 90036 - Batch 477/963 - Train loss: 0.26162090  - Train acc: 0.9009 - Val loss: 1.01989067\n",
      "(18.99 min) Epoch 94/300 -- Iteration 90045 - Batch 486/963 - Train loss: 0.26133611  - Train acc: 0.9010 - Val loss: 1.01989067\n",
      "(18.99 min) Epoch 94/300 -- Iteration 90054 - Batch 495/963 - Train loss: 0.26089895  - Train acc: 0.9012 - Val loss: 1.01989067\n",
      "(18.99 min) Epoch 94/300 -- Iteration 90063 - Batch 504/963 - Train loss: 0.26096846  - Train acc: 0.9011 - Val loss: 1.01989067\n",
      "(19.00 min) Epoch 94/300 -- Iteration 90072 - Batch 513/963 - Train loss: 0.26075228  - Train acc: 0.9013 - Val loss: 1.01989067\n",
      "(19.00 min) Epoch 94/300 -- Iteration 90081 - Batch 522/963 - Train loss: 0.26090015  - Train acc: 0.9011 - Val loss: 1.01989067\n",
      "(19.00 min) Epoch 94/300 -- Iteration 90090 - Batch 531/963 - Train loss: 0.26131310  - Train acc: 0.9009 - Val loss: 1.01989067\n",
      "(19.00 min) Epoch 94/300 -- Iteration 90099 - Batch 540/963 - Train loss: 0.26126861  - Train acc: 0.9011 - Val loss: 1.01989067\n",
      "(19.00 min) Epoch 94/300 -- Iteration 90108 - Batch 549/963 - Train loss: 0.26086973  - Train acc: 0.9013 - Val loss: 1.01989067\n",
      "(19.01 min) Epoch 94/300 -- Iteration 90117 - Batch 558/963 - Train loss: 0.26034212  - Train acc: 0.9015 - Val loss: 1.01989067\n",
      "(19.01 min) Epoch 94/300 -- Iteration 90126 - Batch 567/963 - Train loss: 0.26071215  - Train acc: 0.9013 - Val loss: 1.01989067\n",
      "(19.01 min) Epoch 94/300 -- Iteration 90135 - Batch 576/963 - Train loss: 0.26074632  - Train acc: 0.9015 - Val loss: 1.01989067\n",
      "(19.01 min) Epoch 94/300 -- Iteration 90144 - Batch 585/963 - Train loss: 0.26095144  - Train acc: 0.9014 - Val loss: 1.01989067\n",
      "(19.01 min) Epoch 94/300 -- Iteration 90153 - Batch 594/963 - Train loss: 0.26084452  - Train acc: 0.9015 - Val loss: 1.01989067\n",
      "(19.02 min) Epoch 94/300 -- Iteration 90162 - Batch 603/963 - Train loss: 0.26071818  - Train acc: 0.9017 - Val loss: 1.01989067\n",
      "(19.02 min) Epoch 94/300 -- Iteration 90171 - Batch 612/963 - Train loss: 0.26058692  - Train acc: 0.9017 - Val loss: 1.01989067\n",
      "(19.02 min) Epoch 94/300 -- Iteration 90180 - Batch 621/963 - Train loss: 0.26050601  - Train acc: 0.9017 - Val loss: 1.01989067\n",
      "(19.02 min) Epoch 94/300 -- Iteration 90189 - Batch 630/963 - Train loss: 0.26031732  - Train acc: 0.9019 - Val loss: 1.01989067\n",
      "(19.02 min) Epoch 94/300 -- Iteration 90198 - Batch 639/963 - Train loss: 0.26006956  - Train acc: 0.9019 - Val loss: 1.01989067\n",
      "(19.02 min) Epoch 94/300 -- Iteration 90207 - Batch 648/963 - Train loss: 0.26029627  - Train acc: 0.9019 - Val loss: 1.01989067\n",
      "(19.03 min) Epoch 94/300 -- Iteration 90216 - Batch 657/963 - Train loss: 0.26004440  - Train acc: 0.9020 - Val loss: 1.01989067\n",
      "(19.03 min) Epoch 94/300 -- Iteration 90225 - Batch 666/963 - Train loss: 0.26018472  - Train acc: 0.9020 - Val loss: 1.01989067\n",
      "(19.03 min) Epoch 94/300 -- Iteration 90234 - Batch 675/963 - Train loss: 0.26043737  - Train acc: 0.9018 - Val loss: 1.01989067\n",
      "(19.03 min) Epoch 94/300 -- Iteration 90243 - Batch 684/963 - Train loss: 0.25994011  - Train acc: 0.9020 - Val loss: 1.01989067\n",
      "(19.03 min) Epoch 94/300 -- Iteration 90252 - Batch 693/963 - Train loss: 0.26010557  - Train acc: 0.9019 - Val loss: 1.01989067\n",
      "(19.04 min) Epoch 94/300 -- Iteration 90261 - Batch 702/963 - Train loss: 0.26008396  - Train acc: 0.9019 - Val loss: 1.01989067\n",
      "(19.04 min) Epoch 94/300 -- Iteration 90270 - Batch 711/963 - Train loss: 0.25991946  - Train acc: 0.9019 - Val loss: 1.01989067\n",
      "(19.04 min) Epoch 94/300 -- Iteration 90279 - Batch 720/963 - Train loss: 0.25974544  - Train acc: 0.9020 - Val loss: 1.01989067\n",
      "(19.04 min) Epoch 94/300 -- Iteration 90288 - Batch 729/963 - Train loss: 0.25957079  - Train acc: 0.9021 - Val loss: 1.01989067\n",
      "(19.04 min) Epoch 94/300 -- Iteration 90297 - Batch 738/963 - Train loss: 0.25929439  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.05 min) Epoch 94/300 -- Iteration 90306 - Batch 747/963 - Train loss: 0.25919301  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.05 min) Epoch 94/300 -- Iteration 90315 - Batch 756/963 - Train loss: 0.25922980  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.05 min) Epoch 94/300 -- Iteration 90324 - Batch 765/963 - Train loss: 0.25917153  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(19.05 min) Epoch 94/300 -- Iteration 90333 - Batch 774/963 - Train loss: 0.25920929  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(19.05 min) Epoch 94/300 -- Iteration 90342 - Batch 783/963 - Train loss: 0.25926373  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.06 min) Epoch 94/300 -- Iteration 90351 - Batch 792/963 - Train loss: 0.25922166  - Train acc: 0.9021 - Val loss: 1.01989067\n",
      "(19.06 min) Epoch 94/300 -- Iteration 90360 - Batch 801/963 - Train loss: 0.25934978  - Train acc: 0.9021 - Val loss: 1.01989067\n",
      "(19.06 min) Epoch 94/300 -- Iteration 90369 - Batch 810/963 - Train loss: 0.25905087  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.06 min) Epoch 94/300 -- Iteration 90378 - Batch 819/963 - Train loss: 0.25902029  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.06 min) Epoch 94/300 -- Iteration 90387 - Batch 828/963 - Train loss: 0.25912909  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.07 min) Epoch 94/300 -- Iteration 90396 - Batch 837/963 - Train loss: 0.25907806  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.07 min) Epoch 94/300 -- Iteration 90405 - Batch 846/963 - Train loss: 0.25919690  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.07 min) Epoch 94/300 -- Iteration 90414 - Batch 855/963 - Train loss: 0.25907548  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(19.07 min) Epoch 94/300 -- Iteration 90423 - Batch 864/963 - Train loss: 0.25922984  - Train acc: 0.9022 - Val loss: 1.01989067\n",
      "(19.07 min) Epoch 94/300 -- Iteration 90432 - Batch 873/963 - Train loss: 0.25894960  - Train acc: 0.9024 - Val loss: 1.01989067\n",
      "(19.08 min) Epoch 94/300 -- Iteration 90441 - Batch 882/963 - Train loss: 0.25909637  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(19.08 min) Epoch 94/300 -- Iteration 90450 - Batch 891/963 - Train loss: 0.25900101  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(19.08 min) Epoch 94/300 -- Iteration 90459 - Batch 900/963 - Train loss: 0.25887726  - Train acc: 0.9024 - Val loss: 1.01989067\n",
      "(19.08 min) Epoch 94/300 -- Iteration 90468 - Batch 909/963 - Train loss: 0.25891316  - Train acc: 0.9024 - Val loss: 1.01989067\n",
      "(19.08 min) Epoch 94/300 -- Iteration 90477 - Batch 918/963 - Train loss: 0.25889314  - Train acc: 0.9024 - Val loss: 1.01989067\n",
      "(19.09 min) Epoch 94/300 -- Iteration 90486 - Batch 927/963 - Train loss: 0.25920611  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(19.09 min) Epoch 94/300 -- Iteration 90495 - Batch 936/963 - Train loss: 0.25921213  - Train acc: 0.9023 - Val loss: 1.01989067\n",
      "(19.09 min) Epoch 94/300 -- Iteration 90504 - Batch 945/963 - Train loss: 0.25881795  - Train acc: 0.9024 - Val loss: 1.01989067\n",
      "(19.09 min) Epoch 94/300 -- Iteration 90513 - Batch 954/963 - Train loss: 0.25893437  - Train acc: 0.9024 - Val loss: 1.01989067\n",
      "(19.09 min) Epoch 94/300 -- Iteration 90522 - Batch 962/963 - Train loss: 0.25915841  - Train acc: 0.9023 - Val loss: 1.03372622 - Val acc: 0.5733\n",
      "(19.10 min) Epoch 95/300 -- Iteration 90531 - Batch 9/963 - Train loss: 0.27922036  - Train acc: 0.8938 - Val loss: 1.03372622\n",
      "(19.10 min) Epoch 95/300 -- Iteration 90540 - Batch 18/963 - Train loss: 0.26212162  - Train acc: 0.9025 - Val loss: 1.03372622\n",
      "(19.10 min) Epoch 95/300 -- Iteration 90549 - Batch 27/963 - Train loss: 0.26769299  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.10 min) Epoch 95/300 -- Iteration 90558 - Batch 36/963 - Train loss: 0.26491658  - Train acc: 0.9024 - Val loss: 1.03372622\n",
      "(19.10 min) Epoch 95/300 -- Iteration 90567 - Batch 45/963 - Train loss: 0.26692863  - Train acc: 0.9020 - Val loss: 1.03372622\n",
      "(19.10 min) Epoch 95/300 -- Iteration 90576 - Batch 54/963 - Train loss: 0.27056290  - Train acc: 0.8990 - Val loss: 1.03372622\n",
      "(19.11 min) Epoch 95/300 -- Iteration 90585 - Batch 63/963 - Train loss: 0.27248232  - Train acc: 0.8978 - Val loss: 1.03372622\n",
      "(19.11 min) Epoch 95/300 -- Iteration 90594 - Batch 72/963 - Train loss: 0.27369002  - Train acc: 0.8970 - Val loss: 1.03372622\n",
      "(19.11 min) Epoch 95/300 -- Iteration 90603 - Batch 81/963 - Train loss: 0.27371123  - Train acc: 0.8962 - Val loss: 1.03372622\n",
      "(19.11 min) Epoch 95/300 -- Iteration 90612 - Batch 90/963 - Train loss: 0.27052680  - Train acc: 0.8978 - Val loss: 1.03372622\n",
      "(19.11 min) Epoch 95/300 -- Iteration 90621 - Batch 99/963 - Train loss: 0.27025599  - Train acc: 0.8981 - Val loss: 1.03372622\n",
      "(19.12 min) Epoch 95/300 -- Iteration 90630 - Batch 108/963 - Train loss: 0.26688954  - Train acc: 0.8993 - Val loss: 1.03372622\n",
      "(19.12 min) Epoch 95/300 -- Iteration 90639 - Batch 117/963 - Train loss: 0.26574867  - Train acc: 0.8990 - Val loss: 1.03372622\n",
      "(19.12 min) Epoch 95/300 -- Iteration 90648 - Batch 126/963 - Train loss: 0.26478395  - Train acc: 0.8989 - Val loss: 1.03372622\n",
      "(19.12 min) Epoch 95/300 -- Iteration 90657 - Batch 135/963 - Train loss: 0.26556124  - Train acc: 0.8992 - Val loss: 1.03372622\n",
      "(19.12 min) Epoch 95/300 -- Iteration 90666 - Batch 144/963 - Train loss: 0.26577367  - Train acc: 0.8992 - Val loss: 1.03372622\n",
      "(19.13 min) Epoch 95/300 -- Iteration 90675 - Batch 153/963 - Train loss: 0.26510660  - Train acc: 0.8997 - Val loss: 1.03372622\n",
      "(19.13 min) Epoch 95/300 -- Iteration 90684 - Batch 162/963 - Train loss: 0.26540735  - Train acc: 0.8995 - Val loss: 1.03372622\n",
      "(19.13 min) Epoch 95/300 -- Iteration 90693 - Batch 171/963 - Train loss: 0.26489154  - Train acc: 0.8993 - Val loss: 1.03372622\n",
      "(19.13 min) Epoch 95/300 -- Iteration 90702 - Batch 180/963 - Train loss: 0.26286952  - Train acc: 0.9005 - Val loss: 1.03372622\n",
      "(19.13 min) Epoch 95/300 -- Iteration 90711 - Batch 189/963 - Train loss: 0.26270207  - Train acc: 0.9005 - Val loss: 1.03372622\n",
      "(19.14 min) Epoch 95/300 -- Iteration 90720 - Batch 198/963 - Train loss: 0.26211480  - Train acc: 0.9011 - Val loss: 1.03372622\n",
      "(19.14 min) Epoch 95/300 -- Iteration 90729 - Batch 207/963 - Train loss: 0.26269922  - Train acc: 0.9008 - Val loss: 1.03372622\n",
      "(19.14 min) Epoch 95/300 -- Iteration 90738 - Batch 216/963 - Train loss: 0.26170437  - Train acc: 0.9012 - Val loss: 1.03372622\n",
      "(19.14 min) Epoch 95/300 -- Iteration 90747 - Batch 225/963 - Train loss: 0.26222963  - Train acc: 0.9009 - Val loss: 1.03372622\n",
      "(19.14 min) Epoch 95/300 -- Iteration 90756 - Batch 234/963 - Train loss: 0.26250956  - Train acc: 0.9008 - Val loss: 1.03372622\n",
      "(19.15 min) Epoch 95/300 -- Iteration 90765 - Batch 243/963 - Train loss: 0.26140808  - Train acc: 0.9010 - Val loss: 1.03372622\n",
      "(19.15 min) Epoch 95/300 -- Iteration 90774 - Batch 252/963 - Train loss: 0.26125796  - Train acc: 0.9011 - Val loss: 1.03372622\n",
      "(19.15 min) Epoch 95/300 -- Iteration 90783 - Batch 261/963 - Train loss: 0.26124594  - Train acc: 0.9011 - Val loss: 1.03372622\n",
      "(19.15 min) Epoch 95/300 -- Iteration 90792 - Batch 270/963 - Train loss: 0.26081911  - Train acc: 0.9013 - Val loss: 1.03372622\n",
      "(19.15 min) Epoch 95/300 -- Iteration 90801 - Batch 279/963 - Train loss: 0.26006936  - Train acc: 0.9014 - Val loss: 1.03372622\n",
      "(19.15 min) Epoch 95/300 -- Iteration 90810 - Batch 288/963 - Train loss: 0.25977184  - Train acc: 0.9017 - Val loss: 1.03372622\n",
      "(19.16 min) Epoch 95/300 -- Iteration 90819 - Batch 297/963 - Train loss: 0.25950946  - Train acc: 0.9020 - Val loss: 1.03372622\n",
      "(19.16 min) Epoch 95/300 -- Iteration 90828 - Batch 306/963 - Train loss: 0.25889948  - Train acc: 0.9024 - Val loss: 1.03372622\n",
      "(19.16 min) Epoch 95/300 -- Iteration 90837 - Batch 315/963 - Train loss: 0.25881018  - Train acc: 0.9024 - Val loss: 1.03372622\n",
      "(19.16 min) Epoch 95/300 -- Iteration 90846 - Batch 324/963 - Train loss: 0.25815136  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.16 min) Epoch 95/300 -- Iteration 90855 - Batch 333/963 - Train loss: 0.25854249  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.17 min) Epoch 95/300 -- Iteration 90864 - Batch 342/963 - Train loss: 0.25838885  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.17 min) Epoch 95/300 -- Iteration 90873 - Batch 351/963 - Train loss: 0.25842694  - Train acc: 0.9027 - Val loss: 1.03372622\n",
      "(19.17 min) Epoch 95/300 -- Iteration 90882 - Batch 360/963 - Train loss: 0.25797837  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.17 min) Epoch 95/300 -- Iteration 90891 - Batch 369/963 - Train loss: 0.25807054  - Train acc: 0.9025 - Val loss: 1.03372622\n",
      "(19.17 min) Epoch 95/300 -- Iteration 90900 - Batch 378/963 - Train loss: 0.25841839  - Train acc: 0.9024 - Val loss: 1.03372622\n",
      "(19.18 min) Epoch 95/300 -- Iteration 90909 - Batch 387/963 - Train loss: 0.25810381  - Train acc: 0.9025 - Val loss: 1.03372622\n",
      "(19.18 min) Epoch 95/300 -- Iteration 90918 - Batch 396/963 - Train loss: 0.25881151  - Train acc: 0.9023 - Val loss: 1.03372622\n",
      "(19.18 min) Epoch 95/300 -- Iteration 90927 - Batch 405/963 - Train loss: 0.25906045  - Train acc: 0.9022 - Val loss: 1.03372622\n",
      "(19.18 min) Epoch 95/300 -- Iteration 90936 - Batch 414/963 - Train loss: 0.25904408  - Train acc: 0.9023 - Val loss: 1.03372622\n",
      "(19.18 min) Epoch 95/300 -- Iteration 90945 - Batch 423/963 - Train loss: 0.25861296  - Train acc: 0.9023 - Val loss: 1.03372622\n",
      "(19.19 min) Epoch 95/300 -- Iteration 90954 - Batch 432/963 - Train loss: 0.25824986  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.19 min) Epoch 95/300 -- Iteration 90963 - Batch 441/963 - Train loss: 0.25845093  - Train acc: 0.9021 - Val loss: 1.03372622\n",
      "(19.19 min) Epoch 95/300 -- Iteration 90972 - Batch 450/963 - Train loss: 0.25902705  - Train acc: 0.9019 - Val loss: 1.03372622\n",
      "(19.19 min) Epoch 95/300 -- Iteration 90981 - Batch 459/963 - Train loss: 0.25882578  - Train acc: 0.9021 - Val loss: 1.03372622\n",
      "(19.19 min) Epoch 95/300 -- Iteration 90990 - Batch 468/963 - Train loss: 0.25851990  - Train acc: 0.9023 - Val loss: 1.03372622\n",
      "(19.20 min) Epoch 95/300 -- Iteration 90999 - Batch 477/963 - Train loss: 0.25835810  - Train acc: 0.9025 - Val loss: 1.03372622\n",
      "(19.20 min) Epoch 95/300 -- Iteration 91008 - Batch 486/963 - Train loss: 0.25823171  - Train acc: 0.9027 - Val loss: 1.03372622\n",
      "(19.20 min) Epoch 95/300 -- Iteration 91017 - Batch 495/963 - Train loss: 0.25829232  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.20 min) Epoch 95/300 -- Iteration 91026 - Batch 504/963 - Train loss: 0.25834622  - Train acc: 0.9025 - Val loss: 1.03372622\n",
      "(19.20 min) Epoch 95/300 -- Iteration 91035 - Batch 513/963 - Train loss: 0.25873814  - Train acc: 0.9024 - Val loss: 1.03372622\n",
      "(19.20 min) Epoch 95/300 -- Iteration 91044 - Batch 522/963 - Train loss: 0.25862913  - Train acc: 0.9023 - Val loss: 1.03372622\n",
      "(19.21 min) Epoch 95/300 -- Iteration 91053 - Batch 531/963 - Train loss: 0.25859774  - Train acc: 0.9024 - Val loss: 1.03372622\n",
      "(19.21 min) Epoch 95/300 -- Iteration 91062 - Batch 540/963 - Train loss: 0.25855105  - Train acc: 0.9023 - Val loss: 1.03372622\n",
      "(19.21 min) Epoch 95/300 -- Iteration 91071 - Batch 549/963 - Train loss: 0.25820411  - Train acc: 0.9025 - Val loss: 1.03372622\n",
      "(19.21 min) Epoch 95/300 -- Iteration 91080 - Batch 558/963 - Train loss: 0.25805724  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.21 min) Epoch 95/300 -- Iteration 91089 - Batch 567/963 - Train loss: 0.25766740  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.22 min) Epoch 95/300 -- Iteration 91098 - Batch 576/963 - Train loss: 0.25724279  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.22 min) Epoch 95/300 -- Iteration 91107 - Batch 585/963 - Train loss: 0.25732489  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.22 min) Epoch 95/300 -- Iteration 91116 - Batch 594/963 - Train loss: 0.25765016  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.22 min) Epoch 95/300 -- Iteration 91125 - Batch 603/963 - Train loss: 0.25800023  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.22 min) Epoch 95/300 -- Iteration 91134 - Batch 612/963 - Train loss: 0.25815771  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.23 min) Epoch 95/300 -- Iteration 91143 - Batch 621/963 - Train loss: 0.25816065  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.23 min) Epoch 95/300 -- Iteration 91152 - Batch 630/963 - Train loss: 0.25800961  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.23 min) Epoch 95/300 -- Iteration 91161 - Batch 639/963 - Train loss: 0.25825577  - Train acc: 0.9027 - Val loss: 1.03372622\n",
      "(19.23 min) Epoch 95/300 -- Iteration 91170 - Batch 648/963 - Train loss: 0.25830091  - Train acc: 0.9027 - Val loss: 1.03372622\n",
      "(19.23 min) Epoch 95/300 -- Iteration 91179 - Batch 657/963 - Train loss: 0.25815018  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.24 min) Epoch 95/300 -- Iteration 91188 - Batch 666/963 - Train loss: 0.25829250  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.24 min) Epoch 95/300 -- Iteration 91197 - Batch 675/963 - Train loss: 0.25807534  - Train acc: 0.9030 - Val loss: 1.03372622\n",
      "(19.24 min) Epoch 95/300 -- Iteration 91206 - Batch 684/963 - Train loss: 0.25804894  - Train acc: 0.9032 - Val loss: 1.03372622\n",
      "(19.24 min) Epoch 95/300 -- Iteration 91215 - Batch 693/963 - Train loss: 0.25806602  - Train acc: 0.9030 - Val loss: 1.03372622\n",
      "(19.24 min) Epoch 95/300 -- Iteration 91224 - Batch 702/963 - Train loss: 0.25803203  - Train acc: 0.9030 - Val loss: 1.03372622\n",
      "(19.25 min) Epoch 95/300 -- Iteration 91233 - Batch 711/963 - Train loss: 0.25822703  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.25 min) Epoch 95/300 -- Iteration 91242 - Batch 720/963 - Train loss: 0.25828955  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.25 min) Epoch 95/300 -- Iteration 91251 - Batch 729/963 - Train loss: 0.25880494  - Train acc: 0.9027 - Val loss: 1.03372622\n",
      "(19.25 min) Epoch 95/300 -- Iteration 91260 - Batch 738/963 - Train loss: 0.25862306  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.25 min) Epoch 95/300 -- Iteration 91269 - Batch 747/963 - Train loss: 0.25857473  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.26 min) Epoch 95/300 -- Iteration 91278 - Batch 756/963 - Train loss: 0.25868308  - Train acc: 0.9027 - Val loss: 1.03372622\n",
      "(19.26 min) Epoch 95/300 -- Iteration 91287 - Batch 765/963 - Train loss: 0.25853505  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.26 min) Epoch 95/300 -- Iteration 91296 - Batch 774/963 - Train loss: 0.25866989  - Train acc: 0.9027 - Val loss: 1.03372622\n",
      "(19.26 min) Epoch 95/300 -- Iteration 91305 - Batch 783/963 - Train loss: 0.25854638  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.26 min) Epoch 95/300 -- Iteration 91314 - Batch 792/963 - Train loss: 0.25851104  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.26 min) Epoch 95/300 -- Iteration 91323 - Batch 801/963 - Train loss: 0.25861536  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.27 min) Epoch 95/300 -- Iteration 91332 - Batch 810/963 - Train loss: 0.25883231  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.27 min) Epoch 95/300 -- Iteration 91341 - Batch 819/963 - Train loss: 0.25905149  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.27 min) Epoch 95/300 -- Iteration 91350 - Batch 828/963 - Train loss: 0.25872728  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.27 min) Epoch 95/300 -- Iteration 91359 - Batch 837/963 - Train loss: 0.25882905  - Train acc: 0.9030 - Val loss: 1.03372622\n",
      "(19.27 min) Epoch 95/300 -- Iteration 91368 - Batch 846/963 - Train loss: 0.25910788  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.28 min) Epoch 95/300 -- Iteration 91377 - Batch 855/963 - Train loss: 0.25916147  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.28 min) Epoch 95/300 -- Iteration 91386 - Batch 864/963 - Train loss: 0.25944846  - Train acc: 0.9024 - Val loss: 1.03372622\n",
      "(19.28 min) Epoch 95/300 -- Iteration 91395 - Batch 873/963 - Train loss: 0.25924965  - Train acc: 0.9025 - Val loss: 1.03372622\n",
      "(19.28 min) Epoch 95/300 -- Iteration 91404 - Batch 882/963 - Train loss: 0.25946106  - Train acc: 0.9024 - Val loss: 1.03372622\n",
      "(19.28 min) Epoch 95/300 -- Iteration 91413 - Batch 891/963 - Train loss: 0.25939098  - Train acc: 0.9025 - Val loss: 1.03372622\n",
      "(19.29 min) Epoch 95/300 -- Iteration 91422 - Batch 900/963 - Train loss: 0.25915846  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.29 min) Epoch 95/300 -- Iteration 91431 - Batch 909/963 - Train loss: 0.25895210  - Train acc: 0.9026 - Val loss: 1.03372622\n",
      "(19.29 min) Epoch 95/300 -- Iteration 91440 - Batch 918/963 - Train loss: 0.25867861  - Train acc: 0.9027 - Val loss: 1.03372622\n",
      "(19.29 min) Epoch 95/300 -- Iteration 91449 - Batch 927/963 - Train loss: 0.25850607  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.29 min) Epoch 95/300 -- Iteration 91458 - Batch 936/963 - Train loss: 0.25853124  - Train acc: 0.9027 - Val loss: 1.03372622\n",
      "(19.29 min) Epoch 95/300 -- Iteration 91467 - Batch 945/963 - Train loss: 0.25834549  - Train acc: 0.9028 - Val loss: 1.03372622\n",
      "(19.30 min) Epoch 95/300 -- Iteration 91476 - Batch 954/963 - Train loss: 0.25839005  - Train acc: 0.9029 - Val loss: 1.03372622\n",
      "(19.30 min) Epoch 95/300 -- Iteration 91485 - Batch 962/963 - Train loss: 0.25846624  - Train acc: 0.9028 - Val loss: 1.02334487 - Val acc: 0.5800\n",
      "(19.30 min) Epoch 96/300 -- Iteration 91494 - Batch 9/963 - Train loss: 0.21613702  - Train acc: 0.9195 - Val loss: 1.02334487\n",
      "(19.30 min) Epoch 96/300 -- Iteration 91503 - Batch 18/963 - Train loss: 0.23855341  - Train acc: 0.9095 - Val loss: 1.02334487\n",
      "(19.30 min) Epoch 96/300 -- Iteration 91512 - Batch 27/963 - Train loss: 0.24994277  - Train acc: 0.9049 - Val loss: 1.02334487\n",
      "(19.31 min) Epoch 96/300 -- Iteration 91521 - Batch 36/963 - Train loss: 0.25550689  - Train acc: 0.9016 - Val loss: 1.02334487\n",
      "(19.31 min) Epoch 96/300 -- Iteration 91530 - Batch 45/963 - Train loss: 0.25342652  - Train acc: 0.9025 - Val loss: 1.02334487\n",
      "(19.31 min) Epoch 96/300 -- Iteration 91539 - Batch 54/963 - Train loss: 0.25198037  - Train acc: 0.9037 - Val loss: 1.02334487\n",
      "(19.31 min) Epoch 96/300 -- Iteration 91548 - Batch 63/963 - Train loss: 0.25740052  - Train acc: 0.9015 - Val loss: 1.02334487\n",
      "(19.31 min) Epoch 96/300 -- Iteration 91557 - Batch 72/963 - Train loss: 0.25741039  - Train acc: 0.9007 - Val loss: 1.02334487\n",
      "(19.32 min) Epoch 96/300 -- Iteration 91566 - Batch 81/963 - Train loss: 0.25775881  - Train acc: 0.9004 - Val loss: 1.02334487\n",
      "(19.32 min) Epoch 96/300 -- Iteration 91575 - Batch 90/963 - Train loss: 0.26030333  - Train acc: 0.8992 - Val loss: 1.02334487\n",
      "(19.32 min) Epoch 96/300 -- Iteration 91584 - Batch 99/963 - Train loss: 0.26050522  - Train acc: 0.8996 - Val loss: 1.02334487\n",
      "(19.32 min) Epoch 96/300 -- Iteration 91593 - Batch 108/963 - Train loss: 0.26274115  - Train acc: 0.8981 - Val loss: 1.02334487\n",
      "(19.32 min) Epoch 96/300 -- Iteration 91602 - Batch 117/963 - Train loss: 0.26255472  - Train acc: 0.8988 - Val loss: 1.02334487\n",
      "(19.32 min) Epoch 96/300 -- Iteration 91611 - Batch 126/963 - Train loss: 0.26164066  - Train acc: 0.8989 - Val loss: 1.02334487\n",
      "(19.33 min) Epoch 96/300 -- Iteration 91620 - Batch 135/963 - Train loss: 0.26092750  - Train acc: 0.8990 - Val loss: 1.02334487\n",
      "(19.33 min) Epoch 96/300 -- Iteration 91629 - Batch 144/963 - Train loss: 0.26110797  - Train acc: 0.8992 - Val loss: 1.02334487\n",
      "(19.33 min) Epoch 96/300 -- Iteration 91638 - Batch 153/963 - Train loss: 0.25985791  - Train acc: 0.9003 - Val loss: 1.02334487\n",
      "(19.33 min) Epoch 96/300 -- Iteration 91647 - Batch 162/963 - Train loss: 0.25953533  - Train acc: 0.9001 - Val loss: 1.02334487\n",
      "(19.33 min) Epoch 96/300 -- Iteration 91656 - Batch 171/963 - Train loss: 0.25907183  - Train acc: 0.9003 - Val loss: 1.02334487\n",
      "(19.34 min) Epoch 96/300 -- Iteration 91665 - Batch 180/963 - Train loss: 0.25957289  - Train acc: 0.9000 - Val loss: 1.02334487\n",
      "(19.34 min) Epoch 96/300 -- Iteration 91674 - Batch 189/963 - Train loss: 0.25903094  - Train acc: 0.9003 - Val loss: 1.02334487\n",
      "(19.34 min) Epoch 96/300 -- Iteration 91683 - Batch 198/963 - Train loss: 0.25974539  - Train acc: 0.9001 - Val loss: 1.02334487\n",
      "(19.34 min) Epoch 96/300 -- Iteration 91692 - Batch 207/963 - Train loss: 0.25993264  - Train acc: 0.9002 - Val loss: 1.02334487\n",
      "(19.34 min) Epoch 96/300 -- Iteration 91701 - Batch 216/963 - Train loss: 0.26020345  - Train acc: 0.9000 - Val loss: 1.02334487\n",
      "(19.35 min) Epoch 96/300 -- Iteration 91710 - Batch 225/963 - Train loss: 0.26022055  - Train acc: 0.8999 - Val loss: 1.02334487\n",
      "(19.35 min) Epoch 96/300 -- Iteration 91719 - Batch 234/963 - Train loss: 0.25955239  - Train acc: 0.9001 - Val loss: 1.02334487\n",
      "(19.35 min) Epoch 96/300 -- Iteration 91728 - Batch 243/963 - Train loss: 0.25956344  - Train acc: 0.9002 - Val loss: 1.02334487\n",
      "(19.35 min) Epoch 96/300 -- Iteration 91737 - Batch 252/963 - Train loss: 0.25869368  - Train acc: 0.9008 - Val loss: 1.02334487\n",
      "(19.35 min) Epoch 96/300 -- Iteration 91746 - Batch 261/963 - Train loss: 0.25834278  - Train acc: 0.9012 - Val loss: 1.02334487\n",
      "(19.35 min) Epoch 96/300 -- Iteration 91755 - Batch 270/963 - Train loss: 0.25974955  - Train acc: 0.9009 - Val loss: 1.02334487\n",
      "(19.36 min) Epoch 96/300 -- Iteration 91764 - Batch 279/963 - Train loss: 0.25957933  - Train acc: 0.9008 - Val loss: 1.02334487\n",
      "(19.36 min) Epoch 96/300 -- Iteration 91773 - Batch 288/963 - Train loss: 0.25935093  - Train acc: 0.9011 - Val loss: 1.02334487\n",
      "(19.36 min) Epoch 96/300 -- Iteration 91782 - Batch 297/963 - Train loss: 0.25962552  - Train acc: 0.9012 - Val loss: 1.02334487\n",
      "(19.36 min) Epoch 96/300 -- Iteration 91791 - Batch 306/963 - Train loss: 0.25985027  - Train acc: 0.9009 - Val loss: 1.02334487\n",
      "(19.36 min) Epoch 96/300 -- Iteration 91800 - Batch 315/963 - Train loss: 0.26012453  - Train acc: 0.9010 - Val loss: 1.02334487\n",
      "(19.37 min) Epoch 96/300 -- Iteration 91809 - Batch 324/963 - Train loss: 0.26014724  - Train acc: 0.9009 - Val loss: 1.02334487\n",
      "(19.37 min) Epoch 96/300 -- Iteration 91818 - Batch 333/963 - Train loss: 0.26026083  - Train acc: 0.9008 - Val loss: 1.02334487\n",
      "(19.37 min) Epoch 96/300 -- Iteration 91827 - Batch 342/963 - Train loss: 0.26029313  - Train acc: 0.9010 - Val loss: 1.02334487\n",
      "(19.37 min) Epoch 96/300 -- Iteration 91836 - Batch 351/963 - Train loss: 0.26094778  - Train acc: 0.9011 - Val loss: 1.02334487\n",
      "(19.37 min) Epoch 96/300 -- Iteration 91845 - Batch 360/963 - Train loss: 0.26121643  - Train acc: 0.9011 - Val loss: 1.02334487\n",
      "(19.38 min) Epoch 96/300 -- Iteration 91854 - Batch 369/963 - Train loss: 0.26062574  - Train acc: 0.9013 - Val loss: 1.02334487\n",
      "(19.38 min) Epoch 96/300 -- Iteration 91863 - Batch 378/963 - Train loss: 0.26111266  - Train acc: 0.9011 - Val loss: 1.02334487\n",
      "(19.38 min) Epoch 96/300 -- Iteration 91872 - Batch 387/963 - Train loss: 0.26159358  - Train acc: 0.9011 - Val loss: 1.02334487\n",
      "(19.38 min) Epoch 96/300 -- Iteration 91881 - Batch 396/963 - Train loss: 0.26171863  - Train acc: 0.9011 - Val loss: 1.02334487\n",
      "(19.38 min) Epoch 96/300 -- Iteration 91890 - Batch 405/963 - Train loss: 0.26161814  - Train acc: 0.9014 - Val loss: 1.02334487\n",
      "(19.38 min) Epoch 96/300 -- Iteration 91899 - Batch 414/963 - Train loss: 0.26151693  - Train acc: 0.9014 - Val loss: 1.02334487\n",
      "(19.39 min) Epoch 96/300 -- Iteration 91908 - Batch 423/963 - Train loss: 0.26181554  - Train acc: 0.9015 - Val loss: 1.02334487\n",
      "(19.39 min) Epoch 96/300 -- Iteration 91917 - Batch 432/963 - Train loss: 0.26209201  - Train acc: 0.9012 - Val loss: 1.02334487\n",
      "(19.39 min) Epoch 96/300 -- Iteration 91926 - Batch 441/963 - Train loss: 0.26229755  - Train acc: 0.9012 - Val loss: 1.02334487\n",
      "(19.39 min) Epoch 96/300 -- Iteration 91935 - Batch 450/963 - Train loss: 0.26192254  - Train acc: 0.9016 - Val loss: 1.02334487\n",
      "(19.39 min) Epoch 96/300 -- Iteration 91944 - Batch 459/963 - Train loss: 0.26143186  - Train acc: 0.9019 - Val loss: 1.02334487\n",
      "(19.40 min) Epoch 96/300 -- Iteration 91953 - Batch 468/963 - Train loss: 0.26098390  - Train acc: 0.9020 - Val loss: 1.02334487\n",
      "(19.40 min) Epoch 96/300 -- Iteration 91962 - Batch 477/963 - Train loss: 0.26053855  - Train acc: 0.9022 - Val loss: 1.02334487\n",
      "(19.40 min) Epoch 96/300 -- Iteration 91971 - Batch 486/963 - Train loss: 0.26022396  - Train acc: 0.9023 - Val loss: 1.02334487\n",
      "(19.40 min) Epoch 96/300 -- Iteration 91980 - Batch 495/963 - Train loss: 0.26034616  - Train acc: 0.9022 - Val loss: 1.02334487\n",
      "(19.40 min) Epoch 96/300 -- Iteration 91989 - Batch 504/963 - Train loss: 0.26037690  - Train acc: 0.9021 - Val loss: 1.02334487\n",
      "(19.40 min) Epoch 96/300 -- Iteration 91998 - Batch 513/963 - Train loss: 0.26034248  - Train acc: 0.9021 - Val loss: 1.02334487\n",
      "(19.41 min) Epoch 96/300 -- Iteration 92007 - Batch 522/963 - Train loss: 0.25977751  - Train acc: 0.9025 - Val loss: 1.02334487\n",
      "(19.41 min) Epoch 96/300 -- Iteration 92016 - Batch 531/963 - Train loss: 0.25953565  - Train acc: 0.9025 - Val loss: 1.02334487\n",
      "(19.41 min) Epoch 96/300 -- Iteration 92025 - Batch 540/963 - Train loss: 0.25984788  - Train acc: 0.9023 - Val loss: 1.02334487\n",
      "(19.41 min) Epoch 96/300 -- Iteration 92034 - Batch 549/963 - Train loss: 0.25996495  - Train acc: 0.9023 - Val loss: 1.02334487\n",
      "(19.41 min) Epoch 96/300 -- Iteration 92043 - Batch 558/963 - Train loss: 0.25998761  - Train acc: 0.9024 - Val loss: 1.02334487\n",
      "(19.42 min) Epoch 96/300 -- Iteration 92052 - Batch 567/963 - Train loss: 0.25989893  - Train acc: 0.9024 - Val loss: 1.02334487\n",
      "(19.42 min) Epoch 96/300 -- Iteration 92061 - Batch 576/963 - Train loss: 0.25971801  - Train acc: 0.9024 - Val loss: 1.02334487\n",
      "(19.42 min) Epoch 96/300 -- Iteration 92070 - Batch 585/963 - Train loss: 0.25944525  - Train acc: 0.9025 - Val loss: 1.02334487\n",
      "(19.42 min) Epoch 96/300 -- Iteration 92079 - Batch 594/963 - Train loss: 0.25985021  - Train acc: 0.9022 - Val loss: 1.02334487\n",
      "(19.42 min) Epoch 96/300 -- Iteration 92088 - Batch 603/963 - Train loss: 0.25974248  - Train acc: 0.9022 - Val loss: 1.02334487\n",
      "(19.43 min) Epoch 96/300 -- Iteration 92097 - Batch 612/963 - Train loss: 0.25980938  - Train acc: 0.9022 - Val loss: 1.02334487\n",
      "(19.43 min) Epoch 96/300 -- Iteration 92106 - Batch 621/963 - Train loss: 0.26023767  - Train acc: 0.9020 - Val loss: 1.02334487\n",
      "(19.43 min) Epoch 96/300 -- Iteration 92115 - Batch 630/963 - Train loss: 0.26020408  - Train acc: 0.9021 - Val loss: 1.02334487\n",
      "(19.43 min) Epoch 96/300 -- Iteration 92124 - Batch 639/963 - Train loss: 0.26059334  - Train acc: 0.9020 - Val loss: 1.02334487\n",
      "(19.43 min) Epoch 96/300 -- Iteration 92133 - Batch 648/963 - Train loss: 0.26111317  - Train acc: 0.9018 - Val loss: 1.02334487\n",
      "(19.43 min) Epoch 96/300 -- Iteration 92142 - Batch 657/963 - Train loss: 0.26125683  - Train acc: 0.9017 - Val loss: 1.02334487\n",
      "(19.44 min) Epoch 96/300 -- Iteration 92151 - Batch 666/963 - Train loss: 0.26119435  - Train acc: 0.9017 - Val loss: 1.02334487\n",
      "(19.44 min) Epoch 96/300 -- Iteration 92160 - Batch 675/963 - Train loss: 0.26141579  - Train acc: 0.9017 - Val loss: 1.02334487\n",
      "(19.44 min) Epoch 96/300 -- Iteration 92169 - Batch 684/963 - Train loss: 0.26139944  - Train acc: 0.9018 - Val loss: 1.02334487\n",
      "(19.44 min) Epoch 96/300 -- Iteration 92178 - Batch 693/963 - Train loss: 0.26125635  - Train acc: 0.9018 - Val loss: 1.02334487\n",
      "(19.44 min) Epoch 96/300 -- Iteration 92187 - Batch 702/963 - Train loss: 0.26102505  - Train acc: 0.9020 - Val loss: 1.02334487\n",
      "(19.45 min) Epoch 96/300 -- Iteration 92196 - Batch 711/963 - Train loss: 0.26123082  - Train acc: 0.9018 - Val loss: 1.02334487\n",
      "(19.45 min) Epoch 96/300 -- Iteration 92205 - Batch 720/963 - Train loss: 0.26162248  - Train acc: 0.9016 - Val loss: 1.02334487\n",
      "(19.45 min) Epoch 96/300 -- Iteration 92214 - Batch 729/963 - Train loss: 0.26152172  - Train acc: 0.9017 - Val loss: 1.02334487\n",
      "(19.45 min) Epoch 96/300 -- Iteration 92223 - Batch 738/963 - Train loss: 0.26115782  - Train acc: 0.9018 - Val loss: 1.02334487\n",
      "(19.45 min) Epoch 96/300 -- Iteration 92232 - Batch 747/963 - Train loss: 0.26107801  - Train acc: 0.9017 - Val loss: 1.02334487\n",
      "(19.45 min) Epoch 96/300 -- Iteration 92241 - Batch 756/963 - Train loss: 0.26124991  - Train acc: 0.9016 - Val loss: 1.02334487\n",
      "(19.46 min) Epoch 96/300 -- Iteration 92250 - Batch 765/963 - Train loss: 0.26149826  - Train acc: 0.9016 - Val loss: 1.02334487\n",
      "(19.46 min) Epoch 96/300 -- Iteration 92259 - Batch 774/963 - Train loss: 0.26114883  - Train acc: 0.9018 - Val loss: 1.02334487\n",
      "(19.46 min) Epoch 96/300 -- Iteration 92268 - Batch 783/963 - Train loss: 0.26152248  - Train acc: 0.9017 - Val loss: 1.02334487\n",
      "(19.46 min) Epoch 96/300 -- Iteration 92277 - Batch 792/963 - Train loss: 0.26169567  - Train acc: 0.9016 - Val loss: 1.02334487\n",
      "(19.46 min) Epoch 96/300 -- Iteration 92286 - Batch 801/963 - Train loss: 0.26125602  - Train acc: 0.9018 - Val loss: 1.02334487\n",
      "(19.47 min) Epoch 96/300 -- Iteration 92295 - Batch 810/963 - Train loss: 0.26108480  - Train acc: 0.9019 - Val loss: 1.02334487\n",
      "(19.47 min) Epoch 96/300 -- Iteration 92304 - Batch 819/963 - Train loss: 0.26128290  - Train acc: 0.9017 - Val loss: 1.02334487\n",
      "(19.47 min) Epoch 96/300 -- Iteration 92313 - Batch 828/963 - Train loss: 0.26145378  - Train acc: 0.9016 - Val loss: 1.02334487\n",
      "(19.47 min) Epoch 96/300 -- Iteration 92322 - Batch 837/963 - Train loss: 0.26169742  - Train acc: 0.9015 - Val loss: 1.02334487\n",
      "(19.47 min) Epoch 96/300 -- Iteration 92331 - Batch 846/963 - Train loss: 0.26190742  - Train acc: 0.9014 - Val loss: 1.02334487\n",
      "(19.48 min) Epoch 96/300 -- Iteration 92340 - Batch 855/963 - Train loss: 0.26223041  - Train acc: 0.9013 - Val loss: 1.02334487\n",
      "(19.48 min) Epoch 96/300 -- Iteration 92349 - Batch 864/963 - Train loss: 0.26237185  - Train acc: 0.9012 - Val loss: 1.02334487\n",
      "(19.48 min) Epoch 96/300 -- Iteration 92358 - Batch 873/963 - Train loss: 0.26246520  - Train acc: 0.9013 - Val loss: 1.02334487\n",
      "(19.48 min) Epoch 96/300 -- Iteration 92367 - Batch 882/963 - Train loss: 0.26254388  - Train acc: 0.9012 - Val loss: 1.02334487\n",
      "(19.48 min) Epoch 96/300 -- Iteration 92376 - Batch 891/963 - Train loss: 0.26243539  - Train acc: 0.9012 - Val loss: 1.02334487\n",
      "(19.48 min) Epoch 96/300 -- Iteration 92385 - Batch 900/963 - Train loss: 0.26241163  - Train acc: 0.9012 - Val loss: 1.02334487\n",
      "(19.49 min) Epoch 96/300 -- Iteration 92394 - Batch 909/963 - Train loss: 0.26238362  - Train acc: 0.9013 - Val loss: 1.02334487\n",
      "(19.49 min) Epoch 96/300 -- Iteration 92403 - Batch 918/963 - Train loss: 0.26233977  - Train acc: 0.9013 - Val loss: 1.02334487\n",
      "(19.49 min) Epoch 96/300 -- Iteration 92412 - Batch 927/963 - Train loss: 0.26244798  - Train acc: 0.9013 - Val loss: 1.02334487\n",
      "(19.49 min) Epoch 96/300 -- Iteration 92421 - Batch 936/963 - Train loss: 0.26226636  - Train acc: 0.9013 - Val loss: 1.02334487\n",
      "(19.49 min) Epoch 96/300 -- Iteration 92430 - Batch 945/963 - Train loss: 0.26228018  - Train acc: 0.9013 - Val loss: 1.02334487\n",
      "(19.50 min) Epoch 96/300 -- Iteration 92439 - Batch 954/963 - Train loss: 0.26228423  - Train acc: 0.9013 - Val loss: 1.02334487\n",
      "(19.50 min) Epoch 96/300 -- Iteration 92448 - Batch 962/963 - Train loss: 0.26216919  - Train acc: 0.9014 - Val loss: 1.01953161 - Val acc: 0.5800\n",
      "(19.50 min) Epoch 97/300 -- Iteration 92457 - Batch 9/963 - Train loss: 0.25327370  - Train acc: 0.8984 - Val loss: 1.01953161\n",
      "(19.50 min) Epoch 97/300 -- Iteration 92466 - Batch 18/963 - Train loss: 0.26447102  - Train acc: 0.8919 - Val loss: 1.01953161\n",
      "(19.50 min) Epoch 97/300 -- Iteration 92475 - Batch 27/963 - Train loss: 0.26092257  - Train acc: 0.8951 - Val loss: 1.01953161\n",
      "(19.51 min) Epoch 97/300 -- Iteration 92484 - Batch 36/963 - Train loss: 0.26434930  - Train acc: 0.8957 - Val loss: 1.01953161\n",
      "(19.51 min) Epoch 97/300 -- Iteration 92493 - Batch 45/963 - Train loss: 0.26585488  - Train acc: 0.8957 - Val loss: 1.01953161\n",
      "(19.51 min) Epoch 97/300 -- Iteration 92502 - Batch 54/963 - Train loss: 0.26758443  - Train acc: 0.8952 - Val loss: 1.01953161\n",
      "(19.51 min) Epoch 97/300 -- Iteration 92511 - Batch 63/963 - Train loss: 0.26396112  - Train acc: 0.8979 - Val loss: 1.01953161\n",
      "(19.51 min) Epoch 97/300 -- Iteration 92520 - Batch 72/963 - Train loss: 0.26371955  - Train acc: 0.8977 - Val loss: 1.01953161\n",
      "(19.51 min) Epoch 97/300 -- Iteration 92529 - Batch 81/963 - Train loss: 0.26218070  - Train acc: 0.8988 - Val loss: 1.01953161\n",
      "(19.52 min) Epoch 97/300 -- Iteration 92538 - Batch 90/963 - Train loss: 0.26137435  - Train acc: 0.8992 - Val loss: 1.01953161\n",
      "(19.52 min) Epoch 97/300 -- Iteration 92547 - Batch 99/963 - Train loss: 0.26028227  - Train acc: 0.8998 - Val loss: 1.01953161\n",
      "(19.52 min) Epoch 97/300 -- Iteration 92556 - Batch 108/963 - Train loss: 0.26015566  - Train acc: 0.8998 - Val loss: 1.01953161\n",
      "(19.52 min) Epoch 97/300 -- Iteration 92565 - Batch 117/963 - Train loss: 0.25989186  - Train acc: 0.9003 - Val loss: 1.01953161\n",
      "(19.52 min) Epoch 97/300 -- Iteration 92574 - Batch 126/963 - Train loss: 0.26041596  - Train acc: 0.9005 - Val loss: 1.01953161\n",
      "(19.53 min) Epoch 97/300 -- Iteration 92583 - Batch 135/963 - Train loss: 0.26207499  - Train acc: 0.9001 - Val loss: 1.01953161\n",
      "(19.53 min) Epoch 97/300 -- Iteration 92592 - Batch 144/963 - Train loss: 0.26270421  - Train acc: 0.9004 - Val loss: 1.01953161\n",
      "(19.53 min) Epoch 97/300 -- Iteration 92601 - Batch 153/963 - Train loss: 0.26285318  - Train acc: 0.9007 - Val loss: 1.01953161\n",
      "(19.53 min) Epoch 97/300 -- Iteration 92610 - Batch 162/963 - Train loss: 0.26370891  - Train acc: 0.9004 - Val loss: 1.01953161\n",
      "(19.53 min) Epoch 97/300 -- Iteration 92619 - Batch 171/963 - Train loss: 0.26413953  - Train acc: 0.8999 - Val loss: 1.01953161\n",
      "(19.54 min) Epoch 97/300 -- Iteration 92628 - Batch 180/963 - Train loss: 0.26448823  - Train acc: 0.9000 - Val loss: 1.01953161\n",
      "(19.54 min) Epoch 97/300 -- Iteration 92637 - Batch 189/963 - Train loss: 0.26466437  - Train acc: 0.9000 - Val loss: 1.01953161\n",
      "(19.54 min) Epoch 97/300 -- Iteration 92646 - Batch 198/963 - Train loss: 0.26431246  - Train acc: 0.9003 - Val loss: 1.01953161\n",
      "(19.54 min) Epoch 97/300 -- Iteration 92655 - Batch 207/963 - Train loss: 0.26324954  - Train acc: 0.9008 - Val loss: 1.01953161\n",
      "(19.54 min) Epoch 97/300 -- Iteration 92664 - Batch 216/963 - Train loss: 0.26351383  - Train acc: 0.9005 - Val loss: 1.01953161\n",
      "(19.54 min) Epoch 97/300 -- Iteration 92673 - Batch 225/963 - Train loss: 0.26302799  - Train acc: 0.9004 - Val loss: 1.01953161\n",
      "(19.55 min) Epoch 97/300 -- Iteration 92682 - Batch 234/963 - Train loss: 0.26130262  - Train acc: 0.9012 - Val loss: 1.01953161\n",
      "(19.55 min) Epoch 97/300 -- Iteration 92691 - Batch 243/963 - Train loss: 0.26032968  - Train acc: 0.9017 - Val loss: 1.01953161\n",
      "(19.55 min) Epoch 97/300 -- Iteration 92700 - Batch 252/963 - Train loss: 0.25957302  - Train acc: 0.9019 - Val loss: 1.01953161\n",
      "(19.55 min) Epoch 97/300 -- Iteration 92709 - Batch 261/963 - Train loss: 0.25963063  - Train acc: 0.9021 - Val loss: 1.01953161\n",
      "(19.55 min) Epoch 97/300 -- Iteration 92718 - Batch 270/963 - Train loss: 0.25972173  - Train acc: 0.9021 - Val loss: 1.01953161\n",
      "(19.56 min) Epoch 97/300 -- Iteration 92727 - Batch 279/963 - Train loss: 0.26025092  - Train acc: 0.9021 - Val loss: 1.01953161\n",
      "(19.56 min) Epoch 97/300 -- Iteration 92736 - Batch 288/963 - Train loss: 0.25965925  - Train acc: 0.9022 - Val loss: 1.01953161\n",
      "(19.56 min) Epoch 97/300 -- Iteration 92745 - Batch 297/963 - Train loss: 0.25961153  - Train acc: 0.9021 - Val loss: 1.01953161\n",
      "(19.56 min) Epoch 97/300 -- Iteration 92754 - Batch 306/963 - Train loss: 0.26063266  - Train acc: 0.9017 - Val loss: 1.01953161\n",
      "(19.56 min) Epoch 97/300 -- Iteration 92763 - Batch 315/963 - Train loss: 0.26066115  - Train acc: 0.9015 - Val loss: 1.01953161\n",
      "(19.56 min) Epoch 97/300 -- Iteration 92772 - Batch 324/963 - Train loss: 0.26047994  - Train acc: 0.9018 - Val loss: 1.01953161\n",
      "(19.57 min) Epoch 97/300 -- Iteration 92781 - Batch 333/963 - Train loss: 0.26083981  - Train acc: 0.9016 - Val loss: 1.01953161\n",
      "(19.57 min) Epoch 97/300 -- Iteration 92790 - Batch 342/963 - Train loss: 0.26056828  - Train acc: 0.9017 - Val loss: 1.01953161\n",
      "(19.57 min) Epoch 97/300 -- Iteration 92799 - Batch 351/963 - Train loss: 0.26071959  - Train acc: 0.9018 - Val loss: 1.01953161\n",
      "(19.57 min) Epoch 97/300 -- Iteration 92808 - Batch 360/963 - Train loss: 0.26137416  - Train acc: 0.9014 - Val loss: 1.01953161\n",
      "(19.57 min) Epoch 97/300 -- Iteration 92817 - Batch 369/963 - Train loss: 0.26141042  - Train acc: 0.9013 - Val loss: 1.01953161\n",
      "(19.58 min) Epoch 97/300 -- Iteration 92826 - Batch 378/963 - Train loss: 0.26166283  - Train acc: 0.9013 - Val loss: 1.01953161\n",
      "(19.58 min) Epoch 97/300 -- Iteration 92835 - Batch 387/963 - Train loss: 0.26172138  - Train acc: 0.9014 - Val loss: 1.01953161\n",
      "(19.58 min) Epoch 97/300 -- Iteration 92844 - Batch 396/963 - Train loss: 0.26173214  - Train acc: 0.9013 - Val loss: 1.01953161\n",
      "(19.58 min) Epoch 97/300 -- Iteration 92853 - Batch 405/963 - Train loss: 0.26117853  - Train acc: 0.9016 - Val loss: 1.01953161\n",
      "(19.58 min) Epoch 97/300 -- Iteration 92862 - Batch 414/963 - Train loss: 0.26152740  - Train acc: 0.9015 - Val loss: 1.01953161\n",
      "(19.59 min) Epoch 97/300 -- Iteration 92871 - Batch 423/963 - Train loss: 0.26146251  - Train acc: 0.9015 - Val loss: 1.01953161\n",
      "(19.59 min) Epoch 97/300 -- Iteration 92880 - Batch 432/963 - Train loss: 0.26186637  - Train acc: 0.9014 - Val loss: 1.01953161\n",
      "(19.59 min) Epoch 97/300 -- Iteration 92889 - Batch 441/963 - Train loss: 0.26162682  - Train acc: 0.9016 - Val loss: 1.01953161\n",
      "(19.59 min) Epoch 97/300 -- Iteration 92898 - Batch 450/963 - Train loss: 0.26178754  - Train acc: 0.9015 - Val loss: 1.01953161\n",
      "(19.59 min) Epoch 97/300 -- Iteration 92907 - Batch 459/963 - Train loss: 0.26181201  - Train acc: 0.9015 - Val loss: 1.01953161\n",
      "(19.59 min) Epoch 97/300 -- Iteration 92916 - Batch 468/963 - Train loss: 0.26261118  - Train acc: 0.9012 - Val loss: 1.01953161\n",
      "(19.60 min) Epoch 97/300 -- Iteration 92925 - Batch 477/963 - Train loss: 0.26266921  - Train acc: 0.9012 - Val loss: 1.01953161\n",
      "(19.60 min) Epoch 97/300 -- Iteration 92934 - Batch 486/963 - Train loss: 0.26306631  - Train acc: 0.9011 - Val loss: 1.01953161\n",
      "(19.60 min) Epoch 97/300 -- Iteration 92943 - Batch 495/963 - Train loss: 0.26313013  - Train acc: 0.9010 - Val loss: 1.01953161\n",
      "(19.60 min) Epoch 97/300 -- Iteration 92952 - Batch 504/963 - Train loss: 0.26319441  - Train acc: 0.9011 - Val loss: 1.01953161\n",
      "(19.60 min) Epoch 97/300 -- Iteration 92961 - Batch 513/963 - Train loss: 0.26277496  - Train acc: 0.9014 - Val loss: 1.01953161\n",
      "(19.61 min) Epoch 97/300 -- Iteration 92970 - Batch 522/963 - Train loss: 0.26287612  - Train acc: 0.9013 - Val loss: 1.01953161\n",
      "(19.61 min) Epoch 97/300 -- Iteration 92979 - Batch 531/963 - Train loss: 0.26292617  - Train acc: 0.9012 - Val loss: 1.01953161\n",
      "(19.61 min) Epoch 97/300 -- Iteration 92988 - Batch 540/963 - Train loss: 0.26240358  - Train acc: 0.9014 - Val loss: 1.01953161\n",
      "(19.61 min) Epoch 97/300 -- Iteration 92997 - Batch 549/963 - Train loss: 0.26217656  - Train acc: 0.9016 - Val loss: 1.01953161\n",
      "(19.61 min) Epoch 97/300 -- Iteration 93006 - Batch 558/963 - Train loss: 0.26244783  - Train acc: 0.9015 - Val loss: 1.01953161\n",
      "(19.61 min) Epoch 97/300 -- Iteration 93015 - Batch 567/963 - Train loss: 0.26254691  - Train acc: 0.9015 - Val loss: 1.01953161\n",
      "(19.62 min) Epoch 97/300 -- Iteration 93024 - Batch 576/963 - Train loss: 0.26223132  - Train acc: 0.9016 - Val loss: 1.01953161\n",
      "(19.62 min) Epoch 97/300 -- Iteration 93033 - Batch 585/963 - Train loss: 0.26163492  - Train acc: 0.9018 - Val loss: 1.01953161\n",
      "(19.62 min) Epoch 97/300 -- Iteration 93042 - Batch 594/963 - Train loss: 0.26170109  - Train acc: 0.9018 - Val loss: 1.01953161\n",
      "(19.62 min) Epoch 97/300 -- Iteration 93051 - Batch 603/963 - Train loss: 0.26169736  - Train acc: 0.9018 - Val loss: 1.01953161\n",
      "(19.62 min) Epoch 97/300 -- Iteration 93060 - Batch 612/963 - Train loss: 0.26200320  - Train acc: 0.9017 - Val loss: 1.01953161\n",
      "(19.63 min) Epoch 97/300 -- Iteration 93069 - Batch 621/963 - Train loss: 0.26170275  - Train acc: 0.9018 - Val loss: 1.01953161\n",
      "(19.63 min) Epoch 97/300 -- Iteration 93078 - Batch 630/963 - Train loss: 0.26164147  - Train acc: 0.9019 - Val loss: 1.01953161\n",
      "(19.63 min) Epoch 97/300 -- Iteration 93087 - Batch 639/963 - Train loss: 0.26202794  - Train acc: 0.9017 - Val loss: 1.01953161\n",
      "(19.63 min) Epoch 97/300 -- Iteration 93096 - Batch 648/963 - Train loss: 0.26144480  - Train acc: 0.9019 - Val loss: 1.01953161\n",
      "(19.63 min) Epoch 97/300 -- Iteration 93105 - Batch 657/963 - Train loss: 0.26160799  - Train acc: 0.9018 - Val loss: 1.01953161\n",
      "(19.64 min) Epoch 97/300 -- Iteration 93114 - Batch 666/963 - Train loss: 0.26174480  - Train acc: 0.9018 - Val loss: 1.01953161\n",
      "(19.64 min) Epoch 97/300 -- Iteration 93123 - Batch 675/963 - Train loss: 0.26140623  - Train acc: 0.9019 - Val loss: 1.01953161\n",
      "(19.64 min) Epoch 97/300 -- Iteration 93132 - Batch 684/963 - Train loss: 0.26157405  - Train acc: 0.9019 - Val loss: 1.01953161\n",
      "(19.64 min) Epoch 97/300 -- Iteration 93141 - Batch 693/963 - Train loss: 0.26168566  - Train acc: 0.9020 - Val loss: 1.01953161\n",
      "(19.64 min) Epoch 97/300 -- Iteration 93150 - Batch 702/963 - Train loss: 0.26149150  - Train acc: 0.9021 - Val loss: 1.01953161\n",
      "(19.64 min) Epoch 97/300 -- Iteration 93159 - Batch 711/963 - Train loss: 0.26160550  - Train acc: 0.9021 - Val loss: 1.01953161\n",
      "(19.65 min) Epoch 97/300 -- Iteration 93168 - Batch 720/963 - Train loss: 0.26172218  - Train acc: 0.9020 - Val loss: 1.01953161\n",
      "(19.65 min) Epoch 97/300 -- Iteration 93177 - Batch 729/963 - Train loss: 0.26195973  - Train acc: 0.9019 - Val loss: 1.01953161\n",
      "(19.65 min) Epoch 97/300 -- Iteration 93186 - Batch 738/963 - Train loss: 0.26173441  - Train acc: 0.9020 - Val loss: 1.01953161\n",
      "(19.65 min) Epoch 97/300 -- Iteration 93195 - Batch 747/963 - Train loss: 0.26166629  - Train acc: 0.9020 - Val loss: 1.01953161\n",
      "(19.65 min) Epoch 97/300 -- Iteration 93204 - Batch 756/963 - Train loss: 0.26175127  - Train acc: 0.9020 - Val loss: 1.01953161\n",
      "(19.66 min) Epoch 97/300 -- Iteration 93213 - Batch 765/963 - Train loss: 0.26170092  - Train acc: 0.9020 - Val loss: 1.01953161\n",
      "(19.66 min) Epoch 97/300 -- Iteration 93222 - Batch 774/963 - Train loss: 0.26191831  - Train acc: 0.9020 - Val loss: 1.01953161\n",
      "(19.66 min) Epoch 97/300 -- Iteration 93231 - Batch 783/963 - Train loss: 0.26162816  - Train acc: 0.9020 - Val loss: 1.01953161\n",
      "(19.66 min) Epoch 97/300 -- Iteration 93240 - Batch 792/963 - Train loss: 0.26150859  - Train acc: 0.9021 - Val loss: 1.01953161\n",
      "(19.66 min) Epoch 97/300 -- Iteration 93249 - Batch 801/963 - Train loss: 0.26164539  - Train acc: 0.9020 - Val loss: 1.01953161\n",
      "(19.66 min) Epoch 97/300 -- Iteration 93258 - Batch 810/963 - Train loss: 0.26132293  - Train acc: 0.9021 - Val loss: 1.01953161\n",
      "(19.67 min) Epoch 97/300 -- Iteration 93267 - Batch 819/963 - Train loss: 0.26112349  - Train acc: 0.9022 - Val loss: 1.01953161\n",
      "(19.67 min) Epoch 97/300 -- Iteration 93276 - Batch 828/963 - Train loss: 0.26107393  - Train acc: 0.9023 - Val loss: 1.01953161\n",
      "(19.67 min) Epoch 97/300 -- Iteration 93285 - Batch 837/963 - Train loss: 0.26081895  - Train acc: 0.9024 - Val loss: 1.01953161\n",
      "(19.67 min) Epoch 97/300 -- Iteration 93294 - Batch 846/963 - Train loss: 0.26087458  - Train acc: 0.9023 - Val loss: 1.01953161\n",
      "(19.67 min) Epoch 97/300 -- Iteration 93303 - Batch 855/963 - Train loss: 0.26115835  - Train acc: 0.9022 - Val loss: 1.01953161\n",
      "(19.68 min) Epoch 97/300 -- Iteration 93312 - Batch 864/963 - Train loss: 0.26108236  - Train acc: 0.9022 - Val loss: 1.01953161\n",
      "(19.68 min) Epoch 97/300 -- Iteration 93321 - Batch 873/963 - Train loss: 0.26089818  - Train acc: 0.9023 - Val loss: 1.01953161\n",
      "(19.68 min) Epoch 97/300 -- Iteration 93330 - Batch 882/963 - Train loss: 0.26093052  - Train acc: 0.9023 - Val loss: 1.01953161\n",
      "(19.68 min) Epoch 97/300 -- Iteration 93339 - Batch 891/963 - Train loss: 0.26067773  - Train acc: 0.9024 - Val loss: 1.01953161\n",
      "(19.68 min) Epoch 97/300 -- Iteration 93348 - Batch 900/963 - Train loss: 0.26043851  - Train acc: 0.9025 - Val loss: 1.01953161\n",
      "(19.69 min) Epoch 97/300 -- Iteration 93357 - Batch 909/963 - Train loss: 0.26047823  - Train acc: 0.9026 - Val loss: 1.01953161\n",
      "(19.69 min) Epoch 97/300 -- Iteration 93366 - Batch 918/963 - Train loss: 0.26076646  - Train acc: 0.9025 - Val loss: 1.01953161\n",
      "(19.69 min) Epoch 97/300 -- Iteration 93375 - Batch 927/963 - Train loss: 0.26039027  - Train acc: 0.9026 - Val loss: 1.01953161\n",
      "(19.69 min) Epoch 97/300 -- Iteration 93384 - Batch 936/963 - Train loss: 0.26047667  - Train acc: 0.9026 - Val loss: 1.01953161\n",
      "(19.69 min) Epoch 97/300 -- Iteration 93393 - Batch 945/963 - Train loss: 0.26048631  - Train acc: 0.9025 - Val loss: 1.01953161\n",
      "(19.69 min) Epoch 97/300 -- Iteration 93402 - Batch 954/963 - Train loss: 0.26034326  - Train acc: 0.9025 - Val loss: 1.01953161\n",
      "(19.70 min) Epoch 97/300 -- Iteration 93411 - Batch 962/963 - Train loss: 0.26026605  - Train acc: 0.9025 - Val loss: 1.02779675 - Val acc: 0.5700\n",
      "(19.70 min) Epoch 98/300 -- Iteration 93420 - Batch 9/963 - Train loss: 0.25078238  - Train acc: 0.9000 - Val loss: 1.02779675\n",
      "(19.70 min) Epoch 98/300 -- Iteration 93429 - Batch 18/963 - Train loss: 0.25427764  - Train acc: 0.9030 - Val loss: 1.02779675\n",
      "(19.70 min) Epoch 98/300 -- Iteration 93438 - Batch 27/963 - Train loss: 0.25950852  - Train acc: 0.8984 - Val loss: 1.02779675\n",
      "(19.70 min) Epoch 98/300 -- Iteration 93447 - Batch 36/963 - Train loss: 0.25711798  - Train acc: 0.9003 - Val loss: 1.02779675\n",
      "(19.71 min) Epoch 98/300 -- Iteration 93456 - Batch 45/963 - Train loss: 0.24919772  - Train acc: 0.9035 - Val loss: 1.02779675\n",
      "(19.71 min) Epoch 98/300 -- Iteration 93465 - Batch 54/963 - Train loss: 0.25150401  - Train acc: 0.9030 - Val loss: 1.02779675\n",
      "(19.71 min) Epoch 98/300 -- Iteration 93474 - Batch 63/963 - Train loss: 0.25225562  - Train acc: 0.9030 - Val loss: 1.02779675\n",
      "(19.71 min) Epoch 98/300 -- Iteration 93483 - Batch 72/963 - Train loss: 0.25239246  - Train acc: 0.9026 - Val loss: 1.02779675\n",
      "(19.71 min) Epoch 98/300 -- Iteration 93492 - Batch 81/963 - Train loss: 0.25271069  - Train acc: 0.9024 - Val loss: 1.02779675\n",
      "(19.72 min) Epoch 98/300 -- Iteration 93501 - Batch 90/963 - Train loss: 0.25346229  - Train acc: 0.9024 - Val loss: 1.02779675\n",
      "(19.72 min) Epoch 98/300 -- Iteration 93510 - Batch 99/963 - Train loss: 0.25408091  - Train acc: 0.9029 - Val loss: 1.02779675\n",
      "(19.72 min) Epoch 98/300 -- Iteration 93519 - Batch 108/963 - Train loss: 0.25778741  - Train acc: 0.9017 - Val loss: 1.02779675\n",
      "(19.72 min) Epoch 98/300 -- Iteration 93528 - Batch 117/963 - Train loss: 0.25645353  - Train acc: 0.9018 - Val loss: 1.02779675\n",
      "(19.72 min) Epoch 98/300 -- Iteration 93537 - Batch 126/963 - Train loss: 0.25637748  - Train acc: 0.9011 - Val loss: 1.02779675\n",
      "(19.72 min) Epoch 98/300 -- Iteration 93546 - Batch 135/963 - Train loss: 0.25714314  - Train acc: 0.9009 - Val loss: 1.02779675\n",
      "(19.73 min) Epoch 98/300 -- Iteration 93555 - Batch 144/963 - Train loss: 0.25810304  - Train acc: 0.9001 - Val loss: 1.02779675\n",
      "(19.73 min) Epoch 98/300 -- Iteration 93564 - Batch 153/963 - Train loss: 0.25851260  - Train acc: 0.9000 - Val loss: 1.02779675\n",
      "(19.73 min) Epoch 98/300 -- Iteration 93573 - Batch 162/963 - Train loss: 0.25897288  - Train acc: 0.9003 - Val loss: 1.02779675\n",
      "(19.73 min) Epoch 98/300 -- Iteration 93582 - Batch 171/963 - Train loss: 0.25806638  - Train acc: 0.9007 - Val loss: 1.02779675\n",
      "(19.73 min) Epoch 98/300 -- Iteration 93591 - Batch 180/963 - Train loss: 0.25808588  - Train acc: 0.9005 - Val loss: 1.02779675\n",
      "(19.74 min) Epoch 98/300 -- Iteration 93600 - Batch 189/963 - Train loss: 0.25797915  - Train acc: 0.9005 - Val loss: 1.02779675\n",
      "(19.74 min) Epoch 98/300 -- Iteration 93609 - Batch 198/963 - Train loss: 0.25864074  - Train acc: 0.9004 - Val loss: 1.02779675\n",
      "(19.74 min) Epoch 98/300 -- Iteration 93618 - Batch 207/963 - Train loss: 0.25813659  - Train acc: 0.9005 - Val loss: 1.02779675\n",
      "(19.74 min) Epoch 98/300 -- Iteration 93627 - Batch 216/963 - Train loss: 0.25842659  - Train acc: 0.9006 - Val loss: 1.02779675\n",
      "(19.74 min) Epoch 98/300 -- Iteration 93636 - Batch 225/963 - Train loss: 0.25810701  - Train acc: 0.9010 - Val loss: 1.02779675\n",
      "(19.75 min) Epoch 98/300 -- Iteration 93645 - Batch 234/963 - Train loss: 0.25810700  - Train acc: 0.9011 - Val loss: 1.02779675\n",
      "(19.75 min) Epoch 98/300 -- Iteration 93654 - Batch 243/963 - Train loss: 0.25708883  - Train acc: 0.9016 - Val loss: 1.02779675\n",
      "(19.75 min) Epoch 98/300 -- Iteration 93663 - Batch 252/963 - Train loss: 0.25701580  - Train acc: 0.9014 - Val loss: 1.02779675\n",
      "(19.75 min) Epoch 98/300 -- Iteration 93672 - Batch 261/963 - Train loss: 0.25654557  - Train acc: 0.9017 - Val loss: 1.02779675\n",
      "(19.75 min) Epoch 98/300 -- Iteration 93681 - Batch 270/963 - Train loss: 0.25696696  - Train acc: 0.9015 - Val loss: 1.02779675\n",
      "(19.75 min) Epoch 98/300 -- Iteration 93690 - Batch 279/963 - Train loss: 0.25636002  - Train acc: 0.9021 - Val loss: 1.02779675\n",
      "(19.76 min) Epoch 98/300 -- Iteration 93699 - Batch 288/963 - Train loss: 0.25618232  - Train acc: 0.9022 - Val loss: 1.02779675\n",
      "(19.76 min) Epoch 98/300 -- Iteration 93708 - Batch 297/963 - Train loss: 0.25565556  - Train acc: 0.9024 - Val loss: 1.02779675\n",
      "(19.76 min) Epoch 98/300 -- Iteration 93717 - Batch 306/963 - Train loss: 0.25595082  - Train acc: 0.9025 - Val loss: 1.02779675\n",
      "(19.76 min) Epoch 98/300 -- Iteration 93726 - Batch 315/963 - Train loss: 0.25573195  - Train acc: 0.9027 - Val loss: 1.02779675\n",
      "(19.76 min) Epoch 98/300 -- Iteration 93735 - Batch 324/963 - Train loss: 0.25618145  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.77 min) Epoch 98/300 -- Iteration 93744 - Batch 333/963 - Train loss: 0.25580463  - Train acc: 0.9031 - Val loss: 1.02779675\n",
      "(19.77 min) Epoch 98/300 -- Iteration 93753 - Batch 342/963 - Train loss: 0.25583744  - Train acc: 0.9032 - Val loss: 1.02779675\n",
      "(19.77 min) Epoch 98/300 -- Iteration 93762 - Batch 351/963 - Train loss: 0.25697671  - Train acc: 0.9027 - Val loss: 1.02779675\n",
      "(19.77 min) Epoch 98/300 -- Iteration 93771 - Batch 360/963 - Train loss: 0.25738735  - Train acc: 0.9025 - Val loss: 1.02779675\n",
      "(19.77 min) Epoch 98/300 -- Iteration 93780 - Batch 369/963 - Train loss: 0.25712468  - Train acc: 0.9026 - Val loss: 1.02779675\n",
      "(19.77 min) Epoch 98/300 -- Iteration 93789 - Batch 378/963 - Train loss: 0.25779796  - Train acc: 0.9025 - Val loss: 1.02779675\n",
      "(19.78 min) Epoch 98/300 -- Iteration 93798 - Batch 387/963 - Train loss: 0.25777001  - Train acc: 0.9022 - Val loss: 1.02779675\n",
      "(19.78 min) Epoch 98/300 -- Iteration 93807 - Batch 396/963 - Train loss: 0.25737823  - Train acc: 0.9024 - Val loss: 1.02779675\n",
      "(19.78 min) Epoch 98/300 -- Iteration 93816 - Batch 405/963 - Train loss: 0.25710907  - Train acc: 0.9024 - Val loss: 1.02779675\n",
      "(19.78 min) Epoch 98/300 -- Iteration 93825 - Batch 414/963 - Train loss: 0.25715628  - Train acc: 0.9022 - Val loss: 1.02779675\n",
      "(19.78 min) Epoch 98/300 -- Iteration 93834 - Batch 423/963 - Train loss: 0.25764951  - Train acc: 0.9021 - Val loss: 1.02779675\n",
      "(19.79 min) Epoch 98/300 -- Iteration 93843 - Batch 432/963 - Train loss: 0.25733951  - Train acc: 0.9024 - Val loss: 1.02779675\n",
      "(19.79 min) Epoch 98/300 -- Iteration 93852 - Batch 441/963 - Train loss: 0.25718574  - Train acc: 0.9026 - Val loss: 1.02779675\n",
      "(19.79 min) Epoch 98/300 -- Iteration 93861 - Batch 450/963 - Train loss: 0.25691790  - Train acc: 0.9026 - Val loss: 1.02779675\n",
      "(19.79 min) Epoch 98/300 -- Iteration 93870 - Batch 459/963 - Train loss: 0.25702044  - Train acc: 0.9026 - Val loss: 1.02779675\n",
      "(19.79 min) Epoch 98/300 -- Iteration 93879 - Batch 468/963 - Train loss: 0.25694031  - Train acc: 0.9027 - Val loss: 1.02779675\n",
      "(19.80 min) Epoch 98/300 -- Iteration 93888 - Batch 477/963 - Train loss: 0.25680436  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.80 min) Epoch 98/300 -- Iteration 93897 - Batch 486/963 - Train loss: 0.25699824  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.80 min) Epoch 98/300 -- Iteration 93906 - Batch 495/963 - Train loss: 0.25757599  - Train acc: 0.9025 - Val loss: 1.02779675\n",
      "(19.80 min) Epoch 98/300 -- Iteration 93915 - Batch 504/963 - Train loss: 0.25797776  - Train acc: 0.9024 - Val loss: 1.02779675\n",
      "(19.80 min) Epoch 98/300 -- Iteration 93924 - Batch 513/963 - Train loss: 0.25821526  - Train acc: 0.9022 - Val loss: 1.02779675\n",
      "(19.80 min) Epoch 98/300 -- Iteration 93933 - Batch 522/963 - Train loss: 0.25819762  - Train acc: 0.9023 - Val loss: 1.02779675\n",
      "(19.81 min) Epoch 98/300 -- Iteration 93942 - Batch 531/963 - Train loss: 0.25854597  - Train acc: 0.9022 - Val loss: 1.02779675\n",
      "(19.81 min) Epoch 98/300 -- Iteration 93951 - Batch 540/963 - Train loss: 0.25865667  - Train acc: 0.9022 - Val loss: 1.02779675\n",
      "(19.81 min) Epoch 98/300 -- Iteration 93960 - Batch 549/963 - Train loss: 0.25879889  - Train acc: 0.9021 - Val loss: 1.02779675\n",
      "(19.81 min) Epoch 98/300 -- Iteration 93969 - Batch 558/963 - Train loss: 0.25810067  - Train acc: 0.9023 - Val loss: 1.02779675\n",
      "(19.81 min) Epoch 98/300 -- Iteration 93978 - Batch 567/963 - Train loss: 0.25799190  - Train acc: 0.9024 - Val loss: 1.02779675\n",
      "(19.82 min) Epoch 98/300 -- Iteration 93987 - Batch 576/963 - Train loss: 0.25794485  - Train acc: 0.9022 - Val loss: 1.02779675\n",
      "(19.82 min) Epoch 98/300 -- Iteration 93996 - Batch 585/963 - Train loss: 0.25773857  - Train acc: 0.9022 - Val loss: 1.02779675\n",
      "(19.82 min) Epoch 98/300 -- Iteration 94005 - Batch 594/963 - Train loss: 0.25783515  - Train acc: 0.9023 - Val loss: 1.02779675\n",
      "(19.82 min) Epoch 98/300 -- Iteration 94014 - Batch 603/963 - Train loss: 0.25759046  - Train acc: 0.9025 - Val loss: 1.02779675\n",
      "(19.82 min) Epoch 98/300 -- Iteration 94023 - Batch 612/963 - Train loss: 0.25726561  - Train acc: 0.9027 - Val loss: 1.02779675\n",
      "(19.82 min) Epoch 98/300 -- Iteration 94032 - Batch 621/963 - Train loss: 0.25753873  - Train acc: 0.9026 - Val loss: 1.02779675\n",
      "(19.83 min) Epoch 98/300 -- Iteration 94041 - Batch 630/963 - Train loss: 0.25715053  - Train acc: 0.9027 - Val loss: 1.02779675\n",
      "(19.83 min) Epoch 98/300 -- Iteration 94050 - Batch 639/963 - Train loss: 0.25733702  - Train acc: 0.9027 - Val loss: 1.02779675\n",
      "(19.83 min) Epoch 98/300 -- Iteration 94059 - Batch 648/963 - Train loss: 0.25714295  - Train acc: 0.9029 - Val loss: 1.02779675\n",
      "(19.83 min) Epoch 98/300 -- Iteration 94068 - Batch 657/963 - Train loss: 0.25730285  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.83 min) Epoch 98/300 -- Iteration 94077 - Batch 666/963 - Train loss: 0.25761930  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.84 min) Epoch 98/300 -- Iteration 94086 - Batch 675/963 - Train loss: 0.25731583  - Train acc: 0.9029 - Val loss: 1.02779675\n",
      "(19.84 min) Epoch 98/300 -- Iteration 94095 - Batch 684/963 - Train loss: 0.25704591  - Train acc: 0.9030 - Val loss: 1.02779675\n",
      "(19.84 min) Epoch 98/300 -- Iteration 94104 - Batch 693/963 - Train loss: 0.25697099  - Train acc: 0.9030 - Val loss: 1.02779675\n",
      "(19.84 min) Epoch 98/300 -- Iteration 94113 - Batch 702/963 - Train loss: 0.25744876  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.84 min) Epoch 98/300 -- Iteration 94122 - Batch 711/963 - Train loss: 0.25754289  - Train acc: 0.9027 - Val loss: 1.02779675\n",
      "(19.85 min) Epoch 98/300 -- Iteration 94131 - Batch 720/963 - Train loss: 0.25720885  - Train acc: 0.9027 - Val loss: 1.02779675\n",
      "(19.85 min) Epoch 98/300 -- Iteration 94140 - Batch 729/963 - Train loss: 0.25722558  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.85 min) Epoch 98/300 -- Iteration 94149 - Batch 738/963 - Train loss: 0.25746143  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.85 min) Epoch 98/300 -- Iteration 94158 - Batch 747/963 - Train loss: 0.25733196  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.85 min) Epoch 98/300 -- Iteration 94167 - Batch 756/963 - Train loss: 0.25741843  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.85 min) Epoch 98/300 -- Iteration 94176 - Batch 765/963 - Train loss: 0.25743443  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.86 min) Epoch 98/300 -- Iteration 94185 - Batch 774/963 - Train loss: 0.25719156  - Train acc: 0.9029 - Val loss: 1.02779675\n",
      "(19.86 min) Epoch 98/300 -- Iteration 94194 - Batch 783/963 - Train loss: 0.25735233  - Train acc: 0.9029 - Val loss: 1.02779675\n",
      "(19.86 min) Epoch 98/300 -- Iteration 94203 - Batch 792/963 - Train loss: 0.25747875  - Train acc: 0.9028 - Val loss: 1.02779675\n",
      "(19.86 min) Epoch 98/300 -- Iteration 94212 - Batch 801/963 - Train loss: 0.25717218  - Train acc: 0.9030 - Val loss: 1.02779675\n",
      "(19.86 min) Epoch 98/300 -- Iteration 94221 - Batch 810/963 - Train loss: 0.25731247  - Train acc: 0.9029 - Val loss: 1.02779675\n",
      "(19.87 min) Epoch 98/300 -- Iteration 94230 - Batch 819/963 - Train loss: 0.25719880  - Train acc: 0.9029 - Val loss: 1.02779675\n",
      "(19.87 min) Epoch 98/300 -- Iteration 94239 - Batch 828/963 - Train loss: 0.25708237  - Train acc: 0.9030 - Val loss: 1.02779675\n",
      "(19.87 min) Epoch 98/300 -- Iteration 94248 - Batch 837/963 - Train loss: 0.25712744  - Train acc: 0.9030 - Val loss: 1.02779675\n",
      "(19.87 min) Epoch 98/300 -- Iteration 94257 - Batch 846/963 - Train loss: 0.25684909  - Train acc: 0.9032 - Val loss: 1.02779675\n",
      "(19.87 min) Epoch 98/300 -- Iteration 94266 - Batch 855/963 - Train loss: 0.25680390  - Train acc: 0.9032 - Val loss: 1.02779675\n",
      "(19.88 min) Epoch 98/300 -- Iteration 94275 - Batch 864/963 - Train loss: 0.25660604  - Train acc: 0.9033 - Val loss: 1.02779675\n",
      "(19.88 min) Epoch 98/300 -- Iteration 94284 - Batch 873/963 - Train loss: 0.25671263  - Train acc: 0.9033 - Val loss: 1.02779675\n",
      "(19.88 min) Epoch 98/300 -- Iteration 94293 - Batch 882/963 - Train loss: 0.25660838  - Train acc: 0.9033 - Val loss: 1.02779675\n",
      "(19.88 min) Epoch 98/300 -- Iteration 94302 - Batch 891/963 - Train loss: 0.25660194  - Train acc: 0.9033 - Val loss: 1.02779675\n",
      "(19.88 min) Epoch 98/300 -- Iteration 94311 - Batch 900/963 - Train loss: 0.25648449  - Train acc: 0.9034 - Val loss: 1.02779675\n",
      "(19.88 min) Epoch 98/300 -- Iteration 94320 - Batch 909/963 - Train loss: 0.25653708  - Train acc: 0.9033 - Val loss: 1.02779675\n",
      "(19.89 min) Epoch 98/300 -- Iteration 94329 - Batch 918/963 - Train loss: 0.25656254  - Train acc: 0.9033 - Val loss: 1.02779675\n",
      "(19.89 min) Epoch 98/300 -- Iteration 94338 - Batch 927/963 - Train loss: 0.25672760  - Train acc: 0.9032 - Val loss: 1.02779675\n",
      "(19.89 min) Epoch 98/300 -- Iteration 94347 - Batch 936/963 - Train loss: 0.25673997  - Train acc: 0.9032 - Val loss: 1.02779675\n",
      "(19.89 min) Epoch 98/300 -- Iteration 94356 - Batch 945/963 - Train loss: 0.25683102  - Train acc: 0.9033 - Val loss: 1.02779675\n",
      "(19.89 min) Epoch 98/300 -- Iteration 94365 - Batch 954/963 - Train loss: 0.25689287  - Train acc: 0.9033 - Val loss: 1.02779675\n",
      "(19.90 min) Epoch 98/300 -- Iteration 94374 - Batch 962/963 - Train loss: 0.25675560  - Train acc: 0.9033 - Val loss: 1.03105426 - Val acc: 0.5750\n",
      "(19.90 min) Epoch 99/300 -- Iteration 94383 - Batch 9/963 - Train loss: 0.28524627  - Train acc: 0.8914 - Val loss: 1.03105426\n",
      "(19.90 min) Epoch 99/300 -- Iteration 94392 - Batch 18/963 - Train loss: 0.28197954  - Train acc: 0.8898 - Val loss: 1.03105426\n",
      "(19.90 min) Epoch 99/300 -- Iteration 94401 - Batch 27/963 - Train loss: 0.27501262  - Train acc: 0.8937 - Val loss: 1.03105426\n",
      "(19.90 min) Epoch 99/300 -- Iteration 94410 - Batch 36/963 - Train loss: 0.26900794  - Train acc: 0.8989 - Val loss: 1.03105426\n",
      "(19.91 min) Epoch 99/300 -- Iteration 94419 - Batch 45/963 - Train loss: 0.26436071  - Train acc: 0.9005 - Val loss: 1.03105426\n",
      "(19.91 min) Epoch 99/300 -- Iteration 94428 - Batch 54/963 - Train loss: 0.26446726  - Train acc: 0.9007 - Val loss: 1.03105426\n",
      "(19.91 min) Epoch 99/300 -- Iteration 94437 - Batch 63/963 - Train loss: 0.26261224  - Train acc: 0.9016 - Val loss: 1.03105426\n",
      "(19.91 min) Epoch 99/300 -- Iteration 94446 - Batch 72/963 - Train loss: 0.26061875  - Train acc: 0.9025 - Val loss: 1.03105426\n",
      "(19.91 min) Epoch 99/300 -- Iteration 94455 - Batch 81/963 - Train loss: 0.26303224  - Train acc: 0.9014 - Val loss: 1.03105426\n",
      "(19.91 min) Epoch 99/300 -- Iteration 94464 - Batch 90/963 - Train loss: 0.26004874  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(19.92 min) Epoch 99/300 -- Iteration 94473 - Batch 99/963 - Train loss: 0.26169378  - Train acc: 0.9017 - Val loss: 1.03105426\n",
      "(19.92 min) Epoch 99/300 -- Iteration 94482 - Batch 108/963 - Train loss: 0.26294258  - Train acc: 0.9018 - Val loss: 1.03105426\n",
      "(19.92 min) Epoch 99/300 -- Iteration 94491 - Batch 117/963 - Train loss: 0.26250865  - Train acc: 0.9020 - Val loss: 1.03105426\n",
      "(19.92 min) Epoch 99/300 -- Iteration 94500 - Batch 126/963 - Train loss: 0.26191451  - Train acc: 0.9021 - Val loss: 1.03105426\n",
      "(19.92 min) Epoch 99/300 -- Iteration 94509 - Batch 135/963 - Train loss: 0.26070101  - Train acc: 0.9023 - Val loss: 1.03105426\n",
      "(19.93 min) Epoch 99/300 -- Iteration 94518 - Batch 144/963 - Train loss: 0.26054224  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(19.93 min) Epoch 99/300 -- Iteration 94527 - Batch 153/963 - Train loss: 0.26126039  - Train acc: 0.9023 - Val loss: 1.03105426\n",
      "(19.93 min) Epoch 99/300 -- Iteration 94536 - Batch 162/963 - Train loss: 0.26088630  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(19.93 min) Epoch 99/300 -- Iteration 94545 - Batch 171/963 - Train loss: 0.26152846  - Train acc: 0.9021 - Val loss: 1.03105426\n",
      "(19.93 min) Epoch 99/300 -- Iteration 94554 - Batch 180/963 - Train loss: 0.26194344  - Train acc: 0.9020 - Val loss: 1.03105426\n",
      "(19.94 min) Epoch 99/300 -- Iteration 94563 - Batch 189/963 - Train loss: 0.26367255  - Train acc: 0.9015 - Val loss: 1.03105426\n",
      "(19.94 min) Epoch 99/300 -- Iteration 94572 - Batch 198/963 - Train loss: 0.26349586  - Train acc: 0.9011 - Val loss: 1.03105426\n",
      "(19.94 min) Epoch 99/300 -- Iteration 94581 - Batch 207/963 - Train loss: 0.26363493  - Train acc: 0.9008 - Val loss: 1.03105426\n",
      "(19.94 min) Epoch 99/300 -- Iteration 94590 - Batch 216/963 - Train loss: 0.26333970  - Train acc: 0.9007 - Val loss: 1.03105426\n",
      "(19.94 min) Epoch 99/300 -- Iteration 94599 - Batch 225/963 - Train loss: 0.26266770  - Train acc: 0.9009 - Val loss: 1.03105426\n",
      "(19.94 min) Epoch 99/300 -- Iteration 94608 - Batch 234/963 - Train loss: 0.26198077  - Train acc: 0.9010 - Val loss: 1.03105426\n",
      "(19.95 min) Epoch 99/300 -- Iteration 94617 - Batch 243/963 - Train loss: 0.26242057  - Train acc: 0.9008 - Val loss: 1.03105426\n",
      "(19.95 min) Epoch 99/300 -- Iteration 94626 - Batch 252/963 - Train loss: 0.26252505  - Train acc: 0.9007 - Val loss: 1.03105426\n",
      "(19.95 min) Epoch 99/300 -- Iteration 94635 - Batch 261/963 - Train loss: 0.26189882  - Train acc: 0.9010 - Val loss: 1.03105426\n",
      "(19.95 min) Epoch 99/300 -- Iteration 94644 - Batch 270/963 - Train loss: 0.26168460  - Train acc: 0.9010 - Val loss: 1.03105426\n",
      "(19.95 min) Epoch 99/300 -- Iteration 94653 - Batch 279/963 - Train loss: 0.26230241  - Train acc: 0.9008 - Val loss: 1.03105426\n",
      "(19.96 min) Epoch 99/300 -- Iteration 94662 - Batch 288/963 - Train loss: 0.26210199  - Train acc: 0.9010 - Val loss: 1.03105426\n",
      "(19.96 min) Epoch 99/300 -- Iteration 94671 - Batch 297/963 - Train loss: 0.26170120  - Train acc: 0.9013 - Val loss: 1.03105426\n",
      "(19.96 min) Epoch 99/300 -- Iteration 94680 - Batch 306/963 - Train loss: 0.26146510  - Train acc: 0.9015 - Val loss: 1.03105426\n",
      "(19.96 min) Epoch 99/300 -- Iteration 94689 - Batch 315/963 - Train loss: 0.26238920  - Train acc: 0.9010 - Val loss: 1.03105426\n",
      "(19.96 min) Epoch 99/300 -- Iteration 94698 - Batch 324/963 - Train loss: 0.26211017  - Train acc: 0.9011 - Val loss: 1.03105426\n",
      "(19.96 min) Epoch 99/300 -- Iteration 94707 - Batch 333/963 - Train loss: 0.26196962  - Train acc: 0.9012 - Val loss: 1.03105426\n",
      "(19.97 min) Epoch 99/300 -- Iteration 94716 - Batch 342/963 - Train loss: 0.26142154  - Train acc: 0.9015 - Val loss: 1.03105426\n",
      "(19.97 min) Epoch 99/300 -- Iteration 94725 - Batch 351/963 - Train loss: 0.26094159  - Train acc: 0.9015 - Val loss: 1.03105426\n",
      "(19.97 min) Epoch 99/300 -- Iteration 94734 - Batch 360/963 - Train loss: 0.25990305  - Train acc: 0.9019 - Val loss: 1.03105426\n",
      "(19.97 min) Epoch 99/300 -- Iteration 94743 - Batch 369/963 - Train loss: 0.25980434  - Train acc: 0.9023 - Val loss: 1.03105426\n",
      "(19.97 min) Epoch 99/300 -- Iteration 94752 - Batch 378/963 - Train loss: 0.25962253  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(19.98 min) Epoch 99/300 -- Iteration 94761 - Batch 387/963 - Train loss: 0.25983931  - Train acc: 0.9023 - Val loss: 1.03105426\n",
      "(19.98 min) Epoch 99/300 -- Iteration 94770 - Batch 396/963 - Train loss: 0.26012134  - Train acc: 0.9020 - Val loss: 1.03105426\n",
      "(19.98 min) Epoch 99/300 -- Iteration 94779 - Batch 405/963 - Train loss: 0.26000720  - Train acc: 0.9021 - Val loss: 1.03105426\n",
      "(19.98 min) Epoch 99/300 -- Iteration 94788 - Batch 414/963 - Train loss: 0.25981754  - Train acc: 0.9021 - Val loss: 1.03105426\n",
      "(19.98 min) Epoch 99/300 -- Iteration 94797 - Batch 423/963 - Train loss: 0.25924101  - Train acc: 0.9023 - Val loss: 1.03105426\n",
      "(19.99 min) Epoch 99/300 -- Iteration 94806 - Batch 432/963 - Train loss: 0.25874711  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(19.99 min) Epoch 99/300 -- Iteration 94815 - Batch 441/963 - Train loss: 0.25924318  - Train acc: 0.9023 - Val loss: 1.03105426\n",
      "(19.99 min) Epoch 99/300 -- Iteration 94824 - Batch 450/963 - Train loss: 0.25929701  - Train acc: 0.9022 - Val loss: 1.03105426\n",
      "(19.99 min) Epoch 99/300 -- Iteration 94833 - Batch 459/963 - Train loss: 0.25889076  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(19.99 min) Epoch 99/300 -- Iteration 94842 - Batch 468/963 - Train loss: 0.25877408  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(19.99 min) Epoch 99/300 -- Iteration 94851 - Batch 477/963 - Train loss: 0.25893118  - Train acc: 0.9023 - Val loss: 1.03105426\n",
      "(20.00 min) Epoch 99/300 -- Iteration 94860 - Batch 486/963 - Train loss: 0.25883249  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(20.00 min) Epoch 99/300 -- Iteration 94869 - Batch 495/963 - Train loss: 0.25869354  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(20.00 min) Epoch 99/300 -- Iteration 94878 - Batch 504/963 - Train loss: 0.25888568  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(20.00 min) Epoch 99/300 -- Iteration 94887 - Batch 513/963 - Train loss: 0.25864636  - Train acc: 0.9025 - Val loss: 1.03105426\n",
      "(20.00 min) Epoch 99/300 -- Iteration 94896 - Batch 522/963 - Train loss: 0.25859192  - Train acc: 0.9025 - Val loss: 1.03105426\n",
      "(20.01 min) Epoch 99/300 -- Iteration 94905 - Batch 531/963 - Train loss: 0.25856945  - Train acc: 0.9026 - Val loss: 1.03105426\n",
      "(20.01 min) Epoch 99/300 -- Iteration 94914 - Batch 540/963 - Train loss: 0.25842345  - Train acc: 0.9027 - Val loss: 1.03105426\n",
      "(20.01 min) Epoch 99/300 -- Iteration 94923 - Batch 549/963 - Train loss: 0.25805830  - Train acc: 0.9028 - Val loss: 1.03105426\n",
      "(20.01 min) Epoch 99/300 -- Iteration 94932 - Batch 558/963 - Train loss: 0.25846040  - Train acc: 0.9026 - Val loss: 1.03105426\n",
      "(20.01 min) Epoch 99/300 -- Iteration 94941 - Batch 567/963 - Train loss: 0.25887157  - Train acc: 0.9024 - Val loss: 1.03105426\n",
      "(20.01 min) Epoch 99/300 -- Iteration 94950 - Batch 576/963 - Train loss: 0.25872047  - Train acc: 0.9025 - Val loss: 1.03105426\n",
      "(20.02 min) Epoch 99/300 -- Iteration 94959 - Batch 585/963 - Train loss: 0.25865813  - Train acc: 0.9027 - Val loss: 1.03105426\n",
      "(20.02 min) Epoch 99/300 -- Iteration 94968 - Batch 594/963 - Train loss: 0.25859566  - Train acc: 0.9027 - Val loss: 1.03105426\n",
      "(20.02 min) Epoch 99/300 -- Iteration 94977 - Batch 603/963 - Train loss: 0.25839581  - Train acc: 0.9028 - Val loss: 1.03105426\n",
      "(20.02 min) Epoch 99/300 -- Iteration 94986 - Batch 612/963 - Train loss: 0.25830286  - Train acc: 0.9028 - Val loss: 1.03105426\n",
      "(20.02 min) Epoch 99/300 -- Iteration 94995 - Batch 621/963 - Train loss: 0.25816420  - Train acc: 0.9029 - Val loss: 1.03105426\n",
      "(20.03 min) Epoch 99/300 -- Iteration 95004 - Batch 630/963 - Train loss: 0.25841945  - Train acc: 0.9028 - Val loss: 1.03105426\n",
      "(20.03 min) Epoch 99/300 -- Iteration 95013 - Batch 639/963 - Train loss: 0.25841998  - Train acc: 0.9030 - Val loss: 1.03105426\n",
      "(20.03 min) Epoch 99/300 -- Iteration 95022 - Batch 648/963 - Train loss: 0.25837235  - Train acc: 0.9030 - Val loss: 1.03105426\n",
      "(20.03 min) Epoch 99/300 -- Iteration 95031 - Batch 657/963 - Train loss: 0.25813056  - Train acc: 0.9032 - Val loss: 1.03105426\n",
      "(20.03 min) Epoch 99/300 -- Iteration 95040 - Batch 666/963 - Train loss: 0.25804711  - Train acc: 0.9033 - Val loss: 1.03105426\n",
      "(20.04 min) Epoch 99/300 -- Iteration 95049 - Batch 675/963 - Train loss: 0.25823834  - Train acc: 0.9032 - Val loss: 1.03105426\n",
      "(20.04 min) Epoch 99/300 -- Iteration 95058 - Batch 684/963 - Train loss: 0.25871866  - Train acc: 0.9031 - Val loss: 1.03105426\n",
      "(20.04 min) Epoch 99/300 -- Iteration 95067 - Batch 693/963 - Train loss: 0.25884507  - Train acc: 0.9031 - Val loss: 1.03105426\n",
      "(20.04 min) Epoch 99/300 -- Iteration 95076 - Batch 702/963 - Train loss: 0.25872058  - Train acc: 0.9032 - Val loss: 1.03105426\n",
      "(20.04 min) Epoch 99/300 -- Iteration 95085 - Batch 711/963 - Train loss: 0.25882530  - Train acc: 0.9032 - Val loss: 1.03105426\n",
      "(20.04 min) Epoch 99/300 -- Iteration 95094 - Batch 720/963 - Train loss: 0.25844249  - Train acc: 0.9034 - Val loss: 1.03105426\n",
      "(20.05 min) Epoch 99/300 -- Iteration 95103 - Batch 729/963 - Train loss: 0.25861127  - Train acc: 0.9034 - Val loss: 1.03105426\n",
      "(20.05 min) Epoch 99/300 -- Iteration 95112 - Batch 738/963 - Train loss: 0.25852473  - Train acc: 0.9033 - Val loss: 1.03105426\n",
      "(20.05 min) Epoch 99/300 -- Iteration 95121 - Batch 747/963 - Train loss: 0.25847902  - Train acc: 0.9033 - Val loss: 1.03105426\n",
      "(20.05 min) Epoch 99/300 -- Iteration 95130 - Batch 756/963 - Train loss: 0.25872010  - Train acc: 0.9032 - Val loss: 1.03105426\n",
      "(20.05 min) Epoch 99/300 -- Iteration 95139 - Batch 765/963 - Train loss: 0.25841894  - Train acc: 0.9033 - Val loss: 1.03105426\n",
      "(20.06 min) Epoch 99/300 -- Iteration 95148 - Batch 774/963 - Train loss: 0.25859150  - Train acc: 0.9033 - Val loss: 1.03105426\n",
      "(20.06 min) Epoch 99/300 -- Iteration 95157 - Batch 783/963 - Train loss: 0.25905213  - Train acc: 0.9032 - Val loss: 1.03105426\n",
      "(20.06 min) Epoch 99/300 -- Iteration 95166 - Batch 792/963 - Train loss: 0.25877231  - Train acc: 0.9032 - Val loss: 1.03105426\n",
      "(20.06 min) Epoch 99/300 -- Iteration 95175 - Batch 801/963 - Train loss: 0.25905297  - Train acc: 0.9031 - Val loss: 1.03105426\n",
      "(20.06 min) Epoch 99/300 -- Iteration 95184 - Batch 810/963 - Train loss: 0.25859777  - Train acc: 0.9033 - Val loss: 1.03105426\n",
      "(20.07 min) Epoch 99/300 -- Iteration 95193 - Batch 819/963 - Train loss: 0.25834036  - Train acc: 0.9034 - Val loss: 1.03105426\n",
      "(20.07 min) Epoch 99/300 -- Iteration 95202 - Batch 828/963 - Train loss: 0.25853010  - Train acc: 0.9033 - Val loss: 1.03105426\n",
      "(20.07 min) Epoch 99/300 -- Iteration 95211 - Batch 837/963 - Train loss: 0.25862201  - Train acc: 0.9032 - Val loss: 1.03105426\n",
      "(20.07 min) Epoch 99/300 -- Iteration 95220 - Batch 846/963 - Train loss: 0.25867439  - Train acc: 0.9030 - Val loss: 1.03105426\n",
      "(20.07 min) Epoch 99/300 -- Iteration 95229 - Batch 855/963 - Train loss: 0.25865925  - Train acc: 0.9030 - Val loss: 1.03105426\n",
      "(20.07 min) Epoch 99/300 -- Iteration 95238 - Batch 864/963 - Train loss: 0.25877032  - Train acc: 0.9029 - Val loss: 1.03105426\n",
      "(20.08 min) Epoch 99/300 -- Iteration 95247 - Batch 873/963 - Train loss: 0.25886989  - Train acc: 0.9029 - Val loss: 1.03105426\n",
      "(20.08 min) Epoch 99/300 -- Iteration 95256 - Batch 882/963 - Train loss: 0.25881793  - Train acc: 0.9030 - Val loss: 1.03105426\n",
      "(20.08 min) Epoch 99/300 -- Iteration 95265 - Batch 891/963 - Train loss: 0.25881608  - Train acc: 0.9031 - Val loss: 1.03105426\n",
      "(20.08 min) Epoch 99/300 -- Iteration 95274 - Batch 900/963 - Train loss: 0.25920725  - Train acc: 0.9029 - Val loss: 1.03105426\n",
      "(20.08 min) Epoch 99/300 -- Iteration 95283 - Batch 909/963 - Train loss: 0.25921807  - Train acc: 0.9028 - Val loss: 1.03105426\n",
      "(20.09 min) Epoch 99/300 -- Iteration 95292 - Batch 918/963 - Train loss: 0.25932621  - Train acc: 0.9028 - Val loss: 1.03105426\n",
      "(20.09 min) Epoch 99/300 -- Iteration 95301 - Batch 927/963 - Train loss: 0.25926384  - Train acc: 0.9028 - Val loss: 1.03105426\n",
      "(20.09 min) Epoch 99/300 -- Iteration 95310 - Batch 936/963 - Train loss: 0.25916065  - Train acc: 0.9028 - Val loss: 1.03105426\n",
      "(20.09 min) Epoch 99/300 -- Iteration 95319 - Batch 945/963 - Train loss: 0.25952550  - Train acc: 0.9026 - Val loss: 1.03105426\n",
      "(20.09 min) Epoch 99/300 -- Iteration 95328 - Batch 954/963 - Train loss: 0.25951361  - Train acc: 0.9027 - Val loss: 1.03105426\n",
      "(20.09 min) Epoch 99/300 -- Iteration 95337 - Batch 962/963 - Train loss: 0.25965557  - Train acc: 0.9026 - Val loss: 1.01598990 - Val acc: 0.5817\n",
      "(20.10 min) Epoch 100/300 -- Iteration 95346 - Batch 9/963 - Train loss: 0.25065206  - Train acc: 0.9070 - Val loss: 1.01598990\n",
      "(20.10 min) Epoch 100/300 -- Iteration 95355 - Batch 18/963 - Train loss: 0.25841752  - Train acc: 0.9017 - Val loss: 1.01598990\n",
      "(20.10 min) Epoch 100/300 -- Iteration 95364 - Batch 27/963 - Train loss: 0.25003123  - Train acc: 0.9082 - Val loss: 1.01598990\n",
      "(20.10 min) Epoch 100/300 -- Iteration 95373 - Batch 36/963 - Train loss: 0.24522366  - Train acc: 0.9107 - Val loss: 1.01598990\n",
      "(20.10 min) Epoch 100/300 -- Iteration 95382 - Batch 45/963 - Train loss: 0.24534679  - Train acc: 0.9100 - Val loss: 1.01598990\n",
      "(20.11 min) Epoch 100/300 -- Iteration 95391 - Batch 54/963 - Train loss: 0.24234581  - Train acc: 0.9101 - Val loss: 1.01598990\n",
      "(20.11 min) Epoch 100/300 -- Iteration 95400 - Batch 63/963 - Train loss: 0.24761795  - Train acc: 0.9084 - Val loss: 1.01598990\n",
      "(20.11 min) Epoch 100/300 -- Iteration 95409 - Batch 72/963 - Train loss: 0.25057371  - Train acc: 0.9069 - Val loss: 1.01598990\n",
      "(20.11 min) Epoch 100/300 -- Iteration 95418 - Batch 81/963 - Train loss: 0.24766125  - Train acc: 0.9074 - Val loss: 1.01598990\n",
      "(20.11 min) Epoch 100/300 -- Iteration 95427 - Batch 90/963 - Train loss: 0.25058900  - Train acc: 0.9055 - Val loss: 1.01598990\n",
      "(20.12 min) Epoch 100/300 -- Iteration 95436 - Batch 99/963 - Train loss: 0.25028793  - Train acc: 0.9060 - Val loss: 1.01598990\n",
      "(20.12 min) Epoch 100/300 -- Iteration 95445 - Batch 108/963 - Train loss: 0.25161978  - Train acc: 0.9052 - Val loss: 1.01598990\n",
      "(20.12 min) Epoch 100/300 -- Iteration 95454 - Batch 117/963 - Train loss: 0.25309708  - Train acc: 0.9043 - Val loss: 1.01598990\n",
      "(20.12 min) Epoch 100/300 -- Iteration 95463 - Batch 126/963 - Train loss: 0.25144793  - Train acc: 0.9053 - Val loss: 1.01598990\n",
      "(20.12 min) Epoch 100/300 -- Iteration 95472 - Batch 135/963 - Train loss: 0.25082420  - Train acc: 0.9056 - Val loss: 1.01598990\n",
      "(20.13 min) Epoch 100/300 -- Iteration 95481 - Batch 144/963 - Train loss: 0.25121229  - Train acc: 0.9055 - Val loss: 1.01598990\n",
      "(20.13 min) Epoch 100/300 -- Iteration 95490 - Batch 153/963 - Train loss: 0.25029626  - Train acc: 0.9061 - Val loss: 1.01598990\n",
      "(20.13 min) Epoch 100/300 -- Iteration 95499 - Batch 162/963 - Train loss: 0.25076969  - Train acc: 0.9061 - Val loss: 1.01598990\n",
      "(20.13 min) Epoch 100/300 -- Iteration 95508 - Batch 171/963 - Train loss: 0.25221200  - Train acc: 0.9055 - Val loss: 1.01598990\n",
      "(20.13 min) Epoch 100/300 -- Iteration 95517 - Batch 180/963 - Train loss: 0.25292907  - Train acc: 0.9051 - Val loss: 1.01598990\n",
      "(20.13 min) Epoch 100/300 -- Iteration 95526 - Batch 189/963 - Train loss: 0.25542782  - Train acc: 0.9041 - Val loss: 1.01598990\n",
      "(20.14 min) Epoch 100/300 -- Iteration 95535 - Batch 198/963 - Train loss: 0.25580152  - Train acc: 0.9043 - Val loss: 1.01598990\n",
      "(20.14 min) Epoch 100/300 -- Iteration 95544 - Batch 207/963 - Train loss: 0.25653472  - Train acc: 0.9039 - Val loss: 1.01598990\n",
      "(20.14 min) Epoch 100/300 -- Iteration 95553 - Batch 216/963 - Train loss: 0.25596855  - Train acc: 0.9038 - Val loss: 1.01598990\n",
      "(20.14 min) Epoch 100/300 -- Iteration 95562 - Batch 225/963 - Train loss: 0.25681358  - Train acc: 0.9036 - Val loss: 1.01598990\n",
      "(20.14 min) Epoch 100/300 -- Iteration 95571 - Batch 234/963 - Train loss: 0.25649672  - Train acc: 0.9036 - Val loss: 1.01598990\n",
      "(20.15 min) Epoch 100/300 -- Iteration 95580 - Batch 243/963 - Train loss: 0.25644284  - Train acc: 0.9036 - Val loss: 1.01598990\n",
      "(20.15 min) Epoch 100/300 -- Iteration 95589 - Batch 252/963 - Train loss: 0.25616333  - Train acc: 0.9037 - Val loss: 1.01598990\n",
      "(20.15 min) Epoch 100/300 -- Iteration 95598 - Batch 261/963 - Train loss: 0.25603290  - Train acc: 0.9038 - Val loss: 1.01598990\n",
      "(20.15 min) Epoch 100/300 -- Iteration 95607 - Batch 270/963 - Train loss: 0.25673726  - Train acc: 0.9036 - Val loss: 1.01598990\n",
      "(20.15 min) Epoch 100/300 -- Iteration 95616 - Batch 279/963 - Train loss: 0.25662999  - Train acc: 0.9037 - Val loss: 1.01598990\n",
      "(20.15 min) Epoch 100/300 -- Iteration 95625 - Batch 288/963 - Train loss: 0.25674988  - Train acc: 0.9035 - Val loss: 1.01598990\n",
      "(20.16 min) Epoch 100/300 -- Iteration 95634 - Batch 297/963 - Train loss: 0.25573535  - Train acc: 0.9039 - Val loss: 1.01598990\n",
      "(20.16 min) Epoch 100/300 -- Iteration 95643 - Batch 306/963 - Train loss: 0.25663428  - Train acc: 0.9034 - Val loss: 1.01598990\n",
      "(20.16 min) Epoch 100/300 -- Iteration 95652 - Batch 315/963 - Train loss: 0.25625817  - Train acc: 0.9034 - Val loss: 1.01598990\n",
      "(20.16 min) Epoch 100/300 -- Iteration 95661 - Batch 324/963 - Train loss: 0.25622015  - Train acc: 0.9036 - Val loss: 1.01598990\n",
      "(20.16 min) Epoch 100/300 -- Iteration 95670 - Batch 333/963 - Train loss: 0.25662203  - Train acc: 0.9033 - Val loss: 1.01598990\n",
      "(20.17 min) Epoch 100/300 -- Iteration 95679 - Batch 342/963 - Train loss: 0.25678519  - Train acc: 0.9031 - Val loss: 1.01598990\n",
      "(20.17 min) Epoch 100/300 -- Iteration 95688 - Batch 351/963 - Train loss: 0.25704824  - Train acc: 0.9031 - Val loss: 1.01598990\n",
      "(20.17 min) Epoch 100/300 -- Iteration 95697 - Batch 360/963 - Train loss: 0.25734898  - Train acc: 0.9030 - Val loss: 1.01598990\n",
      "(20.17 min) Epoch 100/300 -- Iteration 95706 - Batch 369/963 - Train loss: 0.25643609  - Train acc: 0.9034 - Val loss: 1.01598990\n",
      "(20.17 min) Epoch 100/300 -- Iteration 95715 - Batch 378/963 - Train loss: 0.25650300  - Train acc: 0.9033 - Val loss: 1.01598990\n",
      "(20.18 min) Epoch 100/300 -- Iteration 95724 - Batch 387/963 - Train loss: 0.25614288  - Train acc: 0.9033 - Val loss: 1.01598990\n",
      "(20.18 min) Epoch 100/300 -- Iteration 95733 - Batch 396/963 - Train loss: 0.25558091  - Train acc: 0.9035 - Val loss: 1.01598990\n",
      "(20.18 min) Epoch 100/300 -- Iteration 95742 - Batch 405/963 - Train loss: 0.25515966  - Train acc: 0.9036 - Val loss: 1.01598990\n",
      "(20.18 min) Epoch 100/300 -- Iteration 95751 - Batch 414/963 - Train loss: 0.25520638  - Train acc: 0.9036 - Val loss: 1.01598990\n",
      "(20.18 min) Epoch 100/300 -- Iteration 95760 - Batch 423/963 - Train loss: 0.25490115  - Train acc: 0.9037 - Val loss: 1.01598990\n",
      "(20.18 min) Epoch 100/300 -- Iteration 95769 - Batch 432/963 - Train loss: 0.25480286  - Train acc: 0.9037 - Val loss: 1.01598990\n",
      "(20.19 min) Epoch 100/300 -- Iteration 95778 - Batch 441/963 - Train loss: 0.25501685  - Train acc: 0.9037 - Val loss: 1.01598990\n",
      "(20.19 min) Epoch 100/300 -- Iteration 95787 - Batch 450/963 - Train loss: 0.25514556  - Train acc: 0.9037 - Val loss: 1.01598990\n",
      "(20.19 min) Epoch 100/300 -- Iteration 95796 - Batch 459/963 - Train loss: 0.25514157  - Train acc: 0.9036 - Val loss: 1.01598990\n",
      "(20.19 min) Epoch 100/300 -- Iteration 95805 - Batch 468/963 - Train loss: 0.25504585  - Train acc: 0.9036 - Val loss: 1.01598990\n",
      "(20.19 min) Epoch 100/300 -- Iteration 95814 - Batch 477/963 - Train loss: 0.25505309  - Train acc: 0.9035 - Val loss: 1.01598990\n",
      "(20.20 min) Epoch 100/300 -- Iteration 95823 - Batch 486/963 - Train loss: 0.25539018  - Train acc: 0.9035 - Val loss: 1.01598990\n",
      "(20.20 min) Epoch 100/300 -- Iteration 95832 - Batch 495/963 - Train loss: 0.25627711  - Train acc: 0.9032 - Val loss: 1.01598990\n",
      "(20.20 min) Epoch 100/300 -- Iteration 95841 - Batch 504/963 - Train loss: 0.25662155  - Train acc: 0.9028 - Val loss: 1.01598990\n",
      "(20.20 min) Epoch 100/300 -- Iteration 95850 - Batch 513/963 - Train loss: 0.25618450  - Train acc: 0.9029 - Val loss: 1.01598990\n",
      "(20.20 min) Epoch 100/300 -- Iteration 95859 - Batch 522/963 - Train loss: 0.25588427  - Train acc: 0.9030 - Val loss: 1.01598990\n",
      "(20.20 min) Epoch 100/300 -- Iteration 95868 - Batch 531/963 - Train loss: 0.25559268  - Train acc: 0.9032 - Val loss: 1.01598990\n",
      "(20.21 min) Epoch 100/300 -- Iteration 95877 - Batch 540/963 - Train loss: 0.25524678  - Train acc: 0.9034 - Val loss: 1.01598990\n",
      "(20.21 min) Epoch 100/300 -- Iteration 95886 - Batch 549/963 - Train loss: 0.25601296  - Train acc: 0.9031 - Val loss: 1.01598990\n",
      "(20.21 min) Epoch 100/300 -- Iteration 95895 - Batch 558/963 - Train loss: 0.25630710  - Train acc: 0.9030 - Val loss: 1.01598990\n",
      "(20.21 min) Epoch 100/300 -- Iteration 95904 - Batch 567/963 - Train loss: 0.25633674  - Train acc: 0.9030 - Val loss: 1.01598990\n",
      "(20.21 min) Epoch 100/300 -- Iteration 95913 - Batch 576/963 - Train loss: 0.25598002  - Train acc: 0.9032 - Val loss: 1.01598990\n",
      "(20.22 min) Epoch 100/300 -- Iteration 95922 - Batch 585/963 - Train loss: 0.25589085  - Train acc: 0.9031 - Val loss: 1.01598990\n",
      "(20.22 min) Epoch 100/300 -- Iteration 95931 - Batch 594/963 - Train loss: 0.25605414  - Train acc: 0.9030 - Val loss: 1.01598990\n",
      "(20.22 min) Epoch 100/300 -- Iteration 95940 - Batch 603/963 - Train loss: 0.25606179  - Train acc: 0.9030 - Val loss: 1.01598990\n",
      "(20.22 min) Epoch 100/300 -- Iteration 95949 - Batch 612/963 - Train loss: 0.25650042  - Train acc: 0.9028 - Val loss: 1.01598990\n",
      "(20.22 min) Epoch 100/300 -- Iteration 95958 - Batch 621/963 - Train loss: 0.25670061  - Train acc: 0.9028 - Val loss: 1.01598990\n",
      "(20.23 min) Epoch 100/300 -- Iteration 95967 - Batch 630/963 - Train loss: 0.25671394  - Train acc: 0.9027 - Val loss: 1.01598990\n",
      "(20.23 min) Epoch 100/300 -- Iteration 95976 - Batch 639/963 - Train loss: 0.25682375  - Train acc: 0.9027 - Val loss: 1.01598990\n",
      "(20.23 min) Epoch 100/300 -- Iteration 95985 - Batch 648/963 - Train loss: 0.25689325  - Train acc: 0.9027 - Val loss: 1.01598990\n",
      "(20.23 min) Epoch 100/300 -- Iteration 95994 - Batch 657/963 - Train loss: 0.25693733  - Train acc: 0.9026 - Val loss: 1.01598990\n",
      "(20.23 min) Epoch 100/300 -- Iteration 96003 - Batch 666/963 - Train loss: 0.25718410  - Train acc: 0.9027 - Val loss: 1.01598990\n",
      "(20.23 min) Epoch 100/300 -- Iteration 96012 - Batch 675/963 - Train loss: 0.25768815  - Train acc: 0.9023 - Val loss: 1.01598990\n",
      "(20.24 min) Epoch 100/300 -- Iteration 96021 - Batch 684/963 - Train loss: 0.25780210  - Train acc: 0.9022 - Val loss: 1.01598990\n",
      "(20.24 min) Epoch 100/300 -- Iteration 96030 - Batch 693/963 - Train loss: 0.25781114  - Train acc: 0.9022 - Val loss: 1.01598990\n",
      "(20.24 min) Epoch 100/300 -- Iteration 96039 - Batch 702/963 - Train loss: 0.25843689  - Train acc: 0.9020 - Val loss: 1.01598990\n",
      "(20.24 min) Epoch 100/300 -- Iteration 96048 - Batch 711/963 - Train loss: 0.25849946  - Train acc: 0.9020 - Val loss: 1.01598990\n",
      "(20.24 min) Epoch 100/300 -- Iteration 96057 - Batch 720/963 - Train loss: 0.25833417  - Train acc: 0.9022 - Val loss: 1.01598990\n",
      "(20.25 min) Epoch 100/300 -- Iteration 96066 - Batch 729/963 - Train loss: 0.25817652  - Train acc: 0.9023 - Val loss: 1.01598990\n",
      "(20.25 min) Epoch 100/300 -- Iteration 96075 - Batch 738/963 - Train loss: 0.25809910  - Train acc: 0.9023 - Val loss: 1.01598990\n",
      "(20.25 min) Epoch 100/300 -- Iteration 96084 - Batch 747/963 - Train loss: 0.25778658  - Train acc: 0.9025 - Val loss: 1.01598990\n",
      "(20.25 min) Epoch 100/300 -- Iteration 96093 - Batch 756/963 - Train loss: 0.25804938  - Train acc: 0.9025 - Val loss: 1.01598990\n",
      "(20.25 min) Epoch 100/300 -- Iteration 96102 - Batch 765/963 - Train loss: 0.25823236  - Train acc: 0.9024 - Val loss: 1.01598990\n",
      "(20.25 min) Epoch 100/300 -- Iteration 96111 - Batch 774/963 - Train loss: 0.25852464  - Train acc: 0.9023 - Val loss: 1.01598990\n",
      "(20.26 min) Epoch 100/300 -- Iteration 96120 - Batch 783/963 - Train loss: 0.25832075  - Train acc: 0.9024 - Val loss: 1.01598990\n",
      "(20.26 min) Epoch 100/300 -- Iteration 96129 - Batch 792/963 - Train loss: 0.25835812  - Train acc: 0.9025 - Val loss: 1.01598990\n",
      "(20.26 min) Epoch 100/300 -- Iteration 96138 - Batch 801/963 - Train loss: 0.25821740  - Train acc: 0.9025 - Val loss: 1.01598990\n",
      "(20.26 min) Epoch 100/300 -- Iteration 96147 - Batch 810/963 - Train loss: 0.25823889  - Train acc: 0.9025 - Val loss: 1.01598990\n",
      "(20.26 min) Epoch 100/300 -- Iteration 96156 - Batch 819/963 - Train loss: 0.25804811  - Train acc: 0.9026 - Val loss: 1.01598990\n",
      "(20.27 min) Epoch 100/300 -- Iteration 96165 - Batch 828/963 - Train loss: 0.25826077  - Train acc: 0.9025 - Val loss: 1.01598990\n",
      "(20.27 min) Epoch 100/300 -- Iteration 96174 - Batch 837/963 - Train loss: 0.25814605  - Train acc: 0.9025 - Val loss: 1.01598990\n",
      "(20.27 min) Epoch 100/300 -- Iteration 96183 - Batch 846/963 - Train loss: 0.25787517  - Train acc: 0.9027 - Val loss: 1.01598990\n",
      "(20.27 min) Epoch 100/300 -- Iteration 96192 - Batch 855/963 - Train loss: 0.25783299  - Train acc: 0.9027 - Val loss: 1.01598990\n",
      "(20.27 min) Epoch 100/300 -- Iteration 96201 - Batch 864/963 - Train loss: 0.25769839  - Train acc: 0.9029 - Val loss: 1.01598990\n",
      "(20.28 min) Epoch 100/300 -- Iteration 96210 - Batch 873/963 - Train loss: 0.25770538  - Train acc: 0.9029 - Val loss: 1.01598990\n",
      "(20.28 min) Epoch 100/300 -- Iteration 96219 - Batch 882/963 - Train loss: 0.25777941  - Train acc: 0.9028 - Val loss: 1.01598990\n",
      "(20.28 min) Epoch 100/300 -- Iteration 96228 - Batch 891/963 - Train loss: 0.25782699  - Train acc: 0.9028 - Val loss: 1.01598990\n",
      "(20.28 min) Epoch 100/300 -- Iteration 96237 - Batch 900/963 - Train loss: 0.25759204  - Train acc: 0.9029 - Val loss: 1.01598990\n",
      "(20.28 min) Epoch 100/300 -- Iteration 96246 - Batch 909/963 - Train loss: 0.25759394  - Train acc: 0.9028 - Val loss: 1.01598990\n",
      "(20.28 min) Epoch 100/300 -- Iteration 96255 - Batch 918/963 - Train loss: 0.25743013  - Train acc: 0.9029 - Val loss: 1.01598990\n",
      "(20.29 min) Epoch 100/300 -- Iteration 96264 - Batch 927/963 - Train loss: 0.25740533  - Train acc: 0.9029 - Val loss: 1.01598990\n",
      "(20.29 min) Epoch 100/300 -- Iteration 96273 - Batch 936/963 - Train loss: 0.25754705  - Train acc: 0.9029 - Val loss: 1.01598990\n",
      "(20.29 min) Epoch 100/300 -- Iteration 96282 - Batch 945/963 - Train loss: 0.25747288  - Train acc: 0.9029 - Val loss: 1.01598990\n",
      "(20.29 min) Epoch 100/300 -- Iteration 96291 - Batch 954/963 - Train loss: 0.25772923  - Train acc: 0.9027 - Val loss: 1.01598990\n",
      "(20.29 min) Epoch 100/300 -- Iteration 96300 - Batch 962/963 - Train loss: 0.25777049  - Train acc: 0.9027 - Val loss: 1.02369726 - Val acc: 0.5767\n",
      "(20.30 min) Epoch 101/300 -- Iteration 96309 - Batch 9/963 - Train loss: 0.27928956  - Train acc: 0.8898 - Val loss: 1.02369726\n",
      "(20.30 min) Epoch 101/300 -- Iteration 96318 - Batch 18/963 - Train loss: 0.27869489  - Train acc: 0.8927 - Val loss: 1.02369726\n",
      "(20.30 min) Epoch 101/300 -- Iteration 96327 - Batch 27/963 - Train loss: 0.26594309  - Train acc: 0.8998 - Val loss: 1.02369726\n",
      "(20.30 min) Epoch 101/300 -- Iteration 96336 - Batch 36/963 - Train loss: 0.25706254  - Train acc: 0.9033 - Val loss: 1.02369726\n",
      "(20.30 min) Epoch 101/300 -- Iteration 96345 - Batch 45/963 - Train loss: 0.25444984  - Train acc: 0.9032 - Val loss: 1.02369726\n",
      "(20.31 min) Epoch 101/300 -- Iteration 96354 - Batch 54/963 - Train loss: 0.25347440  - Train acc: 0.9036 - Val loss: 1.02369726\n",
      "(20.31 min) Epoch 101/300 -- Iteration 96363 - Batch 63/963 - Train loss: 0.24885309  - Train acc: 0.9059 - Val loss: 1.02369726\n",
      "(20.31 min) Epoch 101/300 -- Iteration 96372 - Batch 72/963 - Train loss: 0.25430556  - Train acc: 0.9059 - Val loss: 1.02369726\n",
      "(20.31 min) Epoch 101/300 -- Iteration 96381 - Batch 81/963 - Train loss: 0.25777907  - Train acc: 0.9050 - Val loss: 1.02369726\n",
      "(20.31 min) Epoch 101/300 -- Iteration 96390 - Batch 90/963 - Train loss: 0.25704358  - Train acc: 0.9050 - Val loss: 1.02369726\n",
      "(20.32 min) Epoch 101/300 -- Iteration 96399 - Batch 99/963 - Train loss: 0.25505169  - Train acc: 0.9062 - Val loss: 1.02369726\n",
      "(20.32 min) Epoch 101/300 -- Iteration 96408 - Batch 108/963 - Train loss: 0.25593481  - Train acc: 0.9055 - Val loss: 1.02369726\n",
      "(20.32 min) Epoch 101/300 -- Iteration 96417 - Batch 117/963 - Train loss: 0.25427748  - Train acc: 0.9058 - Val loss: 1.02369726\n",
      "(20.32 min) Epoch 101/300 -- Iteration 96426 - Batch 126/963 - Train loss: 0.25287039  - Train acc: 0.9063 - Val loss: 1.02369726\n",
      "(20.32 min) Epoch 101/300 -- Iteration 96435 - Batch 135/963 - Train loss: 0.25267255  - Train acc: 0.9061 - Val loss: 1.02369726\n",
      "(20.33 min) Epoch 101/300 -- Iteration 96444 - Batch 144/963 - Train loss: 0.25138852  - Train acc: 0.9067 - Val loss: 1.02369726\n",
      "(20.33 min) Epoch 101/300 -- Iteration 96453 - Batch 153/963 - Train loss: 0.25242010  - Train acc: 0.9058 - Val loss: 1.02369726\n",
      "(20.33 min) Epoch 101/300 -- Iteration 96462 - Batch 162/963 - Train loss: 0.25319004  - Train acc: 0.9058 - Val loss: 1.02369726\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m     13\u001b[0m stage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 14\u001b[0m curves \u001b[38;5;241m=\u001b[39m \u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_final_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelo_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mae_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mrnn_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mmax_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                                \u001b[49m\u001b[38;5;241;43m10e-7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mrandom_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43monly_classifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mnum_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(modelo_final\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelo_final\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pmarc\\Universidad\\inteli\\ZTF_alert_8a\\src\\final_training.py:151\u001b[0m, in \u001b[0;36mtrain_final_model\u001b[1;34m(model, stage, train_dataset, validation_dataset, ae_criterion, rnn_criterion, max_epochs, max_time, batch_size, learning_rate, random_sampler, only_classifier, augmentation, early_stop, use_gpu, num_cpu)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid stage value. Pick between \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mae_alone\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mae_rnn\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrnn\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    150\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 151\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    154\u001b[0m cumulative_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\pmarc\\.conda\\envs\\inteli_gpu\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmarc\\.conda\\envs\\inteli_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmarc\\.conda\\envs\\inteli_gpu\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import src.final_model as fm\n",
    "import src.final_training as ft\n",
    "importlib.reload(fm)\n",
    "importlib.reload(ft)\n",
    "importlib.reload(src.final_training)\n",
    "importlib.reload(src.final_model)\n",
    "\n",
    "only_classifier = False\n",
    "early_stop = 300\n",
    "augmentation = False\n",
    "batch_size = 128\n",
    "\n",
    "stage = 'rnn'\n",
    "curves = ft.train_final_model(modelo_final,\n",
    "                                              stage,\n",
    "                                                train_dataset,\n",
    "                                                validation_dataset,\n",
    "                                                ae_loss,\n",
    "                                                rnn_loss,\n",
    "                                                max_epochs,\n",
    "                                                max_time,\n",
    "                                                batch_size,\n",
    "                                                10e-7,\n",
    "                                                random_sampler,\n",
    "                                                only_classifier,\n",
    "                                                augmentation,\n",
    "                                                early_stop,\n",
    "                                                use_gpu,\n",
    "                                                num_cpu)\n",
    "\n",
    "torch.save(modelo_final.state_dict(), f'models/{modelo_final.name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1950x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.show_curves([curves], [modelo_final])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo 2nd_iteration\n",
      "Precision: 0.442\n",
      "Recall: 0.572\n",
      "F1: 0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmarc\\.conda\\envs\\inteli_gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1950x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#validation\n",
    "import src.final_model as fm\n",
    "import src.final_training as ft\n",
    "importlib.reload(fm)\n",
    "importlib.reload(ft)\n",
    "importlib.reload(src.final_training)\n",
    "importlib.reload(src.final_model)\n",
    "\n",
    "modelo_final.cpu()\n",
    "modelo_final.eval()\n",
    "\n",
    "\n",
    "prediction = modelo_final(validation_dataset.tensors[0])\n",
    "\n",
    "cm = confusion_matrix(validation_dataset.tensors[1], prediction.argmax(dim=1), normalize='true')\n",
    "print(f'\\nModelo {modelo_final.name}')\n",
    "metrics.plot_matrix([modelo_final], [cm])\n",
    "precision, recall, f1 = metrics.performance_metrics(validation_dataset.tensors[1], prediction.argmax(dim=1))\n",
    "\n",
    "print(f'Precision: {precision:.3f}'\n",
    "      f'\\nRecall: {recall:.3f}'\n",
    "      f'\\nF1: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo 2nd_iteration\n",
      "Precision: 0.592\n",
      "Recall: 0.593\n",
      "F1: 0.593\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1950x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test\n",
    "modelo_final.eval()\n",
    "\n",
    "prediction = modelo_final(test_dataset.tensors[0])\n",
    "cm = confusion_matrix(test_dataset.tensors[1], prediction.argmax(dim=1), normalize='true')\n",
    "print(f'\\nModelo {modelo_final.name}')\n",
    "metrics.plot_matrix([modelo_final], [cm])\n",
    "precision, recall, f1 = metrics.performance_metrics(test_dataset.tensors[1], prediction.argmax(dim=1))\n",
    "\n",
    "print(f'Precision: {precision:.3f}'\n",
    "      f'\\nRecall: {recall:.3f}'\n",
    "      f'\\nF1: {f1:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "import src.final_model as fm\n",
    "import src.final_training as ft\n",
    "importlib.reload(fm)\n",
    "importlib.reload(ft)\n",
    "importlib.reload(src.final_training)\n",
    "importlib.reload(src.final_model)\n",
    "\n",
    "# modelo_final = fm.FinalModel(latent_dim, n_channels, rnn_type, hidden_dim, num_layers, dropout_prob,  n_classes, name='2nd_iteration')\n",
    "\n",
    "# modelo_final.load_state_dict(torch.load(f'models/aernn_without_aug.pth'))\n",
    "modelo_final.eval()\n",
    "modelo_final.cuda()\n",
    "\n",
    "class_counts = torch.bincount(train_dataset.tensors[1].long())\n",
    "class_weights = 1. / class_counts.float()\n",
    "weights = class_weights[train_dataset.tensors[1].long()]\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, sampler=sampler, num_workers=0, pin_memory=True)\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, y_batch in train_loader:\n",
    "        img = img.cuda()\n",
    "        lat_spc = modelo_final.rnn_latent(img)\n",
    "        features.append(lat_spc.cpu().numpy())\n",
    "        labels.append(y_batch.numpy())\n",
    "\n",
    "features = np.concatenate(features, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123227, 128)\n",
      "(123227,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000, random_state=42, n_jobs = -1)\n",
    "rf_classifier.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5883\n"
     ]
    }
   ],
   "source": [
    "# Extract features from the validation dataset\n",
    "val_loader = DataLoader(validation_dataset, batch_size=len(validation_dataset), shuffle=False)\n",
    "val_features = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, y_batch in val_loader:\n",
    "        if use_gpu:\n",
    "            img = img.cuda()\n",
    "        lat_spc = modelo_final.rnn_latent(img)\n",
    "        val_features.append(lat_spc.cpu().numpy())\n",
    "        val_labels.append(y_batch.numpy())\n",
    "\n",
    "val_features = np.concatenate(val_features, axis=0)\n",
    "val_labels = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "# Predict and evaluate\n",
    "val_predictions = rf_classifier.predict(val_features)\n",
    "accuracy = np.mean(val_predictions == val_labels)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inteli_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
