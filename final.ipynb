{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "#system\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "#ai\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import src\n",
    "importlib.reload(src)\n",
    "\n",
    "import src.utils.metrics as metrics\n",
    "import src.utils.gau as gaussian\n",
    "import src.utils.plots as plots\n",
    "\n",
    "from src.models import AE, RNN, LinearClassifier, get_latent_features, rnn_loss_function, ae_loss_function\n",
    "from src.training import EarlyStopping, train_ae, train_rnn, train_lp, augment_data\n",
    "\n",
    "from src.final_model import ae_loss_function, rnn_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamps_dataset = pd.read_pickle('data/5stamps_dataset.pkl')\n",
    "\n",
    "def rename_labels(dataset, old_value, new_value):\n",
    "    for key in dataset.keys():\n",
    "        if old_value in dataset[key]:\n",
    "            dataset[key][new_value] = dataset[key].pop(old_value)\n",
    "\n",
    "rename_labels(stamps_dataset, 'labels', 'class')\n",
    "rename_labels(stamps_dataset, 'science', 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = torch.tensor(stamps_dataset['Train']['template'], dtype=torch.float32)\n",
    "validation_template = torch.tensor(stamps_dataset['Validation']['template'], dtype=torch.float32)\n",
    "test_template = torch.tensor(stamps_dataset['Test']['template'], dtype=torch.float32)\n",
    "\n",
    "train_difference = torch.tensor(stamps_dataset['Train']['difference'], dtype=torch.float32)\n",
    "validation_difference = torch.tensor(stamps_dataset['Validation']['difference'], dtype=torch.float32)\n",
    "test_difference = torch.tensor(stamps_dataset['Test']['difference'], dtype=torch.float32)\n",
    "\n",
    "train_image = torch.tensor(stamps_dataset['Train']['images'], dtype=torch.float32)\n",
    "validation_image = torch.tensor(stamps_dataset['Validation']['images'], dtype=torch.float32)\n",
    "test_image = torch.tensor(stamps_dataset['Test']['images'], dtype=torch.float32)\n",
    "\n",
    "train_class_0 = torch.tensor(stamps_dataset['Train']['class'], dtype=torch.float32)\n",
    "validation_class_0 = torch.tensor(stamps_dataset['Validation']['class'], dtype=torch.float32)\n",
    "test_class_0 = torch.tensor(stamps_dataset['Test']['class'], dtype=torch.float32)\n",
    "\n",
    "train_template = train_template.unsqueeze(1).repeat(1, 5, 1, 1)\n",
    "validation_template = validation_template.unsqueeze(1).repeat(1, 5, 1, 1)\n",
    "test_template = test_template.unsqueeze(1).repeat(1, 5, 1, 1)\n",
    "\n",
    "\n",
    "train_dataset = torch.stack((train_template, train_image, train_difference), dim=3  )\n",
    "validation_dataset = torch.stack((validation_template, validation_difference, validation_difference), dim=3)\n",
    "test_dataset = torch.stack((test_template, test_image, test_difference), dim=3)\n",
    "\n",
    "train_template = train_template.unsqueeze(2)  # (samples, 5, 1, 21, 21)\n",
    "train_image = train_image.unsqueeze(2)        \n",
    "train_difference = train_difference.unsqueeze(2)  \n",
    "\n",
    "validation_template = validation_template.unsqueeze(2)\n",
    "validation_image = validation_image.unsqueeze(2)\n",
    "validation_difference = validation_difference.unsqueeze(2)\n",
    "\n",
    "test_template = test_template.unsqueeze(2)\n",
    "test_image = test_image.unsqueeze(2)\n",
    "test_difference = test_difference.unsqueeze(2)\n",
    "\n",
    "# Apilar los tensores a lo largo de la dimensi√≥n correcta\n",
    "train_dataset = torch.cat((train_template, train_image, train_difference), dim=2)\n",
    "validation_dataset = torch.cat((validation_template, validation_image, validation_difference), dim=2)\n",
    "test_dataset = torch.cat((test_template, test_image, test_difference), dim=2)\n",
    "\n",
    "# Crear los conjuntos de datos\n",
    "train_dataset = TensorDataset(train_dataset, train_class_0)\n",
    "validation_dataset = TensorDataset(validation_dataset, validation_class_0)\n",
    "test_dataset = TensorDataset(test_dataset, test_class_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn parameters\n",
    "rnn_type = 'RNN'\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "\n",
    "# autoencoder params\n",
    "latent_dim = 128\n",
    "n_channels = 3\n",
    "n_classes = 3\n",
    "\n",
    "# training params\n",
    "ae_loss = ae_loss_function\n",
    "rnn_loss = rnn_loss_function\n",
    "\n",
    "max_epochs = 300\n",
    "max_time = 1000 #minutos\n",
    "lr = 10e-5\n",
    "batch_size = 256\n",
    "dropout_prob = 0.15\n",
    "\n",
    "random_sampler = True\n",
    "use_gpu = True\n",
    "augmentation = True\n",
    "only_classifier = False\n",
    "early_stop = 50\n",
    "num_cpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data...\n",
      "\n",
      "Setup finishied. Starting training...\n",
      "(0.03 min) Epoch 1/300 -- Iteration 77 - Batch 77/7702 - Train loss: 0.03623345  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.05 min) Epoch 1/300 -- Iteration 154 - Batch 154/7702 - Train loss: 0.02885989  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 231 - Batch 231/7702 - Train loss: 0.02518861  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 308 - Batch 308/7702 - Train loss: 0.02244528  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.12 min) Epoch 1/300 -- Iteration 385 - Batch 385/7702 - Train loss: 0.02023491  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.15 min) Epoch 1/300 -- Iteration 462 - Batch 462/7702 - Train loss: 0.01852953  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 539 - Batch 539/7702 - Train loss: 0.01706235  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.19 min) Epoch 1/300 -- Iteration 616 - Batch 616/7702 - Train loss: 0.01579437  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.21 min) Epoch 1/300 -- Iteration 693 - Batch 693/7702 - Train loss: 0.01471125  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.23 min) Epoch 1/300 -- Iteration 770 - Batch 770/7702 - Train loss: 0.01381014  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.26 min) Epoch 1/300 -- Iteration 847 - Batch 847/7702 - Train loss: 0.01305690  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.28 min) Epoch 1/300 -- Iteration 924 - Batch 924/7702 - Train loss: 0.01241121  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.30 min) Epoch 1/300 -- Iteration 1001 - Batch 1001/7702 - Train loss: 0.01184389  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.32 min) Epoch 1/300 -- Iteration 1078 - Batch 1078/7702 - Train loss: 0.01135698  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.34 min) Epoch 1/300 -- Iteration 1155 - Batch 1155/7702 - Train loss: 0.01091924  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.36 min) Epoch 1/300 -- Iteration 1232 - Batch 1232/7702 - Train loss: 0.01053371  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.39 min) Epoch 1/300 -- Iteration 1309 - Batch 1309/7702 - Train loss: 0.01019248  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.41 min) Epoch 1/300 -- Iteration 1386 - Batch 1386/7702 - Train loss: 0.00987947  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.43 min) Epoch 1/300 -- Iteration 1463 - Batch 1463/7702 - Train loss: 0.00959820  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.45 min) Epoch 1/300 -- Iteration 1540 - Batch 1540/7702 - Train loss: 0.00933805  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.47 min) Epoch 1/300 -- Iteration 1617 - Batch 1617/7702 - Train loss: 0.00910054  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.50 min) Epoch 1/300 -- Iteration 1694 - Batch 1694/7702 - Train loss: 0.00888299  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.52 min) Epoch 1/300 -- Iteration 1771 - Batch 1771/7702 - Train loss: 0.00868282  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.54 min) Epoch 1/300 -- Iteration 1848 - Batch 1848/7702 - Train loss: 0.00849150  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.56 min) Epoch 1/300 -- Iteration 1925 - Batch 1925/7702 - Train loss: 0.00831449  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.58 min) Epoch 1/300 -- Iteration 2002 - Batch 2002/7702 - Train loss: 0.00815097  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.60 min) Epoch 1/300 -- Iteration 2079 - Batch 2079/7702 - Train loss: 0.00799787  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.63 min) Epoch 1/300 -- Iteration 2156 - Batch 2156/7702 - Train loss: 0.00785514  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.65 min) Epoch 1/300 -- Iteration 2233 - Batch 2233/7702 - Train loss: 0.00772195  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.67 min) Epoch 1/300 -- Iteration 2310 - Batch 2310/7702 - Train loss: 0.00759712  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.69 min) Epoch 1/300 -- Iteration 2387 - Batch 2387/7702 - Train loss: 0.00747744  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.71 min) Epoch 1/300 -- Iteration 2464 - Batch 2464/7702 - Train loss: 0.00736399  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.73 min) Epoch 1/300 -- Iteration 2541 - Batch 2541/7702 - Train loss: 0.00725650  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.76 min) Epoch 1/300 -- Iteration 2618 - Batch 2618/7702 - Train loss: 0.00715572  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.78 min) Epoch 1/300 -- Iteration 2695 - Batch 2695/7702 - Train loss: 0.00705950  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.80 min) Epoch 1/300 -- Iteration 2772 - Batch 2772/7702 - Train loss: 0.00696761  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.82 min) Epoch 1/300 -- Iteration 2849 - Batch 2849/7702 - Train loss: 0.00687880  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.84 min) Epoch 1/300 -- Iteration 2926 - Batch 2926/7702 - Train loss: 0.00679587  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.86 min) Epoch 1/300 -- Iteration 3003 - Batch 3003/7702 - Train loss: 0.00671542  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.89 min) Epoch 1/300 -- Iteration 3080 - Batch 3080/7702 - Train loss: 0.00663692  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.91 min) Epoch 1/300 -- Iteration 3157 - Batch 3157/7702 - Train loss: 0.00656087  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.93 min) Epoch 1/300 -- Iteration 3234 - Batch 3234/7702 - Train loss: 0.00648905  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.95 min) Epoch 1/300 -- Iteration 3311 - Batch 3311/7702 - Train loss: 0.00642084  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.97 min) Epoch 1/300 -- Iteration 3388 - Batch 3388/7702 - Train loss: 0.00635713  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(0.99 min) Epoch 1/300 -- Iteration 3465 - Batch 3465/7702 - Train loss: 0.00629375  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.01 min) Epoch 1/300 -- Iteration 3542 - Batch 3542/7702 - Train loss: 0.00623188  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.04 min) Epoch 1/300 -- Iteration 3619 - Batch 3619/7702 - Train loss: 0.00617367  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.06 min) Epoch 1/300 -- Iteration 3696 - Batch 3696/7702 - Train loss: 0.00611951  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.08 min) Epoch 1/300 -- Iteration 3773 - Batch 3773/7702 - Train loss: 0.00606491  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.10 min) Epoch 1/300 -- Iteration 3850 - Batch 3850/7702 - Train loss: 0.00601368  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.12 min) Epoch 1/300 -- Iteration 3927 - Batch 3927/7702 - Train loss: 0.00596300  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.14 min) Epoch 1/300 -- Iteration 4004 - Batch 4004/7702 - Train loss: 0.00591380  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.17 min) Epoch 1/300 -- Iteration 4081 - Batch 4081/7702 - Train loss: 0.00586594  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.19 min) Epoch 1/300 -- Iteration 4158 - Batch 4158/7702 - Train loss: 0.00581975  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.21 min) Epoch 1/300 -- Iteration 4235 - Batch 4235/7702 - Train loss: 0.00577377  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.23 min) Epoch 1/300 -- Iteration 4312 - Batch 4312/7702 - Train loss: 0.00572967  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.25 min) Epoch 1/300 -- Iteration 4389 - Batch 4389/7702 - Train loss: 0.00568696  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.27 min) Epoch 1/300 -- Iteration 4466 - Batch 4466/7702 - Train loss: 0.00564511  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.29 min) Epoch 1/300 -- Iteration 4543 - Batch 4543/7702 - Train loss: 0.00560569  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.32 min) Epoch 1/300 -- Iteration 4620 - Batch 4620/7702 - Train loss: 0.00556683  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.34 min) Epoch 1/300 -- Iteration 4697 - Batch 4697/7702 - Train loss: 0.00552890  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.36 min) Epoch 1/300 -- Iteration 4774 - Batch 4774/7702 - Train loss: 0.00549189  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.38 min) Epoch 1/300 -- Iteration 4851 - Batch 4851/7702 - Train loss: 0.00545454  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.40 min) Epoch 1/300 -- Iteration 4928 - Batch 4928/7702 - Train loss: 0.00541826  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.42 min) Epoch 1/300 -- Iteration 5005 - Batch 5005/7702 - Train loss: 0.00538326  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.44 min) Epoch 1/300 -- Iteration 5082 - Batch 5082/7702 - Train loss: 0.00534999  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.46 min) Epoch 1/300 -- Iteration 5159 - Batch 5159/7702 - Train loss: 0.00531648  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.49 min) Epoch 1/300 -- Iteration 5236 - Batch 5236/7702 - Train loss: 0.00528450  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.51 min) Epoch 1/300 -- Iteration 5313 - Batch 5313/7702 - Train loss: 0.00525371  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.53 min) Epoch 1/300 -- Iteration 5390 - Batch 5390/7702 - Train loss: 0.00522212  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.55 min) Epoch 1/300 -- Iteration 5467 - Batch 5467/7702 - Train loss: 0.00519186  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.57 min) Epoch 1/300 -- Iteration 5544 - Batch 5544/7702 - Train loss: 0.00516285  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.59 min) Epoch 1/300 -- Iteration 5621 - Batch 5621/7702 - Train loss: 0.00513288  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.61 min) Epoch 1/300 -- Iteration 5698 - Batch 5698/7702 - Train loss: 0.00510580  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.64 min) Epoch 1/300 -- Iteration 5775 - Batch 5775/7702 - Train loss: 0.00507867  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.66 min) Epoch 1/300 -- Iteration 5852 - Batch 5852/7702 - Train loss: 0.00505162  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.68 min) Epoch 1/300 -- Iteration 5929 - Batch 5929/7702 - Train loss: 0.00502452  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.70 min) Epoch 1/300 -- Iteration 6006 - Batch 6006/7702 - Train loss: 0.00499886  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.72 min) Epoch 1/300 -- Iteration 6083 - Batch 6083/7702 - Train loss: 0.00497349  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.74 min) Epoch 1/300 -- Iteration 6160 - Batch 6160/7702 - Train loss: 0.00494862  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.76 min) Epoch 1/300 -- Iteration 6237 - Batch 6237/7702 - Train loss: 0.00492483  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.78 min) Epoch 1/300 -- Iteration 6314 - Batch 6314/7702 - Train loss: 0.00490037  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.80 min) Epoch 1/300 -- Iteration 6391 - Batch 6391/7702 - Train loss: 0.00487675  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.83 min) Epoch 1/300 -- Iteration 6468 - Batch 6468/7702 - Train loss: 0.00485373  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.85 min) Epoch 1/300 -- Iteration 6545 - Batch 6545/7702 - Train loss: 0.00483151  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.87 min) Epoch 1/300 -- Iteration 6622 - Batch 6622/7702 - Train loss: 0.00480940  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.89 min) Epoch 1/300 -- Iteration 6699 - Batch 6699/7702 - Train loss: 0.00478662  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.91 min) Epoch 1/300 -- Iteration 6776 - Batch 6776/7702 - Train loss: 0.00476515  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.93 min) Epoch 1/300 -- Iteration 6853 - Batch 6853/7702 - Train loss: 0.00474351  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.95 min) Epoch 1/300 -- Iteration 6930 - Batch 6930/7702 - Train loss: 0.00472336  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.97 min) Epoch 1/300 -- Iteration 7007 - Batch 7007/7702 - Train loss: 0.00470333  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(1.99 min) Epoch 1/300 -- Iteration 7084 - Batch 7084/7702 - Train loss: 0.00468361  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(2.02 min) Epoch 1/300 -- Iteration 7161 - Batch 7161/7702 - Train loss: 0.00466456  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(2.04 min) Epoch 1/300 -- Iteration 7238 - Batch 7238/7702 - Train loss: 0.00464514  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(2.06 min) Epoch 1/300 -- Iteration 7315 - Batch 7315/7702 - Train loss: 0.00462631  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(2.08 min) Epoch 1/300 -- Iteration 7392 - Batch 7392/7702 - Train loss: 0.00460810  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(2.10 min) Epoch 1/300 -- Iteration 7469 - Batch 7469/7702 - Train loss: 0.00459031  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(2.12 min) Epoch 1/300 -- Iteration 7546 - Batch 7546/7702 - Train loss: 0.00457259  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(2.14 min) Epoch 1/300 -- Iteration 7623 - Batch 7623/7702 - Train loss: 0.00455552  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(2.16 min) Epoch 1/300 -- Iteration 7700 - Batch 7700/7702 - Train loss: 0.00453815  - Train acc: -0.0000 - Val loss: 0.00000000\n",
      "(2.16 min) Epoch 1/300 -- Iteration 7702 - Batch 7701/7702 - Train loss: 0.00453790  - Train acc: -0.0000 - Val loss: 0.00159934 - Val acc: -0.0000\n",
      "(2.19 min) Epoch 2/300 -- Iteration 7779 - Batch 77/7702 - Train loss: 0.00286317  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.21 min) Epoch 2/300 -- Iteration 7856 - Batch 154/7702 - Train loss: 0.00283497  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.23 min) Epoch 2/300 -- Iteration 7933 - Batch 231/7702 - Train loss: 0.00282013  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.25 min) Epoch 2/300 -- Iteration 8010 - Batch 308/7702 - Train loss: 0.00281807  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.27 min) Epoch 2/300 -- Iteration 8087 - Batch 385/7702 - Train loss: 0.00282053  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.29 min) Epoch 2/300 -- Iteration 8164 - Batch 462/7702 - Train loss: 0.00280430  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.31 min) Epoch 2/300 -- Iteration 8241 - Batch 539/7702 - Train loss: 0.00280765  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.33 min) Epoch 2/300 -- Iteration 8318 - Batch 616/7702 - Train loss: 0.00280463  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.35 min) Epoch 2/300 -- Iteration 8395 - Batch 693/7702 - Train loss: 0.00279608  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.38 min) Epoch 2/300 -- Iteration 8472 - Batch 770/7702 - Train loss: 0.00279378  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.40 min) Epoch 2/300 -- Iteration 8549 - Batch 847/7702 - Train loss: 0.00279205  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.42 min) Epoch 2/300 -- Iteration 8626 - Batch 924/7702 - Train loss: 0.00279140  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.44 min) Epoch 2/300 -- Iteration 8703 - Batch 1001/7702 - Train loss: 0.00278649  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.46 min) Epoch 2/300 -- Iteration 8780 - Batch 1078/7702 - Train loss: 0.00278615  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.48 min) Epoch 2/300 -- Iteration 8857 - Batch 1155/7702 - Train loss: 0.00278712  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.50 min) Epoch 2/300 -- Iteration 8934 - Batch 1232/7702 - Train loss: 0.00278473  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.52 min) Epoch 2/300 -- Iteration 9011 - Batch 1309/7702 - Train loss: 0.00277868  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.54 min) Epoch 2/300 -- Iteration 9088 - Batch 1386/7702 - Train loss: 0.00277582  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.56 min) Epoch 2/300 -- Iteration 9165 - Batch 1463/7702 - Train loss: 0.00277244  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.58 min) Epoch 2/300 -- Iteration 9242 - Batch 1540/7702 - Train loss: 0.00277202  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.61 min) Epoch 2/300 -- Iteration 9319 - Batch 1617/7702 - Train loss: 0.00276706  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.63 min) Epoch 2/300 -- Iteration 9396 - Batch 1694/7702 - Train loss: 0.00276721  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.65 min) Epoch 2/300 -- Iteration 9473 - Batch 1771/7702 - Train loss: 0.00276479  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.67 min) Epoch 2/300 -- Iteration 9550 - Batch 1848/7702 - Train loss: 0.00276173  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.69 min) Epoch 2/300 -- Iteration 9627 - Batch 1925/7702 - Train loss: 0.00275866  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.71 min) Epoch 2/300 -- Iteration 9704 - Batch 2002/7702 - Train loss: 0.00275711  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.73 min) Epoch 2/300 -- Iteration 9781 - Batch 2079/7702 - Train loss: 0.00275628  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.75 min) Epoch 2/300 -- Iteration 9858 - Batch 2156/7702 - Train loss: 0.00275542  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.77 min) Epoch 2/300 -- Iteration 9935 - Batch 2233/7702 - Train loss: 0.00275376  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.79 min) Epoch 2/300 -- Iteration 10012 - Batch 2310/7702 - Train loss: 0.00274997  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.81 min) Epoch 2/300 -- Iteration 10089 - Batch 2387/7702 - Train loss: 0.00274918  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.83 min) Epoch 2/300 -- Iteration 10166 - Batch 2464/7702 - Train loss: 0.00274632  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.86 min) Epoch 2/300 -- Iteration 10243 - Batch 2541/7702 - Train loss: 0.00274530  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.88 min) Epoch 2/300 -- Iteration 10320 - Batch 2618/7702 - Train loss: 0.00274338  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.90 min) Epoch 2/300 -- Iteration 10397 - Batch 2695/7702 - Train loss: 0.00273976  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.92 min) Epoch 2/300 -- Iteration 10474 - Batch 2772/7702 - Train loss: 0.00273667  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.94 min) Epoch 2/300 -- Iteration 10551 - Batch 2849/7702 - Train loss: 0.00273414  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.96 min) Epoch 2/300 -- Iteration 10628 - Batch 2926/7702 - Train loss: 0.00273303  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(2.98 min) Epoch 2/300 -- Iteration 10705 - Batch 3003/7702 - Train loss: 0.00273094  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.00 min) Epoch 2/300 -- Iteration 10782 - Batch 3080/7702 - Train loss: 0.00272934  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.02 min) Epoch 2/300 -- Iteration 10859 - Batch 3157/7702 - Train loss: 0.00272743  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.04 min) Epoch 2/300 -- Iteration 10936 - Batch 3234/7702 - Train loss: 0.00272565  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.06 min) Epoch 2/300 -- Iteration 11013 - Batch 3311/7702 - Train loss: 0.00272509  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.08 min) Epoch 2/300 -- Iteration 11090 - Batch 3388/7702 - Train loss: 0.00272313  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.11 min) Epoch 2/300 -- Iteration 11167 - Batch 3465/7702 - Train loss: 0.00272253  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.13 min) Epoch 2/300 -- Iteration 11244 - Batch 3542/7702 - Train loss: 0.00272181  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.15 min) Epoch 2/300 -- Iteration 11321 - Batch 3619/7702 - Train loss: 0.00271905  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.17 min) Epoch 2/300 -- Iteration 11398 - Batch 3696/7702 - Train loss: 0.00271830  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.19 min) Epoch 2/300 -- Iteration 11475 - Batch 3773/7702 - Train loss: 0.00271701  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.21 min) Epoch 2/300 -- Iteration 11552 - Batch 3850/7702 - Train loss: 0.00271468  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.23 min) Epoch 2/300 -- Iteration 11629 - Batch 3927/7702 - Train loss: 0.00271335  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.25 min) Epoch 2/300 -- Iteration 11706 - Batch 4004/7702 - Train loss: 0.00271149  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.27 min) Epoch 2/300 -- Iteration 11783 - Batch 4081/7702 - Train loss: 0.00270947  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.29 min) Epoch 2/300 -- Iteration 11860 - Batch 4158/7702 - Train loss: 0.00270761  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.31 min) Epoch 2/300 -- Iteration 11937 - Batch 4235/7702 - Train loss: 0.00270515  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.34 min) Epoch 2/300 -- Iteration 12014 - Batch 4312/7702 - Train loss: 0.00270369  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.36 min) Epoch 2/300 -- Iteration 12091 - Batch 4389/7702 - Train loss: 0.00270057  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.38 min) Epoch 2/300 -- Iteration 12168 - Batch 4466/7702 - Train loss: 0.00269891  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.40 min) Epoch 2/300 -- Iteration 12245 - Batch 4543/7702 - Train loss: 0.00269741  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.42 min) Epoch 2/300 -- Iteration 12322 - Batch 4620/7702 - Train loss: 0.00269501  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.44 min) Epoch 2/300 -- Iteration 12399 - Batch 4697/7702 - Train loss: 0.00269356  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.46 min) Epoch 2/300 -- Iteration 12476 - Batch 4774/7702 - Train loss: 0.00269127  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.48 min) Epoch 2/300 -- Iteration 12553 - Batch 4851/7702 - Train loss: 0.00268963  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.50 min) Epoch 2/300 -- Iteration 12630 - Batch 4928/7702 - Train loss: 0.00268828  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.52 min) Epoch 2/300 -- Iteration 12707 - Batch 5005/7702 - Train loss: 0.00268656  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.54 min) Epoch 2/300 -- Iteration 12784 - Batch 5082/7702 - Train loss: 0.00268499  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.57 min) Epoch 2/300 -- Iteration 12861 - Batch 5159/7702 - Train loss: 0.00268361  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.59 min) Epoch 2/300 -- Iteration 12938 - Batch 5236/7702 - Train loss: 0.00268148  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.61 min) Epoch 2/300 -- Iteration 13015 - Batch 5313/7702 - Train loss: 0.00268002  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.63 min) Epoch 2/300 -- Iteration 13092 - Batch 5390/7702 - Train loss: 0.00267816  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.65 min) Epoch 2/300 -- Iteration 13169 - Batch 5467/7702 - Train loss: 0.00267674  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.67 min) Epoch 2/300 -- Iteration 13246 - Batch 5544/7702 - Train loss: 0.00267529  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.69 min) Epoch 2/300 -- Iteration 13323 - Batch 5621/7702 - Train loss: 0.00267329  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.71 min) Epoch 2/300 -- Iteration 13400 - Batch 5698/7702 - Train loss: 0.00267140  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.73 min) Epoch 2/300 -- Iteration 13477 - Batch 5775/7702 - Train loss: 0.00266938  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.75 min) Epoch 2/300 -- Iteration 13554 - Batch 5852/7702 - Train loss: 0.00266825  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.77 min) Epoch 2/300 -- Iteration 13631 - Batch 5929/7702 - Train loss: 0.00266629  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.80 min) Epoch 2/300 -- Iteration 13708 - Batch 6006/7702 - Train loss: 0.00266491  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.82 min) Epoch 2/300 -- Iteration 13785 - Batch 6083/7702 - Train loss: 0.00266297  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.84 min) Epoch 2/300 -- Iteration 13862 - Batch 6160/7702 - Train loss: 0.00266101  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.86 min) Epoch 2/300 -- Iteration 13939 - Batch 6237/7702 - Train loss: 0.00265916  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.88 min) Epoch 2/300 -- Iteration 14016 - Batch 6314/7702 - Train loss: 0.00265692  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.90 min) Epoch 2/300 -- Iteration 14093 - Batch 6391/7702 - Train loss: 0.00265489  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.92 min) Epoch 2/300 -- Iteration 14170 - Batch 6468/7702 - Train loss: 0.00265314  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.94 min) Epoch 2/300 -- Iteration 14247 - Batch 6545/7702 - Train loss: 0.00265111  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.96 min) Epoch 2/300 -- Iteration 14324 - Batch 6622/7702 - Train loss: 0.00264903  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(3.98 min) Epoch 2/300 -- Iteration 14401 - Batch 6699/7702 - Train loss: 0.00264691  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.00 min) Epoch 2/300 -- Iteration 14478 - Batch 6776/7702 - Train loss: 0.00264522  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.02 min) Epoch 2/300 -- Iteration 14555 - Batch 6853/7702 - Train loss: 0.00264376  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.04 min) Epoch 2/300 -- Iteration 14632 - Batch 6930/7702 - Train loss: 0.00264173  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.07 min) Epoch 2/300 -- Iteration 14709 - Batch 7007/7702 - Train loss: 0.00264012  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.09 min) Epoch 2/300 -- Iteration 14786 - Batch 7084/7702 - Train loss: 0.00263846  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.11 min) Epoch 2/300 -- Iteration 14863 - Batch 7161/7702 - Train loss: 0.00263621  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.13 min) Epoch 2/300 -- Iteration 14940 - Batch 7238/7702 - Train loss: 0.00263414  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.15 min) Epoch 2/300 -- Iteration 15017 - Batch 7315/7702 - Train loss: 0.00263217  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.17 min) Epoch 2/300 -- Iteration 15094 - Batch 7392/7702 - Train loss: 0.00263000  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.19 min) Epoch 2/300 -- Iteration 15171 - Batch 7469/7702 - Train loss: 0.00262821  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.21 min) Epoch 2/300 -- Iteration 15248 - Batch 7546/7702 - Train loss: 0.00262712  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.23 min) Epoch 2/300 -- Iteration 15325 - Batch 7623/7702 - Train loss: 0.00262581  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.25 min) Epoch 2/300 -- Iteration 15402 - Batch 7700/7702 - Train loss: 0.00262398  - Train acc: -0.0000 - Val loss: 0.00159934\n",
      "(4.25 min) Epoch 2/300 -- Iteration 15404 - Batch 7701/7702 - Train loss: 0.00262396  - Train acc: -0.0000 - Val loss: 0.00144986 - Val acc: -0.0000\n",
      "(4.28 min) Epoch 3/300 -- Iteration 15481 - Batch 77/7702 - Train loss: 0.00247029  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.30 min) Epoch 3/300 -- Iteration 15558 - Batch 154/7702 - Train loss: 0.00244256  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.32 min) Epoch 3/300 -- Iteration 15635 - Batch 231/7702 - Train loss: 0.00244364  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.34 min) Epoch 3/300 -- Iteration 15712 - Batch 308/7702 - Train loss: 0.00243755  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.36 min) Epoch 3/300 -- Iteration 15789 - Batch 385/7702 - Train loss: 0.00243048  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.38 min) Epoch 3/300 -- Iteration 15866 - Batch 462/7702 - Train loss: 0.00243323  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.40 min) Epoch 3/300 -- Iteration 15943 - Batch 539/7702 - Train loss: 0.00243662  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.42 min) Epoch 3/300 -- Iteration 16020 - Batch 616/7702 - Train loss: 0.00243663  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.44 min) Epoch 3/300 -- Iteration 16097 - Batch 693/7702 - Train loss: 0.00243828  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.46 min) Epoch 3/300 -- Iteration 16174 - Batch 770/7702 - Train loss: 0.00243694  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.48 min) Epoch 3/300 -- Iteration 16251 - Batch 847/7702 - Train loss: 0.00243645  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.50 min) Epoch 3/300 -- Iteration 16328 - Batch 924/7702 - Train loss: 0.00243725  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.53 min) Epoch 3/300 -- Iteration 16405 - Batch 1001/7702 - Train loss: 0.00243599  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.55 min) Epoch 3/300 -- Iteration 16482 - Batch 1078/7702 - Train loss: 0.00243450  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.57 min) Epoch 3/300 -- Iteration 16559 - Batch 1155/7702 - Train loss: 0.00243181  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.59 min) Epoch 3/300 -- Iteration 16636 - Batch 1232/7702 - Train loss: 0.00243175  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.61 min) Epoch 3/300 -- Iteration 16713 - Batch 1309/7702 - Train loss: 0.00243233  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.63 min) Epoch 3/300 -- Iteration 16790 - Batch 1386/7702 - Train loss: 0.00243003  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.65 min) Epoch 3/300 -- Iteration 16867 - Batch 1463/7702 - Train loss: 0.00242720  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.67 min) Epoch 3/300 -- Iteration 16944 - Batch 1540/7702 - Train loss: 0.00242737  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.69 min) Epoch 3/300 -- Iteration 17021 - Batch 1617/7702 - Train loss: 0.00242608  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.71 min) Epoch 3/300 -- Iteration 17098 - Batch 1694/7702 - Train loss: 0.00242581  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.73 min) Epoch 3/300 -- Iteration 17175 - Batch 1771/7702 - Train loss: 0.00242442  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.75 min) Epoch 3/300 -- Iteration 17252 - Batch 1848/7702 - Train loss: 0.00242388  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.78 min) Epoch 3/300 -- Iteration 17329 - Batch 1925/7702 - Train loss: 0.00242370  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.80 min) Epoch 3/300 -- Iteration 17406 - Batch 2002/7702 - Train loss: 0.00242347  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.82 min) Epoch 3/300 -- Iteration 17483 - Batch 2079/7702 - Train loss: 0.00242132  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.84 min) Epoch 3/300 -- Iteration 17560 - Batch 2156/7702 - Train loss: 0.00241983  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.86 min) Epoch 3/300 -- Iteration 17637 - Batch 2233/7702 - Train loss: 0.00241936  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.88 min) Epoch 3/300 -- Iteration 17714 - Batch 2310/7702 - Train loss: 0.00241715  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.90 min) Epoch 3/300 -- Iteration 17791 - Batch 2387/7702 - Train loss: 0.00241603  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.92 min) Epoch 3/300 -- Iteration 17868 - Batch 2464/7702 - Train loss: 0.00241567  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.94 min) Epoch 3/300 -- Iteration 17945 - Batch 2541/7702 - Train loss: 0.00241471  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.96 min) Epoch 3/300 -- Iteration 18022 - Batch 2618/7702 - Train loss: 0.00241363  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(4.98 min) Epoch 3/300 -- Iteration 18099 - Batch 2695/7702 - Train loss: 0.00241495  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.00 min) Epoch 3/300 -- Iteration 18176 - Batch 2772/7702 - Train loss: 0.00241329  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.02 min) Epoch 3/300 -- Iteration 18253 - Batch 2849/7702 - Train loss: 0.00241250  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.04 min) Epoch 3/300 -- Iteration 18330 - Batch 2926/7702 - Train loss: 0.00241220  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.07 min) Epoch 3/300 -- Iteration 18407 - Batch 3003/7702 - Train loss: 0.00241298  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.09 min) Epoch 3/300 -- Iteration 18484 - Batch 3080/7702 - Train loss: 0.00241161  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.11 min) Epoch 3/300 -- Iteration 18561 - Batch 3157/7702 - Train loss: 0.00241043  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.13 min) Epoch 3/300 -- Iteration 18638 - Batch 3234/7702 - Train loss: 0.00240880  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.15 min) Epoch 3/300 -- Iteration 18715 - Batch 3311/7702 - Train loss: 0.00240735  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.17 min) Epoch 3/300 -- Iteration 18792 - Batch 3388/7702 - Train loss: 0.00240713  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.19 min) Epoch 3/300 -- Iteration 18869 - Batch 3465/7702 - Train loss: 0.00240514  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.21 min) Epoch 3/300 -- Iteration 18946 - Batch 3542/7702 - Train loss: 0.00240520  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.23 min) Epoch 3/300 -- Iteration 19023 - Batch 3619/7702 - Train loss: 0.00240448  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.25 min) Epoch 3/300 -- Iteration 19100 - Batch 3696/7702 - Train loss: 0.00240542  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.27 min) Epoch 3/300 -- Iteration 19177 - Batch 3773/7702 - Train loss: 0.00240719  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.29 min) Epoch 3/300 -- Iteration 19254 - Batch 3850/7702 - Train loss: 0.00240718  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.31 min) Epoch 3/300 -- Iteration 19331 - Batch 3927/7702 - Train loss: 0.00240718  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.34 min) Epoch 3/300 -- Iteration 19408 - Batch 4004/7702 - Train loss: 0.00240603  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.36 min) Epoch 3/300 -- Iteration 19485 - Batch 4081/7702 - Train loss: 0.00240484  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.38 min) Epoch 3/300 -- Iteration 19562 - Batch 4158/7702 - Train loss: 0.00240432  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.40 min) Epoch 3/300 -- Iteration 19639 - Batch 4235/7702 - Train loss: 0.00240365  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.42 min) Epoch 3/300 -- Iteration 19716 - Batch 4312/7702 - Train loss: 0.00240235  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.44 min) Epoch 3/300 -- Iteration 19793 - Batch 4389/7702 - Train loss: 0.00240138  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.46 min) Epoch 3/300 -- Iteration 19870 - Batch 4466/7702 - Train loss: 0.00240024  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.48 min) Epoch 3/300 -- Iteration 19947 - Batch 4543/7702 - Train loss: 0.00239870  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.50 min) Epoch 3/300 -- Iteration 20024 - Batch 4620/7702 - Train loss: 0.00239739  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.52 min) Epoch 3/300 -- Iteration 20101 - Batch 4697/7702 - Train loss: 0.00239664  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.54 min) Epoch 3/300 -- Iteration 20178 - Batch 4774/7702 - Train loss: 0.00239536  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.56 min) Epoch 3/300 -- Iteration 20255 - Batch 4851/7702 - Train loss: 0.00239461  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.58 min) Epoch 3/300 -- Iteration 20332 - Batch 4928/7702 - Train loss: 0.00239334  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.60 min) Epoch 3/300 -- Iteration 20409 - Batch 5005/7702 - Train loss: 0.00239286  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.62 min) Epoch 3/300 -- Iteration 20486 - Batch 5082/7702 - Train loss: 0.00239202  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.65 min) Epoch 3/300 -- Iteration 20563 - Batch 5159/7702 - Train loss: 0.00239175  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.67 min) Epoch 3/300 -- Iteration 20640 - Batch 5236/7702 - Train loss: 0.00239033  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.69 min) Epoch 3/300 -- Iteration 20717 - Batch 5313/7702 - Train loss: 0.00238971  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.71 min) Epoch 3/300 -- Iteration 20794 - Batch 5390/7702 - Train loss: 0.00238879  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.73 min) Epoch 3/300 -- Iteration 20871 - Batch 5467/7702 - Train loss: 0.00238833  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.75 min) Epoch 3/300 -- Iteration 20948 - Batch 5544/7702 - Train loss: 0.00238771  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.77 min) Epoch 3/300 -- Iteration 21025 - Batch 5621/7702 - Train loss: 0.00238748  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.79 min) Epoch 3/300 -- Iteration 21102 - Batch 5698/7702 - Train loss: 0.00238707  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.81 min) Epoch 3/300 -- Iteration 21179 - Batch 5775/7702 - Train loss: 0.00238660  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.83 min) Epoch 3/300 -- Iteration 21256 - Batch 5852/7702 - Train loss: 0.00238600  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.85 min) Epoch 3/300 -- Iteration 21333 - Batch 5929/7702 - Train loss: 0.00238565  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.87 min) Epoch 3/300 -- Iteration 21410 - Batch 6006/7702 - Train loss: 0.00238521  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.89 min) Epoch 3/300 -- Iteration 21487 - Batch 6083/7702 - Train loss: 0.00238453  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.91 min) Epoch 3/300 -- Iteration 21564 - Batch 6160/7702 - Train loss: 0.00238368  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.94 min) Epoch 3/300 -- Iteration 21641 - Batch 6237/7702 - Train loss: 0.00238328  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.96 min) Epoch 3/300 -- Iteration 21718 - Batch 6314/7702 - Train loss: 0.00238277  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(5.98 min) Epoch 3/300 -- Iteration 21795 - Batch 6391/7702 - Train loss: 0.00238232  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.00 min) Epoch 3/300 -- Iteration 21872 - Batch 6468/7702 - Train loss: 0.00238145  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.02 min) Epoch 3/300 -- Iteration 21949 - Batch 6545/7702 - Train loss: 0.00238069  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.04 min) Epoch 3/300 -- Iteration 22026 - Batch 6622/7702 - Train loss: 0.00238034  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.06 min) Epoch 3/300 -- Iteration 22103 - Batch 6699/7702 - Train loss: 0.00237966  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.08 min) Epoch 3/300 -- Iteration 22180 - Batch 6776/7702 - Train loss: 0.00237894  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.10 min) Epoch 3/300 -- Iteration 22257 - Batch 6853/7702 - Train loss: 0.00237846  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.12 min) Epoch 3/300 -- Iteration 22334 - Batch 6930/7702 - Train loss: 0.00237813  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.14 min) Epoch 3/300 -- Iteration 22411 - Batch 7007/7702 - Train loss: 0.00237732  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.16 min) Epoch 3/300 -- Iteration 22488 - Batch 7084/7702 - Train loss: 0.00237672  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.18 min) Epoch 3/300 -- Iteration 22565 - Batch 7161/7702 - Train loss: 0.00237592  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.20 min) Epoch 3/300 -- Iteration 22642 - Batch 7238/7702 - Train loss: 0.00237539  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.23 min) Epoch 3/300 -- Iteration 22719 - Batch 7315/7702 - Train loss: 0.00237474  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.25 min) Epoch 3/300 -- Iteration 22796 - Batch 7392/7702 - Train loss: 0.00237446  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.27 min) Epoch 3/300 -- Iteration 22873 - Batch 7469/7702 - Train loss: 0.00237356  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.29 min) Epoch 3/300 -- Iteration 22950 - Batch 7546/7702 - Train loss: 0.00237289  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.31 min) Epoch 3/300 -- Iteration 23027 - Batch 7623/7702 - Train loss: 0.00237247  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.33 min) Epoch 3/300 -- Iteration 23104 - Batch 7700/7702 - Train loss: 0.00237139  - Train acc: -0.0000 - Val loss: 0.00144986\n",
      "(6.33 min) Epoch 3/300 -- Iteration 23106 - Batch 7701/7702 - Train loss: 0.00237134  - Train acc: -0.0000 - Val loss: 0.00103869 - Val acc: -0.0000\n",
      "(6.35 min) Epoch 4/300 -- Iteration 23183 - Batch 77/7702 - Train loss: 0.00230819  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.37 min) Epoch 4/300 -- Iteration 23260 - Batch 154/7702 - Train loss: 0.00232801  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.39 min) Epoch 4/300 -- Iteration 23337 - Batch 231/7702 - Train loss: 0.00232350  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.42 min) Epoch 4/300 -- Iteration 23414 - Batch 308/7702 - Train loss: 0.00232308  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.44 min) Epoch 4/300 -- Iteration 23491 - Batch 385/7702 - Train loss: 0.00231767  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.46 min) Epoch 4/300 -- Iteration 23568 - Batch 462/7702 - Train loss: 0.00232146  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.48 min) Epoch 4/300 -- Iteration 23645 - Batch 539/7702 - Train loss: 0.00231810  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.50 min) Epoch 4/300 -- Iteration 23722 - Batch 616/7702 - Train loss: 0.00231583  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.52 min) Epoch 4/300 -- Iteration 23799 - Batch 693/7702 - Train loss: 0.00231800  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.54 min) Epoch 4/300 -- Iteration 23876 - Batch 770/7702 - Train loss: 0.00232074  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.56 min) Epoch 4/300 -- Iteration 23953 - Batch 847/7702 - Train loss: 0.00232070  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.58 min) Epoch 4/300 -- Iteration 24030 - Batch 924/7702 - Train loss: 0.00232003  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.60 min) Epoch 4/300 -- Iteration 24107 - Batch 1001/7702 - Train loss: 0.00231846  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.62 min) Epoch 4/300 -- Iteration 24184 - Batch 1078/7702 - Train loss: 0.00231793  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.64 min) Epoch 4/300 -- Iteration 24261 - Batch 1155/7702 - Train loss: 0.00231638  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.66 min) Epoch 4/300 -- Iteration 24338 - Batch 1232/7702 - Train loss: 0.00231430  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.68 min) Epoch 4/300 -- Iteration 24415 - Batch 1309/7702 - Train loss: 0.00231105  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.71 min) Epoch 4/300 -- Iteration 24492 - Batch 1386/7702 - Train loss: 0.00231042  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.73 min) Epoch 4/300 -- Iteration 24569 - Batch 1463/7702 - Train loss: 0.00230839  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.75 min) Epoch 4/300 -- Iteration 24646 - Batch 1540/7702 - Train loss: 0.00230855  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.77 min) Epoch 4/300 -- Iteration 24723 - Batch 1617/7702 - Train loss: 0.00231030  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.79 min) Epoch 4/300 -- Iteration 24800 - Batch 1694/7702 - Train loss: 0.00230795  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.81 min) Epoch 4/300 -- Iteration 24877 - Batch 1771/7702 - Train loss: 0.00230636  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.83 min) Epoch 4/300 -- Iteration 24954 - Batch 1848/7702 - Train loss: 0.00230769  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.85 min) Epoch 4/300 -- Iteration 25031 - Batch 1925/7702 - Train loss: 0.00230492  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.87 min) Epoch 4/300 -- Iteration 25108 - Batch 2002/7702 - Train loss: 0.00230453  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.89 min) Epoch 4/300 -- Iteration 25185 - Batch 2079/7702 - Train loss: 0.00230310  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.91 min) Epoch 4/300 -- Iteration 25262 - Batch 2156/7702 - Train loss: 0.00230171  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.93 min) Epoch 4/300 -- Iteration 25339 - Batch 2233/7702 - Train loss: 0.00230190  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.95 min) Epoch 4/300 -- Iteration 25416 - Batch 2310/7702 - Train loss: 0.00230102  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.97 min) Epoch 4/300 -- Iteration 25493 - Batch 2387/7702 - Train loss: 0.00230055  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(6.99 min) Epoch 4/300 -- Iteration 25570 - Batch 2464/7702 - Train loss: 0.00230092  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.02 min) Epoch 4/300 -- Iteration 25647 - Batch 2541/7702 - Train loss: 0.00230048  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.04 min) Epoch 4/300 -- Iteration 25724 - Batch 2618/7702 - Train loss: 0.00230060  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.06 min) Epoch 4/300 -- Iteration 25801 - Batch 2695/7702 - Train loss: 0.00229965  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.08 min) Epoch 4/300 -- Iteration 25878 - Batch 2772/7702 - Train loss: 0.00229943  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.10 min) Epoch 4/300 -- Iteration 25955 - Batch 2849/7702 - Train loss: 0.00229821  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.12 min) Epoch 4/300 -- Iteration 26032 - Batch 2926/7702 - Train loss: 0.00229829  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.14 min) Epoch 4/300 -- Iteration 26109 - Batch 3003/7702 - Train loss: 0.00229768  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.16 min) Epoch 4/300 -- Iteration 26186 - Batch 3080/7702 - Train loss: 0.00229730  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.18 min) Epoch 4/300 -- Iteration 26263 - Batch 3157/7702 - Train loss: 0.00229612  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.20 min) Epoch 4/300 -- Iteration 26340 - Batch 3234/7702 - Train loss: 0.00229572  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.22 min) Epoch 4/300 -- Iteration 26417 - Batch 3311/7702 - Train loss: 0.00229475  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.24 min) Epoch 4/300 -- Iteration 26494 - Batch 3388/7702 - Train loss: 0.00229363  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.26 min) Epoch 4/300 -- Iteration 26571 - Batch 3465/7702 - Train loss: 0.00229420  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.28 min) Epoch 4/300 -- Iteration 26648 - Batch 3542/7702 - Train loss: 0.00229344  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.30 min) Epoch 4/300 -- Iteration 26725 - Batch 3619/7702 - Train loss: 0.00229233  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.32 min) Epoch 4/300 -- Iteration 26802 - Batch 3696/7702 - Train loss: 0.00229181  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.35 min) Epoch 4/300 -- Iteration 26879 - Batch 3773/7702 - Train loss: 0.00229200  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.37 min) Epoch 4/300 -- Iteration 26956 - Batch 3850/7702 - Train loss: 0.00229159  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.39 min) Epoch 4/300 -- Iteration 27033 - Batch 3927/7702 - Train loss: 0.00229152  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.41 min) Epoch 4/300 -- Iteration 27110 - Batch 4004/7702 - Train loss: 0.00229066  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.43 min) Epoch 4/300 -- Iteration 27187 - Batch 4081/7702 - Train loss: 0.00228997  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.45 min) Epoch 4/300 -- Iteration 27264 - Batch 4158/7702 - Train loss: 0.00228930  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.47 min) Epoch 4/300 -- Iteration 27341 - Batch 4235/7702 - Train loss: 0.00228911  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.49 min) Epoch 4/300 -- Iteration 27418 - Batch 4312/7702 - Train loss: 0.00228843  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.51 min) Epoch 4/300 -- Iteration 27495 - Batch 4389/7702 - Train loss: 0.00228788  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.53 min) Epoch 4/300 -- Iteration 27572 - Batch 4466/7702 - Train loss: 0.00228715  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.55 min) Epoch 4/300 -- Iteration 27649 - Batch 4543/7702 - Train loss: 0.00228649  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.57 min) Epoch 4/300 -- Iteration 27726 - Batch 4620/7702 - Train loss: 0.00228608  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.59 min) Epoch 4/300 -- Iteration 27803 - Batch 4697/7702 - Train loss: 0.00228578  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.61 min) Epoch 4/300 -- Iteration 27880 - Batch 4774/7702 - Train loss: 0.00228617  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.63 min) Epoch 4/300 -- Iteration 27957 - Batch 4851/7702 - Train loss: 0.00228572  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.65 min) Epoch 4/300 -- Iteration 28034 - Batch 4928/7702 - Train loss: 0.00228542  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.68 min) Epoch 4/300 -- Iteration 28111 - Batch 5005/7702 - Train loss: 0.00228520  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.70 min) Epoch 4/300 -- Iteration 28188 - Batch 5082/7702 - Train loss: 0.00228507  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.72 min) Epoch 4/300 -- Iteration 28265 - Batch 5159/7702 - Train loss: 0.00228472  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.74 min) Epoch 4/300 -- Iteration 28342 - Batch 5236/7702 - Train loss: 0.00228431  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.76 min) Epoch 4/300 -- Iteration 28419 - Batch 5313/7702 - Train loss: 0.00228400  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.78 min) Epoch 4/300 -- Iteration 28496 - Batch 5390/7702 - Train loss: 0.00228399  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.80 min) Epoch 4/300 -- Iteration 28573 - Batch 5467/7702 - Train loss: 0.00228374  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.82 min) Epoch 4/300 -- Iteration 28650 - Batch 5544/7702 - Train loss: 0.00228338  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.84 min) Epoch 4/300 -- Iteration 28727 - Batch 5621/7702 - Train loss: 0.00228306  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.86 min) Epoch 4/300 -- Iteration 28804 - Batch 5698/7702 - Train loss: 0.00228263  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.88 min) Epoch 4/300 -- Iteration 28881 - Batch 5775/7702 - Train loss: 0.00228220  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.90 min) Epoch 4/300 -- Iteration 28958 - Batch 5852/7702 - Train loss: 0.00228246  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.92 min) Epoch 4/300 -- Iteration 29035 - Batch 5929/7702 - Train loss: 0.00228188  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.94 min) Epoch 4/300 -- Iteration 29112 - Batch 6006/7702 - Train loss: 0.00228131  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.96 min) Epoch 4/300 -- Iteration 29189 - Batch 6083/7702 - Train loss: 0.00228085  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(7.98 min) Epoch 4/300 -- Iteration 29266 - Batch 6160/7702 - Train loss: 0.00228059  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.01 min) Epoch 4/300 -- Iteration 29343 - Batch 6237/7702 - Train loss: 0.00228051  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.03 min) Epoch 4/300 -- Iteration 29420 - Batch 6314/7702 - Train loss: 0.00227987  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.05 min) Epoch 4/300 -- Iteration 29497 - Batch 6391/7702 - Train loss: 0.00227962  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.07 min) Epoch 4/300 -- Iteration 29574 - Batch 6468/7702 - Train loss: 0.00227904  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.09 min) Epoch 4/300 -- Iteration 29651 - Batch 6545/7702 - Train loss: 0.00227896  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.11 min) Epoch 4/300 -- Iteration 29728 - Batch 6622/7702 - Train loss: 0.00227888  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.13 min) Epoch 4/300 -- Iteration 29805 - Batch 6699/7702 - Train loss: 0.00227824  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.15 min) Epoch 4/300 -- Iteration 29882 - Batch 6776/7702 - Train loss: 0.00227824  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.17 min) Epoch 4/300 -- Iteration 29959 - Batch 6853/7702 - Train loss: 0.00227731  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.19 min) Epoch 4/300 -- Iteration 30036 - Batch 6930/7702 - Train loss: 0.00227701  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.21 min) Epoch 4/300 -- Iteration 30113 - Batch 7007/7702 - Train loss: 0.00227632  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.23 min) Epoch 4/300 -- Iteration 30190 - Batch 7084/7702 - Train loss: 0.00227614  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.25 min) Epoch 4/300 -- Iteration 30267 - Batch 7161/7702 - Train loss: 0.00227529  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.27 min) Epoch 4/300 -- Iteration 30344 - Batch 7238/7702 - Train loss: 0.00227496  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.29 min) Epoch 4/300 -- Iteration 30421 - Batch 7315/7702 - Train loss: 0.00227429  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.31 min) Epoch 4/300 -- Iteration 30498 - Batch 7392/7702 - Train loss: 0.00227442  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.34 min) Epoch 4/300 -- Iteration 30575 - Batch 7469/7702 - Train loss: 0.00227442  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.36 min) Epoch 4/300 -- Iteration 30652 - Batch 7546/7702 - Train loss: 0.00227400  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.38 min) Epoch 4/300 -- Iteration 30729 - Batch 7623/7702 - Train loss: 0.00227374  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.40 min) Epoch 4/300 -- Iteration 30806 - Batch 7700/7702 - Train loss: 0.00227317  - Train acc: -0.0000 - Val loss: 0.00103869\n",
      "(8.40 min) Epoch 4/300 -- Iteration 30808 - Batch 7701/7702 - Train loss: 0.00227315  - Train acc: -0.0000 - Val loss: 0.00105197 - Val acc: -0.0000\n",
      "(8.42 min) Epoch 5/300 -- Iteration 30885 - Batch 77/7702 - Train loss: 0.00222068  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.44 min) Epoch 5/300 -- Iteration 30962 - Batch 154/7702 - Train loss: 0.00223480  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.46 min) Epoch 5/300 -- Iteration 31039 - Batch 231/7702 - Train loss: 0.00222483  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.48 min) Epoch 5/300 -- Iteration 31116 - Batch 308/7702 - Train loss: 0.00221867  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.50 min) Epoch 5/300 -- Iteration 31193 - Batch 385/7702 - Train loss: 0.00221912  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.52 min) Epoch 5/300 -- Iteration 31270 - Batch 462/7702 - Train loss: 0.00221981  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.54 min) Epoch 5/300 -- Iteration 31347 - Batch 539/7702 - Train loss: 0.00221579  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.56 min) Epoch 5/300 -- Iteration 31424 - Batch 616/7702 - Train loss: 0.00221767  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.59 min) Epoch 5/300 -- Iteration 31501 - Batch 693/7702 - Train loss: 0.00222201  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.61 min) Epoch 5/300 -- Iteration 31578 - Batch 770/7702 - Train loss: 0.00222131  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.63 min) Epoch 5/300 -- Iteration 31655 - Batch 847/7702 - Train loss: 0.00222391  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.65 min) Epoch 5/300 -- Iteration 31732 - Batch 924/7702 - Train loss: 0.00222612  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.67 min) Epoch 5/300 -- Iteration 31809 - Batch 1001/7702 - Train loss: 0.00222493  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.69 min) Epoch 5/300 -- Iteration 31886 - Batch 1078/7702 - Train loss: 0.00222290  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.71 min) Epoch 5/300 -- Iteration 31963 - Batch 1155/7702 - Train loss: 0.00222615  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.73 min) Epoch 5/300 -- Iteration 32040 - Batch 1232/7702 - Train loss: 0.00222840  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.75 min) Epoch 5/300 -- Iteration 32117 - Batch 1309/7702 - Train loss: 0.00222642  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.77 min) Epoch 5/300 -- Iteration 32194 - Batch 1386/7702 - Train loss: 0.00222585  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.79 min) Epoch 5/300 -- Iteration 32271 - Batch 1463/7702 - Train loss: 0.00222700  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.81 min) Epoch 5/300 -- Iteration 32348 - Batch 1540/7702 - Train loss: 0.00222709  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.83 min) Epoch 5/300 -- Iteration 32425 - Batch 1617/7702 - Train loss: 0.00222921  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.85 min) Epoch 5/300 -- Iteration 32502 - Batch 1694/7702 - Train loss: 0.00222864  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.87 min) Epoch 5/300 -- Iteration 32579 - Batch 1771/7702 - Train loss: 0.00222802  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.89 min) Epoch 5/300 -- Iteration 32656 - Batch 1848/7702 - Train loss: 0.00222895  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.92 min) Epoch 5/300 -- Iteration 32733 - Batch 1925/7702 - Train loss: 0.00222742  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.94 min) Epoch 5/300 -- Iteration 32810 - Batch 2002/7702 - Train loss: 0.00222740  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.96 min) Epoch 5/300 -- Iteration 32887 - Batch 2079/7702 - Train loss: 0.00222730  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(8.98 min) Epoch 5/300 -- Iteration 32964 - Batch 2156/7702 - Train loss: 0.00222943  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.00 min) Epoch 5/300 -- Iteration 33041 - Batch 2233/7702 - Train loss: 0.00222967  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.02 min) Epoch 5/300 -- Iteration 33118 - Batch 2310/7702 - Train loss: 0.00222971  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.04 min) Epoch 5/300 -- Iteration 33195 - Batch 2387/7702 - Train loss: 0.00223047  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.06 min) Epoch 5/300 -- Iteration 33272 - Batch 2464/7702 - Train loss: 0.00223052  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.08 min) Epoch 5/300 -- Iteration 33349 - Batch 2541/7702 - Train loss: 0.00223101  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.10 min) Epoch 5/300 -- Iteration 33426 - Batch 2618/7702 - Train loss: 0.00223120  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.12 min) Epoch 5/300 -- Iteration 33503 - Batch 2695/7702 - Train loss: 0.00223181  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.14 min) Epoch 5/300 -- Iteration 33580 - Batch 2772/7702 - Train loss: 0.00223288  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.16 min) Epoch 5/300 -- Iteration 33657 - Batch 2849/7702 - Train loss: 0.00223241  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.18 min) Epoch 5/300 -- Iteration 33734 - Batch 2926/7702 - Train loss: 0.00223338  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.20 min) Epoch 5/300 -- Iteration 33811 - Batch 3003/7702 - Train loss: 0.00223289  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.22 min) Epoch 5/300 -- Iteration 33888 - Batch 3080/7702 - Train loss: 0.00223293  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.24 min) Epoch 5/300 -- Iteration 33965 - Batch 3157/7702 - Train loss: 0.00223335  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.26 min) Epoch 5/300 -- Iteration 34042 - Batch 3234/7702 - Train loss: 0.00223360  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.29 min) Epoch 5/300 -- Iteration 34119 - Batch 3311/7702 - Train loss: 0.00223335  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.31 min) Epoch 5/300 -- Iteration 34196 - Batch 3388/7702 - Train loss: 0.00223232  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.33 min) Epoch 5/300 -- Iteration 34273 - Batch 3465/7702 - Train loss: 0.00223310  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.35 min) Epoch 5/300 -- Iteration 34350 - Batch 3542/7702 - Train loss: 0.00223337  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.37 min) Epoch 5/300 -- Iteration 34427 - Batch 3619/7702 - Train loss: 0.00223394  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.39 min) Epoch 5/300 -- Iteration 34504 - Batch 3696/7702 - Train loss: 0.00223481  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.41 min) Epoch 5/300 -- Iteration 34581 - Batch 3773/7702 - Train loss: 0.00223473  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.43 min) Epoch 5/300 -- Iteration 34658 - Batch 3850/7702 - Train loss: 0.00223464  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.45 min) Epoch 5/300 -- Iteration 34735 - Batch 3927/7702 - Train loss: 0.00223372  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.47 min) Epoch 5/300 -- Iteration 34812 - Batch 4004/7702 - Train loss: 0.00223295  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.49 min) Epoch 5/300 -- Iteration 34889 - Batch 4081/7702 - Train loss: 0.00223347  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.51 min) Epoch 5/300 -- Iteration 34966 - Batch 4158/7702 - Train loss: 0.00223355  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.53 min) Epoch 5/300 -- Iteration 35043 - Batch 4235/7702 - Train loss: 0.00223393  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.55 min) Epoch 5/300 -- Iteration 35120 - Batch 4312/7702 - Train loss: 0.00223326  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.57 min) Epoch 5/300 -- Iteration 35197 - Batch 4389/7702 - Train loss: 0.00223360  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.59 min) Epoch 5/300 -- Iteration 35274 - Batch 4466/7702 - Train loss: 0.00223334  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.61 min) Epoch 5/300 -- Iteration 35351 - Batch 4543/7702 - Train loss: 0.00223360  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.63 min) Epoch 5/300 -- Iteration 35428 - Batch 4620/7702 - Train loss: 0.00223332  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.66 min) Epoch 5/300 -- Iteration 35505 - Batch 4697/7702 - Train loss: 0.00223343  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.68 min) Epoch 5/300 -- Iteration 35582 - Batch 4774/7702 - Train loss: 0.00223284  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.70 min) Epoch 5/300 -- Iteration 35659 - Batch 4851/7702 - Train loss: 0.00223288  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.72 min) Epoch 5/300 -- Iteration 35736 - Batch 4928/7702 - Train loss: 0.00223235  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.74 min) Epoch 5/300 -- Iteration 35813 - Batch 5005/7702 - Train loss: 0.00223274  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.76 min) Epoch 5/300 -- Iteration 35890 - Batch 5082/7702 - Train loss: 0.00223291  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.78 min) Epoch 5/300 -- Iteration 35967 - Batch 5159/7702 - Train loss: 0.00223242  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.80 min) Epoch 5/300 -- Iteration 36044 - Batch 5236/7702 - Train loss: 0.00223204  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.82 min) Epoch 5/300 -- Iteration 36121 - Batch 5313/7702 - Train loss: 0.00223240  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.84 min) Epoch 5/300 -- Iteration 36198 - Batch 5390/7702 - Train loss: 0.00223206  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.86 min) Epoch 5/300 -- Iteration 36275 - Batch 5467/7702 - Train loss: 0.00223232  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.88 min) Epoch 5/300 -- Iteration 36352 - Batch 5544/7702 - Train loss: 0.00223192  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.90 min) Epoch 5/300 -- Iteration 36429 - Batch 5621/7702 - Train loss: 0.00223195  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.92 min) Epoch 5/300 -- Iteration 36506 - Batch 5698/7702 - Train loss: 0.00223186  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.94 min) Epoch 5/300 -- Iteration 36583 - Batch 5775/7702 - Train loss: 0.00223063  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.96 min) Epoch 5/300 -- Iteration 36660 - Batch 5852/7702 - Train loss: 0.00223035  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(9.98 min) Epoch 5/300 -- Iteration 36737 - Batch 5929/7702 - Train loss: 0.00223067  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.00 min) Epoch 5/300 -- Iteration 36814 - Batch 6006/7702 - Train loss: 0.00223023  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.03 min) Epoch 5/300 -- Iteration 36891 - Batch 6083/7702 - Train loss: 0.00222984  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.05 min) Epoch 5/300 -- Iteration 36968 - Batch 6160/7702 - Train loss: 0.00222959  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.07 min) Epoch 5/300 -- Iteration 37045 - Batch 6237/7702 - Train loss: 0.00222894  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.09 min) Epoch 5/300 -- Iteration 37122 - Batch 6314/7702 - Train loss: 0.00222886  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.11 min) Epoch 5/300 -- Iteration 37199 - Batch 6391/7702 - Train loss: 0.00222830  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.13 min) Epoch 5/300 -- Iteration 37276 - Batch 6468/7702 - Train loss: 0.00222868  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.15 min) Epoch 5/300 -- Iteration 37353 - Batch 6545/7702 - Train loss: 0.00222826  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.17 min) Epoch 5/300 -- Iteration 37430 - Batch 6622/7702 - Train loss: 0.00222791  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.19 min) Epoch 5/300 -- Iteration 37507 - Batch 6699/7702 - Train loss: 0.00222746  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.21 min) Epoch 5/300 -- Iteration 37584 - Batch 6776/7702 - Train loss: 0.00222782  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.23 min) Epoch 5/300 -- Iteration 37661 - Batch 6853/7702 - Train loss: 0.00222790  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.25 min) Epoch 5/300 -- Iteration 37738 - Batch 6930/7702 - Train loss: 0.00222809  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.27 min) Epoch 5/300 -- Iteration 37815 - Batch 7007/7702 - Train loss: 0.00222811  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.29 min) Epoch 5/300 -- Iteration 37892 - Batch 7084/7702 - Train loss: 0.00222815  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.31 min) Epoch 5/300 -- Iteration 37969 - Batch 7161/7702 - Train loss: 0.00222804  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.33 min) Epoch 5/300 -- Iteration 38046 - Batch 7238/7702 - Train loss: 0.00222754  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.35 min) Epoch 5/300 -- Iteration 38123 - Batch 7315/7702 - Train loss: 0.00222705  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.37 min) Epoch 5/300 -- Iteration 38200 - Batch 7392/7702 - Train loss: 0.00222660  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.39 min) Epoch 5/300 -- Iteration 38277 - Batch 7469/7702 - Train loss: 0.00222630  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.42 min) Epoch 5/300 -- Iteration 38354 - Batch 7546/7702 - Train loss: 0.00222618  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.44 min) Epoch 5/300 -- Iteration 38431 - Batch 7623/7702 - Train loss: 0.00222570  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.46 min) Epoch 5/300 -- Iteration 38508 - Batch 7700/7702 - Train loss: 0.00222519  - Train acc: -0.0000 - Val loss: 0.00105197\n",
      "(10.46 min) Epoch 5/300 -- Iteration 38510 - Batch 7701/7702 - Train loss: 0.00222523  - Train acc: -0.0000 - Val loss: 0.00067633 - Val acc: -0.0000\n",
      "(10.48 min) Epoch 6/300 -- Iteration 38587 - Batch 77/7702 - Train loss: 0.00219021  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.50 min) Epoch 6/300 -- Iteration 38664 - Batch 154/7702 - Train loss: 0.00221085  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.52 min) Epoch 6/300 -- Iteration 38741 - Batch 231/7702 - Train loss: 0.00221469  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.54 min) Epoch 6/300 -- Iteration 38818 - Batch 308/7702 - Train loss: 0.00220811  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.56 min) Epoch 6/300 -- Iteration 38895 - Batch 385/7702 - Train loss: 0.00220223  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.58 min) Epoch 6/300 -- Iteration 38972 - Batch 462/7702 - Train loss: 0.00220630  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.60 min) Epoch 6/300 -- Iteration 39049 - Batch 539/7702 - Train loss: 0.00219798  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.62 min) Epoch 6/300 -- Iteration 39126 - Batch 616/7702 - Train loss: 0.00219590  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.64 min) Epoch 6/300 -- Iteration 39203 - Batch 693/7702 - Train loss: 0.00219500  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.66 min) Epoch 6/300 -- Iteration 39280 - Batch 770/7702 - Train loss: 0.00219298  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.69 min) Epoch 6/300 -- Iteration 39357 - Batch 847/7702 - Train loss: 0.00219390  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.71 min) Epoch 6/300 -- Iteration 39434 - Batch 924/7702 - Train loss: 0.00219442  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.73 min) Epoch 6/300 -- Iteration 39511 - Batch 1001/7702 - Train loss: 0.00219486  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.75 min) Epoch 6/300 -- Iteration 39588 - Batch 1078/7702 - Train loss: 0.00219272  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.77 min) Epoch 6/300 -- Iteration 39665 - Batch 1155/7702 - Train loss: 0.00219378  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.79 min) Epoch 6/300 -- Iteration 39742 - Batch 1232/7702 - Train loss: 0.00219602  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.81 min) Epoch 6/300 -- Iteration 39819 - Batch 1309/7702 - Train loss: 0.00219441  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.83 min) Epoch 6/300 -- Iteration 39896 - Batch 1386/7702 - Train loss: 0.00219237  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.85 min) Epoch 6/300 -- Iteration 39973 - Batch 1463/7702 - Train loss: 0.00218932  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.87 min) Epoch 6/300 -- Iteration 40050 - Batch 1540/7702 - Train loss: 0.00219071  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.89 min) Epoch 6/300 -- Iteration 40127 - Batch 1617/7702 - Train loss: 0.00219096  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.91 min) Epoch 6/300 -- Iteration 40204 - Batch 1694/7702 - Train loss: 0.00219117  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.93 min) Epoch 6/300 -- Iteration 40281 - Batch 1771/7702 - Train loss: 0.00218963  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.95 min) Epoch 6/300 -- Iteration 40358 - Batch 1848/7702 - Train loss: 0.00218856  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.97 min) Epoch 6/300 -- Iteration 40435 - Batch 1925/7702 - Train loss: 0.00218797  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(10.99 min) Epoch 6/300 -- Iteration 40512 - Batch 2002/7702 - Train loss: 0.00218860  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.01 min) Epoch 6/300 -- Iteration 40589 - Batch 2079/7702 - Train loss: 0.00218775  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.03 min) Epoch 6/300 -- Iteration 40666 - Batch 2156/7702 - Train loss: 0.00218799  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.05 min) Epoch 6/300 -- Iteration 40743 - Batch 2233/7702 - Train loss: 0.00218931  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.08 min) Epoch 6/300 -- Iteration 40820 - Batch 2310/7702 - Train loss: 0.00218874  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.10 min) Epoch 6/300 -- Iteration 40897 - Batch 2387/7702 - Train loss: 0.00218883  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.12 min) Epoch 6/300 -- Iteration 40974 - Batch 2464/7702 - Train loss: 0.00218888  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.14 min) Epoch 6/300 -- Iteration 41051 - Batch 2541/7702 - Train loss: 0.00218919  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.16 min) Epoch 6/300 -- Iteration 41128 - Batch 2618/7702 - Train loss: 0.00218931  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.18 min) Epoch 6/300 -- Iteration 41205 - Batch 2695/7702 - Train loss: 0.00218834  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.20 min) Epoch 6/300 -- Iteration 41282 - Batch 2772/7702 - Train loss: 0.00218815  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.22 min) Epoch 6/300 -- Iteration 41359 - Batch 2849/7702 - Train loss: 0.00218726  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.24 min) Epoch 6/300 -- Iteration 41436 - Batch 2926/7702 - Train loss: 0.00218743  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.26 min) Epoch 6/300 -- Iteration 41513 - Batch 3003/7702 - Train loss: 0.00218662  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.28 min) Epoch 6/300 -- Iteration 41590 - Batch 3080/7702 - Train loss: 0.00218683  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.30 min) Epoch 6/300 -- Iteration 41667 - Batch 3157/7702 - Train loss: 0.00218624  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.32 min) Epoch 6/300 -- Iteration 41744 - Batch 3234/7702 - Train loss: 0.00218640  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.34 min) Epoch 6/300 -- Iteration 41821 - Batch 3311/7702 - Train loss: 0.00218754  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.36 min) Epoch 6/300 -- Iteration 41898 - Batch 3388/7702 - Train loss: 0.00218773  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.38 min) Epoch 6/300 -- Iteration 41975 - Batch 3465/7702 - Train loss: 0.00218779  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.40 min) Epoch 6/300 -- Iteration 42052 - Batch 3542/7702 - Train loss: 0.00218824  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.42 min) Epoch 6/300 -- Iteration 42129 - Batch 3619/7702 - Train loss: 0.00218804  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.45 min) Epoch 6/300 -- Iteration 42206 - Batch 3696/7702 - Train loss: 0.00218806  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.47 min) Epoch 6/300 -- Iteration 42283 - Batch 3773/7702 - Train loss: 0.00218791  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.49 min) Epoch 6/300 -- Iteration 42360 - Batch 3850/7702 - Train loss: 0.00218798  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.51 min) Epoch 6/300 -- Iteration 42437 - Batch 3927/7702 - Train loss: 0.00218816  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.53 min) Epoch 6/300 -- Iteration 42514 - Batch 4004/7702 - Train loss: 0.00218836  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.55 min) Epoch 6/300 -- Iteration 42591 - Batch 4081/7702 - Train loss: 0.00218839  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.57 min) Epoch 6/300 -- Iteration 42668 - Batch 4158/7702 - Train loss: 0.00218798  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.59 min) Epoch 6/300 -- Iteration 42745 - Batch 4235/7702 - Train loss: 0.00218789  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.61 min) Epoch 6/300 -- Iteration 42822 - Batch 4312/7702 - Train loss: 0.00218756  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.63 min) Epoch 6/300 -- Iteration 42899 - Batch 4389/7702 - Train loss: 0.00218710  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.65 min) Epoch 6/300 -- Iteration 42976 - Batch 4466/7702 - Train loss: 0.00218577  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.67 min) Epoch 6/300 -- Iteration 43053 - Batch 4543/7702 - Train loss: 0.00218594  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.69 min) Epoch 6/300 -- Iteration 43130 - Batch 4620/7702 - Train loss: 0.00218523  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.71 min) Epoch 6/300 -- Iteration 43207 - Batch 4697/7702 - Train loss: 0.00218509  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.73 min) Epoch 6/300 -- Iteration 43284 - Batch 4774/7702 - Train loss: 0.00218496  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.75 min) Epoch 6/300 -- Iteration 43361 - Batch 4851/7702 - Train loss: 0.00218476  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.77 min) Epoch 6/300 -- Iteration 43438 - Batch 4928/7702 - Train loss: 0.00218487  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.79 min) Epoch 6/300 -- Iteration 43515 - Batch 5005/7702 - Train loss: 0.00218523  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.81 min) Epoch 6/300 -- Iteration 43592 - Batch 5082/7702 - Train loss: 0.00218540  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.84 min) Epoch 6/300 -- Iteration 43669 - Batch 5159/7702 - Train loss: 0.00218542  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.86 min) Epoch 6/300 -- Iteration 43746 - Batch 5236/7702 - Train loss: 0.00218451  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.88 min) Epoch 6/300 -- Iteration 43823 - Batch 5313/7702 - Train loss: 0.00218409  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.90 min) Epoch 6/300 -- Iteration 43900 - Batch 5390/7702 - Train loss: 0.00218452  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.92 min) Epoch 6/300 -- Iteration 43977 - Batch 5467/7702 - Train loss: 0.00218460  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.94 min) Epoch 6/300 -- Iteration 44054 - Batch 5544/7702 - Train loss: 0.00218442  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.96 min) Epoch 6/300 -- Iteration 44131 - Batch 5621/7702 - Train loss: 0.00218425  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(11.98 min) Epoch 6/300 -- Iteration 44208 - Batch 5698/7702 - Train loss: 0.00218431  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.00 min) Epoch 6/300 -- Iteration 44285 - Batch 5775/7702 - Train loss: 0.00218402  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.02 min) Epoch 6/300 -- Iteration 44362 - Batch 5852/7702 - Train loss: 0.00218443  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.04 min) Epoch 6/300 -- Iteration 44439 - Batch 5929/7702 - Train loss: 0.00218362  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.06 min) Epoch 6/300 -- Iteration 44516 - Batch 6006/7702 - Train loss: 0.00218344  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.08 min) Epoch 6/300 -- Iteration 44593 - Batch 6083/7702 - Train loss: 0.00218303  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.10 min) Epoch 6/300 -- Iteration 44670 - Batch 6160/7702 - Train loss: 0.00218299  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.12 min) Epoch 6/300 -- Iteration 44747 - Batch 6237/7702 - Train loss: 0.00218261  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.14 min) Epoch 6/300 -- Iteration 44824 - Batch 6314/7702 - Train loss: 0.00218243  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.16 min) Epoch 6/300 -- Iteration 44901 - Batch 6391/7702 - Train loss: 0.00218164  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.18 min) Epoch 6/300 -- Iteration 44978 - Batch 6468/7702 - Train loss: 0.00218166  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.20 min) Epoch 6/300 -- Iteration 45055 - Batch 6545/7702 - Train loss: 0.00218206  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.22 min) Epoch 6/300 -- Iteration 45132 - Batch 6622/7702 - Train loss: 0.00218227  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.25 min) Epoch 6/300 -- Iteration 45209 - Batch 6699/7702 - Train loss: 0.00218224  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.27 min) Epoch 6/300 -- Iteration 45286 - Batch 6776/7702 - Train loss: 0.00218197  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.29 min) Epoch 6/300 -- Iteration 45363 - Batch 6853/7702 - Train loss: 0.00218183  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.31 min) Epoch 6/300 -- Iteration 45440 - Batch 6930/7702 - Train loss: 0.00218205  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.33 min) Epoch 6/300 -- Iteration 45517 - Batch 7007/7702 - Train loss: 0.00218238  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.35 min) Epoch 6/300 -- Iteration 45594 - Batch 7084/7702 - Train loss: 0.00218209  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.37 min) Epoch 6/300 -- Iteration 45671 - Batch 7161/7702 - Train loss: 0.00218139  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.39 min) Epoch 6/300 -- Iteration 45748 - Batch 7238/7702 - Train loss: 0.00218156  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.41 min) Epoch 6/300 -- Iteration 45825 - Batch 7315/7702 - Train loss: 0.00218130  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.43 min) Epoch 6/300 -- Iteration 45902 - Batch 7392/7702 - Train loss: 0.00218146  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.45 min) Epoch 6/300 -- Iteration 45979 - Batch 7469/7702 - Train loss: 0.00218141  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.47 min) Epoch 6/300 -- Iteration 46056 - Batch 7546/7702 - Train loss: 0.00218105  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.49 min) Epoch 6/300 -- Iteration 46133 - Batch 7623/7702 - Train loss: 0.00218070  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.51 min) Epoch 6/300 -- Iteration 46210 - Batch 7700/7702 - Train loss: 0.00218031  - Train acc: -0.0000 - Val loss: 0.00067633\n",
      "(12.51 min) Epoch 6/300 -- Iteration 46212 - Batch 7701/7702 - Train loss: 0.00218037  - Train acc: -0.0000 - Val loss: 0.00058632 - Val acc: -0.0000\n",
      "(12.53 min) Epoch 7/300 -- Iteration 46289 - Batch 77/7702 - Train loss: 0.00218789  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.56 min) Epoch 7/300 -- Iteration 46366 - Batch 154/7702 - Train loss: 0.00216887  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.58 min) Epoch 7/300 -- Iteration 46443 - Batch 231/7702 - Train loss: 0.00217371  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.60 min) Epoch 7/300 -- Iteration 46520 - Batch 308/7702 - Train loss: 0.00216596  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.62 min) Epoch 7/300 -- Iteration 46597 - Batch 385/7702 - Train loss: 0.00217232  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.64 min) Epoch 7/300 -- Iteration 46674 - Batch 462/7702 - Train loss: 0.00217182  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.66 min) Epoch 7/300 -- Iteration 46751 - Batch 539/7702 - Train loss: 0.00216951  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.68 min) Epoch 7/300 -- Iteration 46828 - Batch 616/7702 - Train loss: 0.00216859  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.70 min) Epoch 7/300 -- Iteration 46905 - Batch 693/7702 - Train loss: 0.00216935  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.72 min) Epoch 7/300 -- Iteration 46982 - Batch 770/7702 - Train loss: 0.00216361  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.74 min) Epoch 7/300 -- Iteration 47059 - Batch 847/7702 - Train loss: 0.00216437  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.76 min) Epoch 7/300 -- Iteration 47136 - Batch 924/7702 - Train loss: 0.00216599  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.78 min) Epoch 7/300 -- Iteration 47213 - Batch 1001/7702 - Train loss: 0.00216714  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.80 min) Epoch 7/300 -- Iteration 47290 - Batch 1078/7702 - Train loss: 0.00216536  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.82 min) Epoch 7/300 -- Iteration 47367 - Batch 1155/7702 - Train loss: 0.00216459  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.84 min) Epoch 7/300 -- Iteration 47444 - Batch 1232/7702 - Train loss: 0.00216527  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.86 min) Epoch 7/300 -- Iteration 47521 - Batch 1309/7702 - Train loss: 0.00216424  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.88 min) Epoch 7/300 -- Iteration 47598 - Batch 1386/7702 - Train loss: 0.00216464  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.90 min) Epoch 7/300 -- Iteration 47675 - Batch 1463/7702 - Train loss: 0.00216415  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.93 min) Epoch 7/300 -- Iteration 47752 - Batch 1540/7702 - Train loss: 0.00216518  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.95 min) Epoch 7/300 -- Iteration 47829 - Batch 1617/7702 - Train loss: 0.00216797  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.97 min) Epoch 7/300 -- Iteration 47906 - Batch 1694/7702 - Train loss: 0.00216637  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(12.99 min) Epoch 7/300 -- Iteration 47983 - Batch 1771/7702 - Train loss: 0.00216503  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.01 min) Epoch 7/300 -- Iteration 48060 - Batch 1848/7702 - Train loss: 0.00216490  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.03 min) Epoch 7/300 -- Iteration 48137 - Batch 1925/7702 - Train loss: 0.00216388  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.05 min) Epoch 7/300 -- Iteration 48214 - Batch 2002/7702 - Train loss: 0.00216290  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.07 min) Epoch 7/300 -- Iteration 48291 - Batch 2079/7702 - Train loss: 0.00216245  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.09 min) Epoch 7/300 -- Iteration 48368 - Batch 2156/7702 - Train loss: 0.00216181  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.11 min) Epoch 7/300 -- Iteration 48445 - Batch 2233/7702 - Train loss: 0.00216114  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.13 min) Epoch 7/300 -- Iteration 48522 - Batch 2310/7702 - Train loss: 0.00216178  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.15 min) Epoch 7/300 -- Iteration 48599 - Batch 2387/7702 - Train loss: 0.00216031  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.17 min) Epoch 7/300 -- Iteration 48676 - Batch 2464/7702 - Train loss: 0.00216096  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.19 min) Epoch 7/300 -- Iteration 48753 - Batch 2541/7702 - Train loss: 0.00216081  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.21 min) Epoch 7/300 -- Iteration 48830 - Batch 2618/7702 - Train loss: 0.00216010  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.23 min) Epoch 7/300 -- Iteration 48907 - Batch 2695/7702 - Train loss: 0.00215871  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.25 min) Epoch 7/300 -- Iteration 48984 - Batch 2772/7702 - Train loss: 0.00215907  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.27 min) Epoch 7/300 -- Iteration 49061 - Batch 2849/7702 - Train loss: 0.00215861  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.29 min) Epoch 7/300 -- Iteration 49138 - Batch 2926/7702 - Train loss: 0.00215746  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.31 min) Epoch 7/300 -- Iteration 49215 - Batch 3003/7702 - Train loss: 0.00215696  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.33 min) Epoch 7/300 -- Iteration 49292 - Batch 3080/7702 - Train loss: 0.00215822  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.36 min) Epoch 7/300 -- Iteration 49369 - Batch 3157/7702 - Train loss: 0.00215749  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.38 min) Epoch 7/300 -- Iteration 49446 - Batch 3234/7702 - Train loss: 0.00215699  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.40 min) Epoch 7/300 -- Iteration 49523 - Batch 3311/7702 - Train loss: 0.00215754  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.42 min) Epoch 7/300 -- Iteration 49600 - Batch 3388/7702 - Train loss: 0.00215764  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.44 min) Epoch 7/300 -- Iteration 49677 - Batch 3465/7702 - Train loss: 0.00215806  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.46 min) Epoch 7/300 -- Iteration 49754 - Batch 3542/7702 - Train loss: 0.00215835  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.48 min) Epoch 7/300 -- Iteration 49831 - Batch 3619/7702 - Train loss: 0.00215799  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.50 min) Epoch 7/300 -- Iteration 49908 - Batch 3696/7702 - Train loss: 0.00215834  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.52 min) Epoch 7/300 -- Iteration 49985 - Batch 3773/7702 - Train loss: 0.00215851  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.54 min) Epoch 7/300 -- Iteration 50062 - Batch 3850/7702 - Train loss: 0.00215803  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.56 min) Epoch 7/300 -- Iteration 50139 - Batch 3927/7702 - Train loss: 0.00215857  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.58 min) Epoch 7/300 -- Iteration 50216 - Batch 4004/7702 - Train loss: 0.00215849  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.60 min) Epoch 7/300 -- Iteration 50293 - Batch 4081/7702 - Train loss: 0.00215833  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.62 min) Epoch 7/300 -- Iteration 50370 - Batch 4158/7702 - Train loss: 0.00215787  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.64 min) Epoch 7/300 -- Iteration 50447 - Batch 4235/7702 - Train loss: 0.00215747  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.66 min) Epoch 7/300 -- Iteration 50524 - Batch 4312/7702 - Train loss: 0.00215658  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.68 min) Epoch 7/300 -- Iteration 50601 - Batch 4389/7702 - Train loss: 0.00215613  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.70 min) Epoch 7/300 -- Iteration 50678 - Batch 4466/7702 - Train loss: 0.00215583  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.72 min) Epoch 7/300 -- Iteration 50755 - Batch 4543/7702 - Train loss: 0.00215479  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.75 min) Epoch 7/300 -- Iteration 50832 - Batch 4620/7702 - Train loss: 0.00215475  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.77 min) Epoch 7/300 -- Iteration 50909 - Batch 4697/7702 - Train loss: 0.00215459  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.79 min) Epoch 7/300 -- Iteration 50986 - Batch 4774/7702 - Train loss: 0.00215505  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.81 min) Epoch 7/300 -- Iteration 51063 - Batch 4851/7702 - Train loss: 0.00215480  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.83 min) Epoch 7/300 -- Iteration 51140 - Batch 4928/7702 - Train loss: 0.00215458  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.85 min) Epoch 7/300 -- Iteration 51217 - Batch 5005/7702 - Train loss: 0.00215473  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.87 min) Epoch 7/300 -- Iteration 51294 - Batch 5082/7702 - Train loss: 0.00215471  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.89 min) Epoch 7/300 -- Iteration 51371 - Batch 5159/7702 - Train loss: 0.00215501  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.91 min) Epoch 7/300 -- Iteration 51448 - Batch 5236/7702 - Train loss: 0.00215470  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.93 min) Epoch 7/300 -- Iteration 51525 - Batch 5313/7702 - Train loss: 0.00215433  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.95 min) Epoch 7/300 -- Iteration 51602 - Batch 5390/7702 - Train loss: 0.00215468  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.97 min) Epoch 7/300 -- Iteration 51679 - Batch 5467/7702 - Train loss: 0.00215516  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(13.99 min) Epoch 7/300 -- Iteration 51756 - Batch 5544/7702 - Train loss: 0.00215477  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.01 min) Epoch 7/300 -- Iteration 51833 - Batch 5621/7702 - Train loss: 0.00215492  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.03 min) Epoch 7/300 -- Iteration 51910 - Batch 5698/7702 - Train loss: 0.00215546  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.05 min) Epoch 7/300 -- Iteration 51987 - Batch 5775/7702 - Train loss: 0.00215526  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.07 min) Epoch 7/300 -- Iteration 52064 - Batch 5852/7702 - Train loss: 0.00215529  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.09 min) Epoch 7/300 -- Iteration 52141 - Batch 5929/7702 - Train loss: 0.00215478  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.11 min) Epoch 7/300 -- Iteration 52218 - Batch 6006/7702 - Train loss: 0.00215478  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.13 min) Epoch 7/300 -- Iteration 52295 - Batch 6083/7702 - Train loss: 0.00215521  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.16 min) Epoch 7/300 -- Iteration 52372 - Batch 6160/7702 - Train loss: 0.00215528  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.18 min) Epoch 7/300 -- Iteration 52449 - Batch 6237/7702 - Train loss: 0.00215508  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.20 min) Epoch 7/300 -- Iteration 52526 - Batch 6314/7702 - Train loss: 0.00215475  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.22 min) Epoch 7/300 -- Iteration 52603 - Batch 6391/7702 - Train loss: 0.00215426  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.24 min) Epoch 7/300 -- Iteration 52680 - Batch 6468/7702 - Train loss: 0.00215425  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.26 min) Epoch 7/300 -- Iteration 52757 - Batch 6545/7702 - Train loss: 0.00215449  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.28 min) Epoch 7/300 -- Iteration 52834 - Batch 6622/7702 - Train loss: 0.00215436  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.30 min) Epoch 7/300 -- Iteration 52911 - Batch 6699/7702 - Train loss: 0.00215420  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.32 min) Epoch 7/300 -- Iteration 52988 - Batch 6776/7702 - Train loss: 0.00215423  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.34 min) Epoch 7/300 -- Iteration 53065 - Batch 6853/7702 - Train loss: 0.00215423  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.36 min) Epoch 7/300 -- Iteration 53142 - Batch 6930/7702 - Train loss: 0.00215431  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.38 min) Epoch 7/300 -- Iteration 53219 - Batch 7007/7702 - Train loss: 0.00215458  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.40 min) Epoch 7/300 -- Iteration 53296 - Batch 7084/7702 - Train loss: 0.00215453  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.42 min) Epoch 7/300 -- Iteration 53373 - Batch 7161/7702 - Train loss: 0.00215434  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.45 min) Epoch 7/300 -- Iteration 53450 - Batch 7238/7702 - Train loss: 0.00215408  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.47 min) Epoch 7/300 -- Iteration 53527 - Batch 7315/7702 - Train loss: 0.00215386  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.49 min) Epoch 7/300 -- Iteration 53604 - Batch 7392/7702 - Train loss: 0.00215383  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.51 min) Epoch 7/300 -- Iteration 53681 - Batch 7469/7702 - Train loss: 0.00215364  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.53 min) Epoch 7/300 -- Iteration 53758 - Batch 7546/7702 - Train loss: 0.00215342  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.56 min) Epoch 7/300 -- Iteration 53835 - Batch 7623/7702 - Train loss: 0.00215353  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.58 min) Epoch 7/300 -- Iteration 53912 - Batch 7700/7702 - Train loss: 0.00215337  - Train acc: -0.0000 - Val loss: 0.00058632\n",
      "(14.58 min) Epoch 7/300 -- Iteration 53914 - Batch 7701/7702 - Train loss: 0.00215333  - Train acc: -0.0000 - Val loss: 0.00051984 - Val acc: -0.0000\n",
      "(14.60 min) Epoch 8/300 -- Iteration 53991 - Batch 77/7702 - Train loss: 0.00217442  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.63 min) Epoch 8/300 -- Iteration 54068 - Batch 154/7702 - Train loss: 0.00215053  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.65 min) Epoch 8/300 -- Iteration 54145 - Batch 231/7702 - Train loss: 0.00213890  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.67 min) Epoch 8/300 -- Iteration 54222 - Batch 308/7702 - Train loss: 0.00213698  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.69 min) Epoch 8/300 -- Iteration 54299 - Batch 385/7702 - Train loss: 0.00213220  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.71 min) Epoch 8/300 -- Iteration 54376 - Batch 462/7702 - Train loss: 0.00212590  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.73 min) Epoch 8/300 -- Iteration 54453 - Batch 539/7702 - Train loss: 0.00212827  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.75 min) Epoch 8/300 -- Iteration 54530 - Batch 616/7702 - Train loss: 0.00212368  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.77 min) Epoch 8/300 -- Iteration 54607 - Batch 693/7702 - Train loss: 0.00213428  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.79 min) Epoch 8/300 -- Iteration 54684 - Batch 770/7702 - Train loss: 0.00213293  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.81 min) Epoch 8/300 -- Iteration 54761 - Batch 847/7702 - Train loss: 0.00213608  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.84 min) Epoch 8/300 -- Iteration 54838 - Batch 924/7702 - Train loss: 0.00213820  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.86 min) Epoch 8/300 -- Iteration 54915 - Batch 1001/7702 - Train loss: 0.00213835  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.88 min) Epoch 8/300 -- Iteration 54992 - Batch 1078/7702 - Train loss: 0.00214170  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.90 min) Epoch 8/300 -- Iteration 55069 - Batch 1155/7702 - Train loss: 0.00214224  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.92 min) Epoch 8/300 -- Iteration 55146 - Batch 1232/7702 - Train loss: 0.00214250  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.94 min) Epoch 8/300 -- Iteration 55223 - Batch 1309/7702 - Train loss: 0.00214255  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.96 min) Epoch 8/300 -- Iteration 55300 - Batch 1386/7702 - Train loss: 0.00214349  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(14.98 min) Epoch 8/300 -- Iteration 55377 - Batch 1463/7702 - Train loss: 0.00214103  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.00 min) Epoch 8/300 -- Iteration 55454 - Batch 1540/7702 - Train loss: 0.00214228  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.02 min) Epoch 8/300 -- Iteration 55531 - Batch 1617/7702 - Train loss: 0.00214204  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.04 min) Epoch 8/300 -- Iteration 55608 - Batch 1694/7702 - Train loss: 0.00214141  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.06 min) Epoch 8/300 -- Iteration 55685 - Batch 1771/7702 - Train loss: 0.00214344  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.08 min) Epoch 8/300 -- Iteration 55762 - Batch 1848/7702 - Train loss: 0.00214189  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.10 min) Epoch 8/300 -- Iteration 55839 - Batch 1925/7702 - Train loss: 0.00214170  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.12 min) Epoch 8/300 -- Iteration 55916 - Batch 2002/7702 - Train loss: 0.00214203  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.15 min) Epoch 8/300 -- Iteration 55993 - Batch 2079/7702 - Train loss: 0.00214267  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.17 min) Epoch 8/300 -- Iteration 56070 - Batch 2156/7702 - Train loss: 0.00214277  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.19 min) Epoch 8/300 -- Iteration 56147 - Batch 2233/7702 - Train loss: 0.00214429  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.21 min) Epoch 8/300 -- Iteration 56224 - Batch 2310/7702 - Train loss: 0.00214414  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.23 min) Epoch 8/300 -- Iteration 56301 - Batch 2387/7702 - Train loss: 0.00214391  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.25 min) Epoch 8/300 -- Iteration 56378 - Batch 2464/7702 - Train loss: 0.00214450  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.27 min) Epoch 8/300 -- Iteration 56455 - Batch 2541/7702 - Train loss: 0.00214394  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.29 min) Epoch 8/300 -- Iteration 56532 - Batch 2618/7702 - Train loss: 0.00214336  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.31 min) Epoch 8/300 -- Iteration 56609 - Batch 2695/7702 - Train loss: 0.00214287  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.33 min) Epoch 8/300 -- Iteration 56686 - Batch 2772/7702 - Train loss: 0.00214237  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.35 min) Epoch 8/300 -- Iteration 56763 - Batch 2849/7702 - Train loss: 0.00214142  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.37 min) Epoch 8/300 -- Iteration 56840 - Batch 2926/7702 - Train loss: 0.00213981  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.39 min) Epoch 8/300 -- Iteration 56917 - Batch 3003/7702 - Train loss: 0.00213943  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.41 min) Epoch 8/300 -- Iteration 56994 - Batch 3080/7702 - Train loss: 0.00213898  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.43 min) Epoch 8/300 -- Iteration 57071 - Batch 3157/7702 - Train loss: 0.00213910  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.46 min) Epoch 8/300 -- Iteration 57148 - Batch 3234/7702 - Train loss: 0.00213767  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.48 min) Epoch 8/300 -- Iteration 57225 - Batch 3311/7702 - Train loss: 0.00213718  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.50 min) Epoch 8/300 -- Iteration 57302 - Batch 3388/7702 - Train loss: 0.00213621  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.52 min) Epoch 8/300 -- Iteration 57379 - Batch 3465/7702 - Train loss: 0.00213655  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.54 min) Epoch 8/300 -- Iteration 57456 - Batch 3542/7702 - Train loss: 0.00213573  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.56 min) Epoch 8/300 -- Iteration 57533 - Batch 3619/7702 - Train loss: 0.00213523  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.58 min) Epoch 8/300 -- Iteration 57610 - Batch 3696/7702 - Train loss: 0.00213447  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.60 min) Epoch 8/300 -- Iteration 57687 - Batch 3773/7702 - Train loss: 0.00213306  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.62 min) Epoch 8/300 -- Iteration 57764 - Batch 3850/7702 - Train loss: 0.00213253  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.64 min) Epoch 8/300 -- Iteration 57841 - Batch 3927/7702 - Train loss: 0.00213242  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.66 min) Epoch 8/300 -- Iteration 57918 - Batch 4004/7702 - Train loss: 0.00213247  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.68 min) Epoch 8/300 -- Iteration 57995 - Batch 4081/7702 - Train loss: 0.00213171  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.70 min) Epoch 8/300 -- Iteration 58072 - Batch 4158/7702 - Train loss: 0.00213120  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.72 min) Epoch 8/300 -- Iteration 58149 - Batch 4235/7702 - Train loss: 0.00213090  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.74 min) Epoch 8/300 -- Iteration 58226 - Batch 4312/7702 - Train loss: 0.00213126  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.77 min) Epoch 8/300 -- Iteration 58303 - Batch 4389/7702 - Train loss: 0.00213178  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.79 min) Epoch 8/300 -- Iteration 58380 - Batch 4466/7702 - Train loss: 0.00213176  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.81 min) Epoch 8/300 -- Iteration 58457 - Batch 4543/7702 - Train loss: 0.00213133  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.83 min) Epoch 8/300 -- Iteration 58534 - Batch 4620/7702 - Train loss: 0.00213132  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.85 min) Epoch 8/300 -- Iteration 58611 - Batch 4697/7702 - Train loss: 0.00213120  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.87 min) Epoch 8/300 -- Iteration 58688 - Batch 4774/7702 - Train loss: 0.00213121  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.89 min) Epoch 8/300 -- Iteration 58765 - Batch 4851/7702 - Train loss: 0.00213149  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.91 min) Epoch 8/300 -- Iteration 58842 - Batch 4928/7702 - Train loss: 0.00213077  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.93 min) Epoch 8/300 -- Iteration 58919 - Batch 5005/7702 - Train loss: 0.00213065  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.95 min) Epoch 8/300 -- Iteration 58996 - Batch 5082/7702 - Train loss: 0.00213010  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.97 min) Epoch 8/300 -- Iteration 59073 - Batch 5159/7702 - Train loss: 0.00212985  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(15.99 min) Epoch 8/300 -- Iteration 59150 - Batch 5236/7702 - Train loss: 0.00212965  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.01 min) Epoch 8/300 -- Iteration 59227 - Batch 5313/7702 - Train loss: 0.00213001  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.03 min) Epoch 8/300 -- Iteration 59304 - Batch 5390/7702 - Train loss: 0.00212996  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.05 min) Epoch 8/300 -- Iteration 59381 - Batch 5467/7702 - Train loss: 0.00213039  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.07 min) Epoch 8/300 -- Iteration 59458 - Batch 5544/7702 - Train loss: 0.00213040  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.10 min) Epoch 8/300 -- Iteration 59535 - Batch 5621/7702 - Train loss: 0.00213029  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.12 min) Epoch 8/300 -- Iteration 59612 - Batch 5698/7702 - Train loss: 0.00213029  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.14 min) Epoch 8/300 -- Iteration 59689 - Batch 5775/7702 - Train loss: 0.00213002  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.16 min) Epoch 8/300 -- Iteration 59766 - Batch 5852/7702 - Train loss: 0.00212986  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.18 min) Epoch 8/300 -- Iteration 59843 - Batch 5929/7702 - Train loss: 0.00212983  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.20 min) Epoch 8/300 -- Iteration 59920 - Batch 6006/7702 - Train loss: 0.00212971  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.22 min) Epoch 8/300 -- Iteration 59997 - Batch 6083/7702 - Train loss: 0.00212969  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.24 min) Epoch 8/300 -- Iteration 60074 - Batch 6160/7702 - Train loss: 0.00212952  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.26 min) Epoch 8/300 -- Iteration 60151 - Batch 6237/7702 - Train loss: 0.00212910  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.28 min) Epoch 8/300 -- Iteration 60228 - Batch 6314/7702 - Train loss: 0.00212943  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.30 min) Epoch 8/300 -- Iteration 60305 - Batch 6391/7702 - Train loss: 0.00212926  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.32 min) Epoch 8/300 -- Iteration 60382 - Batch 6468/7702 - Train loss: 0.00212914  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.34 min) Epoch 8/300 -- Iteration 60459 - Batch 6545/7702 - Train loss: 0.00212898  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.36 min) Epoch 8/300 -- Iteration 60536 - Batch 6622/7702 - Train loss: 0.00212866  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.38 min) Epoch 8/300 -- Iteration 60613 - Batch 6699/7702 - Train loss: 0.00212853  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.41 min) Epoch 8/300 -- Iteration 60690 - Batch 6776/7702 - Train loss: 0.00212888  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.43 min) Epoch 8/300 -- Iteration 60767 - Batch 6853/7702 - Train loss: 0.00212909  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.45 min) Epoch 8/300 -- Iteration 60844 - Batch 6930/7702 - Train loss: 0.00212871  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.47 min) Epoch 8/300 -- Iteration 60921 - Batch 7007/7702 - Train loss: 0.00212891  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.49 min) Epoch 8/300 -- Iteration 60998 - Batch 7084/7702 - Train loss: 0.00212871  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.51 min) Epoch 8/300 -- Iteration 61075 - Batch 7161/7702 - Train loss: 0.00212863  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.53 min) Epoch 8/300 -- Iteration 61152 - Batch 7238/7702 - Train loss: 0.00212865  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.55 min) Epoch 8/300 -- Iteration 61229 - Batch 7315/7702 - Train loss: 0.00212819  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.57 min) Epoch 8/300 -- Iteration 61306 - Batch 7392/7702 - Train loss: 0.00212820  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.59 min) Epoch 8/300 -- Iteration 61383 - Batch 7469/7702 - Train loss: 0.00212767  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.61 min) Epoch 8/300 -- Iteration 61460 - Batch 7546/7702 - Train loss: 0.00212733  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.63 min) Epoch 8/300 -- Iteration 61537 - Batch 7623/7702 - Train loss: 0.00212727  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.65 min) Epoch 8/300 -- Iteration 61614 - Batch 7700/7702 - Train loss: 0.00212674  - Train acc: -0.0000 - Val loss: 0.00051984\n",
      "(16.65 min) Epoch 8/300 -- Iteration 61616 - Batch 7701/7702 - Train loss: 0.00212679  - Train acc: -0.0000 - Val loss: 0.00052028 - Val acc: -0.0000\n",
      "(16.68 min) Epoch 9/300 -- Iteration 61693 - Batch 77/7702 - Train loss: 0.00214312  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.70 min) Epoch 9/300 -- Iteration 61770 - Batch 154/7702 - Train loss: 0.00214066  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.72 min) Epoch 9/300 -- Iteration 61847 - Batch 231/7702 - Train loss: 0.00214021  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.74 min) Epoch 9/300 -- Iteration 61924 - Batch 308/7702 - Train loss: 0.00213691  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.76 min) Epoch 9/300 -- Iteration 62001 - Batch 385/7702 - Train loss: 0.00213482  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.78 min) Epoch 9/300 -- Iteration 62078 - Batch 462/7702 - Train loss: 0.00213188  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.80 min) Epoch 9/300 -- Iteration 62155 - Batch 539/7702 - Train loss: 0.00213215  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.82 min) Epoch 9/300 -- Iteration 62232 - Batch 616/7702 - Train loss: 0.00212648  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.84 min) Epoch 9/300 -- Iteration 62309 - Batch 693/7702 - Train loss: 0.00212659  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.86 min) Epoch 9/300 -- Iteration 62386 - Batch 770/7702 - Train loss: 0.00212579  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.88 min) Epoch 9/300 -- Iteration 62463 - Batch 847/7702 - Train loss: 0.00212056  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.90 min) Epoch 9/300 -- Iteration 62540 - Batch 924/7702 - Train loss: 0.00212154  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.92 min) Epoch 9/300 -- Iteration 62617 - Batch 1001/7702 - Train loss: 0.00212152  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.94 min) Epoch 9/300 -- Iteration 62694 - Batch 1078/7702 - Train loss: 0.00212029  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.96 min) Epoch 9/300 -- Iteration 62771 - Batch 1155/7702 - Train loss: 0.00211741  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(16.99 min) Epoch 9/300 -- Iteration 62848 - Batch 1232/7702 - Train loss: 0.00211548  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.01 min) Epoch 9/300 -- Iteration 62925 - Batch 1309/7702 - Train loss: 0.00211484  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.03 min) Epoch 9/300 -- Iteration 63002 - Batch 1386/7702 - Train loss: 0.00211521  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.05 min) Epoch 9/300 -- Iteration 63079 - Batch 1463/7702 - Train loss: 0.00211740  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.07 min) Epoch 9/300 -- Iteration 63156 - Batch 1540/7702 - Train loss: 0.00211654  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.09 min) Epoch 9/300 -- Iteration 63233 - Batch 1617/7702 - Train loss: 0.00211758  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.11 min) Epoch 9/300 -- Iteration 63310 - Batch 1694/7702 - Train loss: 0.00211669  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.13 min) Epoch 9/300 -- Iteration 63387 - Batch 1771/7702 - Train loss: 0.00211598  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.15 min) Epoch 9/300 -- Iteration 63464 - Batch 1848/7702 - Train loss: 0.00211509  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.17 min) Epoch 9/300 -- Iteration 63541 - Batch 1925/7702 - Train loss: 0.00211607  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.19 min) Epoch 9/300 -- Iteration 63618 - Batch 2002/7702 - Train loss: 0.00211551  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.21 min) Epoch 9/300 -- Iteration 63695 - Batch 2079/7702 - Train loss: 0.00211551  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.23 min) Epoch 9/300 -- Iteration 63772 - Batch 2156/7702 - Train loss: 0.00211495  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.25 min) Epoch 9/300 -- Iteration 63849 - Batch 2233/7702 - Train loss: 0.00211525  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.27 min) Epoch 9/300 -- Iteration 63926 - Batch 2310/7702 - Train loss: 0.00211566  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.29 min) Epoch 9/300 -- Iteration 64003 - Batch 2387/7702 - Train loss: 0.00211426  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.32 min) Epoch 9/300 -- Iteration 64080 - Batch 2464/7702 - Train loss: 0.00211521  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.34 min) Epoch 9/300 -- Iteration 64157 - Batch 2541/7702 - Train loss: 0.00211518  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.36 min) Epoch 9/300 -- Iteration 64234 - Batch 2618/7702 - Train loss: 0.00211535  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.38 min) Epoch 9/300 -- Iteration 64311 - Batch 2695/7702 - Train loss: 0.00211535  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.40 min) Epoch 9/300 -- Iteration 64388 - Batch 2772/7702 - Train loss: 0.00211502  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.42 min) Epoch 9/300 -- Iteration 64465 - Batch 2849/7702 - Train loss: 0.00211543  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.44 min) Epoch 9/300 -- Iteration 64542 - Batch 2926/7702 - Train loss: 0.00211483  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.46 min) Epoch 9/300 -- Iteration 64619 - Batch 3003/7702 - Train loss: 0.00211445  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.48 min) Epoch 9/300 -- Iteration 64696 - Batch 3080/7702 - Train loss: 0.00211513  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.50 min) Epoch 9/300 -- Iteration 64773 - Batch 3157/7702 - Train loss: 0.00211434  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.52 min) Epoch 9/300 -- Iteration 64850 - Batch 3234/7702 - Train loss: 0.00211415  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.54 min) Epoch 9/300 -- Iteration 64927 - Batch 3311/7702 - Train loss: 0.00211401  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.56 min) Epoch 9/300 -- Iteration 65004 - Batch 3388/7702 - Train loss: 0.00211540  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.58 min) Epoch 9/300 -- Iteration 65081 - Batch 3465/7702 - Train loss: 0.00211589  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.60 min) Epoch 9/300 -- Iteration 65158 - Batch 3542/7702 - Train loss: 0.00211536  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.63 min) Epoch 9/300 -- Iteration 65235 - Batch 3619/7702 - Train loss: 0.00211583  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.65 min) Epoch 9/300 -- Iteration 65312 - Batch 3696/7702 - Train loss: 0.00211536  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.67 min) Epoch 9/300 -- Iteration 65389 - Batch 3773/7702 - Train loss: 0.00211541  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.69 min) Epoch 9/300 -- Iteration 65466 - Batch 3850/7702 - Train loss: 0.00211525  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.71 min) Epoch 9/300 -- Iteration 65543 - Batch 3927/7702 - Train loss: 0.00211551  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.73 min) Epoch 9/300 -- Iteration 65620 - Batch 4004/7702 - Train loss: 0.00211504  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.75 min) Epoch 9/300 -- Iteration 65697 - Batch 4081/7702 - Train loss: 0.00211544  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.77 min) Epoch 9/300 -- Iteration 65774 - Batch 4158/7702 - Train loss: 0.00211455  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.79 min) Epoch 9/300 -- Iteration 65851 - Batch 4235/7702 - Train loss: 0.00211477  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.81 min) Epoch 9/300 -- Iteration 65928 - Batch 4312/7702 - Train loss: 0.00211441  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.83 min) Epoch 9/300 -- Iteration 66005 - Batch 4389/7702 - Train loss: 0.00211421  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.85 min) Epoch 9/300 -- Iteration 66082 - Batch 4466/7702 - Train loss: 0.00211428  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.87 min) Epoch 9/300 -- Iteration 66159 - Batch 4543/7702 - Train loss: 0.00211448  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.89 min) Epoch 9/300 -- Iteration 66236 - Batch 4620/7702 - Train loss: 0.00211413  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.91 min) Epoch 9/300 -- Iteration 66313 - Batch 4697/7702 - Train loss: 0.00211386  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.94 min) Epoch 9/300 -- Iteration 66390 - Batch 4774/7702 - Train loss: 0.00211318  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.96 min) Epoch 9/300 -- Iteration 66467 - Batch 4851/7702 - Train loss: 0.00211305  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(17.98 min) Epoch 9/300 -- Iteration 66544 - Batch 4928/7702 - Train loss: 0.00211305  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.00 min) Epoch 9/300 -- Iteration 66621 - Batch 5005/7702 - Train loss: 0.00211327  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.02 min) Epoch 9/300 -- Iteration 66698 - Batch 5082/7702 - Train loss: 0.00211404  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.04 min) Epoch 9/300 -- Iteration 66775 - Batch 5159/7702 - Train loss: 0.00211420  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.06 min) Epoch 9/300 -- Iteration 66852 - Batch 5236/7702 - Train loss: 0.00211466  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.08 min) Epoch 9/300 -- Iteration 66929 - Batch 5313/7702 - Train loss: 0.00211461  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.10 min) Epoch 9/300 -- Iteration 67006 - Batch 5390/7702 - Train loss: 0.00211442  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.12 min) Epoch 9/300 -- Iteration 67083 - Batch 5467/7702 - Train loss: 0.00211421  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.14 min) Epoch 9/300 -- Iteration 67160 - Batch 5544/7702 - Train loss: 0.00211448  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.16 min) Epoch 9/300 -- Iteration 67237 - Batch 5621/7702 - Train loss: 0.00211475  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.18 min) Epoch 9/300 -- Iteration 67314 - Batch 5698/7702 - Train loss: 0.00211429  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.20 min) Epoch 9/300 -- Iteration 67391 - Batch 5775/7702 - Train loss: 0.00211430  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.23 min) Epoch 9/300 -- Iteration 67468 - Batch 5852/7702 - Train loss: 0.00211383  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.25 min) Epoch 9/300 -- Iteration 67545 - Batch 5929/7702 - Train loss: 0.00211312  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.27 min) Epoch 9/300 -- Iteration 67622 - Batch 6006/7702 - Train loss: 0.00211295  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.29 min) Epoch 9/300 -- Iteration 67699 - Batch 6083/7702 - Train loss: 0.00211271  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.31 min) Epoch 9/300 -- Iteration 67776 - Batch 6160/7702 - Train loss: 0.00211247  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.33 min) Epoch 9/300 -- Iteration 67853 - Batch 6237/7702 - Train loss: 0.00211251  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.35 min) Epoch 9/300 -- Iteration 67930 - Batch 6314/7702 - Train loss: 0.00211246  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.37 min) Epoch 9/300 -- Iteration 68007 - Batch 6391/7702 - Train loss: 0.00211297  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.39 min) Epoch 9/300 -- Iteration 68084 - Batch 6468/7702 - Train loss: 0.00211298  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.41 min) Epoch 9/300 -- Iteration 68161 - Batch 6545/7702 - Train loss: 0.00211301  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.43 min) Epoch 9/300 -- Iteration 68238 - Batch 6622/7702 - Train loss: 0.00211283  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.45 min) Epoch 9/300 -- Iteration 68315 - Batch 6699/7702 - Train loss: 0.00211256  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.47 min) Epoch 9/300 -- Iteration 68392 - Batch 6776/7702 - Train loss: 0.00211266  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.49 min) Epoch 9/300 -- Iteration 68469 - Batch 6853/7702 - Train loss: 0.00211268  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.51 min) Epoch 9/300 -- Iteration 68546 - Batch 6930/7702 - Train loss: 0.00211257  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.53 min) Epoch 9/300 -- Iteration 68623 - Batch 7007/7702 - Train loss: 0.00211203  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.56 min) Epoch 9/300 -- Iteration 68700 - Batch 7084/7702 - Train loss: 0.00211196  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.58 min) Epoch 9/300 -- Iteration 68777 - Batch 7161/7702 - Train loss: 0.00211158  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.60 min) Epoch 9/300 -- Iteration 68854 - Batch 7238/7702 - Train loss: 0.00211151  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.62 min) Epoch 9/300 -- Iteration 68931 - Batch 7315/7702 - Train loss: 0.00211122  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.64 min) Epoch 9/300 -- Iteration 69008 - Batch 7392/7702 - Train loss: 0.00211109  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.66 min) Epoch 9/300 -- Iteration 69085 - Batch 7469/7702 - Train loss: 0.00211106  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.68 min) Epoch 9/300 -- Iteration 69162 - Batch 7546/7702 - Train loss: 0.00211099  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.70 min) Epoch 9/300 -- Iteration 69239 - Batch 7623/7702 - Train loss: 0.00211078  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.72 min) Epoch 9/300 -- Iteration 69316 - Batch 7700/7702 - Train loss: 0.00211078  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(18.72 min) Epoch 9/300 -- Iteration 69318 - Batch 7701/7702 - Train loss: 0.00211078  - Train acc: -0.0000 - Val loss: 0.00054640 - Val acc: -0.0000\n",
      "(18.74 min) Epoch 10/300 -- Iteration 69395 - Batch 77/7702 - Train loss: 0.00210474  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.76 min) Epoch 10/300 -- Iteration 69472 - Batch 154/7702 - Train loss: 0.00210852  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.79 min) Epoch 10/300 -- Iteration 69549 - Batch 231/7702 - Train loss: 0.00210474  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.81 min) Epoch 10/300 -- Iteration 69626 - Batch 308/7702 - Train loss: 0.00211880  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.83 min) Epoch 10/300 -- Iteration 69703 - Batch 385/7702 - Train loss: 0.00211576  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.85 min) Epoch 10/300 -- Iteration 69780 - Batch 462/7702 - Train loss: 0.00210974  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.87 min) Epoch 10/300 -- Iteration 69857 - Batch 539/7702 - Train loss: 0.00210702  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.89 min) Epoch 10/300 -- Iteration 69934 - Batch 616/7702 - Train loss: 0.00210681  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.91 min) Epoch 10/300 -- Iteration 70011 - Batch 693/7702 - Train loss: 0.00210286  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.93 min) Epoch 10/300 -- Iteration 70088 - Batch 770/7702 - Train loss: 0.00211306  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.95 min) Epoch 10/300 -- Iteration 70165 - Batch 847/7702 - Train loss: 0.00211336  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.97 min) Epoch 10/300 -- Iteration 70242 - Batch 924/7702 - Train loss: 0.00211333  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(18.99 min) Epoch 10/300 -- Iteration 70319 - Batch 1001/7702 - Train loss: 0.00210936  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.01 min) Epoch 10/300 -- Iteration 70396 - Batch 1078/7702 - Train loss: 0.00210870  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.03 min) Epoch 10/300 -- Iteration 70473 - Batch 1155/7702 - Train loss: 0.00210982  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.05 min) Epoch 10/300 -- Iteration 70550 - Batch 1232/7702 - Train loss: 0.00211014  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.07 min) Epoch 10/300 -- Iteration 70627 - Batch 1309/7702 - Train loss: 0.00211098  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.09 min) Epoch 10/300 -- Iteration 70704 - Batch 1386/7702 - Train loss: 0.00211110  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.12 min) Epoch 10/300 -- Iteration 70781 - Batch 1463/7702 - Train loss: 0.00211191  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.14 min) Epoch 10/300 -- Iteration 70858 - Batch 1540/7702 - Train loss: 0.00211231  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.16 min) Epoch 10/300 -- Iteration 70935 - Batch 1617/7702 - Train loss: 0.00211224  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.18 min) Epoch 10/300 -- Iteration 71012 - Batch 1694/7702 - Train loss: 0.00210842  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.20 min) Epoch 10/300 -- Iteration 71089 - Batch 1771/7702 - Train loss: 0.00210923  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.22 min) Epoch 10/300 -- Iteration 71166 - Batch 1848/7702 - Train loss: 0.00211156  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.24 min) Epoch 10/300 -- Iteration 71243 - Batch 1925/7702 - Train loss: 0.00211140  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.26 min) Epoch 10/300 -- Iteration 71320 - Batch 2002/7702 - Train loss: 0.00210979  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.28 min) Epoch 10/300 -- Iteration 71397 - Batch 2079/7702 - Train loss: 0.00210919  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.30 min) Epoch 10/300 -- Iteration 71474 - Batch 2156/7702 - Train loss: 0.00210960  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.32 min) Epoch 10/300 -- Iteration 71551 - Batch 2233/7702 - Train loss: 0.00210918  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.34 min) Epoch 10/300 -- Iteration 71628 - Batch 2310/7702 - Train loss: 0.00210723  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.36 min) Epoch 10/300 -- Iteration 71705 - Batch 2387/7702 - Train loss: 0.00210699  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.38 min) Epoch 10/300 -- Iteration 71782 - Batch 2464/7702 - Train loss: 0.00210675  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.40 min) Epoch 10/300 -- Iteration 71859 - Batch 2541/7702 - Train loss: 0.00210530  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.43 min) Epoch 10/300 -- Iteration 71936 - Batch 2618/7702 - Train loss: 0.00210484  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.45 min) Epoch 10/300 -- Iteration 72013 - Batch 2695/7702 - Train loss: 0.00210483  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.47 min) Epoch 10/300 -- Iteration 72090 - Batch 2772/7702 - Train loss: 0.00210502  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.49 min) Epoch 10/300 -- Iteration 72167 - Batch 2849/7702 - Train loss: 0.00210518  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.51 min) Epoch 10/300 -- Iteration 72244 - Batch 2926/7702 - Train loss: 0.00210515  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.53 min) Epoch 10/300 -- Iteration 72321 - Batch 3003/7702 - Train loss: 0.00210399  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.55 min) Epoch 10/300 -- Iteration 72398 - Batch 3080/7702 - Train loss: 0.00210377  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.57 min) Epoch 10/300 -- Iteration 72475 - Batch 3157/7702 - Train loss: 0.00210344  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.59 min) Epoch 10/300 -- Iteration 72552 - Batch 3234/7702 - Train loss: 0.00210332  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.61 min) Epoch 10/300 -- Iteration 72629 - Batch 3311/7702 - Train loss: 0.00210250  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.63 min) Epoch 10/300 -- Iteration 72706 - Batch 3388/7702 - Train loss: 0.00210265  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.65 min) Epoch 10/300 -- Iteration 72783 - Batch 3465/7702 - Train loss: 0.00210230  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.67 min) Epoch 10/300 -- Iteration 72860 - Batch 3542/7702 - Train loss: 0.00210255  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.70 min) Epoch 10/300 -- Iteration 72937 - Batch 3619/7702 - Train loss: 0.00210202  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.72 min) Epoch 10/300 -- Iteration 73014 - Batch 3696/7702 - Train loss: 0.00210117  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.74 min) Epoch 10/300 -- Iteration 73091 - Batch 3773/7702 - Train loss: 0.00210135  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.76 min) Epoch 10/300 -- Iteration 73168 - Batch 3850/7702 - Train loss: 0.00210111  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.78 min) Epoch 10/300 -- Iteration 73245 - Batch 3927/7702 - Train loss: 0.00210099  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.80 min) Epoch 10/300 -- Iteration 73322 - Batch 4004/7702 - Train loss: 0.00210066  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.82 min) Epoch 10/300 -- Iteration 73399 - Batch 4081/7702 - Train loss: 0.00210054  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.84 min) Epoch 10/300 -- Iteration 73476 - Batch 4158/7702 - Train loss: 0.00210010  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.86 min) Epoch 10/300 -- Iteration 73553 - Batch 4235/7702 - Train loss: 0.00210067  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.88 min) Epoch 10/300 -- Iteration 73630 - Batch 4312/7702 - Train loss: 0.00210032  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.90 min) Epoch 10/300 -- Iteration 73707 - Batch 4389/7702 - Train loss: 0.00209941  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.92 min) Epoch 10/300 -- Iteration 73784 - Batch 4466/7702 - Train loss: 0.00209973  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.94 min) Epoch 10/300 -- Iteration 73861 - Batch 4543/7702 - Train loss: 0.00209951  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.96 min) Epoch 10/300 -- Iteration 73938 - Batch 4620/7702 - Train loss: 0.00209873  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(19.98 min) Epoch 10/300 -- Iteration 74015 - Batch 4697/7702 - Train loss: 0.00209865  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.00 min) Epoch 10/300 -- Iteration 74092 - Batch 4774/7702 - Train loss: 0.00209852  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.03 min) Epoch 10/300 -- Iteration 74169 - Batch 4851/7702 - Train loss: 0.00209819  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.05 min) Epoch 10/300 -- Iteration 74246 - Batch 4928/7702 - Train loss: 0.00209792  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.07 min) Epoch 10/300 -- Iteration 74323 - Batch 5005/7702 - Train loss: 0.00209830  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.09 min) Epoch 10/300 -- Iteration 74400 - Batch 5082/7702 - Train loss: 0.00209821  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.11 min) Epoch 10/300 -- Iteration 74477 - Batch 5159/7702 - Train loss: 0.00209840  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.13 min) Epoch 10/300 -- Iteration 74554 - Batch 5236/7702 - Train loss: 0.00209839  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.15 min) Epoch 10/300 -- Iteration 74631 - Batch 5313/7702 - Train loss: 0.00209824  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.17 min) Epoch 10/300 -- Iteration 74708 - Batch 5390/7702 - Train loss: 0.00209846  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.19 min) Epoch 10/300 -- Iteration 74785 - Batch 5467/7702 - Train loss: 0.00209818  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.21 min) Epoch 10/300 -- Iteration 74862 - Batch 5544/7702 - Train loss: 0.00209810  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.23 min) Epoch 10/300 -- Iteration 74939 - Batch 5621/7702 - Train loss: 0.00209798  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.25 min) Epoch 10/300 -- Iteration 75016 - Batch 5698/7702 - Train loss: 0.00209791  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.27 min) Epoch 10/300 -- Iteration 75093 - Batch 5775/7702 - Train loss: 0.00209804  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.29 min) Epoch 10/300 -- Iteration 75170 - Batch 5852/7702 - Train loss: 0.00209804  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.32 min) Epoch 10/300 -- Iteration 75247 - Batch 5929/7702 - Train loss: 0.00209797  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.34 min) Epoch 10/300 -- Iteration 75324 - Batch 6006/7702 - Train loss: 0.00209785  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.36 min) Epoch 10/300 -- Iteration 75401 - Batch 6083/7702 - Train loss: 0.00209769  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.38 min) Epoch 10/300 -- Iteration 75478 - Batch 6160/7702 - Train loss: 0.00209780  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.40 min) Epoch 10/300 -- Iteration 75555 - Batch 6237/7702 - Train loss: 0.00209696  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.42 min) Epoch 10/300 -- Iteration 75632 - Batch 6314/7702 - Train loss: 0.00209713  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.44 min) Epoch 10/300 -- Iteration 75709 - Batch 6391/7702 - Train loss: 0.00209686  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.46 min) Epoch 10/300 -- Iteration 75786 - Batch 6468/7702 - Train loss: 0.00209710  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.48 min) Epoch 10/300 -- Iteration 75863 - Batch 6545/7702 - Train loss: 0.00209675  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.50 min) Epoch 10/300 -- Iteration 75940 - Batch 6622/7702 - Train loss: 0.00209680  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.52 min) Epoch 10/300 -- Iteration 76017 - Batch 6699/7702 - Train loss: 0.00209667  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.54 min) Epoch 10/300 -- Iteration 76094 - Batch 6776/7702 - Train loss: 0.00209636  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.56 min) Epoch 10/300 -- Iteration 76171 - Batch 6853/7702 - Train loss: 0.00209645  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.58 min) Epoch 10/300 -- Iteration 76248 - Batch 6930/7702 - Train loss: 0.00209666  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.60 min) Epoch 10/300 -- Iteration 76325 - Batch 7007/7702 - Train loss: 0.00209631  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.63 min) Epoch 10/300 -- Iteration 76402 - Batch 7084/7702 - Train loss: 0.00209645  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.65 min) Epoch 10/300 -- Iteration 76479 - Batch 7161/7702 - Train loss: 0.00209608  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.67 min) Epoch 10/300 -- Iteration 76556 - Batch 7238/7702 - Train loss: 0.00209646  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.69 min) Epoch 10/300 -- Iteration 76633 - Batch 7315/7702 - Train loss: 0.00209626  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.71 min) Epoch 10/300 -- Iteration 76710 - Batch 7392/7702 - Train loss: 0.00209637  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.73 min) Epoch 10/300 -- Iteration 76787 - Batch 7469/7702 - Train loss: 0.00209624  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.75 min) Epoch 10/300 -- Iteration 76864 - Batch 7546/7702 - Train loss: 0.00209561  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.77 min) Epoch 10/300 -- Iteration 76941 - Batch 7623/7702 - Train loss: 0.00209543  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.79 min) Epoch 10/300 -- Iteration 77018 - Batch 7700/7702 - Train loss: 0.00209518  - Train acc: -0.0000 - Val loss: 0.00054640\n",
      "(20.79 min) Epoch 10/300 -- Iteration 77020 - Batch 7701/7702 - Train loss: 0.00209520  - Train acc: -0.0000 - Val loss: 0.00058589 - Val acc: -0.0000\n",
      "(20.81 min) Epoch 11/300 -- Iteration 77097 - Batch 77/7702 - Train loss: 0.00210699  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(20.83 min) Epoch 11/300 -- Iteration 77174 - Batch 154/7702 - Train loss: 0.00208785  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(20.86 min) Epoch 11/300 -- Iteration 77251 - Batch 231/7702 - Train loss: 0.00209837  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(20.88 min) Epoch 11/300 -- Iteration 77328 - Batch 308/7702 - Train loss: 0.00209928  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(20.90 min) Epoch 11/300 -- Iteration 77405 - Batch 385/7702 - Train loss: 0.00209835  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(20.92 min) Epoch 11/300 -- Iteration 77482 - Batch 462/7702 - Train loss: 0.00210079  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(20.94 min) Epoch 11/300 -- Iteration 77559 - Batch 539/7702 - Train loss: 0.00209859  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(20.96 min) Epoch 11/300 -- Iteration 77636 - Batch 616/7702 - Train loss: 0.00209845  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(20.98 min) Epoch 11/300 -- Iteration 77713 - Batch 693/7702 - Train loss: 0.00209268  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.00 min) Epoch 11/300 -- Iteration 77790 - Batch 770/7702 - Train loss: 0.00209253  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.02 min) Epoch 11/300 -- Iteration 77867 - Batch 847/7702 - Train loss: 0.00209211  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.04 min) Epoch 11/300 -- Iteration 77944 - Batch 924/7702 - Train loss: 0.00209063  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.06 min) Epoch 11/300 -- Iteration 78021 - Batch 1001/7702 - Train loss: 0.00209084  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.08 min) Epoch 11/300 -- Iteration 78098 - Batch 1078/7702 - Train loss: 0.00209213  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.10 min) Epoch 11/300 -- Iteration 78175 - Batch 1155/7702 - Train loss: 0.00208935  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.12 min) Epoch 11/300 -- Iteration 78252 - Batch 1232/7702 - Train loss: 0.00209282  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.15 min) Epoch 11/300 -- Iteration 78329 - Batch 1309/7702 - Train loss: 0.00209439  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.17 min) Epoch 11/300 -- Iteration 78406 - Batch 1386/7702 - Train loss: 0.00209518  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.19 min) Epoch 11/300 -- Iteration 78483 - Batch 1463/7702 - Train loss: 0.00209453  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.21 min) Epoch 11/300 -- Iteration 78560 - Batch 1540/7702 - Train loss: 0.00209391  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.23 min) Epoch 11/300 -- Iteration 78637 - Batch 1617/7702 - Train loss: 0.00209170  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.25 min) Epoch 11/300 -- Iteration 78714 - Batch 1694/7702 - Train loss: 0.00209211  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.27 min) Epoch 11/300 -- Iteration 78791 - Batch 1771/7702 - Train loss: 0.00209347  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.29 min) Epoch 11/300 -- Iteration 78868 - Batch 1848/7702 - Train loss: 0.00209294  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.31 min) Epoch 11/300 -- Iteration 78945 - Batch 1925/7702 - Train loss: 0.00209253  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.33 min) Epoch 11/300 -- Iteration 79022 - Batch 2002/7702 - Train loss: 0.00209344  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.35 min) Epoch 11/300 -- Iteration 79099 - Batch 2079/7702 - Train loss: 0.00209224  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.37 min) Epoch 11/300 -- Iteration 79176 - Batch 2156/7702 - Train loss: 0.00209272  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.39 min) Epoch 11/300 -- Iteration 79253 - Batch 2233/7702 - Train loss: 0.00209207  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.41 min) Epoch 11/300 -- Iteration 79330 - Batch 2310/7702 - Train loss: 0.00209266  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.43 min) Epoch 11/300 -- Iteration 79407 - Batch 2387/7702 - Train loss: 0.00209292  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.45 min) Epoch 11/300 -- Iteration 79484 - Batch 2464/7702 - Train loss: 0.00209336  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.48 min) Epoch 11/300 -- Iteration 79561 - Batch 2541/7702 - Train loss: 0.00209213  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.50 min) Epoch 11/300 -- Iteration 79638 - Batch 2618/7702 - Train loss: 0.00209132  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.52 min) Epoch 11/300 -- Iteration 79715 - Batch 2695/7702 - Train loss: 0.00209240  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.54 min) Epoch 11/300 -- Iteration 79792 - Batch 2772/7702 - Train loss: 0.00209187  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.56 min) Epoch 11/300 -- Iteration 79869 - Batch 2849/7702 - Train loss: 0.00209211  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.58 min) Epoch 11/300 -- Iteration 79946 - Batch 2926/7702 - Train loss: 0.00209075  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.60 min) Epoch 11/300 -- Iteration 80023 - Batch 3003/7702 - Train loss: 0.00209137  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.62 min) Epoch 11/300 -- Iteration 80100 - Batch 3080/7702 - Train loss: 0.00209161  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.64 min) Epoch 11/300 -- Iteration 80177 - Batch 3157/7702 - Train loss: 0.00209079  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.66 min) Epoch 11/300 -- Iteration 80254 - Batch 3234/7702 - Train loss: 0.00209051  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.68 min) Epoch 11/300 -- Iteration 80331 - Batch 3311/7702 - Train loss: 0.00209025  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.70 min) Epoch 11/300 -- Iteration 80408 - Batch 3388/7702 - Train loss: 0.00209006  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.72 min) Epoch 11/300 -- Iteration 80485 - Batch 3465/7702 - Train loss: 0.00208964  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.74 min) Epoch 11/300 -- Iteration 80562 - Batch 3542/7702 - Train loss: 0.00209022  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.77 min) Epoch 11/300 -- Iteration 80639 - Batch 3619/7702 - Train loss: 0.00209032  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.79 min) Epoch 11/300 -- Iteration 80716 - Batch 3696/7702 - Train loss: 0.00209012  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.81 min) Epoch 11/300 -- Iteration 80793 - Batch 3773/7702 - Train loss: 0.00209000  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.83 min) Epoch 11/300 -- Iteration 80870 - Batch 3850/7702 - Train loss: 0.00208908  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.85 min) Epoch 11/300 -- Iteration 80947 - Batch 3927/7702 - Train loss: 0.00208963  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.87 min) Epoch 11/300 -- Iteration 81024 - Batch 4004/7702 - Train loss: 0.00208865  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.89 min) Epoch 11/300 -- Iteration 81101 - Batch 4081/7702 - Train loss: 0.00208942  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.91 min) Epoch 11/300 -- Iteration 81178 - Batch 4158/7702 - Train loss: 0.00208923  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.93 min) Epoch 11/300 -- Iteration 81255 - Batch 4235/7702 - Train loss: 0.00208898  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.95 min) Epoch 11/300 -- Iteration 81332 - Batch 4312/7702 - Train loss: 0.00208862  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.97 min) Epoch 11/300 -- Iteration 81409 - Batch 4389/7702 - Train loss: 0.00208802  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(21.99 min) Epoch 11/300 -- Iteration 81486 - Batch 4466/7702 - Train loss: 0.00208816  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.01 min) Epoch 11/300 -- Iteration 81563 - Batch 4543/7702 - Train loss: 0.00208810  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.03 min) Epoch 11/300 -- Iteration 81640 - Batch 4620/7702 - Train loss: 0.00208857  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.05 min) Epoch 11/300 -- Iteration 81717 - Batch 4697/7702 - Train loss: 0.00208855  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.08 min) Epoch 11/300 -- Iteration 81794 - Batch 4774/7702 - Train loss: 0.00208783  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.10 min) Epoch 11/300 -- Iteration 81871 - Batch 4851/7702 - Train loss: 0.00208768  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.12 min) Epoch 11/300 -- Iteration 81948 - Batch 4928/7702 - Train loss: 0.00208728  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.14 min) Epoch 11/300 -- Iteration 82025 - Batch 5005/7702 - Train loss: 0.00208683  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.16 min) Epoch 11/300 -- Iteration 82102 - Batch 5082/7702 - Train loss: 0.00208684  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.18 min) Epoch 11/300 -- Iteration 82179 - Batch 5159/7702 - Train loss: 0.00208668  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.20 min) Epoch 11/300 -- Iteration 82256 - Batch 5236/7702 - Train loss: 0.00208636  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.22 min) Epoch 11/300 -- Iteration 82333 - Batch 5313/7702 - Train loss: 0.00208606  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.24 min) Epoch 11/300 -- Iteration 82410 - Batch 5390/7702 - Train loss: 0.00208574  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.26 min) Epoch 11/300 -- Iteration 82487 - Batch 5467/7702 - Train loss: 0.00208512  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.28 min) Epoch 11/300 -- Iteration 82564 - Batch 5544/7702 - Train loss: 0.00208488  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.30 min) Epoch 11/300 -- Iteration 82641 - Batch 5621/7702 - Train loss: 0.00208480  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.32 min) Epoch 11/300 -- Iteration 82718 - Batch 5698/7702 - Train loss: 0.00208503  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.34 min) Epoch 11/300 -- Iteration 82795 - Batch 5775/7702 - Train loss: 0.00208520  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.37 min) Epoch 11/300 -- Iteration 82872 - Batch 5852/7702 - Train loss: 0.00208505  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.39 min) Epoch 11/300 -- Iteration 82949 - Batch 5929/7702 - Train loss: 0.00208534  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.41 min) Epoch 11/300 -- Iteration 83026 - Batch 6006/7702 - Train loss: 0.00208546  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.43 min) Epoch 11/300 -- Iteration 83103 - Batch 6083/7702 - Train loss: 0.00208548  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.45 min) Epoch 11/300 -- Iteration 83180 - Batch 6160/7702 - Train loss: 0.00208508  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.47 min) Epoch 11/300 -- Iteration 83257 - Batch 6237/7702 - Train loss: 0.00208513  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.49 min) Epoch 11/300 -- Iteration 83334 - Batch 6314/7702 - Train loss: 0.00208474  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.51 min) Epoch 11/300 -- Iteration 83411 - Batch 6391/7702 - Train loss: 0.00208449  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.53 min) Epoch 11/300 -- Iteration 83488 - Batch 6468/7702 - Train loss: 0.00208443  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.55 min) Epoch 11/300 -- Iteration 83565 - Batch 6545/7702 - Train loss: 0.00208424  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.57 min) Epoch 11/300 -- Iteration 83642 - Batch 6622/7702 - Train loss: 0.00208394  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.59 min) Epoch 11/300 -- Iteration 83719 - Batch 6699/7702 - Train loss: 0.00208357  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.61 min) Epoch 11/300 -- Iteration 83796 - Batch 6776/7702 - Train loss: 0.00208344  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.63 min) Epoch 11/300 -- Iteration 83873 - Batch 6853/7702 - Train loss: 0.00208331  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.65 min) Epoch 11/300 -- Iteration 83950 - Batch 6930/7702 - Train loss: 0.00208299  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.68 min) Epoch 11/300 -- Iteration 84027 - Batch 7007/7702 - Train loss: 0.00208284  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.70 min) Epoch 11/300 -- Iteration 84104 - Batch 7084/7702 - Train loss: 0.00208294  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.72 min) Epoch 11/300 -- Iteration 84181 - Batch 7161/7702 - Train loss: 0.00208278  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.74 min) Epoch 11/300 -- Iteration 84258 - Batch 7238/7702 - Train loss: 0.00208271  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.76 min) Epoch 11/300 -- Iteration 84335 - Batch 7315/7702 - Train loss: 0.00208267  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.78 min) Epoch 11/300 -- Iteration 84412 - Batch 7392/7702 - Train loss: 0.00208236  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.80 min) Epoch 11/300 -- Iteration 84489 - Batch 7469/7702 - Train loss: 0.00208197  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.82 min) Epoch 11/300 -- Iteration 84566 - Batch 7546/7702 - Train loss: 0.00208189  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.84 min) Epoch 11/300 -- Iteration 84643 - Batch 7623/7702 - Train loss: 0.00208158  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.86 min) Epoch 11/300 -- Iteration 84720 - Batch 7700/7702 - Train loss: 0.00208130  - Train acc: -0.0000 - Val loss: 0.00058589\n",
      "(22.86 min) Epoch 11/300 -- Iteration 84722 - Batch 7701/7702 - Train loss: 0.00208134  - Train acc: -0.0000 - Val loss: 0.00051725 - Val acc: -0.0000\n",
      "(22.88 min) Epoch 12/300 -- Iteration 84799 - Batch 77/7702 - Train loss: 0.00203497  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(22.91 min) Epoch 12/300 -- Iteration 84876 - Batch 154/7702 - Train loss: 0.00205874  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(22.93 min) Epoch 12/300 -- Iteration 84953 - Batch 231/7702 - Train loss: 0.00206489  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(22.95 min) Epoch 12/300 -- Iteration 85030 - Batch 308/7702 - Train loss: 0.00207118  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(22.97 min) Epoch 12/300 -- Iteration 85107 - Batch 385/7702 - Train loss: 0.00207216  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(22.99 min) Epoch 12/300 -- Iteration 85184 - Batch 462/7702 - Train loss: 0.00207729  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.01 min) Epoch 12/300 -- Iteration 85261 - Batch 539/7702 - Train loss: 0.00207450  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.03 min) Epoch 12/300 -- Iteration 85338 - Batch 616/7702 - Train loss: 0.00207388  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.05 min) Epoch 12/300 -- Iteration 85415 - Batch 693/7702 - Train loss: 0.00207220  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.07 min) Epoch 12/300 -- Iteration 85492 - Batch 770/7702 - Train loss: 0.00207057  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.09 min) Epoch 12/300 -- Iteration 85569 - Batch 847/7702 - Train loss: 0.00206951  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.11 min) Epoch 12/300 -- Iteration 85646 - Batch 924/7702 - Train loss: 0.00207295  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.13 min) Epoch 12/300 -- Iteration 85723 - Batch 1001/7702 - Train loss: 0.00207103  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.15 min) Epoch 12/300 -- Iteration 85800 - Batch 1078/7702 - Train loss: 0.00207183  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.17 min) Epoch 12/300 -- Iteration 85877 - Batch 1155/7702 - Train loss: 0.00207167  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.19 min) Epoch 12/300 -- Iteration 85954 - Batch 1232/7702 - Train loss: 0.00206929  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.21 min) Epoch 12/300 -- Iteration 86031 - Batch 1309/7702 - Train loss: 0.00206873  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.24 min) Epoch 12/300 -- Iteration 86108 - Batch 1386/7702 - Train loss: 0.00206732  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.26 min) Epoch 12/300 -- Iteration 86185 - Batch 1463/7702 - Train loss: 0.00206756  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.28 min) Epoch 12/300 -- Iteration 86262 - Batch 1540/7702 - Train loss: 0.00206665  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.30 min) Epoch 12/300 -- Iteration 86339 - Batch 1617/7702 - Train loss: 0.00206650  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.32 min) Epoch 12/300 -- Iteration 86416 - Batch 1694/7702 - Train loss: 0.00206618  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.34 min) Epoch 12/300 -- Iteration 86493 - Batch 1771/7702 - Train loss: 0.00206866  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.36 min) Epoch 12/300 -- Iteration 86570 - Batch 1848/7702 - Train loss: 0.00206789  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.38 min) Epoch 12/300 -- Iteration 86647 - Batch 1925/7702 - Train loss: 0.00206818  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.40 min) Epoch 12/300 -- Iteration 86724 - Batch 2002/7702 - Train loss: 0.00206940  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.42 min) Epoch 12/300 -- Iteration 86801 - Batch 2079/7702 - Train loss: 0.00207003  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.44 min) Epoch 12/300 -- Iteration 86878 - Batch 2156/7702 - Train loss: 0.00206843  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.46 min) Epoch 12/300 -- Iteration 86955 - Batch 2233/7702 - Train loss: 0.00206815  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.48 min) Epoch 12/300 -- Iteration 87032 - Batch 2310/7702 - Train loss: 0.00206885  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.50 min) Epoch 12/300 -- Iteration 87109 - Batch 2387/7702 - Train loss: 0.00206892  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.52 min) Epoch 12/300 -- Iteration 87186 - Batch 2464/7702 - Train loss: 0.00206927  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.54 min) Epoch 12/300 -- Iteration 87263 - Batch 2541/7702 - Train loss: 0.00206901  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.57 min) Epoch 12/300 -- Iteration 87340 - Batch 2618/7702 - Train loss: 0.00206832  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.59 min) Epoch 12/300 -- Iteration 87417 - Batch 2695/7702 - Train loss: 0.00206771  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.61 min) Epoch 12/300 -- Iteration 87494 - Batch 2772/7702 - Train loss: 0.00206789  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.63 min) Epoch 12/300 -- Iteration 87571 - Batch 2849/7702 - Train loss: 0.00206782  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.65 min) Epoch 12/300 -- Iteration 87648 - Batch 2926/7702 - Train loss: 0.00206780  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.67 min) Epoch 12/300 -- Iteration 87725 - Batch 3003/7702 - Train loss: 0.00206786  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.69 min) Epoch 12/300 -- Iteration 87802 - Batch 3080/7702 - Train loss: 0.00206812  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.71 min) Epoch 12/300 -- Iteration 87879 - Batch 3157/7702 - Train loss: 0.00206806  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.73 min) Epoch 12/300 -- Iteration 87956 - Batch 3234/7702 - Train loss: 0.00206728  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.75 min) Epoch 12/300 -- Iteration 88033 - Batch 3311/7702 - Train loss: 0.00206686  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.77 min) Epoch 12/300 -- Iteration 88110 - Batch 3388/7702 - Train loss: 0.00206720  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.79 min) Epoch 12/300 -- Iteration 88187 - Batch 3465/7702 - Train loss: 0.00206726  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.81 min) Epoch 12/300 -- Iteration 88264 - Batch 3542/7702 - Train loss: 0.00206792  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.83 min) Epoch 12/300 -- Iteration 88341 - Batch 3619/7702 - Train loss: 0.00206792  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.85 min) Epoch 12/300 -- Iteration 88418 - Batch 3696/7702 - Train loss: 0.00206794  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.88 min) Epoch 12/300 -- Iteration 88495 - Batch 3773/7702 - Train loss: 0.00206709  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.90 min) Epoch 12/300 -- Iteration 88572 - Batch 3850/7702 - Train loss: 0.00206680  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.92 min) Epoch 12/300 -- Iteration 88649 - Batch 3927/7702 - Train loss: 0.00206633  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.94 min) Epoch 12/300 -- Iteration 88726 - Batch 4004/7702 - Train loss: 0.00206624  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.96 min) Epoch 12/300 -- Iteration 88803 - Batch 4081/7702 - Train loss: 0.00206669  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(23.98 min) Epoch 12/300 -- Iteration 88880 - Batch 4158/7702 - Train loss: 0.00206737  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.00 min) Epoch 12/300 -- Iteration 88957 - Batch 4235/7702 - Train loss: 0.00206745  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.02 min) Epoch 12/300 -- Iteration 89034 - Batch 4312/7702 - Train loss: 0.00206746  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.04 min) Epoch 12/300 -- Iteration 89111 - Batch 4389/7702 - Train loss: 0.00206738  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.06 min) Epoch 12/300 -- Iteration 89188 - Batch 4466/7702 - Train loss: 0.00206724  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.08 min) Epoch 12/300 -- Iteration 89265 - Batch 4543/7702 - Train loss: 0.00206711  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.10 min) Epoch 12/300 -- Iteration 89342 - Batch 4620/7702 - Train loss: 0.00206776  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.12 min) Epoch 12/300 -- Iteration 89419 - Batch 4697/7702 - Train loss: 0.00206820  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.15 min) Epoch 12/300 -- Iteration 89496 - Batch 4774/7702 - Train loss: 0.00206787  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.17 min) Epoch 12/300 -- Iteration 89573 - Batch 4851/7702 - Train loss: 0.00206784  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.19 min) Epoch 12/300 -- Iteration 89650 - Batch 4928/7702 - Train loss: 0.00206813  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.21 min) Epoch 12/300 -- Iteration 89727 - Batch 5005/7702 - Train loss: 0.00206828  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.23 min) Epoch 12/300 -- Iteration 89804 - Batch 5082/7702 - Train loss: 0.00206836  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.25 min) Epoch 12/300 -- Iteration 89881 - Batch 5159/7702 - Train loss: 0.00206801  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.27 min) Epoch 12/300 -- Iteration 89958 - Batch 5236/7702 - Train loss: 0.00206834  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.29 min) Epoch 12/300 -- Iteration 90035 - Batch 5313/7702 - Train loss: 0.00206848  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.31 min) Epoch 12/300 -- Iteration 90112 - Batch 5390/7702 - Train loss: 0.00206859  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.33 min) Epoch 12/300 -- Iteration 90189 - Batch 5467/7702 - Train loss: 0.00206824  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.35 min) Epoch 12/300 -- Iteration 90266 - Batch 5544/7702 - Train loss: 0.00206808  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.37 min) Epoch 12/300 -- Iteration 90343 - Batch 5621/7702 - Train loss: 0.00206806  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.39 min) Epoch 12/300 -- Iteration 90420 - Batch 5698/7702 - Train loss: 0.00206844  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.41 min) Epoch 12/300 -- Iteration 90497 - Batch 5775/7702 - Train loss: 0.00206866  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.44 min) Epoch 12/300 -- Iteration 90574 - Batch 5852/7702 - Train loss: 0.00206912  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.46 min) Epoch 12/300 -- Iteration 90651 - Batch 5929/7702 - Train loss: 0.00206885  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.48 min) Epoch 12/300 -- Iteration 90728 - Batch 6006/7702 - Train loss: 0.00206858  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.50 min) Epoch 12/300 -- Iteration 90805 - Batch 6083/7702 - Train loss: 0.00206870  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.52 min) Epoch 12/300 -- Iteration 90882 - Batch 6160/7702 - Train loss: 0.00206871  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.54 min) Epoch 12/300 -- Iteration 90959 - Batch 6237/7702 - Train loss: 0.00206889  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.56 min) Epoch 12/300 -- Iteration 91036 - Batch 6314/7702 - Train loss: 0.00206834  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.58 min) Epoch 12/300 -- Iteration 91113 - Batch 6391/7702 - Train loss: 0.00206818  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.60 min) Epoch 12/300 -- Iteration 91190 - Batch 6468/7702 - Train loss: 0.00206841  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.62 min) Epoch 12/300 -- Iteration 91267 - Batch 6545/7702 - Train loss: 0.00206820  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.64 min) Epoch 12/300 -- Iteration 91344 - Batch 6622/7702 - Train loss: 0.00206865  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.66 min) Epoch 12/300 -- Iteration 91421 - Batch 6699/7702 - Train loss: 0.00206841  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.68 min) Epoch 12/300 -- Iteration 91498 - Batch 6776/7702 - Train loss: 0.00206827  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.70 min) Epoch 12/300 -- Iteration 91575 - Batch 6853/7702 - Train loss: 0.00206834  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.72 min) Epoch 12/300 -- Iteration 91652 - Batch 6930/7702 - Train loss: 0.00206793  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.75 min) Epoch 12/300 -- Iteration 91729 - Batch 7007/7702 - Train loss: 0.00206740  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.77 min) Epoch 12/300 -- Iteration 91806 - Batch 7084/7702 - Train loss: 0.00206749  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.79 min) Epoch 12/300 -- Iteration 91883 - Batch 7161/7702 - Train loss: 0.00206758  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.81 min) Epoch 12/300 -- Iteration 91960 - Batch 7238/7702 - Train loss: 0.00206772  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.83 min) Epoch 12/300 -- Iteration 92037 - Batch 7315/7702 - Train loss: 0.00206757  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.85 min) Epoch 12/300 -- Iteration 92114 - Batch 7392/7702 - Train loss: 0.00206768  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.87 min) Epoch 12/300 -- Iteration 92191 - Batch 7469/7702 - Train loss: 0.00206765  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.89 min) Epoch 12/300 -- Iteration 92268 - Batch 7546/7702 - Train loss: 0.00206729  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.91 min) Epoch 12/300 -- Iteration 92345 - Batch 7623/7702 - Train loss: 0.00206703  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.93 min) Epoch 12/300 -- Iteration 92422 - Batch 7700/7702 - Train loss: 0.00206773  - Train acc: -0.0000 - Val loss: 0.00051725\n",
      "(24.93 min) Epoch 12/300 -- Iteration 92424 - Batch 7701/7702 - Train loss: 0.00206770  - Train acc: -0.0000 - Val loss: 0.00049462 - Val acc: -0.0000\n",
      "(24.96 min) Epoch 13/300 -- Iteration 92501 - Batch 77/7702 - Train loss: 0.00209438  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(24.98 min) Epoch 13/300 -- Iteration 92578 - Batch 154/7702 - Train loss: 0.00207620  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.00 min) Epoch 13/300 -- Iteration 92655 - Batch 231/7702 - Train loss: 0.00206979  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.02 min) Epoch 13/300 -- Iteration 92732 - Batch 308/7702 - Train loss: 0.00206613  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.04 min) Epoch 13/300 -- Iteration 92809 - Batch 385/7702 - Train loss: 0.00206734  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.06 min) Epoch 13/300 -- Iteration 92886 - Batch 462/7702 - Train loss: 0.00206415  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.08 min) Epoch 13/300 -- Iteration 92963 - Batch 539/7702 - Train loss: 0.00206693  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.10 min) Epoch 13/300 -- Iteration 93040 - Batch 616/7702 - Train loss: 0.00207053  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.12 min) Epoch 13/300 -- Iteration 93117 - Batch 693/7702 - Train loss: 0.00207067  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.14 min) Epoch 13/300 -- Iteration 93194 - Batch 770/7702 - Train loss: 0.00206787  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.16 min) Epoch 13/300 -- Iteration 93271 - Batch 847/7702 - Train loss: 0.00206736  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.18 min) Epoch 13/300 -- Iteration 93348 - Batch 924/7702 - Train loss: 0.00206619  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.20 min) Epoch 13/300 -- Iteration 93425 - Batch 1001/7702 - Train loss: 0.00206553  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.22 min) Epoch 13/300 -- Iteration 93502 - Batch 1078/7702 - Train loss: 0.00206394  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.25 min) Epoch 13/300 -- Iteration 93579 - Batch 1155/7702 - Train loss: 0.00206244  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.27 min) Epoch 13/300 -- Iteration 93656 - Batch 1232/7702 - Train loss: 0.00206129  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.29 min) Epoch 13/300 -- Iteration 93733 - Batch 1309/7702 - Train loss: 0.00205879  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.31 min) Epoch 13/300 -- Iteration 93810 - Batch 1386/7702 - Train loss: 0.00205916  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.33 min) Epoch 13/300 -- Iteration 93887 - Batch 1463/7702 - Train loss: 0.00206144  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.35 min) Epoch 13/300 -- Iteration 93964 - Batch 1540/7702 - Train loss: 0.00206078  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.37 min) Epoch 13/300 -- Iteration 94041 - Batch 1617/7702 - Train loss: 0.00206028  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.39 min) Epoch 13/300 -- Iteration 94118 - Batch 1694/7702 - Train loss: 0.00205901  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.41 min) Epoch 13/300 -- Iteration 94195 - Batch 1771/7702 - Train loss: 0.00205684  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.43 min) Epoch 13/300 -- Iteration 94272 - Batch 1848/7702 - Train loss: 0.00205731  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.45 min) Epoch 13/300 -- Iteration 94349 - Batch 1925/7702 - Train loss: 0.00205925  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.47 min) Epoch 13/300 -- Iteration 94426 - Batch 2002/7702 - Train loss: 0.00205781  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.49 min) Epoch 13/300 -- Iteration 94503 - Batch 2079/7702 - Train loss: 0.00205868  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.51 min) Epoch 13/300 -- Iteration 94580 - Batch 2156/7702 - Train loss: 0.00205703  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.53 min) Epoch 13/300 -- Iteration 94657 - Batch 2233/7702 - Train loss: 0.00205633  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.56 min) Epoch 13/300 -- Iteration 94734 - Batch 2310/7702 - Train loss: 0.00205575  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.58 min) Epoch 13/300 -- Iteration 94811 - Batch 2387/7702 - Train loss: 0.00205558  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.60 min) Epoch 13/300 -- Iteration 94888 - Batch 2464/7702 - Train loss: 0.00205529  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.62 min) Epoch 13/300 -- Iteration 94965 - Batch 2541/7702 - Train loss: 0.00205466  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.64 min) Epoch 13/300 -- Iteration 95042 - Batch 2618/7702 - Train loss: 0.00205618  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.66 min) Epoch 13/300 -- Iteration 95119 - Batch 2695/7702 - Train loss: 0.00205693  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.68 min) Epoch 13/300 -- Iteration 95196 - Batch 2772/7702 - Train loss: 0.00205763  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.70 min) Epoch 13/300 -- Iteration 95273 - Batch 2849/7702 - Train loss: 0.00205619  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.72 min) Epoch 13/300 -- Iteration 95350 - Batch 2926/7702 - Train loss: 0.00205554  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.74 min) Epoch 13/300 -- Iteration 95427 - Batch 3003/7702 - Train loss: 0.00205482  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.76 min) Epoch 13/300 -- Iteration 95504 - Batch 3080/7702 - Train loss: 0.00205501  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.78 min) Epoch 13/300 -- Iteration 95581 - Batch 3157/7702 - Train loss: 0.00205528  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.80 min) Epoch 13/300 -- Iteration 95658 - Batch 3234/7702 - Train loss: 0.00205460  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.82 min) Epoch 13/300 -- Iteration 95735 - Batch 3311/7702 - Train loss: 0.00205385  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.84 min) Epoch 13/300 -- Iteration 95812 - Batch 3388/7702 - Train loss: 0.00205463  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.87 min) Epoch 13/300 -- Iteration 95889 - Batch 3465/7702 - Train loss: 0.00205429  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.89 min) Epoch 13/300 -- Iteration 95966 - Batch 3542/7702 - Train loss: 0.00205363  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.91 min) Epoch 13/300 -- Iteration 96043 - Batch 3619/7702 - Train loss: 0.00205318  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.93 min) Epoch 13/300 -- Iteration 96120 - Batch 3696/7702 - Train loss: 0.00205384  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.95 min) Epoch 13/300 -- Iteration 96197 - Batch 3773/7702 - Train loss: 0.00205380  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.97 min) Epoch 13/300 -- Iteration 96274 - Batch 3850/7702 - Train loss: 0.00205354  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(25.99 min) Epoch 13/300 -- Iteration 96351 - Batch 3927/7702 - Train loss: 0.00205374  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.01 min) Epoch 13/300 -- Iteration 96428 - Batch 4004/7702 - Train loss: 0.00205402  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.03 min) Epoch 13/300 -- Iteration 96505 - Batch 4081/7702 - Train loss: 0.00205378  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.05 min) Epoch 13/300 -- Iteration 96582 - Batch 4158/7702 - Train loss: 0.00205458  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.07 min) Epoch 13/300 -- Iteration 96659 - Batch 4235/7702 - Train loss: 0.00205553  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.09 min) Epoch 13/300 -- Iteration 96736 - Batch 4312/7702 - Train loss: 0.00205533  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.11 min) Epoch 13/300 -- Iteration 96813 - Batch 4389/7702 - Train loss: 0.00205465  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.14 min) Epoch 13/300 -- Iteration 96890 - Batch 4466/7702 - Train loss: 0.00205396  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.16 min) Epoch 13/300 -- Iteration 96967 - Batch 4543/7702 - Train loss: 0.00205484  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.18 min) Epoch 13/300 -- Iteration 97044 - Batch 4620/7702 - Train loss: 0.00205411  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.20 min) Epoch 13/300 -- Iteration 97121 - Batch 4697/7702 - Train loss: 0.00205353  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.22 min) Epoch 13/300 -- Iteration 97198 - Batch 4774/7702 - Train loss: 0.00205286  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.24 min) Epoch 13/300 -- Iteration 97275 - Batch 4851/7702 - Train loss: 0.00205275  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.26 min) Epoch 13/300 -- Iteration 97352 - Batch 4928/7702 - Train loss: 0.00205254  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.28 min) Epoch 13/300 -- Iteration 97429 - Batch 5005/7702 - Train loss: 0.00205249  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.30 min) Epoch 13/300 -- Iteration 97506 - Batch 5082/7702 - Train loss: 0.00205250  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.32 min) Epoch 13/300 -- Iteration 97583 - Batch 5159/7702 - Train loss: 0.00205225  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.34 min) Epoch 13/300 -- Iteration 97660 - Batch 5236/7702 - Train loss: 0.00205181  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.36 min) Epoch 13/300 -- Iteration 97737 - Batch 5313/7702 - Train loss: 0.00205164  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.39 min) Epoch 13/300 -- Iteration 97814 - Batch 5390/7702 - Train loss: 0.00205205  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.41 min) Epoch 13/300 -- Iteration 97891 - Batch 5467/7702 - Train loss: 0.00205260  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.43 min) Epoch 13/300 -- Iteration 97968 - Batch 5544/7702 - Train loss: 0.00205302  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.45 min) Epoch 13/300 -- Iteration 98045 - Batch 5621/7702 - Train loss: 0.00205326  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.47 min) Epoch 13/300 -- Iteration 98122 - Batch 5698/7702 - Train loss: 0.00205315  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.49 min) Epoch 13/300 -- Iteration 98199 - Batch 5775/7702 - Train loss: 0.00205310  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.51 min) Epoch 13/300 -- Iteration 98276 - Batch 5852/7702 - Train loss: 0.00205346  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.53 min) Epoch 13/300 -- Iteration 98353 - Batch 5929/7702 - Train loss: 0.00205361  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.55 min) Epoch 13/300 -- Iteration 98430 - Batch 6006/7702 - Train loss: 0.00205342  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.57 min) Epoch 13/300 -- Iteration 98507 - Batch 6083/7702 - Train loss: 0.00205349  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.59 min) Epoch 13/300 -- Iteration 98584 - Batch 6160/7702 - Train loss: 0.00205342  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.61 min) Epoch 13/300 -- Iteration 98661 - Batch 6237/7702 - Train loss: 0.00205337  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.63 min) Epoch 13/300 -- Iteration 98738 - Batch 6314/7702 - Train loss: 0.00205337  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.65 min) Epoch 13/300 -- Iteration 98815 - Batch 6391/7702 - Train loss: 0.00205333  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.68 min) Epoch 13/300 -- Iteration 98892 - Batch 6468/7702 - Train loss: 0.00205309  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.70 min) Epoch 13/300 -- Iteration 98969 - Batch 6545/7702 - Train loss: 0.00205290  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.72 min) Epoch 13/300 -- Iteration 99046 - Batch 6622/7702 - Train loss: 0.00205260  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.74 min) Epoch 13/300 -- Iteration 99123 - Batch 6699/7702 - Train loss: 0.00205292  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.76 min) Epoch 13/300 -- Iteration 99200 - Batch 6776/7702 - Train loss: 0.00205274  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.78 min) Epoch 13/300 -- Iteration 99277 - Batch 6853/7702 - Train loss: 0.00205240  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.80 min) Epoch 13/300 -- Iteration 99354 - Batch 6930/7702 - Train loss: 0.00205190  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.82 min) Epoch 13/300 -- Iteration 99431 - Batch 7007/7702 - Train loss: 0.00205193  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.84 min) Epoch 13/300 -- Iteration 99508 - Batch 7084/7702 - Train loss: 0.00205208  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.86 min) Epoch 13/300 -- Iteration 99585 - Batch 7161/7702 - Train loss: 0.00205205  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.88 min) Epoch 13/300 -- Iteration 99662 - Batch 7238/7702 - Train loss: 0.00205193  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.90 min) Epoch 13/300 -- Iteration 99739 - Batch 7315/7702 - Train loss: 0.00205158  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.92 min) Epoch 13/300 -- Iteration 99816 - Batch 7392/7702 - Train loss: 0.00205143  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.94 min) Epoch 13/300 -- Iteration 99893 - Batch 7469/7702 - Train loss: 0.00205150  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.96 min) Epoch 13/300 -- Iteration 99970 - Batch 7546/7702 - Train loss: 0.00205180  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(26.99 min) Epoch 13/300 -- Iteration 100047 - Batch 7623/7702 - Train loss: 0.00205145  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(27.01 min) Epoch 13/300 -- Iteration 100124 - Batch 7700/7702 - Train loss: 0.00205131  - Train acc: -0.0000 - Val loss: 0.00049462\n",
      "(27.01 min) Epoch 13/300 -- Iteration 100126 - Batch 7701/7702 - Train loss: 0.00205138  - Train acc: -0.0000 - Val loss: 0.00054164 - Val acc: -0.0000\n",
      "(27.03 min) Epoch 14/300 -- Iteration 100203 - Batch 77/7702 - Train loss: 0.00201226  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.05 min) Epoch 14/300 -- Iteration 100280 - Batch 154/7702 - Train loss: 0.00204450  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.07 min) Epoch 14/300 -- Iteration 100357 - Batch 231/7702 - Train loss: 0.00204684  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.09 min) Epoch 14/300 -- Iteration 100434 - Batch 308/7702 - Train loss: 0.00203888  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.11 min) Epoch 14/300 -- Iteration 100511 - Batch 385/7702 - Train loss: 0.00204333  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.13 min) Epoch 14/300 -- Iteration 100588 - Batch 462/7702 - Train loss: 0.00204419  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.15 min) Epoch 14/300 -- Iteration 100665 - Batch 539/7702 - Train loss: 0.00204787  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.17 min) Epoch 14/300 -- Iteration 100742 - Batch 616/7702 - Train loss: 0.00204657  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.20 min) Epoch 14/300 -- Iteration 100819 - Batch 693/7702 - Train loss: 0.00204684  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.22 min) Epoch 14/300 -- Iteration 100896 - Batch 770/7702 - Train loss: 0.00204768  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.24 min) Epoch 14/300 -- Iteration 100973 - Batch 847/7702 - Train loss: 0.00204801  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.26 min) Epoch 14/300 -- Iteration 101050 - Batch 924/7702 - Train loss: 0.00204792  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.28 min) Epoch 14/300 -- Iteration 101127 - Batch 1001/7702 - Train loss: 0.00204495  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.30 min) Epoch 14/300 -- Iteration 101204 - Batch 1078/7702 - Train loss: 0.00204177  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.32 min) Epoch 14/300 -- Iteration 101281 - Batch 1155/7702 - Train loss: 0.00204135  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.34 min) Epoch 14/300 -- Iteration 101358 - Batch 1232/7702 - Train loss: 0.00204173  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.36 min) Epoch 14/300 -- Iteration 101435 - Batch 1309/7702 - Train loss: 0.00204339  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.38 min) Epoch 14/300 -- Iteration 101512 - Batch 1386/7702 - Train loss: 0.00204382  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.40 min) Epoch 14/300 -- Iteration 101589 - Batch 1463/7702 - Train loss: 0.00204625  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.42 min) Epoch 14/300 -- Iteration 101666 - Batch 1540/7702 - Train loss: 0.00204800  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.44 min) Epoch 14/300 -- Iteration 101743 - Batch 1617/7702 - Train loss: 0.00204735  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.46 min) Epoch 14/300 -- Iteration 101820 - Batch 1694/7702 - Train loss: 0.00205007  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.48 min) Epoch 14/300 -- Iteration 101897 - Batch 1771/7702 - Train loss: 0.00204967  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.51 min) Epoch 14/300 -- Iteration 101974 - Batch 1848/7702 - Train loss: 0.00205089  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.53 min) Epoch 14/300 -- Iteration 102051 - Batch 1925/7702 - Train loss: 0.00205086  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.55 min) Epoch 14/300 -- Iteration 102128 - Batch 2002/7702 - Train loss: 0.00204956  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.57 min) Epoch 14/300 -- Iteration 102205 - Batch 2079/7702 - Train loss: 0.00204786  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.59 min) Epoch 14/300 -- Iteration 102282 - Batch 2156/7702 - Train loss: 0.00204861  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.61 min) Epoch 14/300 -- Iteration 102359 - Batch 2233/7702 - Train loss: 0.00204743  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.63 min) Epoch 14/300 -- Iteration 102436 - Batch 2310/7702 - Train loss: 0.00204811  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.65 min) Epoch 14/300 -- Iteration 102513 - Batch 2387/7702 - Train loss: 0.00204924  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.67 min) Epoch 14/300 -- Iteration 102590 - Batch 2464/7702 - Train loss: 0.00204995  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.69 min) Epoch 14/300 -- Iteration 102667 - Batch 2541/7702 - Train loss: 0.00205032  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.71 min) Epoch 14/300 -- Iteration 102744 - Batch 2618/7702 - Train loss: 0.00204891  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.73 min) Epoch 14/300 -- Iteration 102821 - Batch 2695/7702 - Train loss: 0.00204913  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.75 min) Epoch 14/300 -- Iteration 102898 - Batch 2772/7702 - Train loss: 0.00204985  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.77 min) Epoch 14/300 -- Iteration 102975 - Batch 2849/7702 - Train loss: 0.00204928  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.79 min) Epoch 14/300 -- Iteration 103052 - Batch 2926/7702 - Train loss: 0.00204955  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.82 min) Epoch 14/300 -- Iteration 103129 - Batch 3003/7702 - Train loss: 0.00204963  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.84 min) Epoch 14/300 -- Iteration 103206 - Batch 3080/7702 - Train loss: 0.00204931  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.86 min) Epoch 14/300 -- Iteration 103283 - Batch 3157/7702 - Train loss: 0.00204843  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.88 min) Epoch 14/300 -- Iteration 103360 - Batch 3234/7702 - Train loss: 0.00204853  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.90 min) Epoch 14/300 -- Iteration 103437 - Batch 3311/7702 - Train loss: 0.00204749  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.92 min) Epoch 14/300 -- Iteration 103514 - Batch 3388/7702 - Train loss: 0.00204805  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.94 min) Epoch 14/300 -- Iteration 103591 - Batch 3465/7702 - Train loss: 0.00204855  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.96 min) Epoch 14/300 -- Iteration 103668 - Batch 3542/7702 - Train loss: 0.00204858  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(27.98 min) Epoch 14/300 -- Iteration 103745 - Batch 3619/7702 - Train loss: 0.00204870  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.00 min) Epoch 14/300 -- Iteration 103822 - Batch 3696/7702 - Train loss: 0.00204896  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.02 min) Epoch 14/300 -- Iteration 103899 - Batch 3773/7702 - Train loss: 0.00204905  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.04 min) Epoch 14/300 -- Iteration 103976 - Batch 3850/7702 - Train loss: 0.00204895  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.06 min) Epoch 14/300 -- Iteration 104053 - Batch 3927/7702 - Train loss: 0.00204828  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.08 min) Epoch 14/300 -- Iteration 104130 - Batch 4004/7702 - Train loss: 0.00204844  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.10 min) Epoch 14/300 -- Iteration 104207 - Batch 4081/7702 - Train loss: 0.00204826  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.13 min) Epoch 14/300 -- Iteration 104284 - Batch 4158/7702 - Train loss: 0.00204845  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.15 min) Epoch 14/300 -- Iteration 104361 - Batch 4235/7702 - Train loss: 0.00204851  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.17 min) Epoch 14/300 -- Iteration 104438 - Batch 4312/7702 - Train loss: 0.00204812  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.19 min) Epoch 14/300 -- Iteration 104515 - Batch 4389/7702 - Train loss: 0.00204870  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.21 min) Epoch 14/300 -- Iteration 104592 - Batch 4466/7702 - Train loss: 0.00204852  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.23 min) Epoch 14/300 -- Iteration 104669 - Batch 4543/7702 - Train loss: 0.00204811  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.25 min) Epoch 14/300 -- Iteration 104746 - Batch 4620/7702 - Train loss: 0.00204827  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.27 min) Epoch 14/300 -- Iteration 104823 - Batch 4697/7702 - Train loss: 0.00204827  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.29 min) Epoch 14/300 -- Iteration 104900 - Batch 4774/7702 - Train loss: 0.00204824  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.31 min) Epoch 14/300 -- Iteration 104977 - Batch 4851/7702 - Train loss: 0.00204765  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.33 min) Epoch 14/300 -- Iteration 105054 - Batch 4928/7702 - Train loss: 0.00204714  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.35 min) Epoch 14/300 -- Iteration 105131 - Batch 5005/7702 - Train loss: 0.00204694  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.37 min) Epoch 14/300 -- Iteration 105208 - Batch 5082/7702 - Train loss: 0.00204670  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.39 min) Epoch 14/300 -- Iteration 105285 - Batch 5159/7702 - Train loss: 0.00204625  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.42 min) Epoch 14/300 -- Iteration 105362 - Batch 5236/7702 - Train loss: 0.00204666  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.44 min) Epoch 14/300 -- Iteration 105439 - Batch 5313/7702 - Train loss: 0.00204665  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.46 min) Epoch 14/300 -- Iteration 105516 - Batch 5390/7702 - Train loss: 0.00204665  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.48 min) Epoch 14/300 -- Iteration 105593 - Batch 5467/7702 - Train loss: 0.00204688  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.50 min) Epoch 14/300 -- Iteration 105670 - Batch 5544/7702 - Train loss: 0.00204659  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.52 min) Epoch 14/300 -- Iteration 105747 - Batch 5621/7702 - Train loss: 0.00204653  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.54 min) Epoch 14/300 -- Iteration 105824 - Batch 5698/7702 - Train loss: 0.00204640  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.56 min) Epoch 14/300 -- Iteration 105901 - Batch 5775/7702 - Train loss: 0.00204599  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.58 min) Epoch 14/300 -- Iteration 105978 - Batch 5852/7702 - Train loss: 0.00204627  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.60 min) Epoch 14/300 -- Iteration 106055 - Batch 5929/7702 - Train loss: 0.00204626  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.62 min) Epoch 14/300 -- Iteration 106132 - Batch 6006/7702 - Train loss: 0.00204613  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.64 min) Epoch 14/300 -- Iteration 106209 - Batch 6083/7702 - Train loss: 0.00204628  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.66 min) Epoch 14/300 -- Iteration 106286 - Batch 6160/7702 - Train loss: 0.00204682  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.68 min) Epoch 14/300 -- Iteration 106363 - Batch 6237/7702 - Train loss: 0.00204649  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.70 min) Epoch 14/300 -- Iteration 106440 - Batch 6314/7702 - Train loss: 0.00204653  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.73 min) Epoch 14/300 -- Iteration 106517 - Batch 6391/7702 - Train loss: 0.00204676  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.75 min) Epoch 14/300 -- Iteration 106594 - Batch 6468/7702 - Train loss: 0.00204661  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.77 min) Epoch 14/300 -- Iteration 106671 - Batch 6545/7702 - Train loss: 0.00204618  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.79 min) Epoch 14/300 -- Iteration 106748 - Batch 6622/7702 - Train loss: 0.00204599  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.81 min) Epoch 14/300 -- Iteration 106825 - Batch 6699/7702 - Train loss: 0.00204616  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.83 min) Epoch 14/300 -- Iteration 106902 - Batch 6776/7702 - Train loss: 0.00204580  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.85 min) Epoch 14/300 -- Iteration 106979 - Batch 6853/7702 - Train loss: 0.00204566  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.87 min) Epoch 14/300 -- Iteration 107056 - Batch 6930/7702 - Train loss: 0.00204560  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.89 min) Epoch 14/300 -- Iteration 107133 - Batch 7007/7702 - Train loss: 0.00204547  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.91 min) Epoch 14/300 -- Iteration 107210 - Batch 7084/7702 - Train loss: 0.00204542  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.93 min) Epoch 14/300 -- Iteration 107287 - Batch 7161/7702 - Train loss: 0.00204530  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.95 min) Epoch 14/300 -- Iteration 107364 - Batch 7238/7702 - Train loss: 0.00204544  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.97 min) Epoch 14/300 -- Iteration 107441 - Batch 7315/7702 - Train loss: 0.00204528  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(28.99 min) Epoch 14/300 -- Iteration 107518 - Batch 7392/7702 - Train loss: 0.00204521  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(29.01 min) Epoch 14/300 -- Iteration 107595 - Batch 7469/7702 - Train loss: 0.00204502  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(29.04 min) Epoch 14/300 -- Iteration 107672 - Batch 7546/7702 - Train loss: 0.00204500  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(29.06 min) Epoch 14/300 -- Iteration 107749 - Batch 7623/7702 - Train loss: 0.00204510  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(29.08 min) Epoch 14/300 -- Iteration 107826 - Batch 7700/7702 - Train loss: 0.00204540  - Train acc: -0.0000 - Val loss: 0.00054164\n",
      "(29.08 min) Epoch 14/300 -- Iteration 107828 - Batch 7701/7702 - Train loss: 0.00204538  - Train acc: -0.0000 - Val loss: 0.00058651 - Val acc: -0.0000\n",
      "(29.10 min) Epoch 15/300 -- Iteration 107905 - Batch 77/7702 - Train loss: 0.00207380  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.12 min) Epoch 15/300 -- Iteration 107982 - Batch 154/7702 - Train loss: 0.00206816  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.14 min) Epoch 15/300 -- Iteration 108059 - Batch 231/7702 - Train loss: 0.00206626  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.16 min) Epoch 15/300 -- Iteration 108136 - Batch 308/7702 - Train loss: 0.00205784  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.18 min) Epoch 15/300 -- Iteration 108213 - Batch 385/7702 - Train loss: 0.00204850  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.20 min) Epoch 15/300 -- Iteration 108290 - Batch 462/7702 - Train loss: 0.00204659  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.22 min) Epoch 15/300 -- Iteration 108367 - Batch 539/7702 - Train loss: 0.00204758  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.24 min) Epoch 15/300 -- Iteration 108444 - Batch 616/7702 - Train loss: 0.00204832  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.27 min) Epoch 15/300 -- Iteration 108521 - Batch 693/7702 - Train loss: 0.00204930  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.29 min) Epoch 15/300 -- Iteration 108598 - Batch 770/7702 - Train loss: 0.00204663  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.31 min) Epoch 15/300 -- Iteration 108675 - Batch 847/7702 - Train loss: 0.00204445  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.33 min) Epoch 15/300 -- Iteration 108752 - Batch 924/7702 - Train loss: 0.00204553  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.35 min) Epoch 15/300 -- Iteration 108829 - Batch 1001/7702 - Train loss: 0.00204584  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.37 min) Epoch 15/300 -- Iteration 108906 - Batch 1078/7702 - Train loss: 0.00204155  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.39 min) Epoch 15/300 -- Iteration 108983 - Batch 1155/7702 - Train loss: 0.00204092  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.41 min) Epoch 15/300 -- Iteration 109060 - Batch 1232/7702 - Train loss: 0.00204109  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.43 min) Epoch 15/300 -- Iteration 109137 - Batch 1309/7702 - Train loss: 0.00204175  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.45 min) Epoch 15/300 -- Iteration 109214 - Batch 1386/7702 - Train loss: 0.00204343  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.47 min) Epoch 15/300 -- Iteration 109291 - Batch 1463/7702 - Train loss: 0.00204349  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.49 min) Epoch 15/300 -- Iteration 109368 - Batch 1540/7702 - Train loss: 0.00204127  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.51 min) Epoch 15/300 -- Iteration 109445 - Batch 1617/7702 - Train loss: 0.00204177  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.53 min) Epoch 15/300 -- Iteration 109522 - Batch 1694/7702 - Train loss: 0.00204191  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.56 min) Epoch 15/300 -- Iteration 109599 - Batch 1771/7702 - Train loss: 0.00204121  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.58 min) Epoch 15/300 -- Iteration 109676 - Batch 1848/7702 - Train loss: 0.00203913  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.60 min) Epoch 15/300 -- Iteration 109753 - Batch 1925/7702 - Train loss: 0.00203775  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.62 min) Epoch 15/300 -- Iteration 109830 - Batch 2002/7702 - Train loss: 0.00203806  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.64 min) Epoch 15/300 -- Iteration 109907 - Batch 2079/7702 - Train loss: 0.00203935  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.66 min) Epoch 15/300 -- Iteration 109984 - Batch 2156/7702 - Train loss: 0.00203849  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.68 min) Epoch 15/300 -- Iteration 110061 - Batch 2233/7702 - Train loss: 0.00203836  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.70 min) Epoch 15/300 -- Iteration 110138 - Batch 2310/7702 - Train loss: 0.00203843  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.72 min) Epoch 15/300 -- Iteration 110215 - Batch 2387/7702 - Train loss: 0.00203900  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.74 min) Epoch 15/300 -- Iteration 110292 - Batch 2464/7702 - Train loss: 0.00203885  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.76 min) Epoch 15/300 -- Iteration 110369 - Batch 2541/7702 - Train loss: 0.00203771  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.78 min) Epoch 15/300 -- Iteration 110446 - Batch 2618/7702 - Train loss: 0.00203823  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.80 min) Epoch 15/300 -- Iteration 110523 - Batch 2695/7702 - Train loss: 0.00203822  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.82 min) Epoch 15/300 -- Iteration 110600 - Batch 2772/7702 - Train loss: 0.00203877  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.84 min) Epoch 15/300 -- Iteration 110677 - Batch 2849/7702 - Train loss: 0.00203807  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.87 min) Epoch 15/300 -- Iteration 110754 - Batch 2926/7702 - Train loss: 0.00203725  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.89 min) Epoch 15/300 -- Iteration 110831 - Batch 3003/7702 - Train loss: 0.00203648  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.91 min) Epoch 15/300 -- Iteration 110908 - Batch 3080/7702 - Train loss: 0.00203682  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.93 min) Epoch 15/300 -- Iteration 110985 - Batch 3157/7702 - Train loss: 0.00203676  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.95 min) Epoch 15/300 -- Iteration 111062 - Batch 3234/7702 - Train loss: 0.00203693  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.97 min) Epoch 15/300 -- Iteration 111139 - Batch 3311/7702 - Train loss: 0.00203698  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(29.99 min) Epoch 15/300 -- Iteration 111216 - Batch 3388/7702 - Train loss: 0.00203540  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.01 min) Epoch 15/300 -- Iteration 111293 - Batch 3465/7702 - Train loss: 0.00203528  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.03 min) Epoch 15/300 -- Iteration 111370 - Batch 3542/7702 - Train loss: 0.00203515  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.05 min) Epoch 15/300 -- Iteration 111447 - Batch 3619/7702 - Train loss: 0.00203439  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.07 min) Epoch 15/300 -- Iteration 111524 - Batch 3696/7702 - Train loss: 0.00203428  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.09 min) Epoch 15/300 -- Iteration 111601 - Batch 3773/7702 - Train loss: 0.00203374  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.11 min) Epoch 15/300 -- Iteration 111678 - Batch 3850/7702 - Train loss: 0.00203379  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.13 min) Epoch 15/300 -- Iteration 111755 - Batch 3927/7702 - Train loss: 0.00203415  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.16 min) Epoch 15/300 -- Iteration 111832 - Batch 4004/7702 - Train loss: 0.00203443  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.18 min) Epoch 15/300 -- Iteration 111909 - Batch 4081/7702 - Train loss: 0.00203462  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.20 min) Epoch 15/300 -- Iteration 111986 - Batch 4158/7702 - Train loss: 0.00203432  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.22 min) Epoch 15/300 -- Iteration 112063 - Batch 4235/7702 - Train loss: 0.00203422  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.24 min) Epoch 15/300 -- Iteration 112140 - Batch 4312/7702 - Train loss: 0.00203447  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.26 min) Epoch 15/300 -- Iteration 112217 - Batch 4389/7702 - Train loss: 0.00203518  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.28 min) Epoch 15/300 -- Iteration 112294 - Batch 4466/7702 - Train loss: 0.00203491  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.30 min) Epoch 15/300 -- Iteration 112371 - Batch 4543/7702 - Train loss: 0.00203498  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.32 min) Epoch 15/300 -- Iteration 112448 - Batch 4620/7702 - Train loss: 0.00203565  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.34 min) Epoch 15/300 -- Iteration 112525 - Batch 4697/7702 - Train loss: 0.00203512  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.36 min) Epoch 15/300 -- Iteration 112602 - Batch 4774/7702 - Train loss: 0.00203543  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.38 min) Epoch 15/300 -- Iteration 112679 - Batch 4851/7702 - Train loss: 0.00203617  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.40 min) Epoch 15/300 -- Iteration 112756 - Batch 4928/7702 - Train loss: 0.00203611  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.43 min) Epoch 15/300 -- Iteration 112833 - Batch 5005/7702 - Train loss: 0.00203627  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.45 min) Epoch 15/300 -- Iteration 112910 - Batch 5082/7702 - Train loss: 0.00203663  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.47 min) Epoch 15/300 -- Iteration 112987 - Batch 5159/7702 - Train loss: 0.00203648  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.49 min) Epoch 15/300 -- Iteration 113064 - Batch 5236/7702 - Train loss: 0.00203645  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.51 min) Epoch 15/300 -- Iteration 113141 - Batch 5313/7702 - Train loss: 0.00203631  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.53 min) Epoch 15/300 -- Iteration 113218 - Batch 5390/7702 - Train loss: 0.00203604  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.55 min) Epoch 15/300 -- Iteration 113295 - Batch 5467/7702 - Train loss: 0.00203567  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.57 min) Epoch 15/300 -- Iteration 113372 - Batch 5544/7702 - Train loss: 0.00203543  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.59 min) Epoch 15/300 -- Iteration 113449 - Batch 5621/7702 - Train loss: 0.00203582  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.61 min) Epoch 15/300 -- Iteration 113526 - Batch 5698/7702 - Train loss: 0.00203901  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.63 min) Epoch 15/300 -- Iteration 113603 - Batch 5775/7702 - Train loss: 0.00203935  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.65 min) Epoch 15/300 -- Iteration 113680 - Batch 5852/7702 - Train loss: 0.00203949  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.67 min) Epoch 15/300 -- Iteration 113757 - Batch 5929/7702 - Train loss: 0.00203959  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.69 min) Epoch 15/300 -- Iteration 113834 - Batch 6006/7702 - Train loss: 0.00203938  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.72 min) Epoch 15/300 -- Iteration 113911 - Batch 6083/7702 - Train loss: 0.00203938  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.74 min) Epoch 15/300 -- Iteration 113988 - Batch 6160/7702 - Train loss: 0.00203938  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.76 min) Epoch 15/300 -- Iteration 114065 - Batch 6237/7702 - Train loss: 0.00203907  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.78 min) Epoch 15/300 -- Iteration 114142 - Batch 6314/7702 - Train loss: 0.00203901  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.80 min) Epoch 15/300 -- Iteration 114219 - Batch 6391/7702 - Train loss: 0.00203907  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.82 min) Epoch 15/300 -- Iteration 114296 - Batch 6468/7702 - Train loss: 0.00203913  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.84 min) Epoch 15/300 -- Iteration 114373 - Batch 6545/7702 - Train loss: 0.00203907  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.86 min) Epoch 15/300 -- Iteration 114450 - Batch 6622/7702 - Train loss: 0.00203928  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.88 min) Epoch 15/300 -- Iteration 114527 - Batch 6699/7702 - Train loss: 0.00203925  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.90 min) Epoch 15/300 -- Iteration 114604 - Batch 6776/7702 - Train loss: 0.00203933  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.92 min) Epoch 15/300 -- Iteration 114681 - Batch 6853/7702 - Train loss: 0.00203904  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.94 min) Epoch 15/300 -- Iteration 114758 - Batch 6930/7702 - Train loss: 0.00203838  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.96 min) Epoch 15/300 -- Iteration 114835 - Batch 7007/7702 - Train loss: 0.00203841  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(30.98 min) Epoch 15/300 -- Iteration 114912 - Batch 7084/7702 - Train loss: 0.00203835  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(31.00 min) Epoch 15/300 -- Iteration 114989 - Batch 7161/7702 - Train loss: 0.00203817  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(31.02 min) Epoch 15/300 -- Iteration 115066 - Batch 7238/7702 - Train loss: 0.00203758  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(31.05 min) Epoch 15/300 -- Iteration 115143 - Batch 7315/7702 - Train loss: 0.00203749  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(31.07 min) Epoch 15/300 -- Iteration 115220 - Batch 7392/7702 - Train loss: 0.00203729  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(31.09 min) Epoch 15/300 -- Iteration 115297 - Batch 7469/7702 - Train loss: 0.00203691  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(31.11 min) Epoch 15/300 -- Iteration 115374 - Batch 7546/7702 - Train loss: 0.00203678  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(31.13 min) Epoch 15/300 -- Iteration 115451 - Batch 7623/7702 - Train loss: 0.00203707  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(31.15 min) Epoch 15/300 -- Iteration 115528 - Batch 7700/7702 - Train loss: 0.00203690  - Train acc: -0.0000 - Val loss: 0.00058651\n",
      "(31.15 min) Epoch 15/300 -- Iteration 115530 - Batch 7701/7702 - Train loss: 0.00203687  - Train acc: -0.0000 - Val loss: 0.00061055 - Val acc: -0.0000\n",
      "(31.17 min) Epoch 16/300 -- Iteration 115607 - Batch 77/7702 - Train loss: 0.00203644  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.19 min) Epoch 16/300 -- Iteration 115684 - Batch 154/7702 - Train loss: 0.00201689  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.21 min) Epoch 16/300 -- Iteration 115761 - Batch 231/7702 - Train loss: 0.00200493  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.23 min) Epoch 16/300 -- Iteration 115838 - Batch 308/7702 - Train loss: 0.00201899  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.26 min) Epoch 16/300 -- Iteration 115915 - Batch 385/7702 - Train loss: 0.00202364  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.28 min) Epoch 16/300 -- Iteration 115992 - Batch 462/7702 - Train loss: 0.00202782  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.30 min) Epoch 16/300 -- Iteration 116069 - Batch 539/7702 - Train loss: 0.00202460  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.32 min) Epoch 16/300 -- Iteration 116146 - Batch 616/7702 - Train loss: 0.00202908  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.34 min) Epoch 16/300 -- Iteration 116223 - Batch 693/7702 - Train loss: 0.00202971  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.36 min) Epoch 16/300 -- Iteration 116300 - Batch 770/7702 - Train loss: 0.00203135  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.38 min) Epoch 16/300 -- Iteration 116377 - Batch 847/7702 - Train loss: 0.00203260  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.40 min) Epoch 16/300 -- Iteration 116454 - Batch 924/7702 - Train loss: 0.00203311  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.42 min) Epoch 16/300 -- Iteration 116531 - Batch 1001/7702 - Train loss: 0.00203491  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.44 min) Epoch 16/300 -- Iteration 116608 - Batch 1078/7702 - Train loss: 0.00203501  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.46 min) Epoch 16/300 -- Iteration 116685 - Batch 1155/7702 - Train loss: 0.00203464  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.48 min) Epoch 16/300 -- Iteration 116762 - Batch 1232/7702 - Train loss: 0.00203555  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.50 min) Epoch 16/300 -- Iteration 116839 - Batch 1309/7702 - Train loss: 0.00203493  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.52 min) Epoch 16/300 -- Iteration 116916 - Batch 1386/7702 - Train loss: 0.00203383  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.54 min) Epoch 16/300 -- Iteration 116993 - Batch 1463/7702 - Train loss: 0.00203133  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.56 min) Epoch 16/300 -- Iteration 117070 - Batch 1540/7702 - Train loss: 0.00203170  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.59 min) Epoch 16/300 -- Iteration 117147 - Batch 1617/7702 - Train loss: 0.00203173  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.61 min) Epoch 16/300 -- Iteration 117224 - Batch 1694/7702 - Train loss: 0.00203238  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.63 min) Epoch 16/300 -- Iteration 117301 - Batch 1771/7702 - Train loss: 0.00203340  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.65 min) Epoch 16/300 -- Iteration 117378 - Batch 1848/7702 - Train loss: 0.00203427  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.67 min) Epoch 16/300 -- Iteration 117455 - Batch 1925/7702 - Train loss: 0.00203470  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.69 min) Epoch 16/300 -- Iteration 117532 - Batch 2002/7702 - Train loss: 0.00203486  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.71 min) Epoch 16/300 -- Iteration 117609 - Batch 2079/7702 - Train loss: 0.00203446  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.73 min) Epoch 16/300 -- Iteration 117686 - Batch 2156/7702 - Train loss: 0.00203488  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.75 min) Epoch 16/300 -- Iteration 117763 - Batch 2233/7702 - Train loss: 0.00203460  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.77 min) Epoch 16/300 -- Iteration 117840 - Batch 2310/7702 - Train loss: 0.00203518  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.79 min) Epoch 16/300 -- Iteration 117917 - Batch 2387/7702 - Train loss: 0.00203471  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.81 min) Epoch 16/300 -- Iteration 117994 - Batch 2464/7702 - Train loss: 0.00203346  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.83 min) Epoch 16/300 -- Iteration 118071 - Batch 2541/7702 - Train loss: 0.00203235  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.85 min) Epoch 16/300 -- Iteration 118148 - Batch 2618/7702 - Train loss: 0.00203217  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.87 min) Epoch 16/300 -- Iteration 118225 - Batch 2695/7702 - Train loss: 0.00203133  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.90 min) Epoch 16/300 -- Iteration 118302 - Batch 2772/7702 - Train loss: 0.00203129  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.92 min) Epoch 16/300 -- Iteration 118379 - Batch 2849/7702 - Train loss: 0.00203373  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.94 min) Epoch 16/300 -- Iteration 118456 - Batch 2926/7702 - Train loss: 0.00203360  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.96 min) Epoch 16/300 -- Iteration 118533 - Batch 3003/7702 - Train loss: 0.00203357  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(31.98 min) Epoch 16/300 -- Iteration 118610 - Batch 3080/7702 - Train loss: 0.00203325  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.00 min) Epoch 16/300 -- Iteration 118687 - Batch 3157/7702 - Train loss: 0.00203291  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.02 min) Epoch 16/300 -- Iteration 118764 - Batch 3234/7702 - Train loss: 0.00203237  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.04 min) Epoch 16/300 -- Iteration 118841 - Batch 3311/7702 - Train loss: 0.00203267  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.06 min) Epoch 16/300 -- Iteration 118918 - Batch 3388/7702 - Train loss: 0.00203253  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.08 min) Epoch 16/300 -- Iteration 118995 - Batch 3465/7702 - Train loss: 0.00203289  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.10 min) Epoch 16/300 -- Iteration 119072 - Batch 3542/7702 - Train loss: 0.00203261  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.12 min) Epoch 16/300 -- Iteration 119149 - Batch 3619/7702 - Train loss: 0.00203251  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.14 min) Epoch 16/300 -- Iteration 119226 - Batch 3696/7702 - Train loss: 0.00203252  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.16 min) Epoch 16/300 -- Iteration 119303 - Batch 3773/7702 - Train loss: 0.00203249  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.18 min) Epoch 16/300 -- Iteration 119380 - Batch 3850/7702 - Train loss: 0.00203266  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.20 min) Epoch 16/300 -- Iteration 119457 - Batch 3927/7702 - Train loss: 0.00203266  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.23 min) Epoch 16/300 -- Iteration 119534 - Batch 4004/7702 - Train loss: 0.00203320  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.25 min) Epoch 16/300 -- Iteration 119611 - Batch 4081/7702 - Train loss: 0.00203319  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.27 min) Epoch 16/300 -- Iteration 119688 - Batch 4158/7702 - Train loss: 0.00203396  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.29 min) Epoch 16/300 -- Iteration 119765 - Batch 4235/7702 - Train loss: 0.00203381  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.31 min) Epoch 16/300 -- Iteration 119842 - Batch 4312/7702 - Train loss: 0.00203422  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.33 min) Epoch 16/300 -- Iteration 119919 - Batch 4389/7702 - Train loss: 0.00203353  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.35 min) Epoch 16/300 -- Iteration 119996 - Batch 4466/7702 - Train loss: 0.00203363  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.37 min) Epoch 16/300 -- Iteration 120073 - Batch 4543/7702 - Train loss: 0.00203396  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.39 min) Epoch 16/300 -- Iteration 120150 - Batch 4620/7702 - Train loss: 0.00203449  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.41 min) Epoch 16/300 -- Iteration 120227 - Batch 4697/7702 - Train loss: 0.00203439  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.43 min) Epoch 16/300 -- Iteration 120304 - Batch 4774/7702 - Train loss: 0.00203427  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.45 min) Epoch 16/300 -- Iteration 120381 - Batch 4851/7702 - Train loss: 0.00203461  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.47 min) Epoch 16/300 -- Iteration 120458 - Batch 4928/7702 - Train loss: 0.00203475  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.49 min) Epoch 16/300 -- Iteration 120535 - Batch 5005/7702 - Train loss: 0.00203425  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.51 min) Epoch 16/300 -- Iteration 120612 - Batch 5082/7702 - Train loss: 0.00203416  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.53 min) Epoch 16/300 -- Iteration 120689 - Batch 5159/7702 - Train loss: 0.00203410  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.56 min) Epoch 16/300 -- Iteration 120766 - Batch 5236/7702 - Train loss: 0.00203425  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.58 min) Epoch 16/300 -- Iteration 120843 - Batch 5313/7702 - Train loss: 0.00203427  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.60 min) Epoch 16/300 -- Iteration 120920 - Batch 5390/7702 - Train loss: 0.00203437  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.62 min) Epoch 16/300 -- Iteration 120997 - Batch 5467/7702 - Train loss: 0.00203392  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.64 min) Epoch 16/300 -- Iteration 121074 - Batch 5544/7702 - Train loss: 0.00203368  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.66 min) Epoch 16/300 -- Iteration 121151 - Batch 5621/7702 - Train loss: 0.00203339  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.68 min) Epoch 16/300 -- Iteration 121228 - Batch 5698/7702 - Train loss: 0.00203376  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.70 min) Epoch 16/300 -- Iteration 121305 - Batch 5775/7702 - Train loss: 0.00203308  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.72 min) Epoch 16/300 -- Iteration 121382 - Batch 5852/7702 - Train loss: 0.00203203  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.74 min) Epoch 16/300 -- Iteration 121459 - Batch 5929/7702 - Train loss: 0.00203161  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.76 min) Epoch 16/300 -- Iteration 121536 - Batch 6006/7702 - Train loss: 0.00203118  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.78 min) Epoch 16/300 -- Iteration 121613 - Batch 6083/7702 - Train loss: 0.00203097  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.80 min) Epoch 16/300 -- Iteration 121690 - Batch 6160/7702 - Train loss: 0.00203101  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.82 min) Epoch 16/300 -- Iteration 121767 - Batch 6237/7702 - Train loss: 0.00203117  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.84 min) Epoch 16/300 -- Iteration 121844 - Batch 6314/7702 - Train loss: 0.00203131  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.86 min) Epoch 16/300 -- Iteration 121921 - Batch 6391/7702 - Train loss: 0.00203127  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.89 min) Epoch 16/300 -- Iteration 121998 - Batch 6468/7702 - Train loss: 0.00203124  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.91 min) Epoch 16/300 -- Iteration 122075 - Batch 6545/7702 - Train loss: 0.00203126  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.93 min) Epoch 16/300 -- Iteration 122152 - Batch 6622/7702 - Train loss: 0.00203160  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.95 min) Epoch 16/300 -- Iteration 122229 - Batch 6699/7702 - Train loss: 0.00203153  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.97 min) Epoch 16/300 -- Iteration 122306 - Batch 6776/7702 - Train loss: 0.00203205  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(32.99 min) Epoch 16/300 -- Iteration 122383 - Batch 6853/7702 - Train loss: 0.00203239  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.01 min) Epoch 16/300 -- Iteration 122460 - Batch 6930/7702 - Train loss: 0.00203216  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.03 min) Epoch 16/300 -- Iteration 122537 - Batch 7007/7702 - Train loss: 0.00203151  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.05 min) Epoch 16/300 -- Iteration 122614 - Batch 7084/7702 - Train loss: 0.00203105  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.07 min) Epoch 16/300 -- Iteration 122691 - Batch 7161/7702 - Train loss: 0.00203101  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.09 min) Epoch 16/300 -- Iteration 122768 - Batch 7238/7702 - Train loss: 0.00203060  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.11 min) Epoch 16/300 -- Iteration 122845 - Batch 7315/7702 - Train loss: 0.00203051  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.13 min) Epoch 16/300 -- Iteration 122922 - Batch 7392/7702 - Train loss: 0.00202980  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.15 min) Epoch 16/300 -- Iteration 122999 - Batch 7469/7702 - Train loss: 0.00202991  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.17 min) Epoch 16/300 -- Iteration 123076 - Batch 7546/7702 - Train loss: 0.00202996  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.20 min) Epoch 16/300 -- Iteration 123153 - Batch 7623/7702 - Train loss: 0.00203007  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.22 min) Epoch 16/300 -- Iteration 123230 - Batch 7700/7702 - Train loss: 0.00202988  - Train acc: -0.0000 - Val loss: 0.00061055\n",
      "(33.22 min) Epoch 16/300 -- Iteration 123232 - Batch 7701/7702 - Train loss: 0.00202987  - Train acc: -0.0000 - Val loss: 0.00056600 - Val acc: -0.0000\n",
      "(33.24 min) Epoch 17/300 -- Iteration 123309 - Batch 77/7702 - Train loss: 0.00201979  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.26 min) Epoch 17/300 -- Iteration 123386 - Batch 154/7702 - Train loss: 0.00200566  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.28 min) Epoch 17/300 -- Iteration 123463 - Batch 231/7702 - Train loss: 0.00199611  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.30 min) Epoch 17/300 -- Iteration 123540 - Batch 308/7702 - Train loss: 0.00200791  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.32 min) Epoch 17/300 -- Iteration 123617 - Batch 385/7702 - Train loss: 0.00200804  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.34 min) Epoch 17/300 -- Iteration 123694 - Batch 462/7702 - Train loss: 0.00200761  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.36 min) Epoch 17/300 -- Iteration 123771 - Batch 539/7702 - Train loss: 0.00200912  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.38 min) Epoch 17/300 -- Iteration 123848 - Batch 616/7702 - Train loss: 0.00200687  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.40 min) Epoch 17/300 -- Iteration 123925 - Batch 693/7702 - Train loss: 0.00200604  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.42 min) Epoch 17/300 -- Iteration 124002 - Batch 770/7702 - Train loss: 0.00200426  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.44 min) Epoch 17/300 -- Iteration 124079 - Batch 847/7702 - Train loss: 0.00200208  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.47 min) Epoch 17/300 -- Iteration 124156 - Batch 924/7702 - Train loss: 0.00199968  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.49 min) Epoch 17/300 -- Iteration 124233 - Batch 1001/7702 - Train loss: 0.00200265  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.51 min) Epoch 17/300 -- Iteration 124310 - Batch 1078/7702 - Train loss: 0.00200462  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.53 min) Epoch 17/300 -- Iteration 124387 - Batch 1155/7702 - Train loss: 0.00200494  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.55 min) Epoch 17/300 -- Iteration 124464 - Batch 1232/7702 - Train loss: 0.00200519  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.57 min) Epoch 17/300 -- Iteration 124541 - Batch 1309/7702 - Train loss: 0.00200832  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.59 min) Epoch 17/300 -- Iteration 124618 - Batch 1386/7702 - Train loss: 0.00200866  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.61 min) Epoch 17/300 -- Iteration 124695 - Batch 1463/7702 - Train loss: 0.00200813  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.63 min) Epoch 17/300 -- Iteration 124772 - Batch 1540/7702 - Train loss: 0.00200950  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.65 min) Epoch 17/300 -- Iteration 124849 - Batch 1617/7702 - Train loss: 0.00201005  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.67 min) Epoch 17/300 -- Iteration 124926 - Batch 1694/7702 - Train loss: 0.00200771  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.69 min) Epoch 17/300 -- Iteration 125003 - Batch 1771/7702 - Train loss: 0.00200748  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.71 min) Epoch 17/300 -- Iteration 125080 - Batch 1848/7702 - Train loss: 0.00200858  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.73 min) Epoch 17/300 -- Iteration 125157 - Batch 1925/7702 - Train loss: 0.00200899  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.75 min) Epoch 17/300 -- Iteration 125234 - Batch 2002/7702 - Train loss: 0.00200814  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.78 min) Epoch 17/300 -- Iteration 125311 - Batch 2079/7702 - Train loss: 0.00200881  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.80 min) Epoch 17/300 -- Iteration 125388 - Batch 2156/7702 - Train loss: 0.00200858  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.82 min) Epoch 17/300 -- Iteration 125465 - Batch 2233/7702 - Train loss: 0.00200878  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.84 min) Epoch 17/300 -- Iteration 125542 - Batch 2310/7702 - Train loss: 0.00200873  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.86 min) Epoch 17/300 -- Iteration 125619 - Batch 2387/7702 - Train loss: 0.00200859  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.88 min) Epoch 17/300 -- Iteration 125696 - Batch 2464/7702 - Train loss: 0.00200998  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.90 min) Epoch 17/300 -- Iteration 125773 - Batch 2541/7702 - Train loss: 0.00200974  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.92 min) Epoch 17/300 -- Iteration 125850 - Batch 2618/7702 - Train loss: 0.00200963  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.94 min) Epoch 17/300 -- Iteration 125927 - Batch 2695/7702 - Train loss: 0.00200949  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.96 min) Epoch 17/300 -- Iteration 126004 - Batch 2772/7702 - Train loss: 0.00201098  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(33.98 min) Epoch 17/300 -- Iteration 126081 - Batch 2849/7702 - Train loss: 0.00201172  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.00 min) Epoch 17/300 -- Iteration 126158 - Batch 2926/7702 - Train loss: 0.00201123  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.02 min) Epoch 17/300 -- Iteration 126235 - Batch 3003/7702 - Train loss: 0.00201253  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.04 min) Epoch 17/300 -- Iteration 126312 - Batch 3080/7702 - Train loss: 0.00201285  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.06 min) Epoch 17/300 -- Iteration 126389 - Batch 3157/7702 - Train loss: 0.00201284  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.08 min) Epoch 17/300 -- Iteration 126466 - Batch 3234/7702 - Train loss: 0.00201278  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.11 min) Epoch 17/300 -- Iteration 126543 - Batch 3311/7702 - Train loss: 0.00201293  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.13 min) Epoch 17/300 -- Iteration 126620 - Batch 3388/7702 - Train loss: 0.00201379  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.15 min) Epoch 17/300 -- Iteration 126697 - Batch 3465/7702 - Train loss: 0.00201384  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.17 min) Epoch 17/300 -- Iteration 126774 - Batch 3542/7702 - Train loss: 0.00201363  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.19 min) Epoch 17/300 -- Iteration 126851 - Batch 3619/7702 - Train loss: 0.00201353  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.21 min) Epoch 17/300 -- Iteration 126928 - Batch 3696/7702 - Train loss: 0.00201298  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.23 min) Epoch 17/300 -- Iteration 127005 - Batch 3773/7702 - Train loss: 0.00201253  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.25 min) Epoch 17/300 -- Iteration 127082 - Batch 3850/7702 - Train loss: 0.00201329  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.27 min) Epoch 17/300 -- Iteration 127159 - Batch 3927/7702 - Train loss: 0.00201353  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.29 min) Epoch 17/300 -- Iteration 127236 - Batch 4004/7702 - Train loss: 0.00201433  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.31 min) Epoch 17/300 -- Iteration 127313 - Batch 4081/7702 - Train loss: 0.00201451  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.33 min) Epoch 17/300 -- Iteration 127390 - Batch 4158/7702 - Train loss: 0.00201415  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.35 min) Epoch 17/300 -- Iteration 127467 - Batch 4235/7702 - Train loss: 0.00201363  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.37 min) Epoch 17/300 -- Iteration 127544 - Batch 4312/7702 - Train loss: 0.00201415  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.39 min) Epoch 17/300 -- Iteration 127621 - Batch 4389/7702 - Train loss: 0.00201463  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.42 min) Epoch 17/300 -- Iteration 127698 - Batch 4466/7702 - Train loss: 0.00201476  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.44 min) Epoch 17/300 -- Iteration 127775 - Batch 4543/7702 - Train loss: 0.00201447  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.46 min) Epoch 17/300 -- Iteration 127852 - Batch 4620/7702 - Train loss: 0.00201448  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.48 min) Epoch 17/300 -- Iteration 127929 - Batch 4697/7702 - Train loss: 0.00201429  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.50 min) Epoch 17/300 -- Iteration 128006 - Batch 4774/7702 - Train loss: 0.00201496  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.52 min) Epoch 17/300 -- Iteration 128083 - Batch 4851/7702 - Train loss: 0.00201489  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.54 min) Epoch 17/300 -- Iteration 128160 - Batch 4928/7702 - Train loss: 0.00201515  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.56 min) Epoch 17/300 -- Iteration 128237 - Batch 5005/7702 - Train loss: 0.00201528  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.58 min) Epoch 17/300 -- Iteration 128314 - Batch 5082/7702 - Train loss: 0.00201560  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.60 min) Epoch 17/300 -- Iteration 128391 - Batch 5159/7702 - Train loss: 0.00201578  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.62 min) Epoch 17/300 -- Iteration 128468 - Batch 5236/7702 - Train loss: 0.00201604  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.64 min) Epoch 17/300 -- Iteration 128545 - Batch 5313/7702 - Train loss: 0.00201600  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.66 min) Epoch 17/300 -- Iteration 128622 - Batch 5390/7702 - Train loss: 0.00201647  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.68 min) Epoch 17/300 -- Iteration 128699 - Batch 5467/7702 - Train loss: 0.00201715  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.70 min) Epoch 17/300 -- Iteration 128776 - Batch 5544/7702 - Train loss: 0.00201681  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.72 min) Epoch 17/300 -- Iteration 128853 - Batch 5621/7702 - Train loss: 0.00201739  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.74 min) Epoch 17/300 -- Iteration 128930 - Batch 5698/7702 - Train loss: 0.00201722  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.77 min) Epoch 17/300 -- Iteration 129007 - Batch 5775/7702 - Train loss: 0.00201735  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.79 min) Epoch 17/300 -- Iteration 129084 - Batch 5852/7702 - Train loss: 0.00201748  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.81 min) Epoch 17/300 -- Iteration 129161 - Batch 5929/7702 - Train loss: 0.00201725  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.83 min) Epoch 17/300 -- Iteration 129238 - Batch 6006/7702 - Train loss: 0.00201746  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.85 min) Epoch 17/300 -- Iteration 129315 - Batch 6083/7702 - Train loss: 0.00201741  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.87 min) Epoch 17/300 -- Iteration 129392 - Batch 6160/7702 - Train loss: 0.00201740  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.89 min) Epoch 17/300 -- Iteration 129469 - Batch 6237/7702 - Train loss: 0.00201698  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.91 min) Epoch 17/300 -- Iteration 129546 - Batch 6314/7702 - Train loss: 0.00201704  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.93 min) Epoch 17/300 -- Iteration 129623 - Batch 6391/7702 - Train loss: 0.00201700  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.95 min) Epoch 17/300 -- Iteration 129700 - Batch 6468/7702 - Train loss: 0.00201711  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.97 min) Epoch 17/300 -- Iteration 129777 - Batch 6545/7702 - Train loss: 0.00201739  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(34.99 min) Epoch 17/300 -- Iteration 129854 - Batch 6622/7702 - Train loss: 0.00201754  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.01 min) Epoch 17/300 -- Iteration 129931 - Batch 6699/7702 - Train loss: 0.00201724  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.03 min) Epoch 17/300 -- Iteration 130008 - Batch 6776/7702 - Train loss: 0.00201747  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.05 min) Epoch 17/300 -- Iteration 130085 - Batch 6853/7702 - Train loss: 0.00201696  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.07 min) Epoch 17/300 -- Iteration 130162 - Batch 6930/7702 - Train loss: 0.00201695  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.09 min) Epoch 17/300 -- Iteration 130239 - Batch 7007/7702 - Train loss: 0.00201703  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.12 min) Epoch 17/300 -- Iteration 130316 - Batch 7084/7702 - Train loss: 0.00201744  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.14 min) Epoch 17/300 -- Iteration 130393 - Batch 7161/7702 - Train loss: 0.00201757  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.16 min) Epoch 17/300 -- Iteration 130470 - Batch 7238/7702 - Train loss: 0.00201749  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.18 min) Epoch 17/300 -- Iteration 130547 - Batch 7315/7702 - Train loss: 0.00201764  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.20 min) Epoch 17/300 -- Iteration 130624 - Batch 7392/7702 - Train loss: 0.00201771  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.22 min) Epoch 17/300 -- Iteration 130701 - Batch 7469/7702 - Train loss: 0.00201757  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.24 min) Epoch 17/300 -- Iteration 130778 - Batch 7546/7702 - Train loss: 0.00201723  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.26 min) Epoch 17/300 -- Iteration 130855 - Batch 7623/7702 - Train loss: 0.00201714  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.28 min) Epoch 17/300 -- Iteration 130932 - Batch 7700/7702 - Train loss: 0.00201710  - Train acc: -0.0000 - Val loss: 0.00056600\n",
      "(35.28 min) Epoch 17/300 -- Iteration 130934 - Batch 7701/7702 - Train loss: 0.00201710  - Train acc: -0.0000 - Val loss: 0.00058791 - Val acc: -0.0000\n",
      "(35.30 min) Epoch 18/300 -- Iteration 131011 - Batch 77/7702 - Train loss: 0.00198810  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.33 min) Epoch 18/300 -- Iteration 131088 - Batch 154/7702 - Train loss: 0.00198621  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.35 min) Epoch 18/300 -- Iteration 131165 - Batch 231/7702 - Train loss: 0.00200413  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.37 min) Epoch 18/300 -- Iteration 131242 - Batch 308/7702 - Train loss: 0.00201062  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.39 min) Epoch 18/300 -- Iteration 131319 - Batch 385/7702 - Train loss: 0.00200955  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.41 min) Epoch 18/300 -- Iteration 131396 - Batch 462/7702 - Train loss: 0.00200982  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.43 min) Epoch 18/300 -- Iteration 131473 - Batch 539/7702 - Train loss: 0.00200617  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.45 min) Epoch 18/300 -- Iteration 131550 - Batch 616/7702 - Train loss: 0.00200903  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.47 min) Epoch 18/300 -- Iteration 131627 - Batch 693/7702 - Train loss: 0.00200821  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.49 min) Epoch 18/300 -- Iteration 131704 - Batch 770/7702 - Train loss: 0.00200856  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.51 min) Epoch 18/300 -- Iteration 131781 - Batch 847/7702 - Train loss: 0.00201109  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.53 min) Epoch 18/300 -- Iteration 131858 - Batch 924/7702 - Train loss: 0.00201264  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.55 min) Epoch 18/300 -- Iteration 131935 - Batch 1001/7702 - Train loss: 0.00201522  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.57 min) Epoch 18/300 -- Iteration 132012 - Batch 1078/7702 - Train loss: 0.00201515  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.59 min) Epoch 18/300 -- Iteration 132089 - Batch 1155/7702 - Train loss: 0.00201208  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.61 min) Epoch 18/300 -- Iteration 132166 - Batch 1232/7702 - Train loss: 0.00201122  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.63 min) Epoch 18/300 -- Iteration 132243 - Batch 1309/7702 - Train loss: 0.00201149  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.66 min) Epoch 18/300 -- Iteration 132320 - Batch 1386/7702 - Train loss: 0.00201394  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.68 min) Epoch 18/300 -- Iteration 132397 - Batch 1463/7702 - Train loss: 0.00201282  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.70 min) Epoch 18/300 -- Iteration 132474 - Batch 1540/7702 - Train loss: 0.00201352  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.72 min) Epoch 18/300 -- Iteration 132551 - Batch 1617/7702 - Train loss: 0.00201334  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.74 min) Epoch 18/300 -- Iteration 132628 - Batch 1694/7702 - Train loss: 0.00201300  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.76 min) Epoch 18/300 -- Iteration 132705 - Batch 1771/7702 - Train loss: 0.00201368  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.78 min) Epoch 18/300 -- Iteration 132782 - Batch 1848/7702 - Train loss: 0.00201347  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.80 min) Epoch 18/300 -- Iteration 132859 - Batch 1925/7702 - Train loss: 0.00201428  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.82 min) Epoch 18/300 -- Iteration 132936 - Batch 2002/7702 - Train loss: 0.00201395  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.84 min) Epoch 18/300 -- Iteration 133013 - Batch 2079/7702 - Train loss: 0.00201445  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.86 min) Epoch 18/300 -- Iteration 133090 - Batch 2156/7702 - Train loss: 0.00201417  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.88 min) Epoch 18/300 -- Iteration 133167 - Batch 2233/7702 - Train loss: 0.00201432  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.90 min) Epoch 18/300 -- Iteration 133244 - Batch 2310/7702 - Train loss: 0.00201478  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.92 min) Epoch 18/300 -- Iteration 133321 - Batch 2387/7702 - Train loss: 0.00201530  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.94 min) Epoch 18/300 -- Iteration 133398 - Batch 2464/7702 - Train loss: 0.00201448  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.96 min) Epoch 18/300 -- Iteration 133475 - Batch 2541/7702 - Train loss: 0.00201412  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(35.99 min) Epoch 18/300 -- Iteration 133552 - Batch 2618/7702 - Train loss: 0.00201434  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.01 min) Epoch 18/300 -- Iteration 133629 - Batch 2695/7702 - Train loss: 0.00201479  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.03 min) Epoch 18/300 -- Iteration 133706 - Batch 2772/7702 - Train loss: 0.00201449  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.05 min) Epoch 18/300 -- Iteration 133783 - Batch 2849/7702 - Train loss: 0.00201538  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.07 min) Epoch 18/300 -- Iteration 133860 - Batch 2926/7702 - Train loss: 0.00201526  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.09 min) Epoch 18/300 -- Iteration 133937 - Batch 3003/7702 - Train loss: 0.00201520  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.11 min) Epoch 18/300 -- Iteration 134014 - Batch 3080/7702 - Train loss: 0.00201506  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.13 min) Epoch 18/300 -- Iteration 134091 - Batch 3157/7702 - Train loss: 0.00201605  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.15 min) Epoch 18/300 -- Iteration 134168 - Batch 3234/7702 - Train loss: 0.00201649  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.17 min) Epoch 18/300 -- Iteration 134245 - Batch 3311/7702 - Train loss: 0.00201636  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.19 min) Epoch 18/300 -- Iteration 134322 - Batch 3388/7702 - Train loss: 0.00201699  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.21 min) Epoch 18/300 -- Iteration 134399 - Batch 3465/7702 - Train loss: 0.00201788  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.23 min) Epoch 18/300 -- Iteration 134476 - Batch 3542/7702 - Train loss: 0.00201841  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.25 min) Epoch 18/300 -- Iteration 134553 - Batch 3619/7702 - Train loss: 0.00201799  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.27 min) Epoch 18/300 -- Iteration 134630 - Batch 3696/7702 - Train loss: 0.00201776  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.30 min) Epoch 18/300 -- Iteration 134707 - Batch 3773/7702 - Train loss: 0.00201780  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.32 min) Epoch 18/300 -- Iteration 134784 - Batch 3850/7702 - Train loss: 0.00201742  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.34 min) Epoch 18/300 -- Iteration 134861 - Batch 3927/7702 - Train loss: 0.00201681  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.36 min) Epoch 18/300 -- Iteration 134938 - Batch 4004/7702 - Train loss: 0.00201645  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.38 min) Epoch 18/300 -- Iteration 135015 - Batch 4081/7702 - Train loss: 0.00201652  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.40 min) Epoch 18/300 -- Iteration 135092 - Batch 4158/7702 - Train loss: 0.00201687  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.42 min) Epoch 18/300 -- Iteration 135169 - Batch 4235/7702 - Train loss: 0.00201699  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.44 min) Epoch 18/300 -- Iteration 135246 - Batch 4312/7702 - Train loss: 0.00201698  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.46 min) Epoch 18/300 -- Iteration 135323 - Batch 4389/7702 - Train loss: 0.00201718  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.48 min) Epoch 18/300 -- Iteration 135400 - Batch 4466/7702 - Train loss: 0.00201682  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.50 min) Epoch 18/300 -- Iteration 135477 - Batch 4543/7702 - Train loss: 0.00201676  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.52 min) Epoch 18/300 -- Iteration 135554 - Batch 4620/7702 - Train loss: 0.00201630  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.54 min) Epoch 18/300 -- Iteration 135631 - Batch 4697/7702 - Train loss: 0.00201627  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.56 min) Epoch 18/300 -- Iteration 135708 - Batch 4774/7702 - Train loss: 0.00201643  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.58 min) Epoch 18/300 -- Iteration 135785 - Batch 4851/7702 - Train loss: 0.00201631  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.61 min) Epoch 18/300 -- Iteration 135862 - Batch 4928/7702 - Train loss: 0.00201622  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.63 min) Epoch 18/300 -- Iteration 135939 - Batch 5005/7702 - Train loss: 0.00201567  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.65 min) Epoch 18/300 -- Iteration 136016 - Batch 5082/7702 - Train loss: 0.00201547  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.67 min) Epoch 18/300 -- Iteration 136093 - Batch 5159/7702 - Train loss: 0.00201524  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.69 min) Epoch 18/300 -- Iteration 136170 - Batch 5236/7702 - Train loss: 0.00201526  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.71 min) Epoch 18/300 -- Iteration 136247 - Batch 5313/7702 - Train loss: 0.00201502  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.73 min) Epoch 18/300 -- Iteration 136324 - Batch 5390/7702 - Train loss: 0.00201431  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.75 min) Epoch 18/300 -- Iteration 136401 - Batch 5467/7702 - Train loss: 0.00201423  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.77 min) Epoch 18/300 -- Iteration 136478 - Batch 5544/7702 - Train loss: 0.00201410  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.79 min) Epoch 18/300 -- Iteration 136555 - Batch 5621/7702 - Train loss: 0.00201372  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.81 min) Epoch 18/300 -- Iteration 136632 - Batch 5698/7702 - Train loss: 0.00201405  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.83 min) Epoch 18/300 -- Iteration 136709 - Batch 5775/7702 - Train loss: 0.00201375  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.85 min) Epoch 18/300 -- Iteration 136786 - Batch 5852/7702 - Train loss: 0.00201405  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.87 min) Epoch 18/300 -- Iteration 136863 - Batch 5929/7702 - Train loss: 0.00201386  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.89 min) Epoch 18/300 -- Iteration 136940 - Batch 6006/7702 - Train loss: 0.00201379  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.92 min) Epoch 18/300 -- Iteration 137017 - Batch 6083/7702 - Train loss: 0.00201395  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.94 min) Epoch 18/300 -- Iteration 137094 - Batch 6160/7702 - Train loss: 0.00201367  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.96 min) Epoch 18/300 -- Iteration 137171 - Batch 6237/7702 - Train loss: 0.00201320  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(36.98 min) Epoch 18/300 -- Iteration 137248 - Batch 6314/7702 - Train loss: 0.00201258  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.00 min) Epoch 18/300 -- Iteration 137325 - Batch 6391/7702 - Train loss: 0.00201272  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.02 min) Epoch 18/300 -- Iteration 137402 - Batch 6468/7702 - Train loss: 0.00201251  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.04 min) Epoch 18/300 -- Iteration 137479 - Batch 6545/7702 - Train loss: 0.00201284  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.06 min) Epoch 18/300 -- Iteration 137556 - Batch 6622/7702 - Train loss: 0.00201229  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.08 min) Epoch 18/300 -- Iteration 137633 - Batch 6699/7702 - Train loss: 0.00201198  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.10 min) Epoch 18/300 -- Iteration 137710 - Batch 6776/7702 - Train loss: 0.00201191  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.12 min) Epoch 18/300 -- Iteration 137787 - Batch 6853/7702 - Train loss: 0.00201139  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.14 min) Epoch 18/300 -- Iteration 137864 - Batch 6930/7702 - Train loss: 0.00201176  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.16 min) Epoch 18/300 -- Iteration 137941 - Batch 7007/7702 - Train loss: 0.00201176  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.18 min) Epoch 18/300 -- Iteration 138018 - Batch 7084/7702 - Train loss: 0.00201179  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.20 min) Epoch 18/300 -- Iteration 138095 - Batch 7161/7702 - Train loss: 0.00201125  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.23 min) Epoch 18/300 -- Iteration 138172 - Batch 7238/7702 - Train loss: 0.00201158  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.25 min) Epoch 18/300 -- Iteration 138249 - Batch 7315/7702 - Train loss: 0.00201139  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.27 min) Epoch 18/300 -- Iteration 138326 - Batch 7392/7702 - Train loss: 0.00201131  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.29 min) Epoch 18/300 -- Iteration 138403 - Batch 7469/7702 - Train loss: 0.00201147  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.31 min) Epoch 18/300 -- Iteration 138480 - Batch 7546/7702 - Train loss: 0.00201141  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.33 min) Epoch 18/300 -- Iteration 138557 - Batch 7623/7702 - Train loss: 0.00201154  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.35 min) Epoch 18/300 -- Iteration 138634 - Batch 7700/7702 - Train loss: 0.00201123  - Train acc: -0.0000 - Val loss: 0.00058791\n",
      "(37.35 min) Epoch 18/300 -- Iteration 138636 - Batch 7701/7702 - Train loss: 0.00201123  - Train acc: -0.0000 - Val loss: 0.00055343 - Val acc: -0.0000\n",
      "(37.37 min) Epoch 19/300 -- Iteration 138713 - Batch 77/7702 - Train loss: 0.00199435  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.39 min) Epoch 19/300 -- Iteration 138790 - Batch 154/7702 - Train loss: 0.00199796  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.41 min) Epoch 19/300 -- Iteration 138867 - Batch 231/7702 - Train loss: 0.00199912  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.43 min) Epoch 19/300 -- Iteration 138944 - Batch 308/7702 - Train loss: 0.00200441  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.46 min) Epoch 19/300 -- Iteration 139021 - Batch 385/7702 - Train loss: 0.00200861  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.48 min) Epoch 19/300 -- Iteration 139098 - Batch 462/7702 - Train loss: 0.00201101  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.50 min) Epoch 19/300 -- Iteration 139175 - Batch 539/7702 - Train loss: 0.00200701  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.52 min) Epoch 19/300 -- Iteration 139252 - Batch 616/7702 - Train loss: 0.00201029  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.54 min) Epoch 19/300 -- Iteration 139329 - Batch 693/7702 - Train loss: 0.00201001  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.56 min) Epoch 19/300 -- Iteration 139406 - Batch 770/7702 - Train loss: 0.00201251  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.58 min) Epoch 19/300 -- Iteration 139483 - Batch 847/7702 - Train loss: 0.00201015  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.60 min) Epoch 19/300 -- Iteration 139560 - Batch 924/7702 - Train loss: 0.00201063  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.62 min) Epoch 19/300 -- Iteration 139637 - Batch 1001/7702 - Train loss: 0.00201001  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.64 min) Epoch 19/300 -- Iteration 139714 - Batch 1078/7702 - Train loss: 0.00201046  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.66 min) Epoch 19/300 -- Iteration 139791 - Batch 1155/7702 - Train loss: 0.00201136  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.68 min) Epoch 19/300 -- Iteration 139868 - Batch 1232/7702 - Train loss: 0.00201238  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.70 min) Epoch 19/300 -- Iteration 139945 - Batch 1309/7702 - Train loss: 0.00201266  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.72 min) Epoch 19/300 -- Iteration 140022 - Batch 1386/7702 - Train loss: 0.00201355  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.74 min) Epoch 19/300 -- Iteration 140099 - Batch 1463/7702 - Train loss: 0.00201331  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.76 min) Epoch 19/300 -- Iteration 140176 - Batch 1540/7702 - Train loss: 0.00201207  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.78 min) Epoch 19/300 -- Iteration 140253 - Batch 1617/7702 - Train loss: 0.00201174  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.81 min) Epoch 19/300 -- Iteration 140330 - Batch 1694/7702 - Train loss: 0.00201292  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.83 min) Epoch 19/300 -- Iteration 140407 - Batch 1771/7702 - Train loss: 0.00201401  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.85 min) Epoch 19/300 -- Iteration 140484 - Batch 1848/7702 - Train loss: 0.00201373  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.87 min) Epoch 19/300 -- Iteration 140561 - Batch 1925/7702 - Train loss: 0.00201429  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.89 min) Epoch 19/300 -- Iteration 140638 - Batch 2002/7702 - Train loss: 0.00201439  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.91 min) Epoch 19/300 -- Iteration 140715 - Batch 2079/7702 - Train loss: 0.00201320  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.93 min) Epoch 19/300 -- Iteration 140792 - Batch 2156/7702 - Train loss: 0.00201242  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.95 min) Epoch 19/300 -- Iteration 140869 - Batch 2233/7702 - Train loss: 0.00201261  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.97 min) Epoch 19/300 -- Iteration 140946 - Batch 2310/7702 - Train loss: 0.00201209  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(37.99 min) Epoch 19/300 -- Iteration 141023 - Batch 2387/7702 - Train loss: 0.00201190  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.01 min) Epoch 19/300 -- Iteration 141100 - Batch 2464/7702 - Train loss: 0.00201201  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.03 min) Epoch 19/300 -- Iteration 141177 - Batch 2541/7702 - Train loss: 0.00201117  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.05 min) Epoch 19/300 -- Iteration 141254 - Batch 2618/7702 - Train loss: 0.00201067  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.07 min) Epoch 19/300 -- Iteration 141331 - Batch 2695/7702 - Train loss: 0.00201073  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.09 min) Epoch 19/300 -- Iteration 141408 - Batch 2772/7702 - Train loss: 0.00200914  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.12 min) Epoch 19/300 -- Iteration 141485 - Batch 2849/7702 - Train loss: 0.00200883  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.14 min) Epoch 19/300 -- Iteration 141562 - Batch 2926/7702 - Train loss: 0.00200899  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.16 min) Epoch 19/300 -- Iteration 141639 - Batch 3003/7702 - Train loss: 0.00200934  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.18 min) Epoch 19/300 -- Iteration 141716 - Batch 3080/7702 - Train loss: 0.00200946  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.20 min) Epoch 19/300 -- Iteration 141793 - Batch 3157/7702 - Train loss: 0.00200998  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.22 min) Epoch 19/300 -- Iteration 141870 - Batch 3234/7702 - Train loss: 0.00200927  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.24 min) Epoch 19/300 -- Iteration 141947 - Batch 3311/7702 - Train loss: 0.00200846  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.26 min) Epoch 19/300 -- Iteration 142024 - Batch 3388/7702 - Train loss: 0.00201000  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.28 min) Epoch 19/300 -- Iteration 142101 - Batch 3465/7702 - Train loss: 0.00200935  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.30 min) Epoch 19/300 -- Iteration 142178 - Batch 3542/7702 - Train loss: 0.00200935  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.32 min) Epoch 19/300 -- Iteration 142255 - Batch 3619/7702 - Train loss: 0.00200861  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.34 min) Epoch 19/300 -- Iteration 142332 - Batch 3696/7702 - Train loss: 0.00200774  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.36 min) Epoch 19/300 -- Iteration 142409 - Batch 3773/7702 - Train loss: 0.00200686  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.38 min) Epoch 19/300 -- Iteration 142486 - Batch 3850/7702 - Train loss: 0.00200744  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.41 min) Epoch 19/300 -- Iteration 142563 - Batch 3927/7702 - Train loss: 0.00200692  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.43 min) Epoch 19/300 -- Iteration 142640 - Batch 4004/7702 - Train loss: 0.00200700  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.45 min) Epoch 19/300 -- Iteration 142717 - Batch 4081/7702 - Train loss: 0.00200759  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.47 min) Epoch 19/300 -- Iteration 142794 - Batch 4158/7702 - Train loss: 0.00200712  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.49 min) Epoch 19/300 -- Iteration 142871 - Batch 4235/7702 - Train loss: 0.00200687  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.51 min) Epoch 19/300 -- Iteration 142948 - Batch 4312/7702 - Train loss: 0.00200643  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.53 min) Epoch 19/300 -- Iteration 143025 - Batch 4389/7702 - Train loss: 0.00200614  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.55 min) Epoch 19/300 -- Iteration 143102 - Batch 4466/7702 - Train loss: 0.00200636  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.57 min) Epoch 19/300 -- Iteration 143179 - Batch 4543/7702 - Train loss: 0.00200590  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.59 min) Epoch 19/300 -- Iteration 143256 - Batch 4620/7702 - Train loss: 0.00200591  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.61 min) Epoch 19/300 -- Iteration 143333 - Batch 4697/7702 - Train loss: 0.00200621  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.63 min) Epoch 19/300 -- Iteration 143410 - Batch 4774/7702 - Train loss: 0.00200662  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.65 min) Epoch 19/300 -- Iteration 143487 - Batch 4851/7702 - Train loss: 0.00200619  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.67 min) Epoch 19/300 -- Iteration 143564 - Batch 4928/7702 - Train loss: 0.00200705  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.69 min) Epoch 19/300 -- Iteration 143641 - Batch 5005/7702 - Train loss: 0.00200718  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.72 min) Epoch 19/300 -- Iteration 143718 - Batch 5082/7702 - Train loss: 0.00200738  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.74 min) Epoch 19/300 -- Iteration 143795 - Batch 5159/7702 - Train loss: 0.00200747  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.76 min) Epoch 19/300 -- Iteration 143872 - Batch 5236/7702 - Train loss: 0.00200687  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.78 min) Epoch 19/300 -- Iteration 143949 - Batch 5313/7702 - Train loss: 0.00200680  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.80 min) Epoch 19/300 -- Iteration 144026 - Batch 5390/7702 - Train loss: 0.00200676  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.82 min) Epoch 19/300 -- Iteration 144103 - Batch 5467/7702 - Train loss: 0.00200740  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.84 min) Epoch 19/300 -- Iteration 144180 - Batch 5544/7702 - Train loss: 0.00200696  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.86 min) Epoch 19/300 -- Iteration 144257 - Batch 5621/7702 - Train loss: 0.00200674  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.88 min) Epoch 19/300 -- Iteration 144334 - Batch 5698/7702 - Train loss: 0.00200662  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.90 min) Epoch 19/300 -- Iteration 144411 - Batch 5775/7702 - Train loss: 0.00200650  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.92 min) Epoch 19/300 -- Iteration 144488 - Batch 5852/7702 - Train loss: 0.00200658  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.94 min) Epoch 19/300 -- Iteration 144565 - Batch 5929/7702 - Train loss: 0.00200628  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.96 min) Epoch 19/300 -- Iteration 144642 - Batch 6006/7702 - Train loss: 0.00200614  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(38.98 min) Epoch 19/300 -- Iteration 144719 - Batch 6083/7702 - Train loss: 0.00200593  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.00 min) Epoch 19/300 -- Iteration 144796 - Batch 6160/7702 - Train loss: 0.00200575  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.03 min) Epoch 19/300 -- Iteration 144873 - Batch 6237/7702 - Train loss: 0.00200561  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.05 min) Epoch 19/300 -- Iteration 144950 - Batch 6314/7702 - Train loss: 0.00200531  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.07 min) Epoch 19/300 -- Iteration 145027 - Batch 6391/7702 - Train loss: 0.00200544  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.09 min) Epoch 19/300 -- Iteration 145104 - Batch 6468/7702 - Train loss: 0.00200500  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.11 min) Epoch 19/300 -- Iteration 145181 - Batch 6545/7702 - Train loss: 0.00200469  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.13 min) Epoch 19/300 -- Iteration 145258 - Batch 6622/7702 - Train loss: 0.00200509  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.15 min) Epoch 19/300 -- Iteration 145335 - Batch 6699/7702 - Train loss: 0.00200490  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.17 min) Epoch 19/300 -- Iteration 145412 - Batch 6776/7702 - Train loss: 0.00200534  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.19 min) Epoch 19/300 -- Iteration 145489 - Batch 6853/7702 - Train loss: 0.00200583  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.21 min) Epoch 19/300 -- Iteration 145566 - Batch 6930/7702 - Train loss: 0.00200580  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.23 min) Epoch 19/300 -- Iteration 145643 - Batch 7007/7702 - Train loss: 0.00200553  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.25 min) Epoch 19/300 -- Iteration 145720 - Batch 7084/7702 - Train loss: 0.00200574  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.27 min) Epoch 19/300 -- Iteration 145797 - Batch 7161/7702 - Train loss: 0.00200539  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.29 min) Epoch 19/300 -- Iteration 145874 - Batch 7238/7702 - Train loss: 0.00200539  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.31 min) Epoch 19/300 -- Iteration 145951 - Batch 7315/7702 - Train loss: 0.00200553  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.33 min) Epoch 19/300 -- Iteration 146028 - Batch 7392/7702 - Train loss: 0.00200590  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.36 min) Epoch 19/300 -- Iteration 146105 - Batch 7469/7702 - Train loss: 0.00200602  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.38 min) Epoch 19/300 -- Iteration 146182 - Batch 7546/7702 - Train loss: 0.00200647  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.40 min) Epoch 19/300 -- Iteration 146259 - Batch 7623/7702 - Train loss: 0.00200685  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.42 min) Epoch 19/300 -- Iteration 146336 - Batch 7700/7702 - Train loss: 0.00200705  - Train acc: -0.0000 - Val loss: 0.00055343\n",
      "(39.42 min) Epoch 19/300 -- Iteration 146338 - Batch 7701/7702 - Train loss: 0.00200709  - Train acc: -0.0000 - Val loss: 0.00054013 - Val acc: -0.0000\n",
      "(39.44 min) Epoch 20/300 -- Iteration 146415 - Batch 77/7702 - Train loss: 0.00203042  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.46 min) Epoch 20/300 -- Iteration 146492 - Batch 154/7702 - Train loss: 0.00200539  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.48 min) Epoch 20/300 -- Iteration 146569 - Batch 231/7702 - Train loss: 0.00200521  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.50 min) Epoch 20/300 -- Iteration 146646 - Batch 308/7702 - Train loss: 0.00199146  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.52 min) Epoch 20/300 -- Iteration 146723 - Batch 385/7702 - Train loss: 0.00199029  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.54 min) Epoch 20/300 -- Iteration 146800 - Batch 462/7702 - Train loss: 0.00199377  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.56 min) Epoch 20/300 -- Iteration 146877 - Batch 539/7702 - Train loss: 0.00199550  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.59 min) Epoch 20/300 -- Iteration 146954 - Batch 616/7702 - Train loss: 0.00199509  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.61 min) Epoch 20/300 -- Iteration 147031 - Batch 693/7702 - Train loss: 0.00199549  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.63 min) Epoch 20/300 -- Iteration 147108 - Batch 770/7702 - Train loss: 0.00199530  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.65 min) Epoch 20/300 -- Iteration 147185 - Batch 847/7702 - Train loss: 0.00199619  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.67 min) Epoch 20/300 -- Iteration 147262 - Batch 924/7702 - Train loss: 0.00199743  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.69 min) Epoch 20/300 -- Iteration 147339 - Batch 1001/7702 - Train loss: 0.00200091  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.71 min) Epoch 20/300 -- Iteration 147416 - Batch 1078/7702 - Train loss: 0.00200083  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.73 min) Epoch 20/300 -- Iteration 147493 - Batch 1155/7702 - Train loss: 0.00200127  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.75 min) Epoch 20/300 -- Iteration 147570 - Batch 1232/7702 - Train loss: 0.00199869  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.77 min) Epoch 20/300 -- Iteration 147647 - Batch 1309/7702 - Train loss: 0.00199740  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.79 min) Epoch 20/300 -- Iteration 147724 - Batch 1386/7702 - Train loss: 0.00199877  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.81 min) Epoch 20/300 -- Iteration 147801 - Batch 1463/7702 - Train loss: 0.00200147  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.83 min) Epoch 20/300 -- Iteration 147878 - Batch 1540/7702 - Train loss: 0.00199960  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.85 min) Epoch 20/300 -- Iteration 147955 - Batch 1617/7702 - Train loss: 0.00199880  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.88 min) Epoch 20/300 -- Iteration 148032 - Batch 1694/7702 - Train loss: 0.00199845  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.90 min) Epoch 20/300 -- Iteration 148109 - Batch 1771/7702 - Train loss: 0.00199950  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.92 min) Epoch 20/300 -- Iteration 148186 - Batch 1848/7702 - Train loss: 0.00199979  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.94 min) Epoch 20/300 -- Iteration 148263 - Batch 1925/7702 - Train loss: 0.00200184  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.96 min) Epoch 20/300 -- Iteration 148340 - Batch 2002/7702 - Train loss: 0.00200348  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(39.98 min) Epoch 20/300 -- Iteration 148417 - Batch 2079/7702 - Train loss: 0.00200456  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.00 min) Epoch 20/300 -- Iteration 148494 - Batch 2156/7702 - Train loss: 0.00200516  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.02 min) Epoch 20/300 -- Iteration 148571 - Batch 2233/7702 - Train loss: 0.00200552  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.04 min) Epoch 20/300 -- Iteration 148648 - Batch 2310/7702 - Train loss: 0.00200605  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.06 min) Epoch 20/300 -- Iteration 148725 - Batch 2387/7702 - Train loss: 0.00200614  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.08 min) Epoch 20/300 -- Iteration 148802 - Batch 2464/7702 - Train loss: 0.00200573  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.10 min) Epoch 20/300 -- Iteration 148879 - Batch 2541/7702 - Train loss: 0.00200716  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.12 min) Epoch 20/300 -- Iteration 148956 - Batch 2618/7702 - Train loss: 0.00200717  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.14 min) Epoch 20/300 -- Iteration 149033 - Batch 2695/7702 - Train loss: 0.00200653  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.16 min) Epoch 20/300 -- Iteration 149110 - Batch 2772/7702 - Train loss: 0.00200695  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.18 min) Epoch 20/300 -- Iteration 149187 - Batch 2849/7702 - Train loss: 0.00200583  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.21 min) Epoch 20/300 -- Iteration 149264 - Batch 2926/7702 - Train loss: 0.00200508  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.23 min) Epoch 20/300 -- Iteration 149341 - Batch 3003/7702 - Train loss: 0.00200521  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.25 min) Epoch 20/300 -- Iteration 149418 - Batch 3080/7702 - Train loss: 0.00200542  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.27 min) Epoch 20/300 -- Iteration 149495 - Batch 3157/7702 - Train loss: 0.00200579  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.29 min) Epoch 20/300 -- Iteration 149572 - Batch 3234/7702 - Train loss: 0.00200559  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.31 min) Epoch 20/300 -- Iteration 149649 - Batch 3311/7702 - Train loss: 0.00200520  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.33 min) Epoch 20/300 -- Iteration 149726 - Batch 3388/7702 - Train loss: 0.00200533  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.35 min) Epoch 20/300 -- Iteration 149803 - Batch 3465/7702 - Train loss: 0.00200511  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.37 min) Epoch 20/300 -- Iteration 149880 - Batch 3542/7702 - Train loss: 0.00200464  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.39 min) Epoch 20/300 -- Iteration 149957 - Batch 3619/7702 - Train loss: 0.00200389  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.41 min) Epoch 20/300 -- Iteration 150034 - Batch 3696/7702 - Train loss: 0.00200283  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.43 min) Epoch 20/300 -- Iteration 150111 - Batch 3773/7702 - Train loss: 0.00200303  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.45 min) Epoch 20/300 -- Iteration 150188 - Batch 3850/7702 - Train loss: 0.00200277  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.47 min) Epoch 20/300 -- Iteration 150265 - Batch 3927/7702 - Train loss: 0.00200163  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.49 min) Epoch 20/300 -- Iteration 150342 - Batch 4004/7702 - Train loss: 0.00200118  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.52 min) Epoch 20/300 -- Iteration 150419 - Batch 4081/7702 - Train loss: 0.00200106  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.54 min) Epoch 20/300 -- Iteration 150496 - Batch 4158/7702 - Train loss: 0.00200027  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.56 min) Epoch 20/300 -- Iteration 150573 - Batch 4235/7702 - Train loss: 0.00200081  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.58 min) Epoch 20/300 -- Iteration 150650 - Batch 4312/7702 - Train loss: 0.00200070  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.60 min) Epoch 20/300 -- Iteration 150727 - Batch 4389/7702 - Train loss: 0.00200002  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.62 min) Epoch 20/300 -- Iteration 150804 - Batch 4466/7702 - Train loss: 0.00200011  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.64 min) Epoch 20/300 -- Iteration 150881 - Batch 4543/7702 - Train loss: 0.00200024  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.66 min) Epoch 20/300 -- Iteration 150958 - Batch 4620/7702 - Train loss: 0.00200055  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.68 min) Epoch 20/300 -- Iteration 151035 - Batch 4697/7702 - Train loss: 0.00200091  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.70 min) Epoch 20/300 -- Iteration 151112 - Batch 4774/7702 - Train loss: 0.00200117  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.72 min) Epoch 20/300 -- Iteration 151189 - Batch 4851/7702 - Train loss: 0.00200171  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.74 min) Epoch 20/300 -- Iteration 151266 - Batch 4928/7702 - Train loss: 0.00200191  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.76 min) Epoch 20/300 -- Iteration 151343 - Batch 5005/7702 - Train loss: 0.00200185  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.78 min) Epoch 20/300 -- Iteration 151420 - Batch 5082/7702 - Train loss: 0.00200185  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.80 min) Epoch 20/300 -- Iteration 151497 - Batch 5159/7702 - Train loss: 0.00200221  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.82 min) Epoch 20/300 -- Iteration 151574 - Batch 5236/7702 - Train loss: 0.00200175  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.84 min) Epoch 20/300 -- Iteration 151651 - Batch 5313/7702 - Train loss: 0.00200160  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.87 min) Epoch 20/300 -- Iteration 151728 - Batch 5390/7702 - Train loss: 0.00200174  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.89 min) Epoch 20/300 -- Iteration 151805 - Batch 5467/7702 - Train loss: 0.00200122  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.91 min) Epoch 20/300 -- Iteration 151882 - Batch 5544/7702 - Train loss: 0.00200116  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.93 min) Epoch 20/300 -- Iteration 151959 - Batch 5621/7702 - Train loss: 0.00200102  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.95 min) Epoch 20/300 -- Iteration 152036 - Batch 5698/7702 - Train loss: 0.00200060  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.97 min) Epoch 20/300 -- Iteration 152113 - Batch 5775/7702 - Train loss: 0.00200085  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(40.99 min) Epoch 20/300 -- Iteration 152190 - Batch 5852/7702 - Train loss: 0.00200043  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.01 min) Epoch 20/300 -- Iteration 152267 - Batch 5929/7702 - Train loss: 0.00200036  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.03 min) Epoch 20/300 -- Iteration 152344 - Batch 6006/7702 - Train loss: 0.00200004  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.05 min) Epoch 20/300 -- Iteration 152421 - Batch 6083/7702 - Train loss: 0.00199981  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.07 min) Epoch 20/300 -- Iteration 152498 - Batch 6160/7702 - Train loss: 0.00200029  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.09 min) Epoch 20/300 -- Iteration 152575 - Batch 6237/7702 - Train loss: 0.00200067  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.11 min) Epoch 20/300 -- Iteration 152652 - Batch 6314/7702 - Train loss: 0.00200030  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.13 min) Epoch 20/300 -- Iteration 152729 - Batch 6391/7702 - Train loss: 0.00200013  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.15 min) Epoch 20/300 -- Iteration 152806 - Batch 6468/7702 - Train loss: 0.00200015  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.18 min) Epoch 20/300 -- Iteration 152883 - Batch 6545/7702 - Train loss: 0.00200052  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.20 min) Epoch 20/300 -- Iteration 152960 - Batch 6622/7702 - Train loss: 0.00200061  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.22 min) Epoch 20/300 -- Iteration 153037 - Batch 6699/7702 - Train loss: 0.00200063  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.24 min) Epoch 20/300 -- Iteration 153114 - Batch 6776/7702 - Train loss: 0.00200106  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.26 min) Epoch 20/300 -- Iteration 153191 - Batch 6853/7702 - Train loss: 0.00200063  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.28 min) Epoch 20/300 -- Iteration 153268 - Batch 6930/7702 - Train loss: 0.00200066  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.30 min) Epoch 20/300 -- Iteration 153345 - Batch 7007/7702 - Train loss: 0.00200059  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.32 min) Epoch 20/300 -- Iteration 153422 - Batch 7084/7702 - Train loss: 0.00200056  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.34 min) Epoch 20/300 -- Iteration 153499 - Batch 7161/7702 - Train loss: 0.00200082  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.36 min) Epoch 20/300 -- Iteration 153576 - Batch 7238/7702 - Train loss: 0.00200066  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.38 min) Epoch 20/300 -- Iteration 153653 - Batch 7315/7702 - Train loss: 0.00200024  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.40 min) Epoch 20/300 -- Iteration 153730 - Batch 7392/7702 - Train loss: 0.00200010  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.42 min) Epoch 20/300 -- Iteration 153807 - Batch 7469/7702 - Train loss: 0.00199988  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.44 min) Epoch 20/300 -- Iteration 153884 - Batch 7546/7702 - Train loss: 0.00200014  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.46 min) Epoch 20/300 -- Iteration 153961 - Batch 7623/7702 - Train loss: 0.00200062  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.49 min) Epoch 20/300 -- Iteration 154038 - Batch 7700/7702 - Train loss: 0.00200052  - Train acc: -0.0000 - Val loss: 0.00054013\n",
      "(41.49 min) Epoch 20/300 -- Iteration 154040 - Batch 7701/7702 - Train loss: 0.00200055  - Train acc: -0.0000 - Val loss: 0.00055527 - Val acc: -0.0000\n",
      "(41.51 min) Epoch 21/300 -- Iteration 154117 - Batch 77/7702 - Train loss: 0.00198571  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.53 min) Epoch 21/300 -- Iteration 154194 - Batch 154/7702 - Train loss: 0.00200579  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.55 min) Epoch 21/300 -- Iteration 154271 - Batch 231/7702 - Train loss: 0.00201052  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.57 min) Epoch 21/300 -- Iteration 154348 - Batch 308/7702 - Train loss: 0.00200244  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.59 min) Epoch 21/300 -- Iteration 154425 - Batch 385/7702 - Train loss: 0.00200782  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.61 min) Epoch 21/300 -- Iteration 154502 - Batch 462/7702 - Train loss: 0.00201091  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.63 min) Epoch 21/300 -- Iteration 154579 - Batch 539/7702 - Train loss: 0.00200823  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.65 min) Epoch 21/300 -- Iteration 154656 - Batch 616/7702 - Train loss: 0.00201095  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.67 min) Epoch 21/300 -- Iteration 154733 - Batch 693/7702 - Train loss: 0.00200850  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.69 min) Epoch 21/300 -- Iteration 154810 - Batch 770/7702 - Train loss: 0.00200711  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.72 min) Epoch 21/300 -- Iteration 154887 - Batch 847/7702 - Train loss: 0.00201003  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.74 min) Epoch 21/300 -- Iteration 154964 - Batch 924/7702 - Train loss: 0.00200987  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.76 min) Epoch 21/300 -- Iteration 155041 - Batch 1001/7702 - Train loss: 0.00200962  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.78 min) Epoch 21/300 -- Iteration 155118 - Batch 1078/7702 - Train loss: 0.00200728  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.80 min) Epoch 21/300 -- Iteration 155195 - Batch 1155/7702 - Train loss: 0.00200493  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.82 min) Epoch 21/300 -- Iteration 155272 - Batch 1232/7702 - Train loss: 0.00200656  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.84 min) Epoch 21/300 -- Iteration 155349 - Batch 1309/7702 - Train loss: 0.00200655  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.86 min) Epoch 21/300 -- Iteration 155426 - Batch 1386/7702 - Train loss: 0.00200415  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.88 min) Epoch 21/300 -- Iteration 155503 - Batch 1463/7702 - Train loss: 0.00200423  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.90 min) Epoch 21/300 -- Iteration 155580 - Batch 1540/7702 - Train loss: 0.00200443  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.92 min) Epoch 21/300 -- Iteration 155657 - Batch 1617/7702 - Train loss: 0.00200516  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.94 min) Epoch 21/300 -- Iteration 155734 - Batch 1694/7702 - Train loss: 0.00200399  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.96 min) Epoch 21/300 -- Iteration 155811 - Batch 1771/7702 - Train loss: 0.00200506  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(41.98 min) Epoch 21/300 -- Iteration 155888 - Batch 1848/7702 - Train loss: 0.00200343  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.00 min) Epoch 21/300 -- Iteration 155965 - Batch 1925/7702 - Train loss: 0.00200355  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.02 min) Epoch 21/300 -- Iteration 156042 - Batch 2002/7702 - Train loss: 0.00200353  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.05 min) Epoch 21/300 -- Iteration 156119 - Batch 2079/7702 - Train loss: 0.00200508  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.07 min) Epoch 21/300 -- Iteration 156196 - Batch 2156/7702 - Train loss: 0.00200706  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.09 min) Epoch 21/300 -- Iteration 156273 - Batch 2233/7702 - Train loss: 0.00200781  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.11 min) Epoch 21/300 -- Iteration 156350 - Batch 2310/7702 - Train loss: 0.00200799  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.13 min) Epoch 21/300 -- Iteration 156427 - Batch 2387/7702 - Train loss: 0.00200862  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.15 min) Epoch 21/300 -- Iteration 156504 - Batch 2464/7702 - Train loss: 0.00200733  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.17 min) Epoch 21/300 -- Iteration 156581 - Batch 2541/7702 - Train loss: 0.00200654  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.19 min) Epoch 21/300 -- Iteration 156658 - Batch 2618/7702 - Train loss: 0.00200570  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.21 min) Epoch 21/300 -- Iteration 156735 - Batch 2695/7702 - Train loss: 0.00200566  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.23 min) Epoch 21/300 -- Iteration 156812 - Batch 2772/7702 - Train loss: 0.00200555  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.25 min) Epoch 21/300 -- Iteration 156889 - Batch 2849/7702 - Train loss: 0.00200525  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.27 min) Epoch 21/300 -- Iteration 156966 - Batch 2926/7702 - Train loss: 0.00200544  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.29 min) Epoch 21/300 -- Iteration 157043 - Batch 3003/7702 - Train loss: 0.00200602  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.31 min) Epoch 21/300 -- Iteration 157120 - Batch 3080/7702 - Train loss: 0.00200653  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.33 min) Epoch 21/300 -- Iteration 157197 - Batch 3157/7702 - Train loss: 0.00200658  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.36 min) Epoch 21/300 -- Iteration 157274 - Batch 3234/7702 - Train loss: 0.00200641  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.38 min) Epoch 21/300 -- Iteration 157351 - Batch 3311/7702 - Train loss: 0.00200532  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.40 min) Epoch 21/300 -- Iteration 157428 - Batch 3388/7702 - Train loss: 0.00200667  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.42 min) Epoch 21/300 -- Iteration 157505 - Batch 3465/7702 - Train loss: 0.00200783  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.44 min) Epoch 21/300 -- Iteration 157582 - Batch 3542/7702 - Train loss: 0.00200767  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.46 min) Epoch 21/300 -- Iteration 157659 - Batch 3619/7702 - Train loss: 0.00200765  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.48 min) Epoch 21/300 -- Iteration 157736 - Batch 3696/7702 - Train loss: 0.00200710  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.50 min) Epoch 21/300 -- Iteration 157813 - Batch 3773/7702 - Train loss: 0.00200667  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.52 min) Epoch 21/300 -- Iteration 157890 - Batch 3850/7702 - Train loss: 0.00200666  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.54 min) Epoch 21/300 -- Iteration 157967 - Batch 3927/7702 - Train loss: 0.00200631  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.56 min) Epoch 21/300 -- Iteration 158044 - Batch 4004/7702 - Train loss: 0.00200638  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.58 min) Epoch 21/300 -- Iteration 158121 - Batch 4081/7702 - Train loss: 0.00200602  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.60 min) Epoch 21/300 -- Iteration 158198 - Batch 4158/7702 - Train loss: 0.00200582  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.62 min) Epoch 21/300 -- Iteration 158275 - Batch 4235/7702 - Train loss: 0.00200605  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.64 min) Epoch 21/300 -- Iteration 158352 - Batch 4312/7702 - Train loss: 0.00200555  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.67 min) Epoch 21/300 -- Iteration 158429 - Batch 4389/7702 - Train loss: 0.00200516  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.69 min) Epoch 21/300 -- Iteration 158506 - Batch 4466/7702 - Train loss: 0.00200436  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.71 min) Epoch 21/300 -- Iteration 158583 - Batch 4543/7702 - Train loss: 0.00200425  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.73 min) Epoch 21/300 -- Iteration 158660 - Batch 4620/7702 - Train loss: 0.00200454  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.75 min) Epoch 21/300 -- Iteration 158737 - Batch 4697/7702 - Train loss: 0.00200411  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.77 min) Epoch 21/300 -- Iteration 158814 - Batch 4774/7702 - Train loss: 0.00200364  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.79 min) Epoch 21/300 -- Iteration 158891 - Batch 4851/7702 - Train loss: 0.00200398  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.81 min) Epoch 21/300 -- Iteration 158968 - Batch 4928/7702 - Train loss: 0.00200453  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.83 min) Epoch 21/300 -- Iteration 159045 - Batch 5005/7702 - Train loss: 0.00200450  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.85 min) Epoch 21/300 -- Iteration 159122 - Batch 5082/7702 - Train loss: 0.00200394  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.87 min) Epoch 21/300 -- Iteration 159199 - Batch 5159/7702 - Train loss: 0.00200377  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.89 min) Epoch 21/300 -- Iteration 159276 - Batch 5236/7702 - Train loss: 0.00200387  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.91 min) Epoch 21/300 -- Iteration 159353 - Batch 5313/7702 - Train loss: 0.00200365  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.93 min) Epoch 21/300 -- Iteration 159430 - Batch 5390/7702 - Train loss: 0.00200410  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.95 min) Epoch 21/300 -- Iteration 159507 - Batch 5467/7702 - Train loss: 0.00200375  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(42.98 min) Epoch 21/300 -- Iteration 159584 - Batch 5544/7702 - Train loss: 0.00200357  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.00 min) Epoch 21/300 -- Iteration 159661 - Batch 5621/7702 - Train loss: 0.00200348  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.02 min) Epoch 21/300 -- Iteration 159738 - Batch 5698/7702 - Train loss: 0.00200380  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.04 min) Epoch 21/300 -- Iteration 159815 - Batch 5775/7702 - Train loss: 0.00200401  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.06 min) Epoch 21/300 -- Iteration 159892 - Batch 5852/7702 - Train loss: 0.00200377  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.08 min) Epoch 21/300 -- Iteration 159969 - Batch 5929/7702 - Train loss: 0.00200363  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.10 min) Epoch 21/300 -- Iteration 160046 - Batch 6006/7702 - Train loss: 0.00200351  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.12 min) Epoch 21/300 -- Iteration 160123 - Batch 6083/7702 - Train loss: 0.00200354  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.14 min) Epoch 21/300 -- Iteration 160200 - Batch 6160/7702 - Train loss: 0.00200321  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.16 min) Epoch 21/300 -- Iteration 160277 - Batch 6237/7702 - Train loss: 0.00200348  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.18 min) Epoch 21/300 -- Iteration 160354 - Batch 6314/7702 - Train loss: 0.00200353  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.20 min) Epoch 21/300 -- Iteration 160431 - Batch 6391/7702 - Train loss: 0.00200374  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.22 min) Epoch 21/300 -- Iteration 160508 - Batch 6468/7702 - Train loss: 0.00200369  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.24 min) Epoch 21/300 -- Iteration 160585 - Batch 6545/7702 - Train loss: 0.00200356  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.27 min) Epoch 21/300 -- Iteration 160662 - Batch 6622/7702 - Train loss: 0.00200323  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.29 min) Epoch 21/300 -- Iteration 160739 - Batch 6699/7702 - Train loss: 0.00200332  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.31 min) Epoch 21/300 -- Iteration 160816 - Batch 6776/7702 - Train loss: 0.00200313  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.33 min) Epoch 21/300 -- Iteration 160893 - Batch 6853/7702 - Train loss: 0.00200266  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.35 min) Epoch 21/300 -- Iteration 160970 - Batch 6930/7702 - Train loss: 0.00200254  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.37 min) Epoch 21/300 -- Iteration 161047 - Batch 7007/7702 - Train loss: 0.00200203  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.39 min) Epoch 21/300 -- Iteration 161124 - Batch 7084/7702 - Train loss: 0.00200188  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.41 min) Epoch 21/300 -- Iteration 161201 - Batch 7161/7702 - Train loss: 0.00200206  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.43 min) Epoch 21/300 -- Iteration 161278 - Batch 7238/7702 - Train loss: 0.00200155  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.45 min) Epoch 21/300 -- Iteration 161355 - Batch 7315/7702 - Train loss: 0.00200135  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.47 min) Epoch 21/300 -- Iteration 161432 - Batch 7392/7702 - Train loss: 0.00200123  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.49 min) Epoch 21/300 -- Iteration 161509 - Batch 7469/7702 - Train loss: 0.00200096  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.51 min) Epoch 21/300 -- Iteration 161586 - Batch 7546/7702 - Train loss: 0.00200106  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.53 min) Epoch 21/300 -- Iteration 161663 - Batch 7623/7702 - Train loss: 0.00200071  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.55 min) Epoch 21/300 -- Iteration 161740 - Batch 7700/7702 - Train loss: 0.00200080  - Train acc: -0.0000 - Val loss: 0.00055527\n",
      "(43.56 min) Epoch 21/300 -- Iteration 161742 - Batch 7701/7702 - Train loss: 0.00200078  - Train acc: -0.0000 - Val loss: 0.00052529 - Val acc: -0.0000\n",
      "(43.58 min) Epoch 22/300 -- Iteration 161819 - Batch 77/7702 - Train loss: 0.00200523  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.60 min) Epoch 22/300 -- Iteration 161896 - Batch 154/7702 - Train loss: 0.00197999  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.62 min) Epoch 22/300 -- Iteration 161973 - Batch 231/7702 - Train loss: 0.00197784  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.64 min) Epoch 22/300 -- Iteration 162050 - Batch 308/7702 - Train loss: 0.00197619  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.66 min) Epoch 22/300 -- Iteration 162127 - Batch 385/7702 - Train loss: 0.00197998  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.68 min) Epoch 22/300 -- Iteration 162204 - Batch 462/7702 - Train loss: 0.00199102  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.70 min) Epoch 22/300 -- Iteration 162281 - Batch 539/7702 - Train loss: 0.00199311  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.72 min) Epoch 22/300 -- Iteration 162358 - Batch 616/7702 - Train loss: 0.00199757  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.74 min) Epoch 22/300 -- Iteration 162435 - Batch 693/7702 - Train loss: 0.00199907  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.76 min) Epoch 22/300 -- Iteration 162512 - Batch 770/7702 - Train loss: 0.00199940  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.79 min) Epoch 22/300 -- Iteration 162589 - Batch 847/7702 - Train loss: 0.00199622  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.81 min) Epoch 22/300 -- Iteration 162666 - Batch 924/7702 - Train loss: 0.00199395  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.83 min) Epoch 22/300 -- Iteration 162743 - Batch 1001/7702 - Train loss: 0.00199554  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.85 min) Epoch 22/300 -- Iteration 162820 - Batch 1078/7702 - Train loss: 0.00199603  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.87 min) Epoch 22/300 -- Iteration 162897 - Batch 1155/7702 - Train loss: 0.00199219  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.89 min) Epoch 22/300 -- Iteration 162974 - Batch 1232/7702 - Train loss: 0.00199643  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.91 min) Epoch 22/300 -- Iteration 163051 - Batch 1309/7702 - Train loss: 0.00199679  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.93 min) Epoch 22/300 -- Iteration 163128 - Batch 1386/7702 - Train loss: 0.00199630  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.95 min) Epoch 22/300 -- Iteration 163205 - Batch 1463/7702 - Train loss: 0.00199667  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.97 min) Epoch 22/300 -- Iteration 163282 - Batch 1540/7702 - Train loss: 0.00199673  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(43.99 min) Epoch 22/300 -- Iteration 163359 - Batch 1617/7702 - Train loss: 0.00199756  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.01 min) Epoch 22/300 -- Iteration 163436 - Batch 1694/7702 - Train loss: 0.00199658  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.03 min) Epoch 22/300 -- Iteration 163513 - Batch 1771/7702 - Train loss: 0.00199617  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.05 min) Epoch 22/300 -- Iteration 163590 - Batch 1848/7702 - Train loss: 0.00199589  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.07 min) Epoch 22/300 -- Iteration 163667 - Batch 1925/7702 - Train loss: 0.00199502  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.10 min) Epoch 22/300 -- Iteration 163744 - Batch 2002/7702 - Train loss: 0.00199440  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.12 min) Epoch 22/300 -- Iteration 163821 - Batch 2079/7702 - Train loss: 0.00199545  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.14 min) Epoch 22/300 -- Iteration 163898 - Batch 2156/7702 - Train loss: 0.00199684  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.16 min) Epoch 22/300 -- Iteration 163975 - Batch 2233/7702 - Train loss: 0.00199674  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.18 min) Epoch 22/300 -- Iteration 164052 - Batch 2310/7702 - Train loss: 0.00199712  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.20 min) Epoch 22/300 -- Iteration 164129 - Batch 2387/7702 - Train loss: 0.00199693  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.22 min) Epoch 22/300 -- Iteration 164206 - Batch 2464/7702 - Train loss: 0.00199675  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.24 min) Epoch 22/300 -- Iteration 164283 - Batch 2541/7702 - Train loss: 0.00199654  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.26 min) Epoch 22/300 -- Iteration 164360 - Batch 2618/7702 - Train loss: 0.00199657  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.28 min) Epoch 22/300 -- Iteration 164437 - Batch 2695/7702 - Train loss: 0.00199586  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.30 min) Epoch 22/300 -- Iteration 164514 - Batch 2772/7702 - Train loss: 0.00199568  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.32 min) Epoch 22/300 -- Iteration 164591 - Batch 2849/7702 - Train loss: 0.00199622  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.34 min) Epoch 22/300 -- Iteration 164668 - Batch 2926/7702 - Train loss: 0.00199527  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.36 min) Epoch 22/300 -- Iteration 164745 - Batch 3003/7702 - Train loss: 0.00199579  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.38 min) Epoch 22/300 -- Iteration 164822 - Batch 3080/7702 - Train loss: 0.00199443  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.41 min) Epoch 22/300 -- Iteration 164899 - Batch 3157/7702 - Train loss: 0.00199508  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.43 min) Epoch 22/300 -- Iteration 164976 - Batch 3234/7702 - Train loss: 0.00199443  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.45 min) Epoch 22/300 -- Iteration 165053 - Batch 3311/7702 - Train loss: 0.00199508  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.47 min) Epoch 22/300 -- Iteration 165130 - Batch 3388/7702 - Train loss: 0.00199448  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.49 min) Epoch 22/300 -- Iteration 165207 - Batch 3465/7702 - Train loss: 0.00199366  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.51 min) Epoch 22/300 -- Iteration 165284 - Batch 3542/7702 - Train loss: 0.00199290  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.53 min) Epoch 22/300 -- Iteration 165361 - Batch 3619/7702 - Train loss: 0.00199315  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.55 min) Epoch 22/300 -- Iteration 165438 - Batch 3696/7702 - Train loss: 0.00199354  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.57 min) Epoch 22/300 -- Iteration 165515 - Batch 3773/7702 - Train loss: 0.00199344  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.59 min) Epoch 22/300 -- Iteration 165592 - Batch 3850/7702 - Train loss: 0.00199322  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.61 min) Epoch 22/300 -- Iteration 165669 - Batch 3927/7702 - Train loss: 0.00199285  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.63 min) Epoch 22/300 -- Iteration 165746 - Batch 4004/7702 - Train loss: 0.00199208  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.65 min) Epoch 22/300 -- Iteration 165823 - Batch 4081/7702 - Train loss: 0.00199229  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.67 min) Epoch 22/300 -- Iteration 165900 - Batch 4158/7702 - Train loss: 0.00199185  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.69 min) Epoch 22/300 -- Iteration 165977 - Batch 4235/7702 - Train loss: 0.00199186  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.72 min) Epoch 22/300 -- Iteration 166054 - Batch 4312/7702 - Train loss: 0.00199131  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.74 min) Epoch 22/300 -- Iteration 166131 - Batch 4389/7702 - Train loss: 0.00199058  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.76 min) Epoch 22/300 -- Iteration 166208 - Batch 4466/7702 - Train loss: 0.00199009  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.78 min) Epoch 22/300 -- Iteration 166285 - Batch 4543/7702 - Train loss: 0.00199068  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.80 min) Epoch 22/300 -- Iteration 166362 - Batch 4620/7702 - Train loss: 0.00199036  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.82 min) Epoch 22/300 -- Iteration 166439 - Batch 4697/7702 - Train loss: 0.00199026  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.84 min) Epoch 22/300 -- Iteration 166516 - Batch 4774/7702 - Train loss: 0.00199066  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.86 min) Epoch 22/300 -- Iteration 166593 - Batch 4851/7702 - Train loss: 0.00199090  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.88 min) Epoch 22/300 -- Iteration 166670 - Batch 4928/7702 - Train loss: 0.00199080  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.90 min) Epoch 22/300 -- Iteration 166747 - Batch 5005/7702 - Train loss: 0.00199094  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.92 min) Epoch 22/300 -- Iteration 166824 - Batch 5082/7702 - Train loss: 0.00199118  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.94 min) Epoch 22/300 -- Iteration 166901 - Batch 5159/7702 - Train loss: 0.00199067  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.96 min) Epoch 22/300 -- Iteration 166978 - Batch 5236/7702 - Train loss: 0.00199129  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(44.98 min) Epoch 22/300 -- Iteration 167055 - Batch 5313/7702 - Train loss: 0.00199103  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.01 min) Epoch 22/300 -- Iteration 167132 - Batch 5390/7702 - Train loss: 0.00199109  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.03 min) Epoch 22/300 -- Iteration 167209 - Batch 5467/7702 - Train loss: 0.00199090  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.05 min) Epoch 22/300 -- Iteration 167286 - Batch 5544/7702 - Train loss: 0.00199091  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.07 min) Epoch 22/300 -- Iteration 167363 - Batch 5621/7702 - Train loss: 0.00199060  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.09 min) Epoch 22/300 -- Iteration 167440 - Batch 5698/7702 - Train loss: 0.00198997  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.11 min) Epoch 22/300 -- Iteration 167517 - Batch 5775/7702 - Train loss: 0.00198986  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.13 min) Epoch 22/300 -- Iteration 167594 - Batch 5852/7702 - Train loss: 0.00198957  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.15 min) Epoch 22/300 -- Iteration 167671 - Batch 5929/7702 - Train loss: 0.00198960  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.17 min) Epoch 22/300 -- Iteration 167748 - Batch 6006/7702 - Train loss: 0.00198942  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.19 min) Epoch 22/300 -- Iteration 167825 - Batch 6083/7702 - Train loss: 0.00198992  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.21 min) Epoch 22/300 -- Iteration 167902 - Batch 6160/7702 - Train loss: 0.00198954  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.23 min) Epoch 22/300 -- Iteration 167979 - Batch 6237/7702 - Train loss: 0.00198984  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.25 min) Epoch 22/300 -- Iteration 168056 - Batch 6314/7702 - Train loss: 0.00198989  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.27 min) Epoch 22/300 -- Iteration 168133 - Batch 6391/7702 - Train loss: 0.00199017  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.30 min) Epoch 22/300 -- Iteration 168210 - Batch 6468/7702 - Train loss: 0.00199033  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.32 min) Epoch 22/300 -- Iteration 168287 - Batch 6545/7702 - Train loss: 0.00199029  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.34 min) Epoch 22/300 -- Iteration 168364 - Batch 6622/7702 - Train loss: 0.00198997  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.36 min) Epoch 22/300 -- Iteration 168441 - Batch 6699/7702 - Train loss: 0.00199015  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.38 min) Epoch 22/300 -- Iteration 168518 - Batch 6776/7702 - Train loss: 0.00198991  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.40 min) Epoch 22/300 -- Iteration 168595 - Batch 6853/7702 - Train loss: 0.00198987  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.42 min) Epoch 22/300 -- Iteration 168672 - Batch 6930/7702 - Train loss: 0.00199046  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.44 min) Epoch 22/300 -- Iteration 168749 - Batch 7007/7702 - Train loss: 0.00199012  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.46 min) Epoch 22/300 -- Iteration 168826 - Batch 7084/7702 - Train loss: 0.00198995  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.48 min) Epoch 22/300 -- Iteration 168903 - Batch 7161/7702 - Train loss: 0.00199000  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.50 min) Epoch 22/300 -- Iteration 168980 - Batch 7238/7702 - Train loss: 0.00198968  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.52 min) Epoch 22/300 -- Iteration 169057 - Batch 7315/7702 - Train loss: 0.00198982  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.54 min) Epoch 22/300 -- Iteration 169134 - Batch 7392/7702 - Train loss: 0.00198995  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.56 min) Epoch 22/300 -- Iteration 169211 - Batch 7469/7702 - Train loss: 0.00198984  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.58 min) Epoch 22/300 -- Iteration 169288 - Batch 7546/7702 - Train loss: 0.00198931  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.60 min) Epoch 22/300 -- Iteration 169365 - Batch 7623/7702 - Train loss: 0.00198923  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.63 min) Epoch 22/300 -- Iteration 169442 - Batch 7700/7702 - Train loss: 0.00198915  - Train acc: -0.0000 - Val loss: 0.00052529\n",
      "(45.63 min) Epoch 22/300 -- Iteration 169444 - Batch 7701/7702 - Train loss: 0.00198916  - Train acc: -0.0000 - Val loss: 0.00060460 - Val acc: -0.0000\n",
      "(45.65 min) Epoch 23/300 -- Iteration 169521 - Batch 77/7702 - Train loss: 0.00199696  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.67 min) Epoch 23/300 -- Iteration 169598 - Batch 154/7702 - Train loss: 0.00199877  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.69 min) Epoch 23/300 -- Iteration 169675 - Batch 231/7702 - Train loss: 0.00198351  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.71 min) Epoch 23/300 -- Iteration 169752 - Batch 308/7702 - Train loss: 0.00198849  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.73 min) Epoch 23/300 -- Iteration 169829 - Batch 385/7702 - Train loss: 0.00198297  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.75 min) Epoch 23/300 -- Iteration 169906 - Batch 462/7702 - Train loss: 0.00198484  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.77 min) Epoch 23/300 -- Iteration 169983 - Batch 539/7702 - Train loss: 0.00198961  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.79 min) Epoch 23/300 -- Iteration 170060 - Batch 616/7702 - Train loss: 0.00199272  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.81 min) Epoch 23/300 -- Iteration 170137 - Batch 693/7702 - Train loss: 0.00198747  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.84 min) Epoch 23/300 -- Iteration 170214 - Batch 770/7702 - Train loss: 0.00198764  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.86 min) Epoch 23/300 -- Iteration 170291 - Batch 847/7702 - Train loss: 0.00198828  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.88 min) Epoch 23/300 -- Iteration 170368 - Batch 924/7702 - Train loss: 0.00198660  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.90 min) Epoch 23/300 -- Iteration 170445 - Batch 1001/7702 - Train loss: 0.00198587  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.92 min) Epoch 23/300 -- Iteration 170522 - Batch 1078/7702 - Train loss: 0.00198873  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.94 min) Epoch 23/300 -- Iteration 170599 - Batch 1155/7702 - Train loss: 0.00198631  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.96 min) Epoch 23/300 -- Iteration 170676 - Batch 1232/7702 - Train loss: 0.00198673  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(45.98 min) Epoch 23/300 -- Iteration 170753 - Batch 1309/7702 - Train loss: 0.00198677  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.00 min) Epoch 23/300 -- Iteration 170830 - Batch 1386/7702 - Train loss: 0.00198720  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.02 min) Epoch 23/300 -- Iteration 170907 - Batch 1463/7702 - Train loss: 0.00198836  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.04 min) Epoch 23/300 -- Iteration 170984 - Batch 1540/7702 - Train loss: 0.00198809  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.06 min) Epoch 23/300 -- Iteration 171061 - Batch 1617/7702 - Train loss: 0.00198866  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.08 min) Epoch 23/300 -- Iteration 171138 - Batch 1694/7702 - Train loss: 0.00198823  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.10 min) Epoch 23/300 -- Iteration 171215 - Batch 1771/7702 - Train loss: 0.00198839  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.13 min) Epoch 23/300 -- Iteration 171292 - Batch 1848/7702 - Train loss: 0.00198938  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.15 min) Epoch 23/300 -- Iteration 171369 - Batch 1925/7702 - Train loss: 0.00198844  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.17 min) Epoch 23/300 -- Iteration 171446 - Batch 2002/7702 - Train loss: 0.00198912  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.19 min) Epoch 23/300 -- Iteration 171523 - Batch 2079/7702 - Train loss: 0.00198895  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.21 min) Epoch 23/300 -- Iteration 171600 - Batch 2156/7702 - Train loss: 0.00199064  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.23 min) Epoch 23/300 -- Iteration 171677 - Batch 2233/7702 - Train loss: 0.00198988  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.25 min) Epoch 23/300 -- Iteration 171754 - Batch 2310/7702 - Train loss: 0.00198855  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.27 min) Epoch 23/300 -- Iteration 171831 - Batch 2387/7702 - Train loss: 0.00198866  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.29 min) Epoch 23/300 -- Iteration 171908 - Batch 2464/7702 - Train loss: 0.00198848  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.31 min) Epoch 23/300 -- Iteration 171985 - Batch 2541/7702 - Train loss: 0.00198799  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.33 min) Epoch 23/300 -- Iteration 172062 - Batch 2618/7702 - Train loss: 0.00198756  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.35 min) Epoch 23/300 -- Iteration 172139 - Batch 2695/7702 - Train loss: 0.00198696  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.37 min) Epoch 23/300 -- Iteration 172216 - Batch 2772/7702 - Train loss: 0.00198772  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.39 min) Epoch 23/300 -- Iteration 172293 - Batch 2849/7702 - Train loss: 0.00198796  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.41 min) Epoch 23/300 -- Iteration 172370 - Batch 2926/7702 - Train loss: 0.00198771  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.44 min) Epoch 23/300 -- Iteration 172447 - Batch 3003/7702 - Train loss: 0.00198741  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.46 min) Epoch 23/300 -- Iteration 172524 - Batch 3080/7702 - Train loss: 0.00198770  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.48 min) Epoch 23/300 -- Iteration 172601 - Batch 3157/7702 - Train loss: 0.00198780  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.50 min) Epoch 23/300 -- Iteration 172678 - Batch 3234/7702 - Train loss: 0.00198749  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.52 min) Epoch 23/300 -- Iteration 172755 - Batch 3311/7702 - Train loss: 0.00198743  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.54 min) Epoch 23/300 -- Iteration 172832 - Batch 3388/7702 - Train loss: 0.00198854  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.56 min) Epoch 23/300 -- Iteration 172909 - Batch 3465/7702 - Train loss: 0.00198912  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.58 min) Epoch 23/300 -- Iteration 172986 - Batch 3542/7702 - Train loss: 0.00198907  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.60 min) Epoch 23/300 -- Iteration 173063 - Batch 3619/7702 - Train loss: 0.00198857  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.62 min) Epoch 23/300 -- Iteration 173140 - Batch 3696/7702 - Train loss: 0.00198844  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.64 min) Epoch 23/300 -- Iteration 173217 - Batch 3773/7702 - Train loss: 0.00198825  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.66 min) Epoch 23/300 -- Iteration 173294 - Batch 3850/7702 - Train loss: 0.00198821  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.68 min) Epoch 23/300 -- Iteration 173371 - Batch 3927/7702 - Train loss: 0.00198815  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.70 min) Epoch 23/300 -- Iteration 173448 - Batch 4004/7702 - Train loss: 0.00198791  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.72 min) Epoch 23/300 -- Iteration 173525 - Batch 4081/7702 - Train loss: 0.00198826  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.74 min) Epoch 23/300 -- Iteration 173602 - Batch 4158/7702 - Train loss: 0.00198862  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.77 min) Epoch 23/300 -- Iteration 173679 - Batch 4235/7702 - Train loss: 0.00198838  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.79 min) Epoch 23/300 -- Iteration 173756 - Batch 4312/7702 - Train loss: 0.00198737  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.81 min) Epoch 23/300 -- Iteration 173833 - Batch 4389/7702 - Train loss: 0.00198817  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.83 min) Epoch 23/300 -- Iteration 173910 - Batch 4466/7702 - Train loss: 0.00198739  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.85 min) Epoch 23/300 -- Iteration 173987 - Batch 4543/7702 - Train loss: 0.00198683  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.87 min) Epoch 23/300 -- Iteration 174064 - Batch 4620/7702 - Train loss: 0.00198672  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.89 min) Epoch 23/300 -- Iteration 174141 - Batch 4697/7702 - Train loss: 0.00198630  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.91 min) Epoch 23/300 -- Iteration 174218 - Batch 4774/7702 - Train loss: 0.00198613  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.93 min) Epoch 23/300 -- Iteration 174295 - Batch 4851/7702 - Train loss: 0.00198629  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.95 min) Epoch 23/300 -- Iteration 174372 - Batch 4928/7702 - Train loss: 0.00198657  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.97 min) Epoch 23/300 -- Iteration 174449 - Batch 5005/7702 - Train loss: 0.00198652  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(46.99 min) Epoch 23/300 -- Iteration 174526 - Batch 5082/7702 - Train loss: 0.00198663  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.01 min) Epoch 23/300 -- Iteration 174603 - Batch 5159/7702 - Train loss: 0.00198627  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.03 min) Epoch 23/300 -- Iteration 174680 - Batch 5236/7702 - Train loss: 0.00198593  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.05 min) Epoch 23/300 -- Iteration 174757 - Batch 5313/7702 - Train loss: 0.00198568  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.08 min) Epoch 23/300 -- Iteration 174834 - Batch 5390/7702 - Train loss: 0.00198607  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.10 min) Epoch 23/300 -- Iteration 174911 - Batch 5467/7702 - Train loss: 0.00198626  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.12 min) Epoch 23/300 -- Iteration 174988 - Batch 5544/7702 - Train loss: 0.00198583  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.14 min) Epoch 23/300 -- Iteration 175065 - Batch 5621/7702 - Train loss: 0.00198581  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.16 min) Epoch 23/300 -- Iteration 175142 - Batch 5698/7702 - Train loss: 0.00198566  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.18 min) Epoch 23/300 -- Iteration 175219 - Batch 5775/7702 - Train loss: 0.00198605  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.20 min) Epoch 23/300 -- Iteration 175296 - Batch 5852/7702 - Train loss: 0.00198611  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.22 min) Epoch 23/300 -- Iteration 175373 - Batch 5929/7702 - Train loss: 0.00198681  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.24 min) Epoch 23/300 -- Iteration 175450 - Batch 6006/7702 - Train loss: 0.00198707  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.26 min) Epoch 23/300 -- Iteration 175527 - Batch 6083/7702 - Train loss: 0.00198788  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.28 min) Epoch 23/300 -- Iteration 175604 - Batch 6160/7702 - Train loss: 0.00198831  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.30 min) Epoch 23/300 -- Iteration 175681 - Batch 6237/7702 - Train loss: 0.00198853  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.32 min) Epoch 23/300 -- Iteration 175758 - Batch 6314/7702 - Train loss: 0.00198866  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.34 min) Epoch 23/300 -- Iteration 175835 - Batch 6391/7702 - Train loss: 0.00198836  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.36 min) Epoch 23/300 -- Iteration 175912 - Batch 6468/7702 - Train loss: 0.00198833  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.39 min) Epoch 23/300 -- Iteration 175989 - Batch 6545/7702 - Train loss: 0.00198846  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.41 min) Epoch 23/300 -- Iteration 176066 - Batch 6622/7702 - Train loss: 0.00198876  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.43 min) Epoch 23/300 -- Iteration 176143 - Batch 6699/7702 - Train loss: 0.00198860  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.45 min) Epoch 23/300 -- Iteration 176220 - Batch 6776/7702 - Train loss: 0.00198850  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.47 min) Epoch 23/300 -- Iteration 176297 - Batch 6853/7702 - Train loss: 0.00198831  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.49 min) Epoch 23/300 -- Iteration 176374 - Batch 6930/7702 - Train loss: 0.00198840  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.51 min) Epoch 23/300 -- Iteration 176451 - Batch 7007/7702 - Train loss: 0.00198798  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.53 min) Epoch 23/300 -- Iteration 176528 - Batch 7084/7702 - Train loss: 0.00198762  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.55 min) Epoch 23/300 -- Iteration 176605 - Batch 7161/7702 - Train loss: 0.00198714  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.57 min) Epoch 23/300 -- Iteration 176682 - Batch 7238/7702 - Train loss: 0.00198698  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.59 min) Epoch 23/300 -- Iteration 176759 - Batch 7315/7702 - Train loss: 0.00198690  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.61 min) Epoch 23/300 -- Iteration 176836 - Batch 7392/7702 - Train loss: 0.00198689  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.63 min) Epoch 23/300 -- Iteration 176913 - Batch 7469/7702 - Train loss: 0.00198647  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.65 min) Epoch 23/300 -- Iteration 176990 - Batch 7546/7702 - Train loss: 0.00198621  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.68 min) Epoch 23/300 -- Iteration 177067 - Batch 7623/7702 - Train loss: 0.00198626  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.70 min) Epoch 23/300 -- Iteration 177144 - Batch 7700/7702 - Train loss: 0.00198635  - Train acc: -0.0000 - Val loss: 0.00060460\n",
      "(47.70 min) Epoch 23/300 -- Iteration 177146 - Batch 7701/7702 - Train loss: 0.00198640  - Train acc: -0.0000 - Val loss: 0.00061709 - Val acc: -0.0000\n",
      "(47.72 min) Epoch 24/300 -- Iteration 177223 - Batch 77/7702 - Train loss: 0.00196390  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.74 min) Epoch 24/300 -- Iteration 177300 - Batch 154/7702 - Train loss: 0.00197142  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.76 min) Epoch 24/300 -- Iteration 177377 - Batch 231/7702 - Train loss: 0.00198031  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.78 min) Epoch 24/300 -- Iteration 177454 - Batch 308/7702 - Train loss: 0.00198174  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.80 min) Epoch 24/300 -- Iteration 177531 - Batch 385/7702 - Train loss: 0.00197741  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.82 min) Epoch 24/300 -- Iteration 177608 - Batch 462/7702 - Train loss: 0.00197436  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.84 min) Epoch 24/300 -- Iteration 177685 - Batch 539/7702 - Train loss: 0.00197768  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.86 min) Epoch 24/300 -- Iteration 177762 - Batch 616/7702 - Train loss: 0.00197841  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.89 min) Epoch 24/300 -- Iteration 177839 - Batch 693/7702 - Train loss: 0.00197675  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.91 min) Epoch 24/300 -- Iteration 177916 - Batch 770/7702 - Train loss: 0.00198044  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.93 min) Epoch 24/300 -- Iteration 177993 - Batch 847/7702 - Train loss: 0.00198254  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.95 min) Epoch 24/300 -- Iteration 178070 - Batch 924/7702 - Train loss: 0.00197815  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.97 min) Epoch 24/300 -- Iteration 178147 - Batch 1001/7702 - Train loss: 0.00197859  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(47.99 min) Epoch 24/300 -- Iteration 178224 - Batch 1078/7702 - Train loss: 0.00197861  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.01 min) Epoch 24/300 -- Iteration 178301 - Batch 1155/7702 - Train loss: 0.00198100  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.03 min) Epoch 24/300 -- Iteration 178378 - Batch 1232/7702 - Train loss: 0.00197923  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.05 min) Epoch 24/300 -- Iteration 178455 - Batch 1309/7702 - Train loss: 0.00197857  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.07 min) Epoch 24/300 -- Iteration 178532 - Batch 1386/7702 - Train loss: 0.00198188  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.09 min) Epoch 24/300 -- Iteration 178609 - Batch 1463/7702 - Train loss: 0.00198076  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.11 min) Epoch 24/300 -- Iteration 178686 - Batch 1540/7702 - Train loss: 0.00198154  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.13 min) Epoch 24/300 -- Iteration 178763 - Batch 1617/7702 - Train loss: 0.00198061  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.15 min) Epoch 24/300 -- Iteration 178840 - Batch 1694/7702 - Train loss: 0.00197974  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.17 min) Epoch 24/300 -- Iteration 178917 - Batch 1771/7702 - Train loss: 0.00197993  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.20 min) Epoch 24/300 -- Iteration 178994 - Batch 1848/7702 - Train loss: 0.00197984  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.22 min) Epoch 24/300 -- Iteration 179071 - Batch 1925/7702 - Train loss: 0.00198013  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.24 min) Epoch 24/300 -- Iteration 179148 - Batch 2002/7702 - Train loss: 0.00197999  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.26 min) Epoch 24/300 -- Iteration 179225 - Batch 2079/7702 - Train loss: 0.00198092  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.28 min) Epoch 24/300 -- Iteration 179302 - Batch 2156/7702 - Train loss: 0.00198053  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.30 min) Epoch 24/300 -- Iteration 179379 - Batch 2233/7702 - Train loss: 0.00197968  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.32 min) Epoch 24/300 -- Iteration 179456 - Batch 2310/7702 - Train loss: 0.00197958  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.34 min) Epoch 24/300 -- Iteration 179533 - Batch 2387/7702 - Train loss: 0.00198162  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.36 min) Epoch 24/300 -- Iteration 179610 - Batch 2464/7702 - Train loss: 0.00198104  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.38 min) Epoch 24/300 -- Iteration 179687 - Batch 2541/7702 - Train loss: 0.00198022  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.40 min) Epoch 24/300 -- Iteration 179764 - Batch 2618/7702 - Train loss: 0.00198094  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.42 min) Epoch 24/300 -- Iteration 179841 - Batch 2695/7702 - Train loss: 0.00198030  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.44 min) Epoch 24/300 -- Iteration 179918 - Batch 2772/7702 - Train loss: 0.00197965  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.47 min) Epoch 24/300 -- Iteration 179995 - Batch 2849/7702 - Train loss: 0.00198013  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.49 min) Epoch 24/300 -- Iteration 180072 - Batch 2926/7702 - Train loss: 0.00197948  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.51 min) Epoch 24/300 -- Iteration 180149 - Batch 3003/7702 - Train loss: 0.00197961  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.53 min) Epoch 24/300 -- Iteration 180226 - Batch 3080/7702 - Train loss: 0.00197907  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.55 min) Epoch 24/300 -- Iteration 180303 - Batch 3157/7702 - Train loss: 0.00197932  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.57 min) Epoch 24/300 -- Iteration 180380 - Batch 3234/7702 - Train loss: 0.00197918  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.59 min) Epoch 24/300 -- Iteration 180457 - Batch 3311/7702 - Train loss: 0.00197968  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.61 min) Epoch 24/300 -- Iteration 180534 - Batch 3388/7702 - Train loss: 0.00198088  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.63 min) Epoch 24/300 -- Iteration 180611 - Batch 3465/7702 - Train loss: 0.00198048  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.65 min) Epoch 24/300 -- Iteration 180688 - Batch 3542/7702 - Train loss: 0.00198056  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.67 min) Epoch 24/300 -- Iteration 180765 - Batch 3619/7702 - Train loss: 0.00198142  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.69 min) Epoch 24/300 -- Iteration 180842 - Batch 3696/7702 - Train loss: 0.00198108  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.71 min) Epoch 24/300 -- Iteration 180919 - Batch 3773/7702 - Train loss: 0.00198118  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.73 min) Epoch 24/300 -- Iteration 180996 - Batch 3850/7702 - Train loss: 0.00198118  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.76 min) Epoch 24/300 -- Iteration 181073 - Batch 3927/7702 - Train loss: 0.00198134  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.78 min) Epoch 24/300 -- Iteration 181150 - Batch 4004/7702 - Train loss: 0.00198177  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.80 min) Epoch 24/300 -- Iteration 181227 - Batch 4081/7702 - Train loss: 0.00198102  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.82 min) Epoch 24/300 -- Iteration 181304 - Batch 4158/7702 - Train loss: 0.00198083  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.84 min) Epoch 24/300 -- Iteration 181381 - Batch 4235/7702 - Train loss: 0.00198088  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.86 min) Epoch 24/300 -- Iteration 181458 - Batch 4312/7702 - Train loss: 0.00198150  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.88 min) Epoch 24/300 -- Iteration 181535 - Batch 4389/7702 - Train loss: 0.00198177  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.90 min) Epoch 24/300 -- Iteration 181612 - Batch 4466/7702 - Train loss: 0.00198154  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.92 min) Epoch 24/300 -- Iteration 181689 - Batch 4543/7702 - Train loss: 0.00198213  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.94 min) Epoch 24/300 -- Iteration 181766 - Batch 4620/7702 - Train loss: 0.00198220  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.96 min) Epoch 24/300 -- Iteration 181843 - Batch 4697/7702 - Train loss: 0.00198235  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(48.98 min) Epoch 24/300 -- Iteration 181920 - Batch 4774/7702 - Train loss: 0.00198220  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.00 min) Epoch 24/300 -- Iteration 181997 - Batch 4851/7702 - Train loss: 0.00198246  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.02 min) Epoch 24/300 -- Iteration 182074 - Batch 4928/7702 - Train loss: 0.00198264  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.05 min) Epoch 24/300 -- Iteration 182151 - Batch 5005/7702 - Train loss: 0.00198257  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.07 min) Epoch 24/300 -- Iteration 182228 - Batch 5082/7702 - Train loss: 0.00198285  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.09 min) Epoch 24/300 -- Iteration 182305 - Batch 5159/7702 - Train loss: 0.00198255  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.11 min) Epoch 24/300 -- Iteration 182382 - Batch 5236/7702 - Train loss: 0.00198230  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.13 min) Epoch 24/300 -- Iteration 182459 - Batch 5313/7702 - Train loss: 0.00198185  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.15 min) Epoch 24/300 -- Iteration 182536 - Batch 5390/7702 - Train loss: 0.00198191  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.17 min) Epoch 24/300 -- Iteration 182613 - Batch 5467/7702 - Train loss: 0.00198233  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.19 min) Epoch 24/300 -- Iteration 182690 - Batch 5544/7702 - Train loss: 0.00198255  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.21 min) Epoch 24/300 -- Iteration 182767 - Batch 5621/7702 - Train loss: 0.00198278  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.23 min) Epoch 24/300 -- Iteration 182844 - Batch 5698/7702 - Train loss: 0.00198352  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.25 min) Epoch 24/300 -- Iteration 182921 - Batch 5775/7702 - Train loss: 0.00198373  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.27 min) Epoch 24/300 -- Iteration 182998 - Batch 5852/7702 - Train loss: 0.00198384  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.29 min) Epoch 24/300 -- Iteration 183075 - Batch 5929/7702 - Train loss: 0.00198335  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.31 min) Epoch 24/300 -- Iteration 183152 - Batch 6006/7702 - Train loss: 0.00198317  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.33 min) Epoch 24/300 -- Iteration 183229 - Batch 6083/7702 - Train loss: 0.00198296  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.36 min) Epoch 24/300 -- Iteration 183306 - Batch 6160/7702 - Train loss: 0.00198298  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.38 min) Epoch 24/300 -- Iteration 183383 - Batch 6237/7702 - Train loss: 0.00198272  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.40 min) Epoch 24/300 -- Iteration 183460 - Batch 6314/7702 - Train loss: 0.00198256  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.42 min) Epoch 24/300 -- Iteration 183537 - Batch 6391/7702 - Train loss: 0.00198275  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.44 min) Epoch 24/300 -- Iteration 183614 - Batch 6468/7702 - Train loss: 0.00198266  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.46 min) Epoch 24/300 -- Iteration 183691 - Batch 6545/7702 - Train loss: 0.00198237  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.48 min) Epoch 24/300 -- Iteration 183768 - Batch 6622/7702 - Train loss: 0.00198254  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.50 min) Epoch 24/300 -- Iteration 183845 - Batch 6699/7702 - Train loss: 0.00198221  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.52 min) Epoch 24/300 -- Iteration 183922 - Batch 6776/7702 - Train loss: 0.00198203  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.54 min) Epoch 24/300 -- Iteration 183999 - Batch 6853/7702 - Train loss: 0.00198196  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.56 min) Epoch 24/300 -- Iteration 184076 - Batch 6930/7702 - Train loss: 0.00198191  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.58 min) Epoch 24/300 -- Iteration 184153 - Batch 7007/7702 - Train loss: 0.00198216  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.60 min) Epoch 24/300 -- Iteration 184230 - Batch 7084/7702 - Train loss: 0.00198163  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.62 min) Epoch 24/300 -- Iteration 184307 - Batch 7161/7702 - Train loss: 0.00198178  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.65 min) Epoch 24/300 -- Iteration 184384 - Batch 7238/7702 - Train loss: 0.00198187  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.67 min) Epoch 24/300 -- Iteration 184461 - Batch 7315/7702 - Train loss: 0.00198148  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.69 min) Epoch 24/300 -- Iteration 184538 - Batch 7392/7702 - Train loss: 0.00198148  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.71 min) Epoch 24/300 -- Iteration 184615 - Batch 7469/7702 - Train loss: 0.00198120  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.73 min) Epoch 24/300 -- Iteration 184692 - Batch 7546/7702 - Train loss: 0.00198111  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.75 min) Epoch 24/300 -- Iteration 184769 - Batch 7623/7702 - Train loss: 0.00198093  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.77 min) Epoch 24/300 -- Iteration 184846 - Batch 7700/7702 - Train loss: 0.00198067  - Train acc: -0.0000 - Val loss: 0.00061709\n",
      "(49.77 min) Epoch 24/300 -- Iteration 184848 - Batch 7701/7702 - Train loss: 0.00198067  - Train acc: -0.0000 - Val loss: 0.00053873 - Val acc: -0.0000\n",
      "(49.79 min) Epoch 25/300 -- Iteration 184925 - Batch 77/7702 - Train loss: 0.00197031  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(49.81 min) Epoch 25/300 -- Iteration 185002 - Batch 154/7702 - Train loss: 0.00198502  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(49.83 min) Epoch 25/300 -- Iteration 185079 - Batch 231/7702 - Train loss: 0.00198091  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(49.85 min) Epoch 25/300 -- Iteration 185156 - Batch 308/7702 - Train loss: 0.00197725  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(49.88 min) Epoch 25/300 -- Iteration 185233 - Batch 385/7702 - Train loss: 0.00197688  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(49.90 min) Epoch 25/300 -- Iteration 185310 - Batch 462/7702 - Train loss: 0.00198306  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(49.92 min) Epoch 25/300 -- Iteration 185387 - Batch 539/7702 - Train loss: 0.00198770  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(49.94 min) Epoch 25/300 -- Iteration 185464 - Batch 616/7702 - Train loss: 0.00198999  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(49.96 min) Epoch 25/300 -- Iteration 185541 - Batch 693/7702 - Train loss: 0.00198868  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(49.98 min) Epoch 25/300 -- Iteration 185618 - Batch 770/7702 - Train loss: 0.00198541  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.00 min) Epoch 25/300 -- Iteration 185695 - Batch 847/7702 - Train loss: 0.00198793  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.02 min) Epoch 25/300 -- Iteration 185772 - Batch 924/7702 - Train loss: 0.00198997  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.04 min) Epoch 25/300 -- Iteration 185849 - Batch 1001/7702 - Train loss: 0.00199127  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.06 min) Epoch 25/300 -- Iteration 185926 - Batch 1078/7702 - Train loss: 0.00198927  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.08 min) Epoch 25/300 -- Iteration 186003 - Batch 1155/7702 - Train loss: 0.00198900  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.10 min) Epoch 25/300 -- Iteration 186080 - Batch 1232/7702 - Train loss: 0.00199192  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.12 min) Epoch 25/300 -- Iteration 186157 - Batch 1309/7702 - Train loss: 0.00199429  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.15 min) Epoch 25/300 -- Iteration 186234 - Batch 1386/7702 - Train loss: 0.00199410  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.17 min) Epoch 25/300 -- Iteration 186311 - Batch 1463/7702 - Train loss: 0.00199608  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.19 min) Epoch 25/300 -- Iteration 186388 - Batch 1540/7702 - Train loss: 0.00199469  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.21 min) Epoch 25/300 -- Iteration 186465 - Batch 1617/7702 - Train loss: 0.00199293  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.23 min) Epoch 25/300 -- Iteration 186542 - Batch 1694/7702 - Train loss: 0.00199163  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.25 min) Epoch 25/300 -- Iteration 186619 - Batch 1771/7702 - Train loss: 0.00199276  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.27 min) Epoch 25/300 -- Iteration 186696 - Batch 1848/7702 - Train loss: 0.00199297  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.29 min) Epoch 25/300 -- Iteration 186773 - Batch 1925/7702 - Train loss: 0.00199212  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.31 min) Epoch 25/300 -- Iteration 186850 - Batch 2002/7702 - Train loss: 0.00199075  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.33 min) Epoch 25/300 -- Iteration 186927 - Batch 2079/7702 - Train loss: 0.00199003  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.35 min) Epoch 25/300 -- Iteration 187004 - Batch 2156/7702 - Train loss: 0.00198965  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.37 min) Epoch 25/300 -- Iteration 187081 - Batch 2233/7702 - Train loss: 0.00198963  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.39 min) Epoch 25/300 -- Iteration 187158 - Batch 2310/7702 - Train loss: 0.00199037  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.41 min) Epoch 25/300 -- Iteration 187235 - Batch 2387/7702 - Train loss: 0.00199023  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.43 min) Epoch 25/300 -- Iteration 187312 - Batch 2464/7702 - Train loss: 0.00198989  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.46 min) Epoch 25/300 -- Iteration 187389 - Batch 2541/7702 - Train loss: 0.00198908  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.48 min) Epoch 25/300 -- Iteration 187466 - Batch 2618/7702 - Train loss: 0.00198832  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.50 min) Epoch 25/300 -- Iteration 187543 - Batch 2695/7702 - Train loss: 0.00198775  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.52 min) Epoch 25/300 -- Iteration 187620 - Batch 2772/7702 - Train loss: 0.00198783  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.54 min) Epoch 25/300 -- Iteration 187697 - Batch 2849/7702 - Train loss: 0.00198741  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.56 min) Epoch 25/300 -- Iteration 187774 - Batch 2926/7702 - Train loss: 0.00198695  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.58 min) Epoch 25/300 -- Iteration 187851 - Batch 3003/7702 - Train loss: 0.00198689  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.60 min) Epoch 25/300 -- Iteration 187928 - Batch 3080/7702 - Train loss: 0.00198698  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.62 min) Epoch 25/300 -- Iteration 188005 - Batch 3157/7702 - Train loss: 0.00198668  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.64 min) Epoch 25/300 -- Iteration 188082 - Batch 3234/7702 - Train loss: 0.00198690  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.66 min) Epoch 25/300 -- Iteration 188159 - Batch 3311/7702 - Train loss: 0.00198708  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.68 min) Epoch 25/300 -- Iteration 188236 - Batch 3388/7702 - Train loss: 0.00198639  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.70 min) Epoch 25/300 -- Iteration 188313 - Batch 3465/7702 - Train loss: 0.00198600  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.72 min) Epoch 25/300 -- Iteration 188390 - Batch 3542/7702 - Train loss: 0.00198592  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.74 min) Epoch 25/300 -- Iteration 188467 - Batch 3619/7702 - Train loss: 0.00198732  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.77 min) Epoch 25/300 -- Iteration 188544 - Batch 3696/7702 - Train loss: 0.00198759  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.79 min) Epoch 25/300 -- Iteration 188621 - Batch 3773/7702 - Train loss: 0.00198685  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.81 min) Epoch 25/300 -- Iteration 188698 - Batch 3850/7702 - Train loss: 0.00198687  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.83 min) Epoch 25/300 -- Iteration 188775 - Batch 3927/7702 - Train loss: 0.00198681  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.85 min) Epoch 25/300 -- Iteration 188852 - Batch 4004/7702 - Train loss: 0.00198670  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.87 min) Epoch 25/300 -- Iteration 188929 - Batch 4081/7702 - Train loss: 0.00198622  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.89 min) Epoch 25/300 -- Iteration 189006 - Batch 4158/7702 - Train loss: 0.00198587  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.91 min) Epoch 25/300 -- Iteration 189083 - Batch 4235/7702 - Train loss: 0.00198653  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.93 min) Epoch 25/300 -- Iteration 189160 - Batch 4312/7702 - Train loss: 0.00198610  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.95 min) Epoch 25/300 -- Iteration 189237 - Batch 4389/7702 - Train loss: 0.00198608  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.97 min) Epoch 25/300 -- Iteration 189314 - Batch 4466/7702 - Train loss: 0.00198564  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(50.99 min) Epoch 25/300 -- Iteration 189391 - Batch 4543/7702 - Train loss: 0.00198545  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.01 min) Epoch 25/300 -- Iteration 189468 - Batch 4620/7702 - Train loss: 0.00198604  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.03 min) Epoch 25/300 -- Iteration 189545 - Batch 4697/7702 - Train loss: 0.00198590  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.05 min) Epoch 25/300 -- Iteration 189622 - Batch 4774/7702 - Train loss: 0.00198512  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.07 min) Epoch 25/300 -- Iteration 189699 - Batch 4851/7702 - Train loss: 0.00198506  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.09 min) Epoch 25/300 -- Iteration 189776 - Batch 4928/7702 - Train loss: 0.00198507  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.12 min) Epoch 25/300 -- Iteration 189853 - Batch 5005/7702 - Train loss: 0.00198521  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.14 min) Epoch 25/300 -- Iteration 189930 - Batch 5082/7702 - Train loss: 0.00198536  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.16 min) Epoch 25/300 -- Iteration 190007 - Batch 5159/7702 - Train loss: 0.00198557  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.18 min) Epoch 25/300 -- Iteration 190084 - Batch 5236/7702 - Train loss: 0.00198529  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.20 min) Epoch 25/300 -- Iteration 190161 - Batch 5313/7702 - Train loss: 0.00198468  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.22 min) Epoch 25/300 -- Iteration 190238 - Batch 5390/7702 - Train loss: 0.00198431  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.24 min) Epoch 25/300 -- Iteration 190315 - Batch 5467/7702 - Train loss: 0.00198412  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.26 min) Epoch 25/300 -- Iteration 190392 - Batch 5544/7702 - Train loss: 0.00198422  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.28 min) Epoch 25/300 -- Iteration 190469 - Batch 5621/7702 - Train loss: 0.00198442  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.30 min) Epoch 25/300 -- Iteration 190546 - Batch 5698/7702 - Train loss: 0.00198403  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.32 min) Epoch 25/300 -- Iteration 190623 - Batch 5775/7702 - Train loss: 0.00198375  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.34 min) Epoch 25/300 -- Iteration 190700 - Batch 5852/7702 - Train loss: 0.00198391  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.36 min) Epoch 25/300 -- Iteration 190777 - Batch 5929/7702 - Train loss: 0.00198378  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.38 min) Epoch 25/300 -- Iteration 190854 - Batch 6006/7702 - Train loss: 0.00198365  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.41 min) Epoch 25/300 -- Iteration 190931 - Batch 6083/7702 - Train loss: 0.00198366  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.43 min) Epoch 25/300 -- Iteration 191008 - Batch 6160/7702 - Train loss: 0.00198359  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.45 min) Epoch 25/300 -- Iteration 191085 - Batch 6237/7702 - Train loss: 0.00198345  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.47 min) Epoch 25/300 -- Iteration 191162 - Batch 6314/7702 - Train loss: 0.00198316  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.49 min) Epoch 25/300 -- Iteration 191239 - Batch 6391/7702 - Train loss: 0.00198388  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.51 min) Epoch 25/300 -- Iteration 191316 - Batch 6468/7702 - Train loss: 0.00198401  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.53 min) Epoch 25/300 -- Iteration 191393 - Batch 6545/7702 - Train loss: 0.00198373  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.55 min) Epoch 25/300 -- Iteration 191470 - Batch 6622/7702 - Train loss: 0.00198374  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.57 min) Epoch 25/300 -- Iteration 191547 - Batch 6699/7702 - Train loss: 0.00198489  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.59 min) Epoch 25/300 -- Iteration 191624 - Batch 6776/7702 - Train loss: 0.00198525  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.61 min) Epoch 25/300 -- Iteration 191701 - Batch 6853/7702 - Train loss: 0.00198546  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.63 min) Epoch 25/300 -- Iteration 191778 - Batch 6930/7702 - Train loss: 0.00198504  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.65 min) Epoch 25/300 -- Iteration 191855 - Batch 7007/7702 - Train loss: 0.00198509  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.67 min) Epoch 25/300 -- Iteration 191932 - Batch 7084/7702 - Train loss: 0.00198557  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.70 min) Epoch 25/300 -- Iteration 192009 - Batch 7161/7702 - Train loss: 0.00198558  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.72 min) Epoch 25/300 -- Iteration 192086 - Batch 7238/7702 - Train loss: 0.00198505  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.74 min) Epoch 25/300 -- Iteration 192163 - Batch 7315/7702 - Train loss: 0.00198484  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.76 min) Epoch 25/300 -- Iteration 192240 - Batch 7392/7702 - Train loss: 0.00198506  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.78 min) Epoch 25/300 -- Iteration 192317 - Batch 7469/7702 - Train loss: 0.00198535  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.80 min) Epoch 25/300 -- Iteration 192394 - Batch 7546/7702 - Train loss: 0.00198520  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.82 min) Epoch 25/300 -- Iteration 192471 - Batch 7623/7702 - Train loss: 0.00198483  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.84 min) Epoch 25/300 -- Iteration 192548 - Batch 7700/7702 - Train loss: 0.00198465  - Train acc: -0.0000 - Val loss: 0.00053873\n",
      "(51.84 min) Epoch 25/300 -- Iteration 192550 - Batch 7701/7702 - Train loss: 0.00198466  - Train acc: -0.0000 - Val loss: 0.00052028 - Val acc: -0.0000\n",
      "(51.86 min) Epoch 26/300 -- Iteration 192627 - Batch 77/7702 - Train loss: 0.00199613  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(51.88 min) Epoch 26/300 -- Iteration 192704 - Batch 154/7702 - Train loss: 0.00199375  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(51.90 min) Epoch 26/300 -- Iteration 192781 - Batch 231/7702 - Train loss: 0.00197851  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(51.93 min) Epoch 26/300 -- Iteration 192858 - Batch 308/7702 - Train loss: 0.00198241  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(51.95 min) Epoch 26/300 -- Iteration 192935 - Batch 385/7702 - Train loss: 0.00198338  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(51.97 min) Epoch 26/300 -- Iteration 193012 - Batch 462/7702 - Train loss: 0.00198503  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(51.99 min) Epoch 26/300 -- Iteration 193089 - Batch 539/7702 - Train loss: 0.00197533  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.01 min) Epoch 26/300 -- Iteration 193166 - Batch 616/7702 - Train loss: 0.00197871  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.03 min) Epoch 26/300 -- Iteration 193243 - Batch 693/7702 - Train loss: 0.00197810  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.05 min) Epoch 26/300 -- Iteration 193320 - Batch 770/7702 - Train loss: 0.00198111  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.07 min) Epoch 26/300 -- Iteration 193397 - Batch 847/7702 - Train loss: 0.00198370  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.09 min) Epoch 26/300 -- Iteration 193474 - Batch 924/7702 - Train loss: 0.00198389  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.11 min) Epoch 26/300 -- Iteration 193551 - Batch 1001/7702 - Train loss: 0.00198443  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.13 min) Epoch 26/300 -- Iteration 193628 - Batch 1078/7702 - Train loss: 0.00198593  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.15 min) Epoch 26/300 -- Iteration 193705 - Batch 1155/7702 - Train loss: 0.00198397  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.17 min) Epoch 26/300 -- Iteration 193782 - Batch 1232/7702 - Train loss: 0.00198265  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.19 min) Epoch 26/300 -- Iteration 193859 - Batch 1309/7702 - Train loss: 0.00198086  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.21 min) Epoch 26/300 -- Iteration 193936 - Batch 1386/7702 - Train loss: 0.00197900  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.23 min) Epoch 26/300 -- Iteration 194013 - Batch 1463/7702 - Train loss: 0.00197859  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.26 min) Epoch 26/300 -- Iteration 194090 - Batch 1540/7702 - Train loss: 0.00197980  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.28 min) Epoch 26/300 -- Iteration 194167 - Batch 1617/7702 - Train loss: 0.00198252  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.30 min) Epoch 26/300 -- Iteration 194244 - Batch 1694/7702 - Train loss: 0.00198282  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.32 min) Epoch 26/300 -- Iteration 194321 - Batch 1771/7702 - Train loss: 0.00198212  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.34 min) Epoch 26/300 -- Iteration 194398 - Batch 1848/7702 - Train loss: 0.00198317  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.36 min) Epoch 26/300 -- Iteration 194475 - Batch 1925/7702 - Train loss: 0.00198396  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.38 min) Epoch 26/300 -- Iteration 194552 - Batch 2002/7702 - Train loss: 0.00198402  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.40 min) Epoch 26/300 -- Iteration 194629 - Batch 2079/7702 - Train loss: 0.00198306  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.42 min) Epoch 26/300 -- Iteration 194706 - Batch 2156/7702 - Train loss: 0.00198254  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.44 min) Epoch 26/300 -- Iteration 194783 - Batch 2233/7702 - Train loss: 0.00198169  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.46 min) Epoch 26/300 -- Iteration 194860 - Batch 2310/7702 - Train loss: 0.00198105  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.48 min) Epoch 26/300 -- Iteration 194937 - Batch 2387/7702 - Train loss: 0.00198038  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.50 min) Epoch 26/300 -- Iteration 195014 - Batch 2464/7702 - Train loss: 0.00197971  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.52 min) Epoch 26/300 -- Iteration 195091 - Batch 2541/7702 - Train loss: 0.00197924  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.55 min) Epoch 26/300 -- Iteration 195168 - Batch 2618/7702 - Train loss: 0.00197940  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.57 min) Epoch 26/300 -- Iteration 195245 - Batch 2695/7702 - Train loss: 0.00197927  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.59 min) Epoch 26/300 -- Iteration 195322 - Batch 2772/7702 - Train loss: 0.00197968  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.61 min) Epoch 26/300 -- Iteration 195399 - Batch 2849/7702 - Train loss: 0.00198001  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.63 min) Epoch 26/300 -- Iteration 195476 - Batch 2926/7702 - Train loss: 0.00197897  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.65 min) Epoch 26/300 -- Iteration 195553 - Batch 3003/7702 - Train loss: 0.00198003  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.67 min) Epoch 26/300 -- Iteration 195630 - Batch 3080/7702 - Train loss: 0.00198001  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.69 min) Epoch 26/300 -- Iteration 195707 - Batch 3157/7702 - Train loss: 0.00197985  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.71 min) Epoch 26/300 -- Iteration 195784 - Batch 3234/7702 - Train loss: 0.00198036  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.73 min) Epoch 26/300 -- Iteration 195861 - Batch 3311/7702 - Train loss: 0.00198012  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.75 min) Epoch 26/300 -- Iteration 195938 - Batch 3388/7702 - Train loss: 0.00198042  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.77 min) Epoch 26/300 -- Iteration 196015 - Batch 3465/7702 - Train loss: 0.00198112  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.79 min) Epoch 26/300 -- Iteration 196092 - Batch 3542/7702 - Train loss: 0.00198073  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.82 min) Epoch 26/300 -- Iteration 196169 - Batch 3619/7702 - Train loss: 0.00198085  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.84 min) Epoch 26/300 -- Iteration 196246 - Batch 3696/7702 - Train loss: 0.00198133  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.86 min) Epoch 26/300 -- Iteration 196323 - Batch 3773/7702 - Train loss: 0.00198122  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.88 min) Epoch 26/300 -- Iteration 196400 - Batch 3850/7702 - Train loss: 0.00198138  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.90 min) Epoch 26/300 -- Iteration 196477 - Batch 3927/7702 - Train loss: 0.00198059  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.92 min) Epoch 26/300 -- Iteration 196554 - Batch 4004/7702 - Train loss: 0.00198060  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.94 min) Epoch 26/300 -- Iteration 196631 - Batch 4081/7702 - Train loss: 0.00198037  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.96 min) Epoch 26/300 -- Iteration 196708 - Batch 4158/7702 - Train loss: 0.00197953  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(52.98 min) Epoch 26/300 -- Iteration 196785 - Batch 4235/7702 - Train loss: 0.00197951  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.00 min) Epoch 26/300 -- Iteration 196862 - Batch 4312/7702 - Train loss: 0.00197954  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.02 min) Epoch 26/300 -- Iteration 196939 - Batch 4389/7702 - Train loss: 0.00198000  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.04 min) Epoch 26/300 -- Iteration 197016 - Batch 4466/7702 - Train loss: 0.00198001  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.06 min) Epoch 26/300 -- Iteration 197093 - Batch 4543/7702 - Train loss: 0.00198043  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.08 min) Epoch 26/300 -- Iteration 197170 - Batch 4620/7702 - Train loss: 0.00197998  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.11 min) Epoch 26/300 -- Iteration 197247 - Batch 4697/7702 - Train loss: 0.00197966  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.13 min) Epoch 26/300 -- Iteration 197324 - Batch 4774/7702 - Train loss: 0.00197949  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.15 min) Epoch 26/300 -- Iteration 197401 - Batch 4851/7702 - Train loss: 0.00197938  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.17 min) Epoch 26/300 -- Iteration 197478 - Batch 4928/7702 - Train loss: 0.00197957  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.19 min) Epoch 26/300 -- Iteration 197555 - Batch 5005/7702 - Train loss: 0.00197924  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.21 min) Epoch 26/300 -- Iteration 197632 - Batch 5082/7702 - Train loss: 0.00197945  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.23 min) Epoch 26/300 -- Iteration 197709 - Batch 5159/7702 - Train loss: 0.00197911  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.25 min) Epoch 26/300 -- Iteration 197786 - Batch 5236/7702 - Train loss: 0.00197893  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.27 min) Epoch 26/300 -- Iteration 197863 - Batch 5313/7702 - Train loss: 0.00197933  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.29 min) Epoch 26/300 -- Iteration 197940 - Batch 5390/7702 - Train loss: 0.00197897  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.31 min) Epoch 26/300 -- Iteration 198017 - Batch 5467/7702 - Train loss: 0.00197916  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.33 min) Epoch 26/300 -- Iteration 198094 - Batch 5544/7702 - Train loss: 0.00197889  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.35 min) Epoch 26/300 -- Iteration 198171 - Batch 5621/7702 - Train loss: 0.00197848  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.37 min) Epoch 26/300 -- Iteration 198248 - Batch 5698/7702 - Train loss: 0.00197825  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.39 min) Epoch 26/300 -- Iteration 198325 - Batch 5775/7702 - Train loss: 0.00197799  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.41 min) Epoch 26/300 -- Iteration 198402 - Batch 5852/7702 - Train loss: 0.00197749  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.44 min) Epoch 26/300 -- Iteration 198479 - Batch 5929/7702 - Train loss: 0.00197770  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.46 min) Epoch 26/300 -- Iteration 198556 - Batch 6006/7702 - Train loss: 0.00197753  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.48 min) Epoch 26/300 -- Iteration 198633 - Batch 6083/7702 - Train loss: 0.00197701  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.50 min) Epoch 26/300 -- Iteration 198710 - Batch 6160/7702 - Train loss: 0.00197697  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.52 min) Epoch 26/300 -- Iteration 198787 - Batch 6237/7702 - Train loss: 0.00197719  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.54 min) Epoch 26/300 -- Iteration 198864 - Batch 6314/7702 - Train loss: 0.00197742  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.56 min) Epoch 26/300 -- Iteration 198941 - Batch 6391/7702 - Train loss: 0.00197775  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.58 min) Epoch 26/300 -- Iteration 199018 - Batch 6468/7702 - Train loss: 0.00197743  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.60 min) Epoch 26/300 -- Iteration 199095 - Batch 6545/7702 - Train loss: 0.00197764  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.62 min) Epoch 26/300 -- Iteration 199172 - Batch 6622/7702 - Train loss: 0.00197763  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.64 min) Epoch 26/300 -- Iteration 199249 - Batch 6699/7702 - Train loss: 0.00197727  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.66 min) Epoch 26/300 -- Iteration 199326 - Batch 6776/7702 - Train loss: 0.00197739  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.68 min) Epoch 26/300 -- Iteration 199403 - Batch 6853/7702 - Train loss: 0.00197714  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.70 min) Epoch 26/300 -- Iteration 199480 - Batch 6930/7702 - Train loss: 0.00197735  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.73 min) Epoch 26/300 -- Iteration 199557 - Batch 7007/7702 - Train loss: 0.00197693  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.75 min) Epoch 26/300 -- Iteration 199634 - Batch 7084/7702 - Train loss: 0.00197685  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.77 min) Epoch 26/300 -- Iteration 199711 - Batch 7161/7702 - Train loss: 0.00197712  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.79 min) Epoch 26/300 -- Iteration 199788 - Batch 7238/7702 - Train loss: 0.00197714  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.81 min) Epoch 26/300 -- Iteration 199865 - Batch 7315/7702 - Train loss: 0.00197729  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.83 min) Epoch 26/300 -- Iteration 199942 - Batch 7392/7702 - Train loss: 0.00197746  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.85 min) Epoch 26/300 -- Iteration 200019 - Batch 7469/7702 - Train loss: 0.00197702  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.87 min) Epoch 26/300 -- Iteration 200096 - Batch 7546/7702 - Train loss: 0.00197696  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.89 min) Epoch 26/300 -- Iteration 200173 - Batch 7623/7702 - Train loss: 0.00197683  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.91 min) Epoch 26/300 -- Iteration 200250 - Batch 7700/7702 - Train loss: 0.00197702  - Train acc: -0.0000 - Val loss: 0.00052028\n",
      "(53.91 min) Epoch 26/300 -- Iteration 200252 - Batch 7701/7702 - Train loss: 0.00197701  - Train acc: -0.0000 - Val loss: 0.00058845 - Val acc: -0.0000\n",
      "(53.93 min) Epoch 27/300 -- Iteration 200329 - Batch 77/7702 - Train loss: 0.00195960  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(53.96 min) Epoch 27/300 -- Iteration 200406 - Batch 154/7702 - Train loss: 0.00194866  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(53.98 min) Epoch 27/300 -- Iteration 200483 - Batch 231/7702 - Train loss: 0.00195892  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.00 min) Epoch 27/300 -- Iteration 200560 - Batch 308/7702 - Train loss: 0.00196587  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.02 min) Epoch 27/300 -- Iteration 200637 - Batch 385/7702 - Train loss: 0.00197153  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.04 min) Epoch 27/300 -- Iteration 200714 - Batch 462/7702 - Train loss: 0.00197060  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.06 min) Epoch 27/300 -- Iteration 200791 - Batch 539/7702 - Train loss: 0.00197390  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.08 min) Epoch 27/300 -- Iteration 200868 - Batch 616/7702 - Train loss: 0.00197528  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.10 min) Epoch 27/300 -- Iteration 200945 - Batch 693/7702 - Train loss: 0.00197462  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.12 min) Epoch 27/300 -- Iteration 201022 - Batch 770/7702 - Train loss: 0.00197655  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.14 min) Epoch 27/300 -- Iteration 201099 - Batch 847/7702 - Train loss: 0.00197450  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.16 min) Epoch 27/300 -- Iteration 201176 - Batch 924/7702 - Train loss: 0.00197510  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.18 min) Epoch 27/300 -- Iteration 201253 - Batch 1001/7702 - Train loss: 0.00197420  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.20 min) Epoch 27/300 -- Iteration 201330 - Batch 1078/7702 - Train loss: 0.00197083  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.22 min) Epoch 27/300 -- Iteration 201407 - Batch 1155/7702 - Train loss: 0.00197002  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.24 min) Epoch 27/300 -- Iteration 201484 - Batch 1232/7702 - Train loss: 0.00196855  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.27 min) Epoch 27/300 -- Iteration 201561 - Batch 1309/7702 - Train loss: 0.00197013  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.29 min) Epoch 27/300 -- Iteration 201638 - Batch 1386/7702 - Train loss: 0.00197062  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.31 min) Epoch 27/300 -- Iteration 201715 - Batch 1463/7702 - Train loss: 0.00197075  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.33 min) Epoch 27/300 -- Iteration 201792 - Batch 1540/7702 - Train loss: 0.00196900  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.35 min) Epoch 27/300 -- Iteration 201869 - Batch 1617/7702 - Train loss: 0.00196766  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.37 min) Epoch 27/300 -- Iteration 201946 - Batch 1694/7702 - Train loss: 0.00196803  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.39 min) Epoch 27/300 -- Iteration 202023 - Batch 1771/7702 - Train loss: 0.00196850  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.41 min) Epoch 27/300 -- Iteration 202100 - Batch 1848/7702 - Train loss: 0.00196954  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.43 min) Epoch 27/300 -- Iteration 202177 - Batch 1925/7702 - Train loss: 0.00196944  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.45 min) Epoch 27/300 -- Iteration 202254 - Batch 2002/7702 - Train loss: 0.00196824  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.47 min) Epoch 27/300 -- Iteration 202331 - Batch 2079/7702 - Train loss: 0.00196784  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.49 min) Epoch 27/300 -- Iteration 202408 - Batch 2156/7702 - Train loss: 0.00196806  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.51 min) Epoch 27/300 -- Iteration 202485 - Batch 2233/7702 - Train loss: 0.00196813  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.53 min) Epoch 27/300 -- Iteration 202562 - Batch 2310/7702 - Train loss: 0.00196805  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.55 min) Epoch 27/300 -- Iteration 202639 - Batch 2387/7702 - Train loss: 0.00196821  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.57 min) Epoch 27/300 -- Iteration 202716 - Batch 2464/7702 - Train loss: 0.00196821  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.60 min) Epoch 27/300 -- Iteration 202793 - Batch 2541/7702 - Train loss: 0.00196686  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.62 min) Epoch 27/300 -- Iteration 202870 - Batch 2618/7702 - Train loss: 0.00196715  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.64 min) Epoch 27/300 -- Iteration 202947 - Batch 2695/7702 - Train loss: 0.00196839  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.66 min) Epoch 27/300 -- Iteration 203024 - Batch 2772/7702 - Train loss: 0.00196898  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.68 min) Epoch 27/300 -- Iteration 203101 - Batch 2849/7702 - Train loss: 0.00196838  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.70 min) Epoch 27/300 -- Iteration 203178 - Batch 2926/7702 - Train loss: 0.00196802  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.72 min) Epoch 27/300 -- Iteration 203255 - Batch 3003/7702 - Train loss: 0.00196761  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.74 min) Epoch 27/300 -- Iteration 203332 - Batch 3080/7702 - Train loss: 0.00196759  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.76 min) Epoch 27/300 -- Iteration 203409 - Batch 3157/7702 - Train loss: 0.00196752  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.78 min) Epoch 27/300 -- Iteration 203486 - Batch 3234/7702 - Train loss: 0.00196781  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.80 min) Epoch 27/300 -- Iteration 203563 - Batch 3311/7702 - Train loss: 0.00196823  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.82 min) Epoch 27/300 -- Iteration 203640 - Batch 3388/7702 - Train loss: 0.00196857  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.84 min) Epoch 27/300 -- Iteration 203717 - Batch 3465/7702 - Train loss: 0.00196772  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.86 min) Epoch 27/300 -- Iteration 203794 - Batch 3542/7702 - Train loss: 0.00196813  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.88 min) Epoch 27/300 -- Iteration 203871 - Batch 3619/7702 - Train loss: 0.00196833  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.90 min) Epoch 27/300 -- Iteration 203948 - Batch 3696/7702 - Train loss: 0.00196850  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.93 min) Epoch 27/300 -- Iteration 204025 - Batch 3773/7702 - Train loss: 0.00196915  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.95 min) Epoch 27/300 -- Iteration 204102 - Batch 3850/7702 - Train loss: 0.00197033  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.97 min) Epoch 27/300 -- Iteration 204179 - Batch 3927/7702 - Train loss: 0.00197061  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(54.99 min) Epoch 27/300 -- Iteration 204256 - Batch 4004/7702 - Train loss: 0.00197071  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.01 min) Epoch 27/300 -- Iteration 204333 - Batch 4081/7702 - Train loss: 0.00197052  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.03 min) Epoch 27/300 -- Iteration 204410 - Batch 4158/7702 - Train loss: 0.00197095  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.05 min) Epoch 27/300 -- Iteration 204487 - Batch 4235/7702 - Train loss: 0.00197098  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.07 min) Epoch 27/300 -- Iteration 204564 - Batch 4312/7702 - Train loss: 0.00197033  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.09 min) Epoch 27/300 -- Iteration 204641 - Batch 4389/7702 - Train loss: 0.00197019  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.11 min) Epoch 27/300 -- Iteration 204718 - Batch 4466/7702 - Train loss: 0.00196963  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.13 min) Epoch 27/300 -- Iteration 204795 - Batch 4543/7702 - Train loss: 0.00196926  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.15 min) Epoch 27/300 -- Iteration 204872 - Batch 4620/7702 - Train loss: 0.00196898  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.17 min) Epoch 27/300 -- Iteration 204949 - Batch 4697/7702 - Train loss: 0.00196900  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.19 min) Epoch 27/300 -- Iteration 205026 - Batch 4774/7702 - Train loss: 0.00196916  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.22 min) Epoch 27/300 -- Iteration 205103 - Batch 4851/7702 - Train loss: 0.00196933  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.24 min) Epoch 27/300 -- Iteration 205180 - Batch 4928/7702 - Train loss: 0.00196967  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.26 min) Epoch 27/300 -- Iteration 205257 - Batch 5005/7702 - Train loss: 0.00196927  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.28 min) Epoch 27/300 -- Iteration 205334 - Batch 5082/7702 - Train loss: 0.00196909  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.30 min) Epoch 27/300 -- Iteration 205411 - Batch 5159/7702 - Train loss: 0.00196954  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.32 min) Epoch 27/300 -- Iteration 205488 - Batch 5236/7702 - Train loss: 0.00196977  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.34 min) Epoch 27/300 -- Iteration 205565 - Batch 5313/7702 - Train loss: 0.00196986  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.36 min) Epoch 27/300 -- Iteration 205642 - Batch 5390/7702 - Train loss: 0.00197050  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.38 min) Epoch 27/300 -- Iteration 205719 - Batch 5467/7702 - Train loss: 0.00197073  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.40 min) Epoch 27/300 -- Iteration 205796 - Batch 5544/7702 - Train loss: 0.00197097  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.42 min) Epoch 27/300 -- Iteration 205873 - Batch 5621/7702 - Train loss: 0.00197116  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.44 min) Epoch 27/300 -- Iteration 205950 - Batch 5698/7702 - Train loss: 0.00197131  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.46 min) Epoch 27/300 -- Iteration 206027 - Batch 5775/7702 - Train loss: 0.00197082  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.48 min) Epoch 27/300 -- Iteration 206104 - Batch 5852/7702 - Train loss: 0.00197120  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.51 min) Epoch 27/300 -- Iteration 206181 - Batch 5929/7702 - Train loss: 0.00197120  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.53 min) Epoch 27/300 -- Iteration 206258 - Batch 6006/7702 - Train loss: 0.00197152  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.55 min) Epoch 27/300 -- Iteration 206335 - Batch 6083/7702 - Train loss: 0.00197076  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.57 min) Epoch 27/300 -- Iteration 206412 - Batch 6160/7702 - Train loss: 0.00197115  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.59 min) Epoch 27/300 -- Iteration 206489 - Batch 6237/7702 - Train loss: 0.00197085  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.61 min) Epoch 27/300 -- Iteration 206566 - Batch 6314/7702 - Train loss: 0.00197069  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.63 min) Epoch 27/300 -- Iteration 206643 - Batch 6391/7702 - Train loss: 0.00197053  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.65 min) Epoch 27/300 -- Iteration 206720 - Batch 6468/7702 - Train loss: 0.00197108  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.67 min) Epoch 27/300 -- Iteration 206797 - Batch 6545/7702 - Train loss: 0.00197107  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.69 min) Epoch 27/300 -- Iteration 206874 - Batch 6622/7702 - Train loss: 0.00197111  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.71 min) Epoch 27/300 -- Iteration 206951 - Batch 6699/7702 - Train loss: 0.00197130  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.73 min) Epoch 27/300 -- Iteration 207028 - Batch 6776/7702 - Train loss: 0.00197091  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.75 min) Epoch 27/300 -- Iteration 207105 - Batch 6853/7702 - Train loss: 0.00197058  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.77 min) Epoch 27/300 -- Iteration 207182 - Batch 6930/7702 - Train loss: 0.00197074  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.80 min) Epoch 27/300 -- Iteration 207259 - Batch 7007/7702 - Train loss: 0.00197110  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.82 min) Epoch 27/300 -- Iteration 207336 - Batch 7084/7702 - Train loss: 0.00197059  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.84 min) Epoch 27/300 -- Iteration 207413 - Batch 7161/7702 - Train loss: 0.00197067  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.86 min) Epoch 27/300 -- Iteration 207490 - Batch 7238/7702 - Train loss: 0.00197041  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.88 min) Epoch 27/300 -- Iteration 207567 - Batch 7315/7702 - Train loss: 0.00197065  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.90 min) Epoch 27/300 -- Iteration 207644 - Batch 7392/7702 - Train loss: 0.00197058  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.92 min) Epoch 27/300 -- Iteration 207721 - Batch 7469/7702 - Train loss: 0.00197028  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.94 min) Epoch 27/300 -- Iteration 207798 - Batch 7546/7702 - Train loss: 0.00197030  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.96 min) Epoch 27/300 -- Iteration 207875 - Batch 7623/7702 - Train loss: 0.00197014  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.98 min) Epoch 27/300 -- Iteration 207952 - Batch 7700/7702 - Train loss: 0.00196999  - Train acc: -0.0000 - Val loss: 0.00058845\n",
      "(55.98 min) Epoch 27/300 -- Iteration 207954 - Batch 7701/7702 - Train loss: 0.00197004  - Train acc: -0.0000 - Val loss: 0.00051221 - Val acc: -0.0000\n",
      "(56.01 min) Epoch 28/300 -- Iteration 208031 - Batch 77/7702 - Train loss: 0.00196250  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.03 min) Epoch 28/300 -- Iteration 208108 - Batch 154/7702 - Train loss: 0.00196392  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.05 min) Epoch 28/300 -- Iteration 208185 - Batch 231/7702 - Train loss: 0.00196131  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.07 min) Epoch 28/300 -- Iteration 208262 - Batch 308/7702 - Train loss: 0.00196019  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.09 min) Epoch 28/300 -- Iteration 208339 - Batch 385/7702 - Train loss: 0.00196423  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.11 min) Epoch 28/300 -- Iteration 208416 - Batch 462/7702 - Train loss: 0.00196710  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.13 min) Epoch 28/300 -- Iteration 208493 - Batch 539/7702 - Train loss: 0.00196823  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.15 min) Epoch 28/300 -- Iteration 208570 - Batch 616/7702 - Train loss: 0.00196978  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.17 min) Epoch 28/300 -- Iteration 208647 - Batch 693/7702 - Train loss: 0.00196655  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.19 min) Epoch 28/300 -- Iteration 208724 - Batch 770/7702 - Train loss: 0.00196493  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.21 min) Epoch 28/300 -- Iteration 208801 - Batch 847/7702 - Train loss: 0.00196549  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.23 min) Epoch 28/300 -- Iteration 208878 - Batch 924/7702 - Train loss: 0.00196525  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.25 min) Epoch 28/300 -- Iteration 208955 - Batch 1001/7702 - Train loss: 0.00196517  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.27 min) Epoch 28/300 -- Iteration 209032 - Batch 1078/7702 - Train loss: 0.00196198  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.29 min) Epoch 28/300 -- Iteration 209109 - Batch 1155/7702 - Train loss: 0.00196140  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.32 min) Epoch 28/300 -- Iteration 209186 - Batch 1232/7702 - Train loss: 0.00196389  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.34 min) Epoch 28/300 -- Iteration 209263 - Batch 1309/7702 - Train loss: 0.00196346  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.36 min) Epoch 28/300 -- Iteration 209340 - Batch 1386/7702 - Train loss: 0.00196261  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.38 min) Epoch 28/300 -- Iteration 209417 - Batch 1463/7702 - Train loss: 0.00196415  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.40 min) Epoch 28/300 -- Iteration 209494 - Batch 1540/7702 - Train loss: 0.00196566  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.42 min) Epoch 28/300 -- Iteration 209571 - Batch 1617/7702 - Train loss: 0.00196639  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.44 min) Epoch 28/300 -- Iteration 209648 - Batch 1694/7702 - Train loss: 0.00196859  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.46 min) Epoch 28/300 -- Iteration 209725 - Batch 1771/7702 - Train loss: 0.00196836  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.48 min) Epoch 28/300 -- Iteration 209802 - Batch 1848/7702 - Train loss: 0.00196883  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.50 min) Epoch 28/300 -- Iteration 209879 - Batch 1925/7702 - Train loss: 0.00196847  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.52 min) Epoch 28/300 -- Iteration 209956 - Batch 2002/7702 - Train loss: 0.00196848  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.54 min) Epoch 28/300 -- Iteration 210033 - Batch 2079/7702 - Train loss: 0.00196846  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.56 min) Epoch 28/300 -- Iteration 210110 - Batch 2156/7702 - Train loss: 0.00196763  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.58 min) Epoch 28/300 -- Iteration 210187 - Batch 2233/7702 - Train loss: 0.00196785  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.61 min) Epoch 28/300 -- Iteration 210264 - Batch 2310/7702 - Train loss: 0.00196780  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.63 min) Epoch 28/300 -- Iteration 210341 - Batch 2387/7702 - Train loss: 0.00196620  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.65 min) Epoch 28/300 -- Iteration 210418 - Batch 2464/7702 - Train loss: 0.00196606  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.67 min) Epoch 28/300 -- Iteration 210495 - Batch 2541/7702 - Train loss: 0.00196533  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.69 min) Epoch 28/300 -- Iteration 210572 - Batch 2618/7702 - Train loss: 0.00196517  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.71 min) Epoch 28/300 -- Iteration 210649 - Batch 2695/7702 - Train loss: 0.00196694  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.73 min) Epoch 28/300 -- Iteration 210726 - Batch 2772/7702 - Train loss: 0.00196696  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.75 min) Epoch 28/300 -- Iteration 210803 - Batch 2849/7702 - Train loss: 0.00196777  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.77 min) Epoch 28/300 -- Iteration 210880 - Batch 2926/7702 - Train loss: 0.00196660  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.79 min) Epoch 28/300 -- Iteration 210957 - Batch 3003/7702 - Train loss: 0.00196669  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.81 min) Epoch 28/300 -- Iteration 211034 - Batch 3080/7702 - Train loss: 0.00196690  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.83 min) Epoch 28/300 -- Iteration 211111 - Batch 3157/7702 - Train loss: 0.00196709  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.85 min) Epoch 28/300 -- Iteration 211188 - Batch 3234/7702 - Train loss: 0.00196748  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.87 min) Epoch 28/300 -- Iteration 211265 - Batch 3311/7702 - Train loss: 0.00196853  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.89 min) Epoch 28/300 -- Iteration 211342 - Batch 3388/7702 - Train loss: 0.00196874  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.92 min) Epoch 28/300 -- Iteration 211419 - Batch 3465/7702 - Train loss: 0.00196964  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.94 min) Epoch 28/300 -- Iteration 211496 - Batch 3542/7702 - Train loss: 0.00197012  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.96 min) Epoch 28/300 -- Iteration 211573 - Batch 3619/7702 - Train loss: 0.00196983  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(56.98 min) Epoch 28/300 -- Iteration 211650 - Batch 3696/7702 - Train loss: 0.00197009  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.00 min) Epoch 28/300 -- Iteration 211727 - Batch 3773/7702 - Train loss: 0.00197064  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.02 min) Epoch 28/300 -- Iteration 211804 - Batch 3850/7702 - Train loss: 0.00197167  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.04 min) Epoch 28/300 -- Iteration 211881 - Batch 3927/7702 - Train loss: 0.00197114  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.06 min) Epoch 28/300 -- Iteration 211958 - Batch 4004/7702 - Train loss: 0.00197095  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.08 min) Epoch 28/300 -- Iteration 212035 - Batch 4081/7702 - Train loss: 0.00197068  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.10 min) Epoch 28/300 -- Iteration 212112 - Batch 4158/7702 - Train loss: 0.00197090  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.12 min) Epoch 28/300 -- Iteration 212189 - Batch 4235/7702 - Train loss: 0.00197099  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.14 min) Epoch 28/300 -- Iteration 212266 - Batch 4312/7702 - Train loss: 0.00197052  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.16 min) Epoch 28/300 -- Iteration 212343 - Batch 4389/7702 - Train loss: 0.00197101  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.19 min) Epoch 28/300 -- Iteration 212420 - Batch 4466/7702 - Train loss: 0.00197009  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.21 min) Epoch 28/300 -- Iteration 212497 - Batch 4543/7702 - Train loss: 0.00196947  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.23 min) Epoch 28/300 -- Iteration 212574 - Batch 4620/7702 - Train loss: 0.00196941  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.25 min) Epoch 28/300 -- Iteration 212651 - Batch 4697/7702 - Train loss: 0.00196911  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.27 min) Epoch 28/300 -- Iteration 212728 - Batch 4774/7702 - Train loss: 0.00196864  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.29 min) Epoch 28/300 -- Iteration 212805 - Batch 4851/7702 - Train loss: 0.00196845  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.31 min) Epoch 28/300 -- Iteration 212882 - Batch 4928/7702 - Train loss: 0.00196826  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.33 min) Epoch 28/300 -- Iteration 212959 - Batch 5005/7702 - Train loss: 0.00196837  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.35 min) Epoch 28/300 -- Iteration 213036 - Batch 5082/7702 - Train loss: 0.00196826  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.37 min) Epoch 28/300 -- Iteration 213113 - Batch 5159/7702 - Train loss: 0.00196849  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.39 min) Epoch 28/300 -- Iteration 213190 - Batch 5236/7702 - Train loss: 0.00196945  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.41 min) Epoch 28/300 -- Iteration 213267 - Batch 5313/7702 - Train loss: 0.00196911  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.43 min) Epoch 28/300 -- Iteration 213344 - Batch 5390/7702 - Train loss: 0.00196948  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.45 min) Epoch 28/300 -- Iteration 213421 - Batch 5467/7702 - Train loss: 0.00196980  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.48 min) Epoch 28/300 -- Iteration 213498 - Batch 5544/7702 - Train loss: 0.00196993  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.50 min) Epoch 28/300 -- Iteration 213575 - Batch 5621/7702 - Train loss: 0.00197025  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.52 min) Epoch 28/300 -- Iteration 213652 - Batch 5698/7702 - Train loss: 0.00197026  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.54 min) Epoch 28/300 -- Iteration 213729 - Batch 5775/7702 - Train loss: 0.00197025  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.56 min) Epoch 28/300 -- Iteration 213806 - Batch 5852/7702 - Train loss: 0.00197002  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.58 min) Epoch 28/300 -- Iteration 213883 - Batch 5929/7702 - Train loss: 0.00197040  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.60 min) Epoch 28/300 -- Iteration 213960 - Batch 6006/7702 - Train loss: 0.00196996  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.62 min) Epoch 28/300 -- Iteration 214037 - Batch 6083/7702 - Train loss: 0.00196986  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.64 min) Epoch 28/300 -- Iteration 214114 - Batch 6160/7702 - Train loss: 0.00196939  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.66 min) Epoch 28/300 -- Iteration 214191 - Batch 6237/7702 - Train loss: 0.00196923  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.68 min) Epoch 28/300 -- Iteration 214268 - Batch 6314/7702 - Train loss: 0.00196952  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.70 min) Epoch 28/300 -- Iteration 214345 - Batch 6391/7702 - Train loss: 0.00196887  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.72 min) Epoch 28/300 -- Iteration 214422 - Batch 6468/7702 - Train loss: 0.00196839  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.74 min) Epoch 28/300 -- Iteration 214499 - Batch 6545/7702 - Train loss: 0.00196841  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.76 min) Epoch 28/300 -- Iteration 214576 - Batch 6622/7702 - Train loss: 0.00196854  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.79 min) Epoch 28/300 -- Iteration 214653 - Batch 6699/7702 - Train loss: 0.00196825  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.81 min) Epoch 28/300 -- Iteration 214730 - Batch 6776/7702 - Train loss: 0.00196721  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.83 min) Epoch 28/300 -- Iteration 214807 - Batch 6853/7702 - Train loss: 0.00196700  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.85 min) Epoch 28/300 -- Iteration 214884 - Batch 6930/7702 - Train loss: 0.00196687  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.87 min) Epoch 28/300 -- Iteration 214961 - Batch 7007/7702 - Train loss: 0.00196700  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.89 min) Epoch 28/300 -- Iteration 215038 - Batch 7084/7702 - Train loss: 0.00196707  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.91 min) Epoch 28/300 -- Iteration 215115 - Batch 7161/7702 - Train loss: 0.00196705  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.93 min) Epoch 28/300 -- Iteration 215192 - Batch 7238/7702 - Train loss: 0.00196705  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.95 min) Epoch 28/300 -- Iteration 215269 - Batch 7315/7702 - Train loss: 0.00196680  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.97 min) Epoch 28/300 -- Iteration 215346 - Batch 7392/7702 - Train loss: 0.00196698  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(57.99 min) Epoch 28/300 -- Iteration 215423 - Batch 7469/7702 - Train loss: 0.00196674  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(58.01 min) Epoch 28/300 -- Iteration 215500 - Batch 7546/7702 - Train loss: 0.00196650  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(58.03 min) Epoch 28/300 -- Iteration 215577 - Batch 7623/7702 - Train loss: 0.00196700  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(58.05 min) Epoch 28/300 -- Iteration 215654 - Batch 7700/7702 - Train loss: 0.00196674  - Train acc: -0.0000 - Val loss: 0.00051221\n",
      "(58.05 min) Epoch 28/300 -- Iteration 215656 - Batch 7701/7702 - Train loss: 0.00196677  - Train acc: -0.0000 - Val loss: 0.00055289 - Val acc: -0.0000\n",
      "(58.08 min) Epoch 29/300 -- Iteration 215733 - Batch 77/7702 - Train loss: 0.00193126  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.10 min) Epoch 29/300 -- Iteration 215810 - Batch 154/7702 - Train loss: 0.00195203  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.12 min) Epoch 29/300 -- Iteration 215887 - Batch 231/7702 - Train loss: 0.00195628  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.14 min) Epoch 29/300 -- Iteration 215964 - Batch 308/7702 - Train loss: 0.00196305  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.16 min) Epoch 29/300 -- Iteration 216041 - Batch 385/7702 - Train loss: 0.00196602  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.18 min) Epoch 29/300 -- Iteration 216118 - Batch 462/7702 - Train loss: 0.00198103  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.20 min) Epoch 29/300 -- Iteration 216195 - Batch 539/7702 - Train loss: 0.00198465  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.22 min) Epoch 29/300 -- Iteration 216272 - Batch 616/7702 - Train loss: 0.00198292  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.24 min) Epoch 29/300 -- Iteration 216349 - Batch 693/7702 - Train loss: 0.00198185  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.26 min) Epoch 29/300 -- Iteration 216426 - Batch 770/7702 - Train loss: 0.00197963  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.28 min) Epoch 29/300 -- Iteration 216503 - Batch 847/7702 - Train loss: 0.00197785  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.31 min) Epoch 29/300 -- Iteration 216580 - Batch 924/7702 - Train loss: 0.00197700  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.33 min) Epoch 29/300 -- Iteration 216657 - Batch 1001/7702 - Train loss: 0.00197826  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.35 min) Epoch 29/300 -- Iteration 216734 - Batch 1078/7702 - Train loss: 0.00197504  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.37 min) Epoch 29/300 -- Iteration 216811 - Batch 1155/7702 - Train loss: 0.00197543  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.39 min) Epoch 29/300 -- Iteration 216888 - Batch 1232/7702 - Train loss: 0.00197561  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.41 min) Epoch 29/300 -- Iteration 216965 - Batch 1309/7702 - Train loss: 0.00197485  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.43 min) Epoch 29/300 -- Iteration 217042 - Batch 1386/7702 - Train loss: 0.00197286  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.45 min) Epoch 29/300 -- Iteration 217119 - Batch 1463/7702 - Train loss: 0.00197473  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.47 min) Epoch 29/300 -- Iteration 217196 - Batch 1540/7702 - Train loss: 0.00197496  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.49 min) Epoch 29/300 -- Iteration 217273 - Batch 1617/7702 - Train loss: 0.00197655  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.51 min) Epoch 29/300 -- Iteration 217350 - Batch 1694/7702 - Train loss: 0.00197658  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.53 min) Epoch 29/300 -- Iteration 217427 - Batch 1771/7702 - Train loss: 0.00197650  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.55 min) Epoch 29/300 -- Iteration 217504 - Batch 1848/7702 - Train loss: 0.00197510  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.57 min) Epoch 29/300 -- Iteration 217581 - Batch 1925/7702 - Train loss: 0.00197449  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.59 min) Epoch 29/300 -- Iteration 217658 - Batch 2002/7702 - Train loss: 0.00197369  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.62 min) Epoch 29/300 -- Iteration 217735 - Batch 2079/7702 - Train loss: 0.00197277  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.64 min) Epoch 29/300 -- Iteration 217812 - Batch 2156/7702 - Train loss: 0.00197318  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.66 min) Epoch 29/300 -- Iteration 217889 - Batch 2233/7702 - Train loss: 0.00197345  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.68 min) Epoch 29/300 -- Iteration 217966 - Batch 2310/7702 - Train loss: 0.00197269  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.70 min) Epoch 29/300 -- Iteration 218043 - Batch 2387/7702 - Train loss: 0.00197128  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.72 min) Epoch 29/300 -- Iteration 218120 - Batch 2464/7702 - Train loss: 0.00197235  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.74 min) Epoch 29/300 -- Iteration 218197 - Batch 2541/7702 - Train loss: 0.00197064  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.76 min) Epoch 29/300 -- Iteration 218274 - Batch 2618/7702 - Train loss: 0.00197104  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.78 min) Epoch 29/300 -- Iteration 218351 - Batch 2695/7702 - Train loss: 0.00197132  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.80 min) Epoch 29/300 -- Iteration 218428 - Batch 2772/7702 - Train loss: 0.00197166  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.82 min) Epoch 29/300 -- Iteration 218505 - Batch 2849/7702 - Train loss: 0.00197007  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.84 min) Epoch 29/300 -- Iteration 218582 - Batch 2926/7702 - Train loss: 0.00196986  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.86 min) Epoch 29/300 -- Iteration 218659 - Batch 3003/7702 - Train loss: 0.00197003  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.88 min) Epoch 29/300 -- Iteration 218736 - Batch 3080/7702 - Train loss: 0.00197039  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.90 min) Epoch 29/300 -- Iteration 218813 - Batch 3157/7702 - Train loss: 0.00197150  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.93 min) Epoch 29/300 -- Iteration 218890 - Batch 3234/7702 - Train loss: 0.00197100  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.95 min) Epoch 29/300 -- Iteration 218967 - Batch 3311/7702 - Train loss: 0.00197028  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.97 min) Epoch 29/300 -- Iteration 219044 - Batch 3388/7702 - Train loss: 0.00196967  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(58.99 min) Epoch 29/300 -- Iteration 219121 - Batch 3465/7702 - Train loss: 0.00196963  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.01 min) Epoch 29/300 -- Iteration 219198 - Batch 3542/7702 - Train loss: 0.00196974  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.03 min) Epoch 29/300 -- Iteration 219275 - Batch 3619/7702 - Train loss: 0.00196947  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.05 min) Epoch 29/300 -- Iteration 219352 - Batch 3696/7702 - Train loss: 0.00197011  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.07 min) Epoch 29/300 -- Iteration 219429 - Batch 3773/7702 - Train loss: 0.00196994  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.09 min) Epoch 29/300 -- Iteration 219506 - Batch 3850/7702 - Train loss: 0.00196938  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.11 min) Epoch 29/300 -- Iteration 219583 - Batch 3927/7702 - Train loss: 0.00196938  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.13 min) Epoch 29/300 -- Iteration 219660 - Batch 4004/7702 - Train loss: 0.00196863  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.15 min) Epoch 29/300 -- Iteration 219737 - Batch 4081/7702 - Train loss: 0.00196885  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.17 min) Epoch 29/300 -- Iteration 219814 - Batch 4158/7702 - Train loss: 0.00196827  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.19 min) Epoch 29/300 -- Iteration 219891 - Batch 4235/7702 - Train loss: 0.00196808  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.22 min) Epoch 29/300 -- Iteration 219968 - Batch 4312/7702 - Train loss: 0.00196777  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.24 min) Epoch 29/300 -- Iteration 220045 - Batch 4389/7702 - Train loss: 0.00196759  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.26 min) Epoch 29/300 -- Iteration 220122 - Batch 4466/7702 - Train loss: 0.00196779  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.28 min) Epoch 29/300 -- Iteration 220199 - Batch 4543/7702 - Train loss: 0.00196789  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.30 min) Epoch 29/300 -- Iteration 220276 - Batch 4620/7702 - Train loss: 0.00196777  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.32 min) Epoch 29/300 -- Iteration 220353 - Batch 4697/7702 - Train loss: 0.00196791  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.34 min) Epoch 29/300 -- Iteration 220430 - Batch 4774/7702 - Train loss: 0.00196790  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.36 min) Epoch 29/300 -- Iteration 220507 - Batch 4851/7702 - Train loss: 0.00196705  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.38 min) Epoch 29/300 -- Iteration 220584 - Batch 4928/7702 - Train loss: 0.00196707  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.40 min) Epoch 29/300 -- Iteration 220661 - Batch 5005/7702 - Train loss: 0.00196729  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.42 min) Epoch 29/300 -- Iteration 220738 - Batch 5082/7702 - Train loss: 0.00196679  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.44 min) Epoch 29/300 -- Iteration 220815 - Batch 5159/7702 - Train loss: 0.00196689  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.46 min) Epoch 29/300 -- Iteration 220892 - Batch 5236/7702 - Train loss: 0.00196721  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.48 min) Epoch 29/300 -- Iteration 220969 - Batch 5313/7702 - Train loss: 0.00196704  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.50 min) Epoch 29/300 -- Iteration 221046 - Batch 5390/7702 - Train loss: 0.00196662  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.52 min) Epoch 29/300 -- Iteration 221123 - Batch 5467/7702 - Train loss: 0.00196628  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.55 min) Epoch 29/300 -- Iteration 221200 - Batch 5544/7702 - Train loss: 0.00196623  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.57 min) Epoch 29/300 -- Iteration 221277 - Batch 5621/7702 - Train loss: 0.00196619  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.59 min) Epoch 29/300 -- Iteration 221354 - Batch 5698/7702 - Train loss: 0.00196608  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.61 min) Epoch 29/300 -- Iteration 221431 - Batch 5775/7702 - Train loss: 0.00196613  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.63 min) Epoch 29/300 -- Iteration 221508 - Batch 5852/7702 - Train loss: 0.00196603  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.65 min) Epoch 29/300 -- Iteration 221585 - Batch 5929/7702 - Train loss: 0.00196587  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.67 min) Epoch 29/300 -- Iteration 221662 - Batch 6006/7702 - Train loss: 0.00196545  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.69 min) Epoch 29/300 -- Iteration 221739 - Batch 6083/7702 - Train loss: 0.00196515  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.71 min) Epoch 29/300 -- Iteration 221816 - Batch 6160/7702 - Train loss: 0.00196520  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.73 min) Epoch 29/300 -- Iteration 221893 - Batch 6237/7702 - Train loss: 0.00196528  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.75 min) Epoch 29/300 -- Iteration 221970 - Batch 6314/7702 - Train loss: 0.00196555  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.77 min) Epoch 29/300 -- Iteration 222047 - Batch 6391/7702 - Train loss: 0.00196613  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.79 min) Epoch 29/300 -- Iteration 222124 - Batch 6468/7702 - Train loss: 0.00196626  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.81 min) Epoch 29/300 -- Iteration 222201 - Batch 6545/7702 - Train loss: 0.00196650  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.84 min) Epoch 29/300 -- Iteration 222278 - Batch 6622/7702 - Train loss: 0.00196669  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.86 min) Epoch 29/300 -- Iteration 222355 - Batch 6699/7702 - Train loss: 0.00196670  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.88 min) Epoch 29/300 -- Iteration 222432 - Batch 6776/7702 - Train loss: 0.00196672  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.90 min) Epoch 29/300 -- Iteration 222509 - Batch 6853/7702 - Train loss: 0.00196662  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.92 min) Epoch 29/300 -- Iteration 222586 - Batch 6930/7702 - Train loss: 0.00196625  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.94 min) Epoch 29/300 -- Iteration 222663 - Batch 7007/7702 - Train loss: 0.00196618  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.96 min) Epoch 29/300 -- Iteration 222740 - Batch 7084/7702 - Train loss: 0.00196624  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(59.98 min) Epoch 29/300 -- Iteration 222817 - Batch 7161/7702 - Train loss: 0.00196615  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(60.00 min) Epoch 29/300 -- Iteration 222894 - Batch 7238/7702 - Train loss: 0.00196623  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(60.02 min) Epoch 29/300 -- Iteration 222971 - Batch 7315/7702 - Train loss: 0.00196647  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(60.04 min) Epoch 29/300 -- Iteration 223048 - Batch 7392/7702 - Train loss: 0.00196654  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(60.06 min) Epoch 29/300 -- Iteration 223125 - Batch 7469/7702 - Train loss: 0.00196668  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(60.08 min) Epoch 29/300 -- Iteration 223202 - Batch 7546/7702 - Train loss: 0.00196639  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(60.11 min) Epoch 29/300 -- Iteration 223279 - Batch 7623/7702 - Train loss: 0.00196637  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(60.13 min) Epoch 29/300 -- Iteration 223356 - Batch 7700/7702 - Train loss: 0.00196622  - Train acc: -0.0000 - Val loss: 0.00055289\n",
      "(60.13 min) Epoch 29/300 -- Iteration 223358 - Batch 7701/7702 - Train loss: 0.00196628  - Train acc: -0.0000 - Val loss: 0.00056129 - Val acc: -0.0000\n",
      "(60.15 min) Epoch 30/300 -- Iteration 223435 - Batch 77/7702 - Train loss: 0.00194327  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.17 min) Epoch 30/300 -- Iteration 223512 - Batch 154/7702 - Train loss: 0.00194031  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.19 min) Epoch 30/300 -- Iteration 223589 - Batch 231/7702 - Train loss: 0.00196444  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.21 min) Epoch 30/300 -- Iteration 223666 - Batch 308/7702 - Train loss: 0.00195866  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.23 min) Epoch 30/300 -- Iteration 223743 - Batch 385/7702 - Train loss: 0.00195192  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.25 min) Epoch 30/300 -- Iteration 223820 - Batch 462/7702 - Train loss: 0.00195620  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.27 min) Epoch 30/300 -- Iteration 223897 - Batch 539/7702 - Train loss: 0.00195666  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.29 min) Epoch 30/300 -- Iteration 223974 - Batch 616/7702 - Train loss: 0.00195842  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.31 min) Epoch 30/300 -- Iteration 224051 - Batch 693/7702 - Train loss: 0.00195539  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.34 min) Epoch 30/300 -- Iteration 224128 - Batch 770/7702 - Train loss: 0.00195743  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.36 min) Epoch 30/300 -- Iteration 224205 - Batch 847/7702 - Train loss: 0.00195521  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.38 min) Epoch 30/300 -- Iteration 224282 - Batch 924/7702 - Train loss: 0.00195933  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.40 min) Epoch 30/300 -- Iteration 224359 - Batch 1001/7702 - Train loss: 0.00195797  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.42 min) Epoch 30/300 -- Iteration 224436 - Batch 1078/7702 - Train loss: 0.00195917  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.44 min) Epoch 30/300 -- Iteration 224513 - Batch 1155/7702 - Train loss: 0.00195849  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.46 min) Epoch 30/300 -- Iteration 224590 - Batch 1232/7702 - Train loss: 0.00195843  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.48 min) Epoch 30/300 -- Iteration 224667 - Batch 1309/7702 - Train loss: 0.00195803  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.50 min) Epoch 30/300 -- Iteration 224744 - Batch 1386/7702 - Train loss: 0.00195832  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.52 min) Epoch 30/300 -- Iteration 224821 - Batch 1463/7702 - Train loss: 0.00195851  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.54 min) Epoch 30/300 -- Iteration 224898 - Batch 1540/7702 - Train loss: 0.00195864  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.56 min) Epoch 30/300 -- Iteration 224975 - Batch 1617/7702 - Train loss: 0.00195841  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.58 min) Epoch 30/300 -- Iteration 225052 - Batch 1694/7702 - Train loss: 0.00195915  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.60 min) Epoch 30/300 -- Iteration 225129 - Batch 1771/7702 - Train loss: 0.00195928  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.63 min) Epoch 30/300 -- Iteration 225206 - Batch 1848/7702 - Train loss: 0.00195784  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.65 min) Epoch 30/300 -- Iteration 225283 - Batch 1925/7702 - Train loss: 0.00195959  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.67 min) Epoch 30/300 -- Iteration 225360 - Batch 2002/7702 - Train loss: 0.00195792  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.69 min) Epoch 30/300 -- Iteration 225437 - Batch 2079/7702 - Train loss: 0.00195701  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.71 min) Epoch 30/300 -- Iteration 225514 - Batch 2156/7702 - Train loss: 0.00195897  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.73 min) Epoch 30/300 -- Iteration 225591 - Batch 2233/7702 - Train loss: 0.00195865  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.75 min) Epoch 30/300 -- Iteration 225668 - Batch 2310/7702 - Train loss: 0.00195875  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.77 min) Epoch 30/300 -- Iteration 225745 - Batch 2387/7702 - Train loss: 0.00195867  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.79 min) Epoch 30/300 -- Iteration 225822 - Batch 2464/7702 - Train loss: 0.00195851  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.81 min) Epoch 30/300 -- Iteration 225899 - Batch 2541/7702 - Train loss: 0.00195738  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.83 min) Epoch 30/300 -- Iteration 225976 - Batch 2618/7702 - Train loss: 0.00195691  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.85 min) Epoch 30/300 -- Iteration 226053 - Batch 2695/7702 - Train loss: 0.00195734  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.87 min) Epoch 30/300 -- Iteration 226130 - Batch 2772/7702 - Train loss: 0.00195737  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.89 min) Epoch 30/300 -- Iteration 226207 - Batch 2849/7702 - Train loss: 0.00195750  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.91 min) Epoch 30/300 -- Iteration 226284 - Batch 2926/7702 - Train loss: 0.00195719  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.94 min) Epoch 30/300 -- Iteration 226361 - Batch 3003/7702 - Train loss: 0.00195699  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.96 min) Epoch 30/300 -- Iteration 226438 - Batch 3080/7702 - Train loss: 0.00195771  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(60.98 min) Epoch 30/300 -- Iteration 226515 - Batch 3157/7702 - Train loss: 0.00195693  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.00 min) Epoch 30/300 -- Iteration 226592 - Batch 3234/7702 - Train loss: 0.00195721  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.02 min) Epoch 30/300 -- Iteration 226669 - Batch 3311/7702 - Train loss: 0.00195727  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.04 min) Epoch 30/300 -- Iteration 226746 - Batch 3388/7702 - Train loss: 0.00195753  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.06 min) Epoch 30/300 -- Iteration 226823 - Batch 3465/7702 - Train loss: 0.00195797  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.08 min) Epoch 30/300 -- Iteration 226900 - Batch 3542/7702 - Train loss: 0.00195830  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.10 min) Epoch 30/300 -- Iteration 226977 - Batch 3619/7702 - Train loss: 0.00195862  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.12 min) Epoch 30/300 -- Iteration 227054 - Batch 3696/7702 - Train loss: 0.00195854  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.14 min) Epoch 30/300 -- Iteration 227131 - Batch 3773/7702 - Train loss: 0.00195914  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.16 min) Epoch 30/300 -- Iteration 227208 - Batch 3850/7702 - Train loss: 0.00195997  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.18 min) Epoch 30/300 -- Iteration 227285 - Batch 3927/7702 - Train loss: 0.00196024  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.20 min) Epoch 30/300 -- Iteration 227362 - Batch 4004/7702 - Train loss: 0.00196040  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.22 min) Epoch 30/300 -- Iteration 227439 - Batch 4081/7702 - Train loss: 0.00196079  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.25 min) Epoch 30/300 -- Iteration 227516 - Batch 4158/7702 - Train loss: 0.00196042  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.27 min) Epoch 30/300 -- Iteration 227593 - Batch 4235/7702 - Train loss: 0.00196026  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.29 min) Epoch 30/300 -- Iteration 227670 - Batch 4312/7702 - Train loss: 0.00196004  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.31 min) Epoch 30/300 -- Iteration 227747 - Batch 4389/7702 - Train loss: 0.00195908  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.33 min) Epoch 30/300 -- Iteration 227824 - Batch 4466/7702 - Train loss: 0.00195879  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.35 min) Epoch 30/300 -- Iteration 227901 - Batch 4543/7702 - Train loss: 0.00195830  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.37 min) Epoch 30/300 -- Iteration 227978 - Batch 4620/7702 - Train loss: 0.00195846  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.39 min) Epoch 30/300 -- Iteration 228055 - Batch 4697/7702 - Train loss: 0.00195898  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.41 min) Epoch 30/300 -- Iteration 228132 - Batch 4774/7702 - Train loss: 0.00195934  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.43 min) Epoch 30/300 -- Iteration 228209 - Batch 4851/7702 - Train loss: 0.00195941  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.45 min) Epoch 30/300 -- Iteration 228286 - Batch 4928/7702 - Train loss: 0.00195961  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.47 min) Epoch 30/300 -- Iteration 228363 - Batch 5005/7702 - Train loss: 0.00195951  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.49 min) Epoch 30/300 -- Iteration 228440 - Batch 5082/7702 - Train loss: 0.00195956  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.51 min) Epoch 30/300 -- Iteration 228517 - Batch 5159/7702 - Train loss: 0.00195947  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.54 min) Epoch 30/300 -- Iteration 228594 - Batch 5236/7702 - Train loss: 0.00195932  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.56 min) Epoch 30/300 -- Iteration 228671 - Batch 5313/7702 - Train loss: 0.00195926  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.58 min) Epoch 30/300 -- Iteration 228748 - Batch 5390/7702 - Train loss: 0.00195905  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.60 min) Epoch 30/300 -- Iteration 228825 - Batch 5467/7702 - Train loss: 0.00195928  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.62 min) Epoch 30/300 -- Iteration 228902 - Batch 5544/7702 - Train loss: 0.00195928  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.64 min) Epoch 30/300 -- Iteration 228979 - Batch 5621/7702 - Train loss: 0.00195943  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.66 min) Epoch 30/300 -- Iteration 229056 - Batch 5698/7702 - Train loss: 0.00195984  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.68 min) Epoch 30/300 -- Iteration 229133 - Batch 5775/7702 - Train loss: 0.00196016  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.70 min) Epoch 30/300 -- Iteration 229210 - Batch 5852/7702 - Train loss: 0.00196029  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.72 min) Epoch 30/300 -- Iteration 229287 - Batch 5929/7702 - Train loss: 0.00196062  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.74 min) Epoch 30/300 -- Iteration 229364 - Batch 6006/7702 - Train loss: 0.00196050  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.76 min) Epoch 30/300 -- Iteration 229441 - Batch 6083/7702 - Train loss: 0.00196103  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.78 min) Epoch 30/300 -- Iteration 229518 - Batch 6160/7702 - Train loss: 0.00196092  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.80 min) Epoch 30/300 -- Iteration 229595 - Batch 6237/7702 - Train loss: 0.00196085  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.83 min) Epoch 30/300 -- Iteration 229672 - Batch 6314/7702 - Train loss: 0.00196029  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.85 min) Epoch 30/300 -- Iteration 229749 - Batch 6391/7702 - Train loss: 0.00196002  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.87 min) Epoch 30/300 -- Iteration 229826 - Batch 6468/7702 - Train loss: 0.00195986  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.89 min) Epoch 30/300 -- Iteration 229903 - Batch 6545/7702 - Train loss: 0.00195969  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.91 min) Epoch 30/300 -- Iteration 229980 - Batch 6622/7702 - Train loss: 0.00196016  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.93 min) Epoch 30/300 -- Iteration 230057 - Batch 6699/7702 - Train loss: 0.00196026  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.95 min) Epoch 30/300 -- Iteration 230134 - Batch 6776/7702 - Train loss: 0.00195984  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.97 min) Epoch 30/300 -- Iteration 230211 - Batch 6853/7702 - Train loss: 0.00195995  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(61.99 min) Epoch 30/300 -- Iteration 230288 - Batch 6930/7702 - Train loss: 0.00196007  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.01 min) Epoch 30/300 -- Iteration 230365 - Batch 7007/7702 - Train loss: 0.00196024  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.03 min) Epoch 30/300 -- Iteration 230442 - Batch 7084/7702 - Train loss: 0.00196050  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.05 min) Epoch 30/300 -- Iteration 230519 - Batch 7161/7702 - Train loss: 0.00196019  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.07 min) Epoch 30/300 -- Iteration 230596 - Batch 7238/7702 - Train loss: 0.00195991  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.09 min) Epoch 30/300 -- Iteration 230673 - Batch 7315/7702 - Train loss: 0.00195999  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.11 min) Epoch 30/300 -- Iteration 230750 - Batch 7392/7702 - Train loss: 0.00195954  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.13 min) Epoch 30/300 -- Iteration 230827 - Batch 7469/7702 - Train loss: 0.00195928  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.16 min) Epoch 30/300 -- Iteration 230904 - Batch 7546/7702 - Train loss: 0.00195884  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.18 min) Epoch 30/300 -- Iteration 230981 - Batch 7623/7702 - Train loss: 0.00195881  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.20 min) Epoch 30/300 -- Iteration 231058 - Batch 7700/7702 - Train loss: 0.00195879  - Train acc: -0.0000 - Val loss: 0.00056129\n",
      "(62.20 min) Epoch 30/300 -- Iteration 231060 - Batch 7701/7702 - Train loss: 0.00195882  - Train acc: -0.0000 - Val loss: 0.00056287 - Val acc: -0.0000\n",
      "(62.22 min) Epoch 31/300 -- Iteration 231137 - Batch 77/7702 - Train loss: 0.00194349  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.24 min) Epoch 31/300 -- Iteration 231214 - Batch 154/7702 - Train loss: 0.00196142  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.26 min) Epoch 31/300 -- Iteration 231291 - Batch 231/7702 - Train loss: 0.00196736  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.28 min) Epoch 31/300 -- Iteration 231368 - Batch 308/7702 - Train loss: 0.00196605  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.30 min) Epoch 31/300 -- Iteration 231445 - Batch 385/7702 - Train loss: 0.00195536  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.32 min) Epoch 31/300 -- Iteration 231522 - Batch 462/7702 - Train loss: 0.00195246  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.34 min) Epoch 31/300 -- Iteration 231599 - Batch 539/7702 - Train loss: 0.00195693  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.36 min) Epoch 31/300 -- Iteration 231676 - Batch 616/7702 - Train loss: 0.00196483  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.39 min) Epoch 31/300 -- Iteration 231753 - Batch 693/7702 - Train loss: 0.00196646  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.41 min) Epoch 31/300 -- Iteration 231830 - Batch 770/7702 - Train loss: 0.00196447  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.43 min) Epoch 31/300 -- Iteration 231907 - Batch 847/7702 - Train loss: 0.00196245  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.45 min) Epoch 31/300 -- Iteration 231984 - Batch 924/7702 - Train loss: 0.00196047  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.47 min) Epoch 31/300 -- Iteration 232061 - Batch 1001/7702 - Train loss: 0.00196096  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.49 min) Epoch 31/300 -- Iteration 232138 - Batch 1078/7702 - Train loss: 0.00196021  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.51 min) Epoch 31/300 -- Iteration 232215 - Batch 1155/7702 - Train loss: 0.00195751  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.53 min) Epoch 31/300 -- Iteration 232292 - Batch 1232/7702 - Train loss: 0.00195780  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.55 min) Epoch 31/300 -- Iteration 232369 - Batch 1309/7702 - Train loss: 0.00195713  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.57 min) Epoch 31/300 -- Iteration 232446 - Batch 1386/7702 - Train loss: 0.00195742  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.59 min) Epoch 31/300 -- Iteration 232523 - Batch 1463/7702 - Train loss: 0.00195825  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.61 min) Epoch 31/300 -- Iteration 232600 - Batch 1540/7702 - Train loss: 0.00196018  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.63 min) Epoch 31/300 -- Iteration 232677 - Batch 1617/7702 - Train loss: 0.00195933  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.65 min) Epoch 31/300 -- Iteration 232754 - Batch 1694/7702 - Train loss: 0.00195964  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.68 min) Epoch 31/300 -- Iteration 232831 - Batch 1771/7702 - Train loss: 0.00195811  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.70 min) Epoch 31/300 -- Iteration 232908 - Batch 1848/7702 - Train loss: 0.00195882  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.72 min) Epoch 31/300 -- Iteration 232985 - Batch 1925/7702 - Train loss: 0.00195888  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.74 min) Epoch 31/300 -- Iteration 233062 - Batch 2002/7702 - Train loss: 0.00196089  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.76 min) Epoch 31/300 -- Iteration 233139 - Batch 2079/7702 - Train loss: 0.00196319  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.78 min) Epoch 31/300 -- Iteration 233216 - Batch 2156/7702 - Train loss: 0.00196217  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.80 min) Epoch 31/300 -- Iteration 233293 - Batch 2233/7702 - Train loss: 0.00196105  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.82 min) Epoch 31/300 -- Iteration 233370 - Batch 2310/7702 - Train loss: 0.00196071  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.84 min) Epoch 31/300 -- Iteration 233447 - Batch 2387/7702 - Train loss: 0.00196043  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.86 min) Epoch 31/300 -- Iteration 233524 - Batch 2464/7702 - Train loss: 0.00196082  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.88 min) Epoch 31/300 -- Iteration 233601 - Batch 2541/7702 - Train loss: 0.00196150  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.90 min) Epoch 31/300 -- Iteration 233678 - Batch 2618/7702 - Train loss: 0.00196088  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.92 min) Epoch 31/300 -- Iteration 233755 - Batch 2695/7702 - Train loss: 0.00196036  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.94 min) Epoch 31/300 -- Iteration 233832 - Batch 2772/7702 - Train loss: 0.00196054  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.96 min) Epoch 31/300 -- Iteration 233909 - Batch 2849/7702 - Train loss: 0.00196039  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(62.99 min) Epoch 31/300 -- Iteration 233986 - Batch 2926/7702 - Train loss: 0.00195969  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.01 min) Epoch 31/300 -- Iteration 234063 - Batch 3003/7702 - Train loss: 0.00195942  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.03 min) Epoch 31/300 -- Iteration 234140 - Batch 3080/7702 - Train loss: 0.00195994  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.05 min) Epoch 31/300 -- Iteration 234217 - Batch 3157/7702 - Train loss: 0.00195964  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.07 min) Epoch 31/300 -- Iteration 234294 - Batch 3234/7702 - Train loss: 0.00196030  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.09 min) Epoch 31/300 -- Iteration 234371 - Batch 3311/7702 - Train loss: 0.00196085  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.11 min) Epoch 31/300 -- Iteration 234448 - Batch 3388/7702 - Train loss: 0.00196080  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.13 min) Epoch 31/300 -- Iteration 234525 - Batch 3465/7702 - Train loss: 0.00196041  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.15 min) Epoch 31/300 -- Iteration 234602 - Batch 3542/7702 - Train loss: 0.00195954  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.17 min) Epoch 31/300 -- Iteration 234679 - Batch 3619/7702 - Train loss: 0.00195970  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.19 min) Epoch 31/300 -- Iteration 234756 - Batch 3696/7702 - Train loss: 0.00196003  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.21 min) Epoch 31/300 -- Iteration 234833 - Batch 3773/7702 - Train loss: 0.00195984  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.23 min) Epoch 31/300 -- Iteration 234910 - Batch 3850/7702 - Train loss: 0.00196013  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.25 min) Epoch 31/300 -- Iteration 234987 - Batch 3927/7702 - Train loss: 0.00195950  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.27 min) Epoch 31/300 -- Iteration 235064 - Batch 4004/7702 - Train loss: 0.00195915  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.29 min) Epoch 31/300 -- Iteration 235141 - Batch 4081/7702 - Train loss: 0.00195900  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.32 min) Epoch 31/300 -- Iteration 235218 - Batch 4158/7702 - Train loss: 0.00195883  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.34 min) Epoch 31/300 -- Iteration 235295 - Batch 4235/7702 - Train loss: 0.00195804  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.36 min) Epoch 31/300 -- Iteration 235372 - Batch 4312/7702 - Train loss: 0.00195835  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.38 min) Epoch 31/300 -- Iteration 235449 - Batch 4389/7702 - Train loss: 0.00195895  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.40 min) Epoch 31/300 -- Iteration 235526 - Batch 4466/7702 - Train loss: 0.00195928  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.42 min) Epoch 31/300 -- Iteration 235603 - Batch 4543/7702 - Train loss: 0.00195900  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.44 min) Epoch 31/300 -- Iteration 235680 - Batch 4620/7702 - Train loss: 0.00195895  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.46 min) Epoch 31/300 -- Iteration 235757 - Batch 4697/7702 - Train loss: 0.00195896  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.48 min) Epoch 31/300 -- Iteration 235834 - Batch 4774/7702 - Train loss: 0.00195912  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.50 min) Epoch 31/300 -- Iteration 235911 - Batch 4851/7702 - Train loss: 0.00195919  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.52 min) Epoch 31/300 -- Iteration 235988 - Batch 4928/7702 - Train loss: 0.00195940  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.54 min) Epoch 31/300 -- Iteration 236065 - Batch 5005/7702 - Train loss: 0.00195926  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.56 min) Epoch 31/300 -- Iteration 236142 - Batch 5082/7702 - Train loss: 0.00195855  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.58 min) Epoch 31/300 -- Iteration 236219 - Batch 5159/7702 - Train loss: 0.00195863  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.60 min) Epoch 31/300 -- Iteration 236296 - Batch 5236/7702 - Train loss: 0.00195847  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.63 min) Epoch 31/300 -- Iteration 236373 - Batch 5313/7702 - Train loss: 0.00195881  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.65 min) Epoch 31/300 -- Iteration 236450 - Batch 5390/7702 - Train loss: 0.00195860  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.67 min) Epoch 31/300 -- Iteration 236527 - Batch 5467/7702 - Train loss: 0.00195869  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.69 min) Epoch 31/300 -- Iteration 236604 - Batch 5544/7702 - Train loss: 0.00195852  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.71 min) Epoch 31/300 -- Iteration 236681 - Batch 5621/7702 - Train loss: 0.00195845  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.73 min) Epoch 31/300 -- Iteration 236758 - Batch 5698/7702 - Train loss: 0.00195867  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.75 min) Epoch 31/300 -- Iteration 236835 - Batch 5775/7702 - Train loss: 0.00195853  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.77 min) Epoch 31/300 -- Iteration 236912 - Batch 5852/7702 - Train loss: 0.00195865  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.79 min) Epoch 31/300 -- Iteration 236989 - Batch 5929/7702 - Train loss: 0.00195861  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.81 min) Epoch 31/300 -- Iteration 237066 - Batch 6006/7702 - Train loss: 0.00195848  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.83 min) Epoch 31/300 -- Iteration 237143 - Batch 6083/7702 - Train loss: 0.00195882  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.85 min) Epoch 31/300 -- Iteration 237220 - Batch 6160/7702 - Train loss: 0.00195942  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.87 min) Epoch 31/300 -- Iteration 237297 - Batch 6237/7702 - Train loss: 0.00195924  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.89 min) Epoch 31/300 -- Iteration 237374 - Batch 6314/7702 - Train loss: 0.00195965  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.92 min) Epoch 31/300 -- Iteration 237451 - Batch 6391/7702 - Train loss: 0.00195910  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.94 min) Epoch 31/300 -- Iteration 237528 - Batch 6468/7702 - Train loss: 0.00195891  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.96 min) Epoch 31/300 -- Iteration 237605 - Batch 6545/7702 - Train loss: 0.00195912  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(63.98 min) Epoch 31/300 -- Iteration 237682 - Batch 6622/7702 - Train loss: 0.00195852  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.00 min) Epoch 31/300 -- Iteration 237759 - Batch 6699/7702 - Train loss: 0.00195857  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.02 min) Epoch 31/300 -- Iteration 237836 - Batch 6776/7702 - Train loss: 0.00195849  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.04 min) Epoch 31/300 -- Iteration 237913 - Batch 6853/7702 - Train loss: 0.00195870  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.06 min) Epoch 31/300 -- Iteration 237990 - Batch 6930/7702 - Train loss: 0.00195840  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.08 min) Epoch 31/300 -- Iteration 238067 - Batch 7007/7702 - Train loss: 0.00195788  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.10 min) Epoch 31/300 -- Iteration 238144 - Batch 7084/7702 - Train loss: 0.00195771  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.12 min) Epoch 31/300 -- Iteration 238221 - Batch 7161/7702 - Train loss: 0.00195733  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.14 min) Epoch 31/300 -- Iteration 238298 - Batch 7238/7702 - Train loss: 0.00195762  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.16 min) Epoch 31/300 -- Iteration 238375 - Batch 7315/7702 - Train loss: 0.00195801  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.18 min) Epoch 31/300 -- Iteration 238452 - Batch 7392/7702 - Train loss: 0.00195817  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.21 min) Epoch 31/300 -- Iteration 238529 - Batch 7469/7702 - Train loss: 0.00195819  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.23 min) Epoch 31/300 -- Iteration 238606 - Batch 7546/7702 - Train loss: 0.00195834  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.25 min) Epoch 31/300 -- Iteration 238683 - Batch 7623/7702 - Train loss: 0.00195845  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.27 min) Epoch 31/300 -- Iteration 238760 - Batch 7700/7702 - Train loss: 0.00195887  - Train acc: -0.0000 - Val loss: 0.00056287\n",
      "(64.27 min) Epoch 31/300 -- Iteration 238762 - Batch 7701/7702 - Train loss: 0.00195889  - Train acc: -0.0000 - Val loss: 0.00054526 - Val acc: -0.0000\n",
      "(64.29 min) Epoch 32/300 -- Iteration 238839 - Batch 77/7702 - Train loss: 0.00197700  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.31 min) Epoch 32/300 -- Iteration 238916 - Batch 154/7702 - Train loss: 0.00198539  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.33 min) Epoch 32/300 -- Iteration 238993 - Batch 231/7702 - Train loss: 0.00198510  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.35 min) Epoch 32/300 -- Iteration 239070 - Batch 308/7702 - Train loss: 0.00197490  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.37 min) Epoch 32/300 -- Iteration 239147 - Batch 385/7702 - Train loss: 0.00197193  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.39 min) Epoch 32/300 -- Iteration 239224 - Batch 462/7702 - Train loss: 0.00196967  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.41 min) Epoch 32/300 -- Iteration 239301 - Batch 539/7702 - Train loss: 0.00196690  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.44 min) Epoch 32/300 -- Iteration 239378 - Batch 616/7702 - Train loss: 0.00196323  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.46 min) Epoch 32/300 -- Iteration 239455 - Batch 693/7702 - Train loss: 0.00196122  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.48 min) Epoch 32/300 -- Iteration 239532 - Batch 770/7702 - Train loss: 0.00195942  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.50 min) Epoch 32/300 -- Iteration 239609 - Batch 847/7702 - Train loss: 0.00195669  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.52 min) Epoch 32/300 -- Iteration 239686 - Batch 924/7702 - Train loss: 0.00195302  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.54 min) Epoch 32/300 -- Iteration 239763 - Batch 1001/7702 - Train loss: 0.00195317  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.56 min) Epoch 32/300 -- Iteration 239840 - Batch 1078/7702 - Train loss: 0.00195266  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.58 min) Epoch 32/300 -- Iteration 239917 - Batch 1155/7702 - Train loss: 0.00195430  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.60 min) Epoch 32/300 -- Iteration 239994 - Batch 1232/7702 - Train loss: 0.00195371  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.62 min) Epoch 32/300 -- Iteration 240071 - Batch 1309/7702 - Train loss: 0.00195499  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.64 min) Epoch 32/300 -- Iteration 240148 - Batch 1386/7702 - Train loss: 0.00195393  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.66 min) Epoch 32/300 -- Iteration 240225 - Batch 1463/7702 - Train loss: 0.00195429  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.68 min) Epoch 32/300 -- Iteration 240302 - Batch 1540/7702 - Train loss: 0.00195261  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.70 min) Epoch 32/300 -- Iteration 240379 - Batch 1617/7702 - Train loss: 0.00195454  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.73 min) Epoch 32/300 -- Iteration 240456 - Batch 1694/7702 - Train loss: 0.00195487  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.75 min) Epoch 32/300 -- Iteration 240533 - Batch 1771/7702 - Train loss: 0.00195486  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.77 min) Epoch 32/300 -- Iteration 240610 - Batch 1848/7702 - Train loss: 0.00195517  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.79 min) Epoch 32/300 -- Iteration 240687 - Batch 1925/7702 - Train loss: 0.00195415  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.81 min) Epoch 32/300 -- Iteration 240764 - Batch 2002/7702 - Train loss: 0.00195365  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.83 min) Epoch 32/300 -- Iteration 240841 - Batch 2079/7702 - Train loss: 0.00195303  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.85 min) Epoch 32/300 -- Iteration 240918 - Batch 2156/7702 - Train loss: 0.00195277  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.87 min) Epoch 32/300 -- Iteration 240995 - Batch 2233/7702 - Train loss: 0.00195224  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.89 min) Epoch 32/300 -- Iteration 241072 - Batch 2310/7702 - Train loss: 0.00195227  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.91 min) Epoch 32/300 -- Iteration 241149 - Batch 2387/7702 - Train loss: 0.00195293  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.93 min) Epoch 32/300 -- Iteration 241226 - Batch 2464/7702 - Train loss: 0.00195278  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.95 min) Epoch 32/300 -- Iteration 241303 - Batch 2541/7702 - Train loss: 0.00195394  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.97 min) Epoch 32/300 -- Iteration 241380 - Batch 2618/7702 - Train loss: 0.00195494  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(64.99 min) Epoch 32/300 -- Iteration 241457 - Batch 2695/7702 - Train loss: 0.00195491  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.01 min) Epoch 32/300 -- Iteration 241534 - Batch 2772/7702 - Train loss: 0.00195572  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.04 min) Epoch 32/300 -- Iteration 241611 - Batch 2849/7702 - Train loss: 0.00195562  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.06 min) Epoch 32/300 -- Iteration 241688 - Batch 2926/7702 - Train loss: 0.00195561  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.08 min) Epoch 32/300 -- Iteration 241765 - Batch 3003/7702 - Train loss: 0.00195451  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.10 min) Epoch 32/300 -- Iteration 241842 - Batch 3080/7702 - Train loss: 0.00195462  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.12 min) Epoch 32/300 -- Iteration 241919 - Batch 3157/7702 - Train loss: 0.00195483  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.14 min) Epoch 32/300 -- Iteration 241996 - Batch 3234/7702 - Train loss: 0.00195445  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.16 min) Epoch 32/300 -- Iteration 242073 - Batch 3311/7702 - Train loss: 0.00195579  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.18 min) Epoch 32/300 -- Iteration 242150 - Batch 3388/7702 - Train loss: 0.00195607  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.20 min) Epoch 32/300 -- Iteration 242227 - Batch 3465/7702 - Train loss: 0.00195631  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.22 min) Epoch 32/300 -- Iteration 242304 - Batch 3542/7702 - Train loss: 0.00195598  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.24 min) Epoch 32/300 -- Iteration 242381 - Batch 3619/7702 - Train loss: 0.00195645  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.26 min) Epoch 32/300 -- Iteration 242458 - Batch 3696/7702 - Train loss: 0.00195627  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.28 min) Epoch 32/300 -- Iteration 242535 - Batch 3773/7702 - Train loss: 0.00195668  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.30 min) Epoch 32/300 -- Iteration 242612 - Batch 3850/7702 - Train loss: 0.00195696  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.32 min) Epoch 32/300 -- Iteration 242689 - Batch 3927/7702 - Train loss: 0.00195687  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.35 min) Epoch 32/300 -- Iteration 242766 - Batch 4004/7702 - Train loss: 0.00195720  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.37 min) Epoch 32/300 -- Iteration 242843 - Batch 4081/7702 - Train loss: 0.00195709  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.39 min) Epoch 32/300 -- Iteration 242920 - Batch 4158/7702 - Train loss: 0.00195674  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.41 min) Epoch 32/300 -- Iteration 242997 - Batch 4235/7702 - Train loss: 0.00195720  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.43 min) Epoch 32/300 -- Iteration 243074 - Batch 4312/7702 - Train loss: 0.00195715  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.45 min) Epoch 32/300 -- Iteration 243151 - Batch 4389/7702 - Train loss: 0.00195752  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.47 min) Epoch 32/300 -- Iteration 243228 - Batch 4466/7702 - Train loss: 0.00195713  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.49 min) Epoch 32/300 -- Iteration 243305 - Batch 4543/7702 - Train loss: 0.00195729  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.51 min) Epoch 32/300 -- Iteration 243382 - Batch 4620/7702 - Train loss: 0.00195754  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.53 min) Epoch 32/300 -- Iteration 243459 - Batch 4697/7702 - Train loss: 0.00195783  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.55 min) Epoch 32/300 -- Iteration 243536 - Batch 4774/7702 - Train loss: 0.00195804  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.57 min) Epoch 32/300 -- Iteration 243613 - Batch 4851/7702 - Train loss: 0.00195881  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.59 min) Epoch 32/300 -- Iteration 243690 - Batch 4928/7702 - Train loss: 0.00195904  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.61 min) Epoch 32/300 -- Iteration 243767 - Batch 5005/7702 - Train loss: 0.00195869  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.64 min) Epoch 32/300 -- Iteration 243844 - Batch 5082/7702 - Train loss: 0.00195848  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.66 min) Epoch 32/300 -- Iteration 243921 - Batch 5159/7702 - Train loss: 0.00195882  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.68 min) Epoch 32/300 -- Iteration 243998 - Batch 5236/7702 - Train loss: 0.00195866  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.70 min) Epoch 32/300 -- Iteration 244075 - Batch 5313/7702 - Train loss: 0.00195843  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.72 min) Epoch 32/300 -- Iteration 244152 - Batch 5390/7702 - Train loss: 0.00195794  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.74 min) Epoch 32/300 -- Iteration 244229 - Batch 5467/7702 - Train loss: 0.00195725  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.76 min) Epoch 32/300 -- Iteration 244306 - Batch 5544/7702 - Train loss: 0.00195752  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.78 min) Epoch 32/300 -- Iteration 244383 - Batch 5621/7702 - Train loss: 0.00195766  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.80 min) Epoch 32/300 -- Iteration 244460 - Batch 5698/7702 - Train loss: 0.00195753  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.82 min) Epoch 32/300 -- Iteration 244537 - Batch 5775/7702 - Train loss: 0.00195693  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.84 min) Epoch 32/300 -- Iteration 244614 - Batch 5852/7702 - Train loss: 0.00195695  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.86 min) Epoch 32/300 -- Iteration 244691 - Batch 5929/7702 - Train loss: 0.00195655  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.88 min) Epoch 32/300 -- Iteration 244768 - Batch 6006/7702 - Train loss: 0.00195653  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.90 min) Epoch 32/300 -- Iteration 244845 - Batch 6083/7702 - Train loss: 0.00195603  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.92 min) Epoch 32/300 -- Iteration 244922 - Batch 6160/7702 - Train loss: 0.00195610  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.94 min) Epoch 32/300 -- Iteration 244999 - Batch 6237/7702 - Train loss: 0.00195571  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.97 min) Epoch 32/300 -- Iteration 245076 - Batch 6314/7702 - Train loss: 0.00195567  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(65.99 min) Epoch 32/300 -- Iteration 245153 - Batch 6391/7702 - Train loss: 0.00195544  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.01 min) Epoch 32/300 -- Iteration 245230 - Batch 6468/7702 - Train loss: 0.00195525  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.03 min) Epoch 32/300 -- Iteration 245307 - Batch 6545/7702 - Train loss: 0.00195536  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.05 min) Epoch 32/300 -- Iteration 245384 - Batch 6622/7702 - Train loss: 0.00195526  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.07 min) Epoch 32/300 -- Iteration 245461 - Batch 6699/7702 - Train loss: 0.00195544  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.09 min) Epoch 32/300 -- Iteration 245538 - Batch 6776/7702 - Train loss: 0.00195553  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.11 min) Epoch 32/300 -- Iteration 245615 - Batch 6853/7702 - Train loss: 0.00195601  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.13 min) Epoch 32/300 -- Iteration 245692 - Batch 6930/7702 - Train loss: 0.00195575  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.15 min) Epoch 32/300 -- Iteration 245769 - Batch 7007/7702 - Train loss: 0.00195590  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.17 min) Epoch 32/300 -- Iteration 245846 - Batch 7084/7702 - Train loss: 0.00195598  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.19 min) Epoch 32/300 -- Iteration 245923 - Batch 7161/7702 - Train loss: 0.00195628  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.21 min) Epoch 32/300 -- Iteration 246000 - Batch 7238/7702 - Train loss: 0.00195635  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.23 min) Epoch 32/300 -- Iteration 246077 - Batch 7315/7702 - Train loss: 0.00195689  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.25 min) Epoch 32/300 -- Iteration 246154 - Batch 7392/7702 - Train loss: 0.00195695  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.28 min) Epoch 32/300 -- Iteration 246231 - Batch 7469/7702 - Train loss: 0.00195749  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.30 min) Epoch 32/300 -- Iteration 246308 - Batch 7546/7702 - Train loss: 0.00195681  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.32 min) Epoch 32/300 -- Iteration 246385 - Batch 7623/7702 - Train loss: 0.00195695  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.34 min) Epoch 32/300 -- Iteration 246462 - Batch 7700/7702 - Train loss: 0.00195709  - Train acc: -0.0000 - Val loss: 0.00054526\n",
      "(66.34 min) Epoch 32/300 -- Iteration 246464 - Batch 7701/7702 - Train loss: 0.00195712  - Train acc: -0.0000 - Val loss: 0.00056666 - Val acc: -0.0000\n",
      "(66.36 min) Epoch 33/300 -- Iteration 246541 - Batch 77/7702 - Train loss: 0.00195551  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.38 min) Epoch 33/300 -- Iteration 246618 - Batch 154/7702 - Train loss: 0.00197451  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.40 min) Epoch 33/300 -- Iteration 246695 - Batch 231/7702 - Train loss: 0.00197489  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.42 min) Epoch 33/300 -- Iteration 246772 - Batch 308/7702 - Train loss: 0.00196665  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.44 min) Epoch 33/300 -- Iteration 246849 - Batch 385/7702 - Train loss: 0.00195664  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.46 min) Epoch 33/300 -- Iteration 246926 - Batch 462/7702 - Train loss: 0.00194886  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.48 min) Epoch 33/300 -- Iteration 247003 - Batch 539/7702 - Train loss: 0.00195170  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.51 min) Epoch 33/300 -- Iteration 247080 - Batch 616/7702 - Train loss: 0.00195074  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.53 min) Epoch 33/300 -- Iteration 247157 - Batch 693/7702 - Train loss: 0.00195033  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.55 min) Epoch 33/300 -- Iteration 247234 - Batch 770/7702 - Train loss: 0.00195211  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.57 min) Epoch 33/300 -- Iteration 247311 - Batch 847/7702 - Train loss: 0.00195038  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.59 min) Epoch 33/300 -- Iteration 247388 - Batch 924/7702 - Train loss: 0.00194680  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.61 min) Epoch 33/300 -- Iteration 247465 - Batch 1001/7702 - Train loss: 0.00194753  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.63 min) Epoch 33/300 -- Iteration 247542 - Batch 1078/7702 - Train loss: 0.00194747  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.65 min) Epoch 33/300 -- Iteration 247619 - Batch 1155/7702 - Train loss: 0.00194872  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.67 min) Epoch 33/300 -- Iteration 247696 - Batch 1232/7702 - Train loss: 0.00195017  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.69 min) Epoch 33/300 -- Iteration 247773 - Batch 1309/7702 - Train loss: 0.00195028  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.71 min) Epoch 33/300 -- Iteration 247850 - Batch 1386/7702 - Train loss: 0.00194859  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.73 min) Epoch 33/300 -- Iteration 247927 - Batch 1463/7702 - Train loss: 0.00194739  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.75 min) Epoch 33/300 -- Iteration 248004 - Batch 1540/7702 - Train loss: 0.00194890  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.78 min) Epoch 33/300 -- Iteration 248081 - Batch 1617/7702 - Train loss: 0.00195050  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.80 min) Epoch 33/300 -- Iteration 248158 - Batch 1694/7702 - Train loss: 0.00195153  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.82 min) Epoch 33/300 -- Iteration 248235 - Batch 1771/7702 - Train loss: 0.00195355  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.84 min) Epoch 33/300 -- Iteration 248312 - Batch 1848/7702 - Train loss: 0.00195200  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.86 min) Epoch 33/300 -- Iteration 248389 - Batch 1925/7702 - Train loss: 0.00195203  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.88 min) Epoch 33/300 -- Iteration 248466 - Batch 2002/7702 - Train loss: 0.00195192  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.90 min) Epoch 33/300 -- Iteration 248543 - Batch 2079/7702 - Train loss: 0.00195267  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.92 min) Epoch 33/300 -- Iteration 248620 - Batch 2156/7702 - Train loss: 0.00195214  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.94 min) Epoch 33/300 -- Iteration 248697 - Batch 2233/7702 - Train loss: 0.00195217  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.96 min) Epoch 33/300 -- Iteration 248774 - Batch 2310/7702 - Train loss: 0.00195275  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(66.98 min) Epoch 33/300 -- Iteration 248851 - Batch 2387/7702 - Train loss: 0.00195211  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.00 min) Epoch 33/300 -- Iteration 248928 - Batch 2464/7702 - Train loss: 0.00195344  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.02 min) Epoch 33/300 -- Iteration 249005 - Batch 2541/7702 - Train loss: 0.00195455  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.04 min) Epoch 33/300 -- Iteration 249082 - Batch 2618/7702 - Train loss: 0.00195484  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.07 min) Epoch 33/300 -- Iteration 249159 - Batch 2695/7702 - Train loss: 0.00195398  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.09 min) Epoch 33/300 -- Iteration 249236 - Batch 2772/7702 - Train loss: 0.00195252  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.11 min) Epoch 33/300 -- Iteration 249313 - Batch 2849/7702 - Train loss: 0.00195201  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.13 min) Epoch 33/300 -- Iteration 249390 - Batch 2926/7702 - Train loss: 0.00195085  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.15 min) Epoch 33/300 -- Iteration 249467 - Batch 3003/7702 - Train loss: 0.00195037  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.17 min) Epoch 33/300 -- Iteration 249544 - Batch 3080/7702 - Train loss: 0.00195068  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.19 min) Epoch 33/300 -- Iteration 249621 - Batch 3157/7702 - Train loss: 0.00195043  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.21 min) Epoch 33/300 -- Iteration 249698 - Batch 3234/7702 - Train loss: 0.00195052  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.23 min) Epoch 33/300 -- Iteration 249775 - Batch 3311/7702 - Train loss: 0.00195028  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.25 min) Epoch 33/300 -- Iteration 249852 - Batch 3388/7702 - Train loss: 0.00195041  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.27 min) Epoch 33/300 -- Iteration 249929 - Batch 3465/7702 - Train loss: 0.00195052  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.29 min) Epoch 33/300 -- Iteration 250006 - Batch 3542/7702 - Train loss: 0.00194957  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.31 min) Epoch 33/300 -- Iteration 250083 - Batch 3619/7702 - Train loss: 0.00194931  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.33 min) Epoch 33/300 -- Iteration 250160 - Batch 3696/7702 - Train loss: 0.00194942  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.36 min) Epoch 33/300 -- Iteration 250237 - Batch 3773/7702 - Train loss: 0.00195018  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.38 min) Epoch 33/300 -- Iteration 250314 - Batch 3850/7702 - Train loss: 0.00195029  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.40 min) Epoch 33/300 -- Iteration 250391 - Batch 3927/7702 - Train loss: 0.00194991  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.42 min) Epoch 33/300 -- Iteration 250468 - Batch 4004/7702 - Train loss: 0.00194991  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.44 min) Epoch 33/300 -- Iteration 250545 - Batch 4081/7702 - Train loss: 0.00195014  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.46 min) Epoch 33/300 -- Iteration 250622 - Batch 4158/7702 - Train loss: 0.00195015  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.48 min) Epoch 33/300 -- Iteration 250699 - Batch 4235/7702 - Train loss: 0.00195031  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.50 min) Epoch 33/300 -- Iteration 250776 - Batch 4312/7702 - Train loss: 0.00195007  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.52 min) Epoch 33/300 -- Iteration 250853 - Batch 4389/7702 - Train loss: 0.00195026  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.54 min) Epoch 33/300 -- Iteration 250930 - Batch 4466/7702 - Train loss: 0.00195052  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.56 min) Epoch 33/300 -- Iteration 251007 - Batch 4543/7702 - Train loss: 0.00195036  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.58 min) Epoch 33/300 -- Iteration 251084 - Batch 4620/7702 - Train loss: 0.00194996  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.60 min) Epoch 33/300 -- Iteration 251161 - Batch 4697/7702 - Train loss: 0.00195029  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.62 min) Epoch 33/300 -- Iteration 251238 - Batch 4774/7702 - Train loss: 0.00194998  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.65 min) Epoch 33/300 -- Iteration 251315 - Batch 4851/7702 - Train loss: 0.00194997  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.67 min) Epoch 33/300 -- Iteration 251392 - Batch 4928/7702 - Train loss: 0.00195054  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.69 min) Epoch 33/300 -- Iteration 251469 - Batch 5005/7702 - Train loss: 0.00195022  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.71 min) Epoch 33/300 -- Iteration 251546 - Batch 5082/7702 - Train loss: 0.00195018  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.73 min) Epoch 33/300 -- Iteration 251623 - Batch 5159/7702 - Train loss: 0.00194988  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.75 min) Epoch 33/300 -- Iteration 251700 - Batch 5236/7702 - Train loss: 0.00195024  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.77 min) Epoch 33/300 -- Iteration 251777 - Batch 5313/7702 - Train loss: 0.00195056  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.79 min) Epoch 33/300 -- Iteration 251854 - Batch 5390/7702 - Train loss: 0.00195005  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.81 min) Epoch 33/300 -- Iteration 251931 - Batch 5467/7702 - Train loss: 0.00194985  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.83 min) Epoch 33/300 -- Iteration 252008 - Batch 5544/7702 - Train loss: 0.00194996  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.85 min) Epoch 33/300 -- Iteration 252085 - Batch 5621/7702 - Train loss: 0.00195017  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.87 min) Epoch 33/300 -- Iteration 252162 - Batch 5698/7702 - Train loss: 0.00195014  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.89 min) Epoch 33/300 -- Iteration 252239 - Batch 5775/7702 - Train loss: 0.00195045  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.92 min) Epoch 33/300 -- Iteration 252316 - Batch 5852/7702 - Train loss: 0.00195056  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.94 min) Epoch 33/300 -- Iteration 252393 - Batch 5929/7702 - Train loss: 0.00195028  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.96 min) Epoch 33/300 -- Iteration 252470 - Batch 6006/7702 - Train loss: 0.00195009  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(67.98 min) Epoch 33/300 -- Iteration 252547 - Batch 6083/7702 - Train loss: 0.00195012  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.00 min) Epoch 33/300 -- Iteration 252624 - Batch 6160/7702 - Train loss: 0.00194955  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.02 min) Epoch 33/300 -- Iteration 252701 - Batch 6237/7702 - Train loss: 0.00194942  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.04 min) Epoch 33/300 -- Iteration 252778 - Batch 6314/7702 - Train loss: 0.00194948  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.06 min) Epoch 33/300 -- Iteration 252855 - Batch 6391/7702 - Train loss: 0.00194951  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.08 min) Epoch 33/300 -- Iteration 252932 - Batch 6468/7702 - Train loss: 0.00194896  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.10 min) Epoch 33/300 -- Iteration 253009 - Batch 6545/7702 - Train loss: 0.00194918  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.12 min) Epoch 33/300 -- Iteration 253086 - Batch 6622/7702 - Train loss: 0.00194932  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.14 min) Epoch 33/300 -- Iteration 253163 - Batch 6699/7702 - Train loss: 0.00194951  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.16 min) Epoch 33/300 -- Iteration 253240 - Batch 6776/7702 - Train loss: 0.00194932  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.19 min) Epoch 33/300 -- Iteration 253317 - Batch 6853/7702 - Train loss: 0.00194917  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.21 min) Epoch 33/300 -- Iteration 253394 - Batch 6930/7702 - Train loss: 0.00194914  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.23 min) Epoch 33/300 -- Iteration 253471 - Batch 7007/7702 - Train loss: 0.00194897  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.25 min) Epoch 33/300 -- Iteration 253548 - Batch 7084/7702 - Train loss: 0.00194910  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.27 min) Epoch 33/300 -- Iteration 253625 - Batch 7161/7702 - Train loss: 0.00194915  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.29 min) Epoch 33/300 -- Iteration 253702 - Batch 7238/7702 - Train loss: 0.00194899  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.31 min) Epoch 33/300 -- Iteration 253779 - Batch 7315/7702 - Train loss: 0.00194853  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.33 min) Epoch 33/300 -- Iteration 253856 - Batch 7392/7702 - Train loss: 0.00194864  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.35 min) Epoch 33/300 -- Iteration 253933 - Batch 7469/7702 - Train loss: 0.00194851  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.37 min) Epoch 33/300 -- Iteration 254010 - Batch 7546/7702 - Train loss: 0.00194813  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.39 min) Epoch 33/300 -- Iteration 254087 - Batch 7623/7702 - Train loss: 0.00194811  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.41 min) Epoch 33/300 -- Iteration 254164 - Batch 7700/7702 - Train loss: 0.00194831  - Train acc: -0.0000 - Val loss: 0.00056666\n",
      "(68.41 min) Epoch 33/300 -- Iteration 254166 - Batch 7701/7702 - Train loss: 0.00194827  - Train acc: -0.0000 - Val loss: 0.00050308 - Val acc: -0.0000\n",
      "(68.44 min) Epoch 34/300 -- Iteration 254243 - Batch 77/7702 - Train loss: 0.00193661  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.46 min) Epoch 34/300 -- Iteration 254320 - Batch 154/7702 - Train loss: 0.00193987  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.48 min) Epoch 34/300 -- Iteration 254397 - Batch 231/7702 - Train loss: 0.00195659  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.50 min) Epoch 34/300 -- Iteration 254474 - Batch 308/7702 - Train loss: 0.00195936  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.52 min) Epoch 34/300 -- Iteration 254551 - Batch 385/7702 - Train loss: 0.00196888  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.54 min) Epoch 34/300 -- Iteration 254628 - Batch 462/7702 - Train loss: 0.00196847  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.56 min) Epoch 34/300 -- Iteration 254705 - Batch 539/7702 - Train loss: 0.00196380  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.58 min) Epoch 34/300 -- Iteration 254782 - Batch 616/7702 - Train loss: 0.00196748  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.60 min) Epoch 34/300 -- Iteration 254859 - Batch 693/7702 - Train loss: 0.00196707  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.62 min) Epoch 34/300 -- Iteration 254936 - Batch 770/7702 - Train loss: 0.00196701  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.64 min) Epoch 34/300 -- Iteration 255013 - Batch 847/7702 - Train loss: 0.00196781  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.66 min) Epoch 34/300 -- Iteration 255090 - Batch 924/7702 - Train loss: 0.00196774  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.68 min) Epoch 34/300 -- Iteration 255167 - Batch 1001/7702 - Train loss: 0.00196808  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.71 min) Epoch 34/300 -- Iteration 255244 - Batch 1078/7702 - Train loss: 0.00196785  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.73 min) Epoch 34/300 -- Iteration 255321 - Batch 1155/7702 - Train loss: 0.00196890  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.75 min) Epoch 34/300 -- Iteration 255398 - Batch 1232/7702 - Train loss: 0.00196866  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.77 min) Epoch 34/300 -- Iteration 255475 - Batch 1309/7702 - Train loss: 0.00197200  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.79 min) Epoch 34/300 -- Iteration 255552 - Batch 1386/7702 - Train loss: 0.00196855  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.81 min) Epoch 34/300 -- Iteration 255629 - Batch 1463/7702 - Train loss: 0.00196690  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.83 min) Epoch 34/300 -- Iteration 255706 - Batch 1540/7702 - Train loss: 0.00196411  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.85 min) Epoch 34/300 -- Iteration 255783 - Batch 1617/7702 - Train loss: 0.00196220  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.87 min) Epoch 34/300 -- Iteration 255860 - Batch 1694/7702 - Train loss: 0.00196056  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.89 min) Epoch 34/300 -- Iteration 255937 - Batch 1771/7702 - Train loss: 0.00195956  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.91 min) Epoch 34/300 -- Iteration 256014 - Batch 1848/7702 - Train loss: 0.00195941  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.93 min) Epoch 34/300 -- Iteration 256091 - Batch 1925/7702 - Train loss: 0.00195848  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.95 min) Epoch 34/300 -- Iteration 256168 - Batch 2002/7702 - Train loss: 0.00195866  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.97 min) Epoch 34/300 -- Iteration 256245 - Batch 2079/7702 - Train loss: 0.00195733  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(68.99 min) Epoch 34/300 -- Iteration 256322 - Batch 2156/7702 - Train loss: 0.00195783  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.01 min) Epoch 34/300 -- Iteration 256399 - Batch 2233/7702 - Train loss: 0.00195742  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.04 min) Epoch 34/300 -- Iteration 256476 - Batch 2310/7702 - Train loss: 0.00195782  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.06 min) Epoch 34/300 -- Iteration 256553 - Batch 2387/7702 - Train loss: 0.00195909  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.08 min) Epoch 34/300 -- Iteration 256630 - Batch 2464/7702 - Train loss: 0.00195856  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.10 min) Epoch 34/300 -- Iteration 256707 - Batch 2541/7702 - Train loss: 0.00195735  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.12 min) Epoch 34/300 -- Iteration 256784 - Batch 2618/7702 - Train loss: 0.00195735  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.14 min) Epoch 34/300 -- Iteration 256861 - Batch 2695/7702 - Train loss: 0.00195598  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.16 min) Epoch 34/300 -- Iteration 256938 - Batch 2772/7702 - Train loss: 0.00195592  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.18 min) Epoch 34/300 -- Iteration 257015 - Batch 2849/7702 - Train loss: 0.00195553  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.20 min) Epoch 34/300 -- Iteration 257092 - Batch 2926/7702 - Train loss: 0.00195489  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.22 min) Epoch 34/300 -- Iteration 257169 - Batch 3003/7702 - Train loss: 0.00195512  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.24 min) Epoch 34/300 -- Iteration 257246 - Batch 3080/7702 - Train loss: 0.00195427  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.26 min) Epoch 34/300 -- Iteration 257323 - Batch 3157/7702 - Train loss: 0.00195456  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.28 min) Epoch 34/300 -- Iteration 257400 - Batch 3234/7702 - Train loss: 0.00195500  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.30 min) Epoch 34/300 -- Iteration 257477 - Batch 3311/7702 - Train loss: 0.00195403  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.33 min) Epoch 34/300 -- Iteration 257554 - Batch 3388/7702 - Train loss: 0.00195377  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.35 min) Epoch 34/300 -- Iteration 257631 - Batch 3465/7702 - Train loss: 0.00195340  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.37 min) Epoch 34/300 -- Iteration 257708 - Batch 3542/7702 - Train loss: 0.00195311  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.39 min) Epoch 34/300 -- Iteration 257785 - Batch 3619/7702 - Train loss: 0.00195210  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.41 min) Epoch 34/300 -- Iteration 257862 - Batch 3696/7702 - Train loss: 0.00195174  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.43 min) Epoch 34/300 -- Iteration 257939 - Batch 3773/7702 - Train loss: 0.00195128  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.45 min) Epoch 34/300 -- Iteration 258016 - Batch 3850/7702 - Train loss: 0.00195072  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.47 min) Epoch 34/300 -- Iteration 258093 - Batch 3927/7702 - Train loss: 0.00195088  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.49 min) Epoch 34/300 -- Iteration 258170 - Batch 4004/7702 - Train loss: 0.00195121  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.51 min) Epoch 34/300 -- Iteration 258247 - Batch 4081/7702 - Train loss: 0.00195144  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.53 min) Epoch 34/300 -- Iteration 258324 - Batch 4158/7702 - Train loss: 0.00195121  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.55 min) Epoch 34/300 -- Iteration 258401 - Batch 4235/7702 - Train loss: 0.00195096  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.57 min) Epoch 34/300 -- Iteration 258478 - Batch 4312/7702 - Train loss: 0.00195085  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.59 min) Epoch 34/300 -- Iteration 258555 - Batch 4389/7702 - Train loss: 0.00195001  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.61 min) Epoch 34/300 -- Iteration 258632 - Batch 4466/7702 - Train loss: 0.00194975  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.64 min) Epoch 34/300 -- Iteration 258709 - Batch 4543/7702 - Train loss: 0.00195042  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.66 min) Epoch 34/300 -- Iteration 258786 - Batch 4620/7702 - Train loss: 0.00195063  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.68 min) Epoch 34/300 -- Iteration 258863 - Batch 4697/7702 - Train loss: 0.00195062  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.70 min) Epoch 34/300 -- Iteration 258940 - Batch 4774/7702 - Train loss: 0.00195046  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.72 min) Epoch 34/300 -- Iteration 259017 - Batch 4851/7702 - Train loss: 0.00195054  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.74 min) Epoch 34/300 -- Iteration 259094 - Batch 4928/7702 - Train loss: 0.00195046  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.76 min) Epoch 34/300 -- Iteration 259171 - Batch 5005/7702 - Train loss: 0.00195052  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.78 min) Epoch 34/300 -- Iteration 259248 - Batch 5082/7702 - Train loss: 0.00195027  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.80 min) Epoch 34/300 -- Iteration 259325 - Batch 5159/7702 - Train loss: 0.00195029  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.82 min) Epoch 34/300 -- Iteration 259402 - Batch 5236/7702 - Train loss: 0.00195040  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.84 min) Epoch 34/300 -- Iteration 259479 - Batch 5313/7702 - Train loss: 0.00195014  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.86 min) Epoch 34/300 -- Iteration 259556 - Batch 5390/7702 - Train loss: 0.00195017  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.88 min) Epoch 34/300 -- Iteration 259633 - Batch 5467/7702 - Train loss: 0.00195066  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.90 min) Epoch 34/300 -- Iteration 259710 - Batch 5544/7702 - Train loss: 0.00195023  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.93 min) Epoch 34/300 -- Iteration 259787 - Batch 5621/7702 - Train loss: 0.00195026  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.95 min) Epoch 34/300 -- Iteration 259864 - Batch 5698/7702 - Train loss: 0.00195002  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.97 min) Epoch 34/300 -- Iteration 259941 - Batch 5775/7702 - Train loss: 0.00195004  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(69.99 min) Epoch 34/300 -- Iteration 260018 - Batch 5852/7702 - Train loss: 0.00194999  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.01 min) Epoch 34/300 -- Iteration 260095 - Batch 5929/7702 - Train loss: 0.00194992  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.03 min) Epoch 34/300 -- Iteration 260172 - Batch 6006/7702 - Train loss: 0.00194973  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.05 min) Epoch 34/300 -- Iteration 260249 - Batch 6083/7702 - Train loss: 0.00194943  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.07 min) Epoch 34/300 -- Iteration 260326 - Batch 6160/7702 - Train loss: 0.00194937  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.09 min) Epoch 34/300 -- Iteration 260403 - Batch 6237/7702 - Train loss: 0.00194921  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.11 min) Epoch 34/300 -- Iteration 260480 - Batch 6314/7702 - Train loss: 0.00194891  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.13 min) Epoch 34/300 -- Iteration 260557 - Batch 6391/7702 - Train loss: 0.00194824  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.15 min) Epoch 34/300 -- Iteration 260634 - Batch 6468/7702 - Train loss: 0.00194813  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.17 min) Epoch 34/300 -- Iteration 260711 - Batch 6545/7702 - Train loss: 0.00194829  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.19 min) Epoch 34/300 -- Iteration 260788 - Batch 6622/7702 - Train loss: 0.00194826  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.21 min) Epoch 34/300 -- Iteration 260865 - Batch 6699/7702 - Train loss: 0.00194837  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.23 min) Epoch 34/300 -- Iteration 260942 - Batch 6776/7702 - Train loss: 0.00194858  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.26 min) Epoch 34/300 -- Iteration 261019 - Batch 6853/7702 - Train loss: 0.00194885  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.28 min) Epoch 34/300 -- Iteration 261096 - Batch 6930/7702 - Train loss: 0.00194904  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.30 min) Epoch 34/300 -- Iteration 261173 - Batch 7007/7702 - Train loss: 0.00194905  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.32 min) Epoch 34/300 -- Iteration 261250 - Batch 7084/7702 - Train loss: 0.00194874  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.34 min) Epoch 34/300 -- Iteration 261327 - Batch 7161/7702 - Train loss: 0.00194856  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.36 min) Epoch 34/300 -- Iteration 261404 - Batch 7238/7702 - Train loss: 0.00194889  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.38 min) Epoch 34/300 -- Iteration 261481 - Batch 7315/7702 - Train loss: 0.00194877  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.40 min) Epoch 34/300 -- Iteration 261558 - Batch 7392/7702 - Train loss: 0.00194861  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.42 min) Epoch 34/300 -- Iteration 261635 - Batch 7469/7702 - Train loss: 0.00194862  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.44 min) Epoch 34/300 -- Iteration 261712 - Batch 7546/7702 - Train loss: 0.00194882  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.46 min) Epoch 34/300 -- Iteration 261789 - Batch 7623/7702 - Train loss: 0.00194890  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.48 min) Epoch 34/300 -- Iteration 261866 - Batch 7700/7702 - Train loss: 0.00194924  - Train acc: -0.0000 - Val loss: 0.00050308\n",
      "(70.48 min) Epoch 34/300 -- Iteration 261868 - Batch 7701/7702 - Train loss: 0.00194925  - Train acc: -0.0000 - Val loss: 0.00061722 - Val acc: -0.0000\n",
      "(70.51 min) Epoch 35/300 -- Iteration 261945 - Batch 77/7702 - Train loss: 0.00196528  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.53 min) Epoch 35/300 -- Iteration 262022 - Batch 154/7702 - Train loss: 0.00195854  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.55 min) Epoch 35/300 -- Iteration 262099 - Batch 231/7702 - Train loss: 0.00194523  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.57 min) Epoch 35/300 -- Iteration 262176 - Batch 308/7702 - Train loss: 0.00194706  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.59 min) Epoch 35/300 -- Iteration 262253 - Batch 385/7702 - Train loss: 0.00194710  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.61 min) Epoch 35/300 -- Iteration 262330 - Batch 462/7702 - Train loss: 0.00194690  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.63 min) Epoch 35/300 -- Iteration 262407 - Batch 539/7702 - Train loss: 0.00194825  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.65 min) Epoch 35/300 -- Iteration 262484 - Batch 616/7702 - Train loss: 0.00194515  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.67 min) Epoch 35/300 -- Iteration 262561 - Batch 693/7702 - Train loss: 0.00194298  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.69 min) Epoch 35/300 -- Iteration 262638 - Batch 770/7702 - Train loss: 0.00194204  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.71 min) Epoch 35/300 -- Iteration 262715 - Batch 847/7702 - Train loss: 0.00194281  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.73 min) Epoch 35/300 -- Iteration 262792 - Batch 924/7702 - Train loss: 0.00194405  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.75 min) Epoch 35/300 -- Iteration 262869 - Batch 1001/7702 - Train loss: 0.00194812  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.78 min) Epoch 35/300 -- Iteration 262946 - Batch 1078/7702 - Train loss: 0.00194748  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.80 min) Epoch 35/300 -- Iteration 263023 - Batch 1155/7702 - Train loss: 0.00194946  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.82 min) Epoch 35/300 -- Iteration 263100 - Batch 1232/7702 - Train loss: 0.00194925  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.84 min) Epoch 35/300 -- Iteration 263177 - Batch 1309/7702 - Train loss: 0.00195005  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.86 min) Epoch 35/300 -- Iteration 263254 - Batch 1386/7702 - Train loss: 0.00195034  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.88 min) Epoch 35/300 -- Iteration 263331 - Batch 1463/7702 - Train loss: 0.00194761  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.90 min) Epoch 35/300 -- Iteration 263408 - Batch 1540/7702 - Train loss: 0.00194830  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.92 min) Epoch 35/300 -- Iteration 263485 - Batch 1617/7702 - Train loss: 0.00194825  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.94 min) Epoch 35/300 -- Iteration 263562 - Batch 1694/7702 - Train loss: 0.00194711  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.96 min) Epoch 35/300 -- Iteration 263639 - Batch 1771/7702 - Train loss: 0.00194742  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(70.98 min) Epoch 35/300 -- Iteration 263716 - Batch 1848/7702 - Train loss: 0.00194670  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.00 min) Epoch 35/300 -- Iteration 263793 - Batch 1925/7702 - Train loss: 0.00194661  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.02 min) Epoch 35/300 -- Iteration 263870 - Batch 2002/7702 - Train loss: 0.00194724  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.04 min) Epoch 35/300 -- Iteration 263947 - Batch 2079/7702 - Train loss: 0.00194857  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.07 min) Epoch 35/300 -- Iteration 264024 - Batch 2156/7702 - Train loss: 0.00194886  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.09 min) Epoch 35/300 -- Iteration 264101 - Batch 2233/7702 - Train loss: 0.00194899  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.11 min) Epoch 35/300 -- Iteration 264178 - Batch 2310/7702 - Train loss: 0.00194999  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.13 min) Epoch 35/300 -- Iteration 264255 - Batch 2387/7702 - Train loss: 0.00194881  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.15 min) Epoch 35/300 -- Iteration 264332 - Batch 2464/7702 - Train loss: 0.00194847  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.17 min) Epoch 35/300 -- Iteration 264409 - Batch 2541/7702 - Train loss: 0.00194994  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.19 min) Epoch 35/300 -- Iteration 264486 - Batch 2618/7702 - Train loss: 0.00194869  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.21 min) Epoch 35/300 -- Iteration 264563 - Batch 2695/7702 - Train loss: 0.00194803  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.23 min) Epoch 35/300 -- Iteration 264640 - Batch 2772/7702 - Train loss: 0.00194753  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.25 min) Epoch 35/300 -- Iteration 264717 - Batch 2849/7702 - Train loss: 0.00194749  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.27 min) Epoch 35/300 -- Iteration 264794 - Batch 2926/7702 - Train loss: 0.00194741  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.29 min) Epoch 35/300 -- Iteration 264871 - Batch 3003/7702 - Train loss: 0.00194771  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.31 min) Epoch 35/300 -- Iteration 264948 - Batch 3080/7702 - Train loss: 0.00194772  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.33 min) Epoch 35/300 -- Iteration 265025 - Batch 3157/7702 - Train loss: 0.00194732  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.36 min) Epoch 35/300 -- Iteration 265102 - Batch 3234/7702 - Train loss: 0.00194632  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.38 min) Epoch 35/300 -- Iteration 265179 - Batch 3311/7702 - Train loss: 0.00194647  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.40 min) Epoch 35/300 -- Iteration 265256 - Batch 3388/7702 - Train loss: 0.00194637  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.42 min) Epoch 35/300 -- Iteration 265333 - Batch 3465/7702 - Train loss: 0.00194693  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.44 min) Epoch 35/300 -- Iteration 265410 - Batch 3542/7702 - Train loss: 0.00194647  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.46 min) Epoch 35/300 -- Iteration 265487 - Batch 3619/7702 - Train loss: 0.00194655  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.48 min) Epoch 35/300 -- Iteration 265564 - Batch 3696/7702 - Train loss: 0.00194729  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.50 min) Epoch 35/300 -- Iteration 265641 - Batch 3773/7702 - Train loss: 0.00194769  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.52 min) Epoch 35/300 -- Iteration 265718 - Batch 3850/7702 - Train loss: 0.00194818  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.54 min) Epoch 35/300 -- Iteration 265795 - Batch 3927/7702 - Train loss: 0.00194777  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.56 min) Epoch 35/300 -- Iteration 265872 - Batch 4004/7702 - Train loss: 0.00194813  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.58 min) Epoch 35/300 -- Iteration 265949 - Batch 4081/7702 - Train loss: 0.00194852  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.61 min) Epoch 35/300 -- Iteration 266026 - Batch 4158/7702 - Train loss: 0.00194780  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.63 min) Epoch 35/300 -- Iteration 266103 - Batch 4235/7702 - Train loss: 0.00194784  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.65 min) Epoch 35/300 -- Iteration 266180 - Batch 4312/7702 - Train loss: 0.00194788  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.67 min) Epoch 35/300 -- Iteration 266257 - Batch 4389/7702 - Train loss: 0.00194723  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.69 min) Epoch 35/300 -- Iteration 266334 - Batch 4466/7702 - Train loss: 0.00194715  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.71 min) Epoch 35/300 -- Iteration 266411 - Batch 4543/7702 - Train loss: 0.00194719  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.73 min) Epoch 35/300 -- Iteration 266488 - Batch 4620/7702 - Train loss: 0.00194740  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.75 min) Epoch 35/300 -- Iteration 266565 - Batch 4697/7702 - Train loss: 0.00194767  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.77 min) Epoch 35/300 -- Iteration 266642 - Batch 4774/7702 - Train loss: 0.00194765  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.79 min) Epoch 35/300 -- Iteration 266719 - Batch 4851/7702 - Train loss: 0.00194776  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.81 min) Epoch 35/300 -- Iteration 266796 - Batch 4928/7702 - Train loss: 0.00194674  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.83 min) Epoch 35/300 -- Iteration 266873 - Batch 5005/7702 - Train loss: 0.00194681  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.85 min) Epoch 35/300 -- Iteration 266950 - Batch 5082/7702 - Train loss: 0.00194653  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.87 min) Epoch 35/300 -- Iteration 267027 - Batch 5159/7702 - Train loss: 0.00194673  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.89 min) Epoch 35/300 -- Iteration 267104 - Batch 5236/7702 - Train loss: 0.00194716  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.92 min) Epoch 35/300 -- Iteration 267181 - Batch 5313/7702 - Train loss: 0.00194717  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.94 min) Epoch 35/300 -- Iteration 267258 - Batch 5390/7702 - Train loss: 0.00194680  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.96 min) Epoch 35/300 -- Iteration 267335 - Batch 5467/7702 - Train loss: 0.00194690  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(71.98 min) Epoch 35/300 -- Iteration 267412 - Batch 5544/7702 - Train loss: 0.00194718  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.00 min) Epoch 35/300 -- Iteration 267489 - Batch 5621/7702 - Train loss: 0.00194722  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.02 min) Epoch 35/300 -- Iteration 267566 - Batch 5698/7702 - Train loss: 0.00194765  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.04 min) Epoch 35/300 -- Iteration 267643 - Batch 5775/7702 - Train loss: 0.00194791  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.06 min) Epoch 35/300 -- Iteration 267720 - Batch 5852/7702 - Train loss: 0.00194770  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.08 min) Epoch 35/300 -- Iteration 267797 - Batch 5929/7702 - Train loss: 0.00194725  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.10 min) Epoch 35/300 -- Iteration 267874 - Batch 6006/7702 - Train loss: 0.00194764  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.12 min) Epoch 35/300 -- Iteration 267951 - Batch 6083/7702 - Train loss: 0.00194736  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.14 min) Epoch 35/300 -- Iteration 268028 - Batch 6160/7702 - Train loss: 0.00194774  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.16 min) Epoch 35/300 -- Iteration 268105 - Batch 6237/7702 - Train loss: 0.00194737  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.19 min) Epoch 35/300 -- Iteration 268182 - Batch 6314/7702 - Train loss: 0.00194730  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.21 min) Epoch 35/300 -- Iteration 268259 - Batch 6391/7702 - Train loss: 0.00194744  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.23 min) Epoch 35/300 -- Iteration 268336 - Batch 6468/7702 - Train loss: 0.00194758  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.25 min) Epoch 35/300 -- Iteration 268413 - Batch 6545/7702 - Train loss: 0.00194745  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.27 min) Epoch 35/300 -- Iteration 268490 - Batch 6622/7702 - Train loss: 0.00194742  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.29 min) Epoch 35/300 -- Iteration 268567 - Batch 6699/7702 - Train loss: 0.00194751  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.31 min) Epoch 35/300 -- Iteration 268644 - Batch 6776/7702 - Train loss: 0.00194734  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.33 min) Epoch 35/300 -- Iteration 268721 - Batch 6853/7702 - Train loss: 0.00194681  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.35 min) Epoch 35/300 -- Iteration 268798 - Batch 6930/7702 - Train loss: 0.00194705  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.37 min) Epoch 35/300 -- Iteration 268875 - Batch 7007/7702 - Train loss: 0.00194734  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.39 min) Epoch 35/300 -- Iteration 268952 - Batch 7084/7702 - Train loss: 0.00194730  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.41 min) Epoch 35/300 -- Iteration 269029 - Batch 7161/7702 - Train loss: 0.00194703  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.43 min) Epoch 35/300 -- Iteration 269106 - Batch 7238/7702 - Train loss: 0.00194674  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.45 min) Epoch 35/300 -- Iteration 269183 - Batch 7315/7702 - Train loss: 0.00194649  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.48 min) Epoch 35/300 -- Iteration 269260 - Batch 7392/7702 - Train loss: 0.00194666  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.50 min) Epoch 35/300 -- Iteration 269337 - Batch 7469/7702 - Train loss: 0.00194633  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.52 min) Epoch 35/300 -- Iteration 269414 - Batch 7546/7702 - Train loss: 0.00194623  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.54 min) Epoch 35/300 -- Iteration 269491 - Batch 7623/7702 - Train loss: 0.00194627  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.56 min) Epoch 35/300 -- Iteration 269568 - Batch 7700/7702 - Train loss: 0.00194643  - Train acc: -0.0000 - Val loss: 0.00061722\n",
      "(72.56 min) Epoch 35/300 -- Iteration 269570 - Batch 7701/7702 - Train loss: 0.00194648  - Train acc: -0.0000 - Val loss: 0.00050611 - Val acc: -0.0000\n",
      "(72.58 min) Epoch 36/300 -- Iteration 269647 - Batch 77/7702 - Train loss: 0.00190173  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.60 min) Epoch 36/300 -- Iteration 269724 - Batch 154/7702 - Train loss: 0.00190556  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.62 min) Epoch 36/300 -- Iteration 269801 - Batch 231/7702 - Train loss: 0.00191888  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.64 min) Epoch 36/300 -- Iteration 269878 - Batch 308/7702 - Train loss: 0.00192824  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.67 min) Epoch 36/300 -- Iteration 269955 - Batch 385/7702 - Train loss: 0.00192729  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.69 min) Epoch 36/300 -- Iteration 270032 - Batch 462/7702 - Train loss: 0.00193219  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.71 min) Epoch 36/300 -- Iteration 270109 - Batch 539/7702 - Train loss: 0.00193171  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.73 min) Epoch 36/300 -- Iteration 270186 - Batch 616/7702 - Train loss: 0.00193241  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.75 min) Epoch 36/300 -- Iteration 270263 - Batch 693/7702 - Train loss: 0.00193539  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.77 min) Epoch 36/300 -- Iteration 270340 - Batch 770/7702 - Train loss: 0.00193165  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.79 min) Epoch 36/300 -- Iteration 270417 - Batch 847/7702 - Train loss: 0.00193326  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.81 min) Epoch 36/300 -- Iteration 270494 - Batch 924/7702 - Train loss: 0.00193593  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.83 min) Epoch 36/300 -- Iteration 270571 - Batch 1001/7702 - Train loss: 0.00193788  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.85 min) Epoch 36/300 -- Iteration 270648 - Batch 1078/7702 - Train loss: 0.00193556  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.87 min) Epoch 36/300 -- Iteration 270725 - Batch 1155/7702 - Train loss: 0.00193514  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.89 min) Epoch 36/300 -- Iteration 270802 - Batch 1232/7702 - Train loss: 0.00193282  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.91 min) Epoch 36/300 -- Iteration 270879 - Batch 1309/7702 - Train loss: 0.00193353  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.93 min) Epoch 36/300 -- Iteration 270956 - Batch 1386/7702 - Train loss: 0.00193353  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.95 min) Epoch 36/300 -- Iteration 271033 - Batch 1463/7702 - Train loss: 0.00193408  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(72.98 min) Epoch 36/300 -- Iteration 271110 - Batch 1540/7702 - Train loss: 0.00193594  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.00 min) Epoch 36/300 -- Iteration 271187 - Batch 1617/7702 - Train loss: 0.00193581  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.02 min) Epoch 36/300 -- Iteration 271264 - Batch 1694/7702 - Train loss: 0.00193646  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.04 min) Epoch 36/300 -- Iteration 271341 - Batch 1771/7702 - Train loss: 0.00193606  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.06 min) Epoch 36/300 -- Iteration 271418 - Batch 1848/7702 - Train loss: 0.00193581  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.08 min) Epoch 36/300 -- Iteration 271495 - Batch 1925/7702 - Train loss: 0.00193713  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.10 min) Epoch 36/300 -- Iteration 271572 - Batch 2002/7702 - Train loss: 0.00193626  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.12 min) Epoch 36/300 -- Iteration 271649 - Batch 2079/7702 - Train loss: 0.00193812  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.14 min) Epoch 36/300 -- Iteration 271726 - Batch 2156/7702 - Train loss: 0.00193931  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.16 min) Epoch 36/300 -- Iteration 271803 - Batch 2233/7702 - Train loss: 0.00193971  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.18 min) Epoch 36/300 -- Iteration 271880 - Batch 2310/7702 - Train loss: 0.00193939  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.20 min) Epoch 36/300 -- Iteration 271957 - Batch 2387/7702 - Train loss: 0.00194063  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.23 min) Epoch 36/300 -- Iteration 272034 - Batch 2464/7702 - Train loss: 0.00194041  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.25 min) Epoch 36/300 -- Iteration 272111 - Batch 2541/7702 - Train loss: 0.00193944  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.27 min) Epoch 36/300 -- Iteration 272188 - Batch 2618/7702 - Train loss: 0.00193963  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.29 min) Epoch 36/300 -- Iteration 272265 - Batch 2695/7702 - Train loss: 0.00194059  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.31 min) Epoch 36/300 -- Iteration 272342 - Batch 2772/7702 - Train loss: 0.00194062  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.33 min) Epoch 36/300 -- Iteration 272419 - Batch 2849/7702 - Train loss: 0.00194045  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.35 min) Epoch 36/300 -- Iteration 272496 - Batch 2926/7702 - Train loss: 0.00194164  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.37 min) Epoch 36/300 -- Iteration 272573 - Batch 3003/7702 - Train loss: 0.00194235  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.39 min) Epoch 36/300 -- Iteration 272650 - Batch 3080/7702 - Train loss: 0.00194224  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.41 min) Epoch 36/300 -- Iteration 272727 - Batch 3157/7702 - Train loss: 0.00194196  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.43 min) Epoch 36/300 -- Iteration 272804 - Batch 3234/7702 - Train loss: 0.00194189  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.45 min) Epoch 36/300 -- Iteration 272881 - Batch 3311/7702 - Train loss: 0.00194121  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.47 min) Epoch 36/300 -- Iteration 272958 - Batch 3388/7702 - Train loss: 0.00194096  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.49 min) Epoch 36/300 -- Iteration 273035 - Batch 3465/7702 - Train loss: 0.00194176  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.52 min) Epoch 36/300 -- Iteration 273112 - Batch 3542/7702 - Train loss: 0.00194330  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.54 min) Epoch 36/300 -- Iteration 273189 - Batch 3619/7702 - Train loss: 0.00194322  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.56 min) Epoch 36/300 -- Iteration 273266 - Batch 3696/7702 - Train loss: 0.00194370  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.58 min) Epoch 36/300 -- Iteration 273343 - Batch 3773/7702 - Train loss: 0.00194368  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.60 min) Epoch 36/300 -- Iteration 273420 - Batch 3850/7702 - Train loss: 0.00194467  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.62 min) Epoch 36/300 -- Iteration 273497 - Batch 3927/7702 - Train loss: 0.00194455  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.64 min) Epoch 36/300 -- Iteration 273574 - Batch 4004/7702 - Train loss: 0.00194415  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.66 min) Epoch 36/300 -- Iteration 273651 - Batch 4081/7702 - Train loss: 0.00194435  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.68 min) Epoch 36/300 -- Iteration 273728 - Batch 4158/7702 - Train loss: 0.00194431  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.70 min) Epoch 36/300 -- Iteration 273805 - Batch 4235/7702 - Train loss: 0.00194466  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.72 min) Epoch 36/300 -- Iteration 273882 - Batch 4312/7702 - Train loss: 0.00194475  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.74 min) Epoch 36/300 -- Iteration 273959 - Batch 4389/7702 - Train loss: 0.00194492  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.76 min) Epoch 36/300 -- Iteration 274036 - Batch 4466/7702 - Train loss: 0.00194401  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.78 min) Epoch 36/300 -- Iteration 274113 - Batch 4543/7702 - Train loss: 0.00194440  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.81 min) Epoch 36/300 -- Iteration 274190 - Batch 4620/7702 - Train loss: 0.00194446  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.83 min) Epoch 36/300 -- Iteration 274267 - Batch 4697/7702 - Train loss: 0.00194479  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.85 min) Epoch 36/300 -- Iteration 274344 - Batch 4774/7702 - Train loss: 0.00194491  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.87 min) Epoch 36/300 -- Iteration 274421 - Batch 4851/7702 - Train loss: 0.00194459  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.89 min) Epoch 36/300 -- Iteration 274498 - Batch 4928/7702 - Train loss: 0.00194470  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.91 min) Epoch 36/300 -- Iteration 274575 - Batch 5005/7702 - Train loss: 0.00194468  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.93 min) Epoch 36/300 -- Iteration 274652 - Batch 5082/7702 - Train loss: 0.00194476  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.95 min) Epoch 36/300 -- Iteration 274729 - Batch 5159/7702 - Train loss: 0.00194470  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.97 min) Epoch 36/300 -- Iteration 274806 - Batch 5236/7702 - Train loss: 0.00194427  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(73.99 min) Epoch 36/300 -- Iteration 274883 - Batch 5313/7702 - Train loss: 0.00194431  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.01 min) Epoch 36/300 -- Iteration 274960 - Batch 5390/7702 - Train loss: 0.00194445  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.03 min) Epoch 36/300 -- Iteration 275037 - Batch 5467/7702 - Train loss: 0.00194447  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.05 min) Epoch 36/300 -- Iteration 275114 - Batch 5544/7702 - Train loss: 0.00194464  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.07 min) Epoch 36/300 -- Iteration 275191 - Batch 5621/7702 - Train loss: 0.00194472  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.09 min) Epoch 36/300 -- Iteration 275268 - Batch 5698/7702 - Train loss: 0.00194444  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.12 min) Epoch 36/300 -- Iteration 275345 - Batch 5775/7702 - Train loss: 0.00194417  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.14 min) Epoch 36/300 -- Iteration 275422 - Batch 5852/7702 - Train loss: 0.00194410  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.16 min) Epoch 36/300 -- Iteration 275499 - Batch 5929/7702 - Train loss: 0.00194422  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.18 min) Epoch 36/300 -- Iteration 275576 - Batch 6006/7702 - Train loss: 0.00194400  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.20 min) Epoch 36/300 -- Iteration 275653 - Batch 6083/7702 - Train loss: 0.00194386  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.22 min) Epoch 36/300 -- Iteration 275730 - Batch 6160/7702 - Train loss: 0.00194360  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.24 min) Epoch 36/300 -- Iteration 275807 - Batch 6237/7702 - Train loss: 0.00194366  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.26 min) Epoch 36/300 -- Iteration 275884 - Batch 6314/7702 - Train loss: 0.00194406  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.28 min) Epoch 36/300 -- Iteration 275961 - Batch 6391/7702 - Train loss: 0.00194420  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.30 min) Epoch 36/300 -- Iteration 276038 - Batch 6468/7702 - Train loss: 0.00194418  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.32 min) Epoch 36/300 -- Iteration 276115 - Batch 6545/7702 - Train loss: 0.00194412  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.34 min) Epoch 36/300 -- Iteration 276192 - Batch 6622/7702 - Train loss: 0.00194415  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.36 min) Epoch 36/300 -- Iteration 276269 - Batch 6699/7702 - Train loss: 0.00194442  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.38 min) Epoch 36/300 -- Iteration 276346 - Batch 6776/7702 - Train loss: 0.00194451  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.41 min) Epoch 36/300 -- Iteration 276423 - Batch 6853/7702 - Train loss: 0.00194464  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.43 min) Epoch 36/300 -- Iteration 276500 - Batch 6930/7702 - Train loss: 0.00194468  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.45 min) Epoch 36/300 -- Iteration 276577 - Batch 7007/7702 - Train loss: 0.00194466  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.47 min) Epoch 36/300 -- Iteration 276654 - Batch 7084/7702 - Train loss: 0.00194462  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.49 min) Epoch 36/300 -- Iteration 276731 - Batch 7161/7702 - Train loss: 0.00194473  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.51 min) Epoch 36/300 -- Iteration 276808 - Batch 7238/7702 - Train loss: 0.00194486  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.53 min) Epoch 36/300 -- Iteration 276885 - Batch 7315/7702 - Train loss: 0.00194448  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.55 min) Epoch 36/300 -- Iteration 276962 - Batch 7392/7702 - Train loss: 0.00194439  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.57 min) Epoch 36/300 -- Iteration 277039 - Batch 7469/7702 - Train loss: 0.00194397  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.59 min) Epoch 36/300 -- Iteration 277116 - Batch 7546/7702 - Train loss: 0.00194358  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.61 min) Epoch 36/300 -- Iteration 277193 - Batch 7623/7702 - Train loss: 0.00194377  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.63 min) Epoch 36/300 -- Iteration 277270 - Batch 7700/7702 - Train loss: 0.00194393  - Train acc: -0.0000 - Val loss: 0.00050611\n",
      "(74.63 min) Epoch 36/300 -- Iteration 277272 - Batch 7701/7702 - Train loss: 0.00194398  - Train acc: -0.0000 - Val loss: 0.00046433 - Val acc: -0.0000\n",
      "(74.66 min) Epoch 37/300 -- Iteration 277349 - Batch 77/7702 - Train loss: 0.00191455  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.68 min) Epoch 37/300 -- Iteration 277426 - Batch 154/7702 - Train loss: 0.00194296  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.70 min) Epoch 37/300 -- Iteration 277503 - Batch 231/7702 - Train loss: 0.00200767  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.72 min) Epoch 37/300 -- Iteration 277580 - Batch 308/7702 - Train loss: 0.00201026  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.74 min) Epoch 37/300 -- Iteration 277657 - Batch 385/7702 - Train loss: 0.00200359  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.76 min) Epoch 37/300 -- Iteration 277734 - Batch 462/7702 - Train loss: 0.00199641  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.78 min) Epoch 37/300 -- Iteration 277811 - Batch 539/7702 - Train loss: 0.00199400  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.80 min) Epoch 37/300 -- Iteration 277888 - Batch 616/7702 - Train loss: 0.00198621  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.82 min) Epoch 37/300 -- Iteration 277965 - Batch 693/7702 - Train loss: 0.00198162  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.84 min) Epoch 37/300 -- Iteration 278042 - Batch 770/7702 - Train loss: 0.00197425  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.86 min) Epoch 37/300 -- Iteration 278119 - Batch 847/7702 - Train loss: 0.00196977  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.88 min) Epoch 37/300 -- Iteration 278196 - Batch 924/7702 - Train loss: 0.00196712  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.90 min) Epoch 37/300 -- Iteration 278273 - Batch 1001/7702 - Train loss: 0.00196559  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.93 min) Epoch 37/300 -- Iteration 278350 - Batch 1078/7702 - Train loss: 0.00196587  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.95 min) Epoch 37/300 -- Iteration 278427 - Batch 1155/7702 - Train loss: 0.00196448  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.97 min) Epoch 37/300 -- Iteration 278504 - Batch 1232/7702 - Train loss: 0.00196472  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(74.99 min) Epoch 37/300 -- Iteration 278581 - Batch 1309/7702 - Train loss: 0.00196481  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.01 min) Epoch 37/300 -- Iteration 278658 - Batch 1386/7702 - Train loss: 0.00196390  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.03 min) Epoch 37/300 -- Iteration 278735 - Batch 1463/7702 - Train loss: 0.00196144  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.05 min) Epoch 37/300 -- Iteration 278812 - Batch 1540/7702 - Train loss: 0.00196049  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.07 min) Epoch 37/300 -- Iteration 278889 - Batch 1617/7702 - Train loss: 0.00195859  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.09 min) Epoch 37/300 -- Iteration 278966 - Batch 1694/7702 - Train loss: 0.00195815  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.11 min) Epoch 37/300 -- Iteration 279043 - Batch 1771/7702 - Train loss: 0.00195708  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.13 min) Epoch 37/300 -- Iteration 279120 - Batch 1848/7702 - Train loss: 0.00195525  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.15 min) Epoch 37/300 -- Iteration 279197 - Batch 1925/7702 - Train loss: 0.00195308  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.17 min) Epoch 37/300 -- Iteration 279274 - Batch 2002/7702 - Train loss: 0.00195386  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.19 min) Epoch 37/300 -- Iteration 279351 - Batch 2079/7702 - Train loss: 0.00195374  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.22 min) Epoch 37/300 -- Iteration 279428 - Batch 2156/7702 - Train loss: 0.00195490  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.24 min) Epoch 37/300 -- Iteration 279505 - Batch 2233/7702 - Train loss: 0.00195522  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.26 min) Epoch 37/300 -- Iteration 279582 - Batch 2310/7702 - Train loss: 0.00195444  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.28 min) Epoch 37/300 -- Iteration 279659 - Batch 2387/7702 - Train loss: 0.00195454  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.30 min) Epoch 37/300 -- Iteration 279736 - Batch 2464/7702 - Train loss: 0.00195382  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.32 min) Epoch 37/300 -- Iteration 279813 - Batch 2541/7702 - Train loss: 0.00195372  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.34 min) Epoch 37/300 -- Iteration 279890 - Batch 2618/7702 - Train loss: 0.00195371  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.36 min) Epoch 37/300 -- Iteration 279967 - Batch 2695/7702 - Train loss: 0.00195472  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.38 min) Epoch 37/300 -- Iteration 280044 - Batch 2772/7702 - Train loss: 0.00195403  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.40 min) Epoch 37/300 -- Iteration 280121 - Batch 2849/7702 - Train loss: 0.00195324  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.42 min) Epoch 37/300 -- Iteration 280198 - Batch 2926/7702 - Train loss: 0.00195289  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.44 min) Epoch 37/300 -- Iteration 280275 - Batch 3003/7702 - Train loss: 0.00195279  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.46 min) Epoch 37/300 -- Iteration 280352 - Batch 3080/7702 - Train loss: 0.00195235  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.48 min) Epoch 37/300 -- Iteration 280429 - Batch 3157/7702 - Train loss: 0.00195315  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.51 min) Epoch 37/300 -- Iteration 280506 - Batch 3234/7702 - Train loss: 0.00195273  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.53 min) Epoch 37/300 -- Iteration 280583 - Batch 3311/7702 - Train loss: 0.00195226  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.55 min) Epoch 37/300 -- Iteration 280660 - Batch 3388/7702 - Train loss: 0.00195269  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.57 min) Epoch 37/300 -- Iteration 280737 - Batch 3465/7702 - Train loss: 0.00195260  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.59 min) Epoch 37/300 -- Iteration 280814 - Batch 3542/7702 - Train loss: 0.00195281  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.61 min) Epoch 37/300 -- Iteration 280891 - Batch 3619/7702 - Train loss: 0.00195263  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.63 min) Epoch 37/300 -- Iteration 280968 - Batch 3696/7702 - Train loss: 0.00195300  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.65 min) Epoch 37/300 -- Iteration 281045 - Batch 3773/7702 - Train loss: 0.00195268  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.67 min) Epoch 37/300 -- Iteration 281122 - Batch 3850/7702 - Train loss: 0.00195337  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.69 min) Epoch 37/300 -- Iteration 281199 - Batch 3927/7702 - Train loss: 0.00195241  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.71 min) Epoch 37/300 -- Iteration 281276 - Batch 4004/7702 - Train loss: 0.00195262  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.73 min) Epoch 37/300 -- Iteration 281353 - Batch 4081/7702 - Train loss: 0.00195230  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.75 min) Epoch 37/300 -- Iteration 281430 - Batch 4158/7702 - Train loss: 0.00195190  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.78 min) Epoch 37/300 -- Iteration 281507 - Batch 4235/7702 - Train loss: 0.00195205  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.80 min) Epoch 37/300 -- Iteration 281584 - Batch 4312/7702 - Train loss: 0.00195213  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.82 min) Epoch 37/300 -- Iteration 281661 - Batch 4389/7702 - Train loss: 0.00195142  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.84 min) Epoch 37/300 -- Iteration 281738 - Batch 4466/7702 - Train loss: 0.00195097  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.86 min) Epoch 37/300 -- Iteration 281815 - Batch 4543/7702 - Train loss: 0.00195084  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.88 min) Epoch 37/300 -- Iteration 281892 - Batch 4620/7702 - Train loss: 0.00195060  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.90 min) Epoch 37/300 -- Iteration 281969 - Batch 4697/7702 - Train loss: 0.00195057  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.92 min) Epoch 37/300 -- Iteration 282046 - Batch 4774/7702 - Train loss: 0.00195033  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.94 min) Epoch 37/300 -- Iteration 282123 - Batch 4851/7702 - Train loss: 0.00195011  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.96 min) Epoch 37/300 -- Iteration 282200 - Batch 4928/7702 - Train loss: 0.00195019  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(75.98 min) Epoch 37/300 -- Iteration 282277 - Batch 5005/7702 - Train loss: 0.00195033  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.00 min) Epoch 37/300 -- Iteration 282354 - Batch 5082/7702 - Train loss: 0.00195017  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.02 min) Epoch 37/300 -- Iteration 282431 - Batch 5159/7702 - Train loss: 0.00195023  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.04 min) Epoch 37/300 -- Iteration 282508 - Batch 5236/7702 - Train loss: 0.00194922  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.07 min) Epoch 37/300 -- Iteration 282585 - Batch 5313/7702 - Train loss: 0.00194885  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.09 min) Epoch 37/300 -- Iteration 282662 - Batch 5390/7702 - Train loss: 0.00194901  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.11 min) Epoch 37/300 -- Iteration 282739 - Batch 5467/7702 - Train loss: 0.00194874  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.13 min) Epoch 37/300 -- Iteration 282816 - Batch 5544/7702 - Train loss: 0.00194894  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.15 min) Epoch 37/300 -- Iteration 282893 - Batch 5621/7702 - Train loss: 0.00194888  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.17 min) Epoch 37/300 -- Iteration 282970 - Batch 5698/7702 - Train loss: 0.00194922  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.19 min) Epoch 37/300 -- Iteration 283047 - Batch 5775/7702 - Train loss: 0.00194938  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.21 min) Epoch 37/300 -- Iteration 283124 - Batch 5852/7702 - Train loss: 0.00194981  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.23 min) Epoch 37/300 -- Iteration 283201 - Batch 5929/7702 - Train loss: 0.00194984  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.25 min) Epoch 37/300 -- Iteration 283278 - Batch 6006/7702 - Train loss: 0.00194999  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.27 min) Epoch 37/300 -- Iteration 283355 - Batch 6083/7702 - Train loss: 0.00195029  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.29 min) Epoch 37/300 -- Iteration 283432 - Batch 6160/7702 - Train loss: 0.00195066  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.31 min) Epoch 37/300 -- Iteration 283509 - Batch 6237/7702 - Train loss: 0.00195030  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.33 min) Epoch 37/300 -- Iteration 283586 - Batch 6314/7702 - Train loss: 0.00195009  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.36 min) Epoch 37/300 -- Iteration 283663 - Batch 6391/7702 - Train loss: 0.00195019  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.38 min) Epoch 37/300 -- Iteration 283740 - Batch 6468/7702 - Train loss: 0.00195067  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.40 min) Epoch 37/300 -- Iteration 283817 - Batch 6545/7702 - Train loss: 0.00195046  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.42 min) Epoch 37/300 -- Iteration 283894 - Batch 6622/7702 - Train loss: 0.00195036  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.44 min) Epoch 37/300 -- Iteration 283971 - Batch 6699/7702 - Train loss: 0.00194977  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.46 min) Epoch 37/300 -- Iteration 284048 - Batch 6776/7702 - Train loss: 0.00194968  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.48 min) Epoch 37/300 -- Iteration 284125 - Batch 6853/7702 - Train loss: 0.00195004  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.50 min) Epoch 37/300 -- Iteration 284202 - Batch 6930/7702 - Train loss: 0.00194978  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.52 min) Epoch 37/300 -- Iteration 284279 - Batch 7007/7702 - Train loss: 0.00194990  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.54 min) Epoch 37/300 -- Iteration 284356 - Batch 7084/7702 - Train loss: 0.00195009  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.56 min) Epoch 37/300 -- Iteration 284433 - Batch 7161/7702 - Train loss: 0.00195001  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.58 min) Epoch 37/300 -- Iteration 284510 - Batch 7238/7702 - Train loss: 0.00195041  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.60 min) Epoch 37/300 -- Iteration 284587 - Batch 7315/7702 - Train loss: 0.00195068  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.63 min) Epoch 37/300 -- Iteration 284664 - Batch 7392/7702 - Train loss: 0.00195108  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.65 min) Epoch 37/300 -- Iteration 284741 - Batch 7469/7702 - Train loss: 0.00195080  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.67 min) Epoch 37/300 -- Iteration 284818 - Batch 7546/7702 - Train loss: 0.00195040  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.69 min) Epoch 37/300 -- Iteration 284895 - Batch 7623/7702 - Train loss: 0.00195000  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.71 min) Epoch 37/300 -- Iteration 284972 - Batch 7700/7702 - Train loss: 0.00194995  - Train acc: -0.0000 - Val loss: 0.00046433\n",
      "(76.71 min) Epoch 37/300 -- Iteration 284974 - Batch 7701/7702 - Train loss: 0.00194993  - Train acc: -0.0000 - Val loss: 0.00052587 - Val acc: -0.0000\n",
      "(76.73 min) Epoch 38/300 -- Iteration 285051 - Batch 77/7702 - Train loss: 0.00193973  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.75 min) Epoch 38/300 -- Iteration 285128 - Batch 154/7702 - Train loss: 0.00194835  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.77 min) Epoch 38/300 -- Iteration 285205 - Batch 231/7702 - Train loss: 0.00194843  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.79 min) Epoch 38/300 -- Iteration 285282 - Batch 308/7702 - Train loss: 0.00194264  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.82 min) Epoch 38/300 -- Iteration 285359 - Batch 385/7702 - Train loss: 0.00193989  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.84 min) Epoch 38/300 -- Iteration 285436 - Batch 462/7702 - Train loss: 0.00193881  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.86 min) Epoch 38/300 -- Iteration 285513 - Batch 539/7702 - Train loss: 0.00193635  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.88 min) Epoch 38/300 -- Iteration 285590 - Batch 616/7702 - Train loss: 0.00193600  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.90 min) Epoch 38/300 -- Iteration 285667 - Batch 693/7702 - Train loss: 0.00193198  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.92 min) Epoch 38/300 -- Iteration 285744 - Batch 770/7702 - Train loss: 0.00193388  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.94 min) Epoch 38/300 -- Iteration 285821 - Batch 847/7702 - Train loss: 0.00193529  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.96 min) Epoch 38/300 -- Iteration 285898 - Batch 924/7702 - Train loss: 0.00193700  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(76.98 min) Epoch 38/300 -- Iteration 285975 - Batch 1001/7702 - Train loss: 0.00193690  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.00 min) Epoch 38/300 -- Iteration 286052 - Batch 1078/7702 - Train loss: 0.00193749  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.02 min) Epoch 38/300 -- Iteration 286129 - Batch 1155/7702 - Train loss: 0.00193788  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.04 min) Epoch 38/300 -- Iteration 286206 - Batch 1232/7702 - Train loss: 0.00193796  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.06 min) Epoch 38/300 -- Iteration 286283 - Batch 1309/7702 - Train loss: 0.00193736  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.08 min) Epoch 38/300 -- Iteration 286360 - Batch 1386/7702 - Train loss: 0.00193889  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.11 min) Epoch 38/300 -- Iteration 286437 - Batch 1463/7702 - Train loss: 0.00193725  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.13 min) Epoch 38/300 -- Iteration 286514 - Batch 1540/7702 - Train loss: 0.00193590  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.15 min) Epoch 38/300 -- Iteration 286591 - Batch 1617/7702 - Train loss: 0.00193692  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.17 min) Epoch 38/300 -- Iteration 286668 - Batch 1694/7702 - Train loss: 0.00193829  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.19 min) Epoch 38/300 -- Iteration 286745 - Batch 1771/7702 - Train loss: 0.00193920  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.21 min) Epoch 38/300 -- Iteration 286822 - Batch 1848/7702 - Train loss: 0.00193886  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.23 min) Epoch 38/300 -- Iteration 286899 - Batch 1925/7702 - Train loss: 0.00193957  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.25 min) Epoch 38/300 -- Iteration 286976 - Batch 2002/7702 - Train loss: 0.00193940  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.27 min) Epoch 38/300 -- Iteration 287053 - Batch 2079/7702 - Train loss: 0.00193757  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.29 min) Epoch 38/300 -- Iteration 287130 - Batch 2156/7702 - Train loss: 0.00193755  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.31 min) Epoch 38/300 -- Iteration 287207 - Batch 2233/7702 - Train loss: 0.00193775  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.33 min) Epoch 38/300 -- Iteration 287284 - Batch 2310/7702 - Train loss: 0.00193831  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.35 min) Epoch 38/300 -- Iteration 287361 - Batch 2387/7702 - Train loss: 0.00193855  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.37 min) Epoch 38/300 -- Iteration 287438 - Batch 2464/7702 - Train loss: 0.00193818  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.40 min) Epoch 38/300 -- Iteration 287515 - Batch 2541/7702 - Train loss: 0.00193885  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.42 min) Epoch 38/300 -- Iteration 287592 - Batch 2618/7702 - Train loss: 0.00193998  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.44 min) Epoch 38/300 -- Iteration 287669 - Batch 2695/7702 - Train loss: 0.00194093  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.46 min) Epoch 38/300 -- Iteration 287746 - Batch 2772/7702 - Train loss: 0.00193997  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.48 min) Epoch 38/300 -- Iteration 287823 - Batch 2849/7702 - Train loss: 0.00193923  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.50 min) Epoch 38/300 -- Iteration 287900 - Batch 2926/7702 - Train loss: 0.00194029  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.52 min) Epoch 38/300 -- Iteration 287977 - Batch 3003/7702 - Train loss: 0.00194069  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.54 min) Epoch 38/300 -- Iteration 288054 - Batch 3080/7702 - Train loss: 0.00194175  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.56 min) Epoch 38/300 -- Iteration 288131 - Batch 3157/7702 - Train loss: 0.00194083  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.58 min) Epoch 38/300 -- Iteration 288208 - Batch 3234/7702 - Train loss: 0.00193989  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.60 min) Epoch 38/300 -- Iteration 288285 - Batch 3311/7702 - Train loss: 0.00193971  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.62 min) Epoch 38/300 -- Iteration 288362 - Batch 3388/7702 - Train loss: 0.00194044  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.64 min) Epoch 38/300 -- Iteration 288439 - Batch 3465/7702 - Train loss: 0.00194042  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.66 min) Epoch 38/300 -- Iteration 288516 - Batch 3542/7702 - Train loss: 0.00194111  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.68 min) Epoch 38/300 -- Iteration 288593 - Batch 3619/7702 - Train loss: 0.00194128  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.71 min) Epoch 38/300 -- Iteration 288670 - Batch 3696/7702 - Train loss: 0.00194054  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.73 min) Epoch 38/300 -- Iteration 288747 - Batch 3773/7702 - Train loss: 0.00194020  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.75 min) Epoch 38/300 -- Iteration 288824 - Batch 3850/7702 - Train loss: 0.00194044  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.77 min) Epoch 38/300 -- Iteration 288901 - Batch 3927/7702 - Train loss: 0.00194001  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.79 min) Epoch 38/300 -- Iteration 288978 - Batch 4004/7702 - Train loss: 0.00193923  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.81 min) Epoch 38/300 -- Iteration 289055 - Batch 4081/7702 - Train loss: 0.00193909  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.83 min) Epoch 38/300 -- Iteration 289132 - Batch 4158/7702 - Train loss: 0.00193886  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.85 min) Epoch 38/300 -- Iteration 289209 - Batch 4235/7702 - Train loss: 0.00193870  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.87 min) Epoch 38/300 -- Iteration 289286 - Batch 4312/7702 - Train loss: 0.00193791  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.89 min) Epoch 38/300 -- Iteration 289363 - Batch 4389/7702 - Train loss: 0.00193798  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.91 min) Epoch 38/300 -- Iteration 289440 - Batch 4466/7702 - Train loss: 0.00193819  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.93 min) Epoch 38/300 -- Iteration 289517 - Batch 4543/7702 - Train loss: 0.00193840  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.95 min) Epoch 38/300 -- Iteration 289594 - Batch 4620/7702 - Train loss: 0.00193782  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(77.98 min) Epoch 38/300 -- Iteration 289671 - Batch 4697/7702 - Train loss: 0.00193757  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.00 min) Epoch 38/300 -- Iteration 289748 - Batch 4774/7702 - Train loss: 0.00193753  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.02 min) Epoch 38/300 -- Iteration 289825 - Batch 4851/7702 - Train loss: 0.00193755  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.04 min) Epoch 38/300 -- Iteration 289902 - Batch 4928/7702 - Train loss: 0.00193858  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.06 min) Epoch 38/300 -- Iteration 289979 - Batch 5005/7702 - Train loss: 0.00193794  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.08 min) Epoch 38/300 -- Iteration 290056 - Batch 5082/7702 - Train loss: 0.00193773  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.10 min) Epoch 38/300 -- Iteration 290133 - Batch 5159/7702 - Train loss: 0.00193802  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.12 min) Epoch 38/300 -- Iteration 290210 - Batch 5236/7702 - Train loss: 0.00193813  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.14 min) Epoch 38/300 -- Iteration 290287 - Batch 5313/7702 - Train loss: 0.00193827  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.16 min) Epoch 38/300 -- Iteration 290364 - Batch 5390/7702 - Train loss: 0.00193758  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.18 min) Epoch 38/300 -- Iteration 290441 - Batch 5467/7702 - Train loss: 0.00193779  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.20 min) Epoch 38/300 -- Iteration 290518 - Batch 5544/7702 - Train loss: 0.00193819  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.22 min) Epoch 38/300 -- Iteration 290595 - Batch 5621/7702 - Train loss: 0.00193794  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.24 min) Epoch 38/300 -- Iteration 290672 - Batch 5698/7702 - Train loss: 0.00193768  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.27 min) Epoch 38/300 -- Iteration 290749 - Batch 5775/7702 - Train loss: 0.00193718  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.29 min) Epoch 38/300 -- Iteration 290826 - Batch 5852/7702 - Train loss: 0.00193735  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.31 min) Epoch 38/300 -- Iteration 290903 - Batch 5929/7702 - Train loss: 0.00193743  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.33 min) Epoch 38/300 -- Iteration 290980 - Batch 6006/7702 - Train loss: 0.00193747  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.35 min) Epoch 38/300 -- Iteration 291057 - Batch 6083/7702 - Train loss: 0.00193704  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.37 min) Epoch 38/300 -- Iteration 291134 - Batch 6160/7702 - Train loss: 0.00193696  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.39 min) Epoch 38/300 -- Iteration 291211 - Batch 6237/7702 - Train loss: 0.00193718  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.41 min) Epoch 38/300 -- Iteration 291288 - Batch 6314/7702 - Train loss: 0.00193750  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.43 min) Epoch 38/300 -- Iteration 291365 - Batch 6391/7702 - Train loss: 0.00193758  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.45 min) Epoch 38/300 -- Iteration 291442 - Batch 6468/7702 - Train loss: 0.00193765  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.47 min) Epoch 38/300 -- Iteration 291519 - Batch 6545/7702 - Train loss: 0.00193783  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.49 min) Epoch 38/300 -- Iteration 291596 - Batch 6622/7702 - Train loss: 0.00193820  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.51 min) Epoch 38/300 -- Iteration 291673 - Batch 6699/7702 - Train loss: 0.00193824  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.53 min) Epoch 38/300 -- Iteration 291750 - Batch 6776/7702 - Train loss: 0.00193801  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.56 min) Epoch 38/300 -- Iteration 291827 - Batch 6853/7702 - Train loss: 0.00193781  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.58 min) Epoch 38/300 -- Iteration 291904 - Batch 6930/7702 - Train loss: 0.00193810  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.60 min) Epoch 38/300 -- Iteration 291981 - Batch 7007/7702 - Train loss: 0.00193788  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.62 min) Epoch 38/300 -- Iteration 292058 - Batch 7084/7702 - Train loss: 0.00193769  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.64 min) Epoch 38/300 -- Iteration 292135 - Batch 7161/7702 - Train loss: 0.00193751  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.66 min) Epoch 38/300 -- Iteration 292212 - Batch 7238/7702 - Train loss: 0.00193740  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.68 min) Epoch 38/300 -- Iteration 292289 - Batch 7315/7702 - Train loss: 0.00193799  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.70 min) Epoch 38/300 -- Iteration 292366 - Batch 7392/7702 - Train loss: 0.00193765  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.72 min) Epoch 38/300 -- Iteration 292443 - Batch 7469/7702 - Train loss: 0.00193748  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.74 min) Epoch 38/300 -- Iteration 292520 - Batch 7546/7702 - Train loss: 0.00193737  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.76 min) Epoch 38/300 -- Iteration 292597 - Batch 7623/7702 - Train loss: 0.00193715  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.78 min) Epoch 38/300 -- Iteration 292674 - Batch 7700/7702 - Train loss: 0.00193730  - Train acc: -0.0000 - Val loss: 0.00052587\n",
      "(78.78 min) Epoch 38/300 -- Iteration 292676 - Batch 7701/7702 - Train loss: 0.00193730  - Train acc: -0.0000 - Val loss: 0.00049976 - Val acc: -0.0000\n",
      "(78.81 min) Epoch 39/300 -- Iteration 292753 - Batch 77/7702 - Train loss: 0.00193135  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(78.83 min) Epoch 39/300 -- Iteration 292830 - Batch 154/7702 - Train loss: 0.00192581  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(78.85 min) Epoch 39/300 -- Iteration 292907 - Batch 231/7702 - Train loss: 0.00192185  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(78.87 min) Epoch 39/300 -- Iteration 292984 - Batch 308/7702 - Train loss: 0.00193044  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(78.89 min) Epoch 39/300 -- Iteration 293061 - Batch 385/7702 - Train loss: 0.00193667  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(78.91 min) Epoch 39/300 -- Iteration 293138 - Batch 462/7702 - Train loss: 0.00192945  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(78.93 min) Epoch 39/300 -- Iteration 293215 - Batch 539/7702 - Train loss: 0.00192674  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(78.95 min) Epoch 39/300 -- Iteration 293292 - Batch 616/7702 - Train loss: 0.00192714  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(78.97 min) Epoch 39/300 -- Iteration 293369 - Batch 693/7702 - Train loss: 0.00192859  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(78.99 min) Epoch 39/300 -- Iteration 293446 - Batch 770/7702 - Train loss: 0.00192715  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.01 min) Epoch 39/300 -- Iteration 293523 - Batch 847/7702 - Train loss: 0.00192839  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.03 min) Epoch 39/300 -- Iteration 293600 - Batch 924/7702 - Train loss: 0.00193010  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.05 min) Epoch 39/300 -- Iteration 293677 - Batch 1001/7702 - Train loss: 0.00193418  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.08 min) Epoch 39/300 -- Iteration 293754 - Batch 1078/7702 - Train loss: 0.00193498  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.10 min) Epoch 39/300 -- Iteration 293831 - Batch 1155/7702 - Train loss: 0.00193354  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.12 min) Epoch 39/300 -- Iteration 293908 - Batch 1232/7702 - Train loss: 0.00193339  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.14 min) Epoch 39/300 -- Iteration 293985 - Batch 1309/7702 - Train loss: 0.00193204  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.16 min) Epoch 39/300 -- Iteration 294062 - Batch 1386/7702 - Train loss: 0.00193122  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.18 min) Epoch 39/300 -- Iteration 294139 - Batch 1463/7702 - Train loss: 0.00193298  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.20 min) Epoch 39/300 -- Iteration 294216 - Batch 1540/7702 - Train loss: 0.00193274  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.22 min) Epoch 39/300 -- Iteration 294293 - Batch 1617/7702 - Train loss: 0.00193066  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.24 min) Epoch 39/300 -- Iteration 294370 - Batch 1694/7702 - Train loss: 0.00193157  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.26 min) Epoch 39/300 -- Iteration 294447 - Batch 1771/7702 - Train loss: 0.00193139  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.28 min) Epoch 39/300 -- Iteration 294524 - Batch 1848/7702 - Train loss: 0.00193121  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.30 min) Epoch 39/300 -- Iteration 294601 - Batch 1925/7702 - Train loss: 0.00193181  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.32 min) Epoch 39/300 -- Iteration 294678 - Batch 2002/7702 - Train loss: 0.00193220  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.35 min) Epoch 39/300 -- Iteration 294755 - Batch 2079/7702 - Train loss: 0.00193277  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.37 min) Epoch 39/300 -- Iteration 294832 - Batch 2156/7702 - Train loss: 0.00193327  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.39 min) Epoch 39/300 -- Iteration 294909 - Batch 2233/7702 - Train loss: 0.00193407  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.41 min) Epoch 39/300 -- Iteration 294986 - Batch 2310/7702 - Train loss: 0.00193449  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.43 min) Epoch 39/300 -- Iteration 295063 - Batch 2387/7702 - Train loss: 0.00193501  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.45 min) Epoch 39/300 -- Iteration 295140 - Batch 2464/7702 - Train loss: 0.00193455  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.47 min) Epoch 39/300 -- Iteration 295217 - Batch 2541/7702 - Train loss: 0.00193391  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.49 min) Epoch 39/300 -- Iteration 295294 - Batch 2618/7702 - Train loss: 0.00193508  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.51 min) Epoch 39/300 -- Iteration 295371 - Batch 2695/7702 - Train loss: 0.00193407  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.53 min) Epoch 39/300 -- Iteration 295448 - Batch 2772/7702 - Train loss: 0.00193426  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.55 min) Epoch 39/300 -- Iteration 295525 - Batch 2849/7702 - Train loss: 0.00193419  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.57 min) Epoch 39/300 -- Iteration 295602 - Batch 2926/7702 - Train loss: 0.00193345  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.59 min) Epoch 39/300 -- Iteration 295679 - Batch 3003/7702 - Train loss: 0.00193409  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.61 min) Epoch 39/300 -- Iteration 295756 - Batch 3080/7702 - Train loss: 0.00193342  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.64 min) Epoch 39/300 -- Iteration 295833 - Batch 3157/7702 - Train loss: 0.00193254  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.66 min) Epoch 39/300 -- Iteration 295910 - Batch 3234/7702 - Train loss: 0.00193215  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.68 min) Epoch 39/300 -- Iteration 295987 - Batch 3311/7702 - Train loss: 0.00193218  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.70 min) Epoch 39/300 -- Iteration 296064 - Batch 3388/7702 - Train loss: 0.00193257  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.72 min) Epoch 39/300 -- Iteration 296141 - Batch 3465/7702 - Train loss: 0.00193331  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.74 min) Epoch 39/300 -- Iteration 296218 - Batch 3542/7702 - Train loss: 0.00193249  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.76 min) Epoch 39/300 -- Iteration 296295 - Batch 3619/7702 - Train loss: 0.00193315  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.78 min) Epoch 39/300 -- Iteration 296372 - Batch 3696/7702 - Train loss: 0.00193217  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.80 min) Epoch 39/300 -- Iteration 296449 - Batch 3773/7702 - Train loss: 0.00193267  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.82 min) Epoch 39/300 -- Iteration 296526 - Batch 3850/7702 - Train loss: 0.00193295  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.84 min) Epoch 39/300 -- Iteration 296603 - Batch 3927/7702 - Train loss: 0.00193230  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.86 min) Epoch 39/300 -- Iteration 296680 - Batch 4004/7702 - Train loss: 0.00193217  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.88 min) Epoch 39/300 -- Iteration 296757 - Batch 4081/7702 - Train loss: 0.00193207  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.90 min) Epoch 39/300 -- Iteration 296834 - Batch 4158/7702 - Train loss: 0.00193298  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.93 min) Epoch 39/300 -- Iteration 296911 - Batch 4235/7702 - Train loss: 0.00193281  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.95 min) Epoch 39/300 -- Iteration 296988 - Batch 4312/7702 - Train loss: 0.00193289  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.97 min) Epoch 39/300 -- Iteration 297065 - Batch 4389/7702 - Train loss: 0.00193305  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(79.99 min) Epoch 39/300 -- Iteration 297142 - Batch 4466/7702 - Train loss: 0.00193301  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.01 min) Epoch 39/300 -- Iteration 297219 - Batch 4543/7702 - Train loss: 0.00193258  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.03 min) Epoch 39/300 -- Iteration 297296 - Batch 4620/7702 - Train loss: 0.00193262  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.05 min) Epoch 39/300 -- Iteration 297373 - Batch 4697/7702 - Train loss: 0.00193347  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.07 min) Epoch 39/300 -- Iteration 297450 - Batch 4774/7702 - Train loss: 0.00193380  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.09 min) Epoch 39/300 -- Iteration 297527 - Batch 4851/7702 - Train loss: 0.00193347  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.11 min) Epoch 39/300 -- Iteration 297604 - Batch 4928/7702 - Train loss: 0.00193353  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.13 min) Epoch 39/300 -- Iteration 297681 - Batch 5005/7702 - Train loss: 0.00193334  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.15 min) Epoch 39/300 -- Iteration 297758 - Batch 5082/7702 - Train loss: 0.00193357  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.17 min) Epoch 39/300 -- Iteration 297835 - Batch 5159/7702 - Train loss: 0.00193350  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.19 min) Epoch 39/300 -- Iteration 297912 - Batch 5236/7702 - Train loss: 0.00193417  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.21 min) Epoch 39/300 -- Iteration 297989 - Batch 5313/7702 - Train loss: 0.00193403  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.24 min) Epoch 39/300 -- Iteration 298066 - Batch 5390/7702 - Train loss: 0.00193491  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.26 min) Epoch 39/300 -- Iteration 298143 - Batch 5467/7702 - Train loss: 0.00193473  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.28 min) Epoch 39/300 -- Iteration 298220 - Batch 5544/7702 - Train loss: 0.00193534  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.30 min) Epoch 39/300 -- Iteration 298297 - Batch 5621/7702 - Train loss: 0.00193613  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.32 min) Epoch 39/300 -- Iteration 298374 - Batch 5698/7702 - Train loss: 0.00193645  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.34 min) Epoch 39/300 -- Iteration 298451 - Batch 5775/7702 - Train loss: 0.00193656  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.36 min) Epoch 39/300 -- Iteration 298528 - Batch 5852/7702 - Train loss: 0.00193705  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.38 min) Epoch 39/300 -- Iteration 298605 - Batch 5929/7702 - Train loss: 0.00193668  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.40 min) Epoch 39/300 -- Iteration 298682 - Batch 6006/7702 - Train loss: 0.00193693  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.42 min) Epoch 39/300 -- Iteration 298759 - Batch 6083/7702 - Train loss: 0.00193675  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.44 min) Epoch 39/300 -- Iteration 298836 - Batch 6160/7702 - Train loss: 0.00193646  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.46 min) Epoch 39/300 -- Iteration 298913 - Batch 6237/7702 - Train loss: 0.00193682  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.48 min) Epoch 39/300 -- Iteration 298990 - Batch 6314/7702 - Train loss: 0.00193675  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.51 min) Epoch 39/300 -- Iteration 299067 - Batch 6391/7702 - Train loss: 0.00193688  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.53 min) Epoch 39/300 -- Iteration 299144 - Batch 6468/7702 - Train loss: 0.00193715  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.55 min) Epoch 39/300 -- Iteration 299221 - Batch 6545/7702 - Train loss: 0.00193744  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.57 min) Epoch 39/300 -- Iteration 299298 - Batch 6622/7702 - Train loss: 0.00193740  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.59 min) Epoch 39/300 -- Iteration 299375 - Batch 6699/7702 - Train loss: 0.00193725  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.61 min) Epoch 39/300 -- Iteration 299452 - Batch 6776/7702 - Train loss: 0.00193783  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.63 min) Epoch 39/300 -- Iteration 299529 - Batch 6853/7702 - Train loss: 0.00193817  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.65 min) Epoch 39/300 -- Iteration 299606 - Batch 6930/7702 - Train loss: 0.00193815  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.67 min) Epoch 39/300 -- Iteration 299683 - Batch 7007/7702 - Train loss: 0.00193787  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.69 min) Epoch 39/300 -- Iteration 299760 - Batch 7084/7702 - Train loss: 0.00193771  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.71 min) Epoch 39/300 -- Iteration 299837 - Batch 7161/7702 - Train loss: 0.00193779  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.73 min) Epoch 39/300 -- Iteration 299914 - Batch 7238/7702 - Train loss: 0.00193756  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.75 min) Epoch 39/300 -- Iteration 299991 - Batch 7315/7702 - Train loss: 0.00193755  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.77 min) Epoch 39/300 -- Iteration 300068 - Batch 7392/7702 - Train loss: 0.00193790  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.80 min) Epoch 39/300 -- Iteration 300145 - Batch 7469/7702 - Train loss: 0.00193772  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.82 min) Epoch 39/300 -- Iteration 300222 - Batch 7546/7702 - Train loss: 0.00193778  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.84 min) Epoch 39/300 -- Iteration 300299 - Batch 7623/7702 - Train loss: 0.00193751  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.86 min) Epoch 39/300 -- Iteration 300376 - Batch 7700/7702 - Train loss: 0.00193757  - Train acc: -0.0000 - Val loss: 0.00049976\n",
      "(80.86 min) Epoch 39/300 -- Iteration 300378 - Batch 7701/7702 - Train loss: 0.00193763  - Train acc: -0.0000 - Val loss: 0.00058699 - Val acc: -0.0000\n",
      "(80.88 min) Epoch 40/300 -- Iteration 300455 - Batch 77/7702 - Train loss: 0.00193400  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(80.90 min) Epoch 40/300 -- Iteration 300532 - Batch 154/7702 - Train loss: 0.00195726  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(80.92 min) Epoch 40/300 -- Iteration 300609 - Batch 231/7702 - Train loss: 0.00194470  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(80.94 min) Epoch 40/300 -- Iteration 300686 - Batch 308/7702 - Train loss: 0.00193507  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(80.96 min) Epoch 40/300 -- Iteration 300763 - Batch 385/7702 - Train loss: 0.00193773  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(80.98 min) Epoch 40/300 -- Iteration 300840 - Batch 462/7702 - Train loss: 0.00193607  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.01 min) Epoch 40/300 -- Iteration 300917 - Batch 539/7702 - Train loss: 0.00193458  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.03 min) Epoch 40/300 -- Iteration 300994 - Batch 616/7702 - Train loss: 0.00193430  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.05 min) Epoch 40/300 -- Iteration 301071 - Batch 693/7702 - Train loss: 0.00193337  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.07 min) Epoch 40/300 -- Iteration 301148 - Batch 770/7702 - Train loss: 0.00193240  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.09 min) Epoch 40/300 -- Iteration 301225 - Batch 847/7702 - Train loss: 0.00193564  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.11 min) Epoch 40/300 -- Iteration 301302 - Batch 924/7702 - Train loss: 0.00193690  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.13 min) Epoch 40/300 -- Iteration 301379 - Batch 1001/7702 - Train loss: 0.00193542  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.15 min) Epoch 40/300 -- Iteration 301456 - Batch 1078/7702 - Train loss: 0.00193611  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.17 min) Epoch 40/300 -- Iteration 301533 - Batch 1155/7702 - Train loss: 0.00193721  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.19 min) Epoch 40/300 -- Iteration 301610 - Batch 1232/7702 - Train loss: 0.00193876  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.21 min) Epoch 40/300 -- Iteration 301687 - Batch 1309/7702 - Train loss: 0.00193829  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.23 min) Epoch 40/300 -- Iteration 301764 - Batch 1386/7702 - Train loss: 0.00193846  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.25 min) Epoch 40/300 -- Iteration 301841 - Batch 1463/7702 - Train loss: 0.00193725  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.28 min) Epoch 40/300 -- Iteration 301918 - Batch 1540/7702 - Train loss: 0.00193770  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.30 min) Epoch 40/300 -- Iteration 301995 - Batch 1617/7702 - Train loss: 0.00193717  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.32 min) Epoch 40/300 -- Iteration 302072 - Batch 1694/7702 - Train loss: 0.00193750  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.34 min) Epoch 40/300 -- Iteration 302149 - Batch 1771/7702 - Train loss: 0.00193801  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.36 min) Epoch 40/300 -- Iteration 302226 - Batch 1848/7702 - Train loss: 0.00193572  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.38 min) Epoch 40/300 -- Iteration 302303 - Batch 1925/7702 - Train loss: 0.00193602  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.40 min) Epoch 40/300 -- Iteration 302380 - Batch 2002/7702 - Train loss: 0.00193504  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.42 min) Epoch 40/300 -- Iteration 302457 - Batch 2079/7702 - Train loss: 0.00193510  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.44 min) Epoch 40/300 -- Iteration 302534 - Batch 2156/7702 - Train loss: 0.00193516  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.46 min) Epoch 40/300 -- Iteration 302611 - Batch 2233/7702 - Train loss: 0.00193637  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.48 min) Epoch 40/300 -- Iteration 302688 - Batch 2310/7702 - Train loss: 0.00193563  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.50 min) Epoch 40/300 -- Iteration 302765 - Batch 2387/7702 - Train loss: 0.00193613  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.52 min) Epoch 40/300 -- Iteration 302842 - Batch 2464/7702 - Train loss: 0.00193595  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.54 min) Epoch 40/300 -- Iteration 302919 - Batch 2541/7702 - Train loss: 0.00193554  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.57 min) Epoch 40/300 -- Iteration 302996 - Batch 2618/7702 - Train loss: 0.00193615  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.59 min) Epoch 40/300 -- Iteration 303073 - Batch 2695/7702 - Train loss: 0.00193687  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.61 min) Epoch 40/300 -- Iteration 303150 - Batch 2772/7702 - Train loss: 0.00193668  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.63 min) Epoch 40/300 -- Iteration 303227 - Batch 2849/7702 - Train loss: 0.00193775  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.65 min) Epoch 40/300 -- Iteration 303304 - Batch 2926/7702 - Train loss: 0.00193828  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.67 min) Epoch 40/300 -- Iteration 303381 - Batch 3003/7702 - Train loss: 0.00193742  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.69 min) Epoch 40/300 -- Iteration 303458 - Batch 3080/7702 - Train loss: 0.00193831  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.71 min) Epoch 40/300 -- Iteration 303535 - Batch 3157/7702 - Train loss: 0.00193876  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.73 min) Epoch 40/300 -- Iteration 303612 - Batch 3234/7702 - Train loss: 0.00193910  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.75 min) Epoch 40/300 -- Iteration 303689 - Batch 3311/7702 - Train loss: 0.00193905  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.77 min) Epoch 40/300 -- Iteration 303766 - Batch 3388/7702 - Train loss: 0.00194034  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.79 min) Epoch 40/300 -- Iteration 303843 - Batch 3465/7702 - Train loss: 0.00194028  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.81 min) Epoch 40/300 -- Iteration 303920 - Batch 3542/7702 - Train loss: 0.00194018  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.84 min) Epoch 40/300 -- Iteration 303997 - Batch 3619/7702 - Train loss: 0.00194019  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.86 min) Epoch 40/300 -- Iteration 304074 - Batch 3696/7702 - Train loss: 0.00194028  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.88 min) Epoch 40/300 -- Iteration 304151 - Batch 3773/7702 - Train loss: 0.00193954  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.90 min) Epoch 40/300 -- Iteration 304228 - Batch 3850/7702 - Train loss: 0.00193921  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.92 min) Epoch 40/300 -- Iteration 304305 - Batch 3927/7702 - Train loss: 0.00193910  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.94 min) Epoch 40/300 -- Iteration 304382 - Batch 4004/7702 - Train loss: 0.00193778  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.96 min) Epoch 40/300 -- Iteration 304459 - Batch 4081/7702 - Train loss: 0.00193792  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(81.98 min) Epoch 40/300 -- Iteration 304536 - Batch 4158/7702 - Train loss: 0.00193815  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.00 min) Epoch 40/300 -- Iteration 304613 - Batch 4235/7702 - Train loss: 0.00193863  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.02 min) Epoch 40/300 -- Iteration 304690 - Batch 4312/7702 - Train loss: 0.00193880  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.04 min) Epoch 40/300 -- Iteration 304767 - Batch 4389/7702 - Train loss: 0.00193840  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.06 min) Epoch 40/300 -- Iteration 304844 - Batch 4466/7702 - Train loss: 0.00193805  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.08 min) Epoch 40/300 -- Iteration 304921 - Batch 4543/7702 - Train loss: 0.00193743  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.11 min) Epoch 40/300 -- Iteration 304998 - Batch 4620/7702 - Train loss: 0.00193682  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.13 min) Epoch 40/300 -- Iteration 305075 - Batch 4697/7702 - Train loss: 0.00193630  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.15 min) Epoch 40/300 -- Iteration 305152 - Batch 4774/7702 - Train loss: 0.00193639  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.17 min) Epoch 40/300 -- Iteration 305229 - Batch 4851/7702 - Train loss: 0.00193696  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.19 min) Epoch 40/300 -- Iteration 305306 - Batch 4928/7702 - Train loss: 0.00193686  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.21 min) Epoch 40/300 -- Iteration 305383 - Batch 5005/7702 - Train loss: 0.00193671  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.23 min) Epoch 40/300 -- Iteration 305460 - Batch 5082/7702 - Train loss: 0.00193642  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.25 min) Epoch 40/300 -- Iteration 305537 - Batch 5159/7702 - Train loss: 0.00193622  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.27 min) Epoch 40/300 -- Iteration 305614 - Batch 5236/7702 - Train loss: 0.00193572  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.29 min) Epoch 40/300 -- Iteration 305691 - Batch 5313/7702 - Train loss: 0.00193598  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.31 min) Epoch 40/300 -- Iteration 305768 - Batch 5390/7702 - Train loss: 0.00193575  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.33 min) Epoch 40/300 -- Iteration 305845 - Batch 5467/7702 - Train loss: 0.00193575  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.35 min) Epoch 40/300 -- Iteration 305922 - Batch 5544/7702 - Train loss: 0.00193644  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.37 min) Epoch 40/300 -- Iteration 305999 - Batch 5621/7702 - Train loss: 0.00193607  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.39 min) Epoch 40/300 -- Iteration 306076 - Batch 5698/7702 - Train loss: 0.00193580  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.42 min) Epoch 40/300 -- Iteration 306153 - Batch 5775/7702 - Train loss: 0.00193623  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.44 min) Epoch 40/300 -- Iteration 306230 - Batch 5852/7702 - Train loss: 0.00193650  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.46 min) Epoch 40/300 -- Iteration 306307 - Batch 5929/7702 - Train loss: 0.00193625  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.48 min) Epoch 40/300 -- Iteration 306384 - Batch 6006/7702 - Train loss: 0.00193558  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.50 min) Epoch 40/300 -- Iteration 306461 - Batch 6083/7702 - Train loss: 0.00193575  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.52 min) Epoch 40/300 -- Iteration 306538 - Batch 6160/7702 - Train loss: 0.00193534  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.54 min) Epoch 40/300 -- Iteration 306615 - Batch 6237/7702 - Train loss: 0.00193558  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.56 min) Epoch 40/300 -- Iteration 306692 - Batch 6314/7702 - Train loss: 0.00193524  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.58 min) Epoch 40/300 -- Iteration 306769 - Batch 6391/7702 - Train loss: 0.00193492  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.60 min) Epoch 40/300 -- Iteration 306846 - Batch 6468/7702 - Train loss: 0.00193469  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.62 min) Epoch 40/300 -- Iteration 306923 - Batch 6545/7702 - Train loss: 0.00193458  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.64 min) Epoch 40/300 -- Iteration 307000 - Batch 6622/7702 - Train loss: 0.00193462  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.66 min) Epoch 40/300 -- Iteration 307077 - Batch 6699/7702 - Train loss: 0.00193440  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.68 min) Epoch 40/300 -- Iteration 307154 - Batch 6776/7702 - Train loss: 0.00193463  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.71 min) Epoch 40/300 -- Iteration 307231 - Batch 6853/7702 - Train loss: 0.00193513  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.73 min) Epoch 40/300 -- Iteration 307308 - Batch 6930/7702 - Train loss: 0.00193571  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.75 min) Epoch 40/300 -- Iteration 307385 - Batch 7007/7702 - Train loss: 0.00193587  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.77 min) Epoch 40/300 -- Iteration 307462 - Batch 7084/7702 - Train loss: 0.00193604  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.79 min) Epoch 40/300 -- Iteration 307539 - Batch 7161/7702 - Train loss: 0.00193605  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.81 min) Epoch 40/300 -- Iteration 307616 - Batch 7238/7702 - Train loss: 0.00193600  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.83 min) Epoch 40/300 -- Iteration 307693 - Batch 7315/7702 - Train loss: 0.00193599  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.85 min) Epoch 40/300 -- Iteration 307770 - Batch 7392/7702 - Train loss: 0.00193595  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.87 min) Epoch 40/300 -- Iteration 307847 - Batch 7469/7702 - Train loss: 0.00193559  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.89 min) Epoch 40/300 -- Iteration 307924 - Batch 7546/7702 - Train loss: 0.00193551  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.91 min) Epoch 40/300 -- Iteration 308001 - Batch 7623/7702 - Train loss: 0.00193529  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.93 min) Epoch 40/300 -- Iteration 308078 - Batch 7700/7702 - Train loss: 0.00193563  - Train acc: -0.0000 - Val loss: 0.00058699\n",
      "(82.93 min) Epoch 40/300 -- Iteration 308080 - Batch 7701/7702 - Train loss: 0.00193570  - Train acc: -0.0000 - Val loss: 0.00047827 - Val acc: -0.0000\n",
      "(82.96 min) Epoch 41/300 -- Iteration 308157 - Batch 77/7702 - Train loss: 0.00195849  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(82.98 min) Epoch 41/300 -- Iteration 308234 - Batch 154/7702 - Train loss: 0.00194149  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.00 min) Epoch 41/300 -- Iteration 308311 - Batch 231/7702 - Train loss: 0.00192300  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.02 min) Epoch 41/300 -- Iteration 308388 - Batch 308/7702 - Train loss: 0.00191357  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.04 min) Epoch 41/300 -- Iteration 308465 - Batch 385/7702 - Train loss: 0.00191419  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.06 min) Epoch 41/300 -- Iteration 308542 - Batch 462/7702 - Train loss: 0.00191829  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.08 min) Epoch 41/300 -- Iteration 308619 - Batch 539/7702 - Train loss: 0.00192101  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.10 min) Epoch 41/300 -- Iteration 308696 - Batch 616/7702 - Train loss: 0.00192913  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.12 min) Epoch 41/300 -- Iteration 308773 - Batch 693/7702 - Train loss: 0.00193093  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.14 min) Epoch 41/300 -- Iteration 308850 - Batch 770/7702 - Train loss: 0.00193072  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.16 min) Epoch 41/300 -- Iteration 308927 - Batch 847/7702 - Train loss: 0.00193242  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.18 min) Epoch 41/300 -- Iteration 309004 - Batch 924/7702 - Train loss: 0.00193238  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.21 min) Epoch 41/300 -- Iteration 309081 - Batch 1001/7702 - Train loss: 0.00193330  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.23 min) Epoch 41/300 -- Iteration 309158 - Batch 1078/7702 - Train loss: 0.00193751  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.25 min) Epoch 41/300 -- Iteration 309235 - Batch 1155/7702 - Train loss: 0.00193620  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.27 min) Epoch 41/300 -- Iteration 309312 - Batch 1232/7702 - Train loss: 0.00193346  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.29 min) Epoch 41/300 -- Iteration 309389 - Batch 1309/7702 - Train loss: 0.00193128  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.31 min) Epoch 41/300 -- Iteration 309466 - Batch 1386/7702 - Train loss: 0.00193223  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.33 min) Epoch 41/300 -- Iteration 309543 - Batch 1463/7702 - Train loss: 0.00193130  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.35 min) Epoch 41/300 -- Iteration 309620 - Batch 1540/7702 - Train loss: 0.00193179  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.37 min) Epoch 41/300 -- Iteration 309697 - Batch 1617/7702 - Train loss: 0.00193012  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.39 min) Epoch 41/300 -- Iteration 309774 - Batch 1694/7702 - Train loss: 0.00193075  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.41 min) Epoch 41/300 -- Iteration 309851 - Batch 1771/7702 - Train loss: 0.00192978  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.43 min) Epoch 41/300 -- Iteration 309928 - Batch 1848/7702 - Train loss: 0.00192964  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.45 min) Epoch 41/300 -- Iteration 310005 - Batch 1925/7702 - Train loss: 0.00192848  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.47 min) Epoch 41/300 -- Iteration 310082 - Batch 2002/7702 - Train loss: 0.00192921  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.50 min) Epoch 41/300 -- Iteration 310159 - Batch 2079/7702 - Train loss: 0.00192965  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.52 min) Epoch 41/300 -- Iteration 310236 - Batch 2156/7702 - Train loss: 0.00192969  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.54 min) Epoch 41/300 -- Iteration 310313 - Batch 2233/7702 - Train loss: 0.00192898  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.56 min) Epoch 41/300 -- Iteration 310390 - Batch 2310/7702 - Train loss: 0.00192977  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.58 min) Epoch 41/300 -- Iteration 310467 - Batch 2387/7702 - Train loss: 0.00192897  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.60 min) Epoch 41/300 -- Iteration 310544 - Batch 2464/7702 - Train loss: 0.00192958  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.62 min) Epoch 41/300 -- Iteration 310621 - Batch 2541/7702 - Train loss: 0.00192860  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.64 min) Epoch 41/300 -- Iteration 310698 - Batch 2618/7702 - Train loss: 0.00192760  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.66 min) Epoch 41/300 -- Iteration 310775 - Batch 2695/7702 - Train loss: 0.00192803  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.68 min) Epoch 41/300 -- Iteration 310852 - Batch 2772/7702 - Train loss: 0.00192782  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.70 min) Epoch 41/300 -- Iteration 310929 - Batch 2849/7702 - Train loss: 0.00192759  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.72 min) Epoch 41/300 -- Iteration 311006 - Batch 2926/7702 - Train loss: 0.00192768  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.74 min) Epoch 41/300 -- Iteration 311083 - Batch 3003/7702 - Train loss: 0.00192770  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.76 min) Epoch 41/300 -- Iteration 311160 - Batch 3080/7702 - Train loss: 0.00192712  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.78 min) Epoch 41/300 -- Iteration 311237 - Batch 3157/7702 - Train loss: 0.00192729  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.81 min) Epoch 41/300 -- Iteration 311314 - Batch 3234/7702 - Train loss: 0.00192810  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.83 min) Epoch 41/300 -- Iteration 311391 - Batch 3311/7702 - Train loss: 0.00192870  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.85 min) Epoch 41/300 -- Iteration 311468 - Batch 3388/7702 - Train loss: 0.00192814  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.87 min) Epoch 41/300 -- Iteration 311545 - Batch 3465/7702 - Train loss: 0.00192938  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.89 min) Epoch 41/300 -- Iteration 311622 - Batch 3542/7702 - Train loss: 0.00193012  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.91 min) Epoch 41/300 -- Iteration 311699 - Batch 3619/7702 - Train loss: 0.00193020  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.93 min) Epoch 41/300 -- Iteration 311776 - Batch 3696/7702 - Train loss: 0.00193034  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.95 min) Epoch 41/300 -- Iteration 311853 - Batch 3773/7702 - Train loss: 0.00192985  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.97 min) Epoch 41/300 -- Iteration 311930 - Batch 3850/7702 - Train loss: 0.00192889  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(83.99 min) Epoch 41/300 -- Iteration 312007 - Batch 3927/7702 - Train loss: 0.00192939  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.01 min) Epoch 41/300 -- Iteration 312084 - Batch 4004/7702 - Train loss: 0.00193003  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.03 min) Epoch 41/300 -- Iteration 312161 - Batch 4081/7702 - Train loss: 0.00193059  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.05 min) Epoch 41/300 -- Iteration 312238 - Batch 4158/7702 - Train loss: 0.00193109  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.07 min) Epoch 41/300 -- Iteration 312315 - Batch 4235/7702 - Train loss: 0.00193034  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.10 min) Epoch 41/300 -- Iteration 312392 - Batch 4312/7702 - Train loss: 0.00193004  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.12 min) Epoch 41/300 -- Iteration 312469 - Batch 4389/7702 - Train loss: 0.00193050  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.14 min) Epoch 41/300 -- Iteration 312546 - Batch 4466/7702 - Train loss: 0.00193082  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.16 min) Epoch 41/300 -- Iteration 312623 - Batch 4543/7702 - Train loss: 0.00193045  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.18 min) Epoch 41/300 -- Iteration 312700 - Batch 4620/7702 - Train loss: 0.00193066  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.20 min) Epoch 41/300 -- Iteration 312777 - Batch 4697/7702 - Train loss: 0.00193029  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.22 min) Epoch 41/300 -- Iteration 312854 - Batch 4774/7702 - Train loss: 0.00193098  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.24 min) Epoch 41/300 -- Iteration 312931 - Batch 4851/7702 - Train loss: 0.00193120  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.26 min) Epoch 41/300 -- Iteration 313008 - Batch 4928/7702 - Train loss: 0.00193102  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.28 min) Epoch 41/300 -- Iteration 313085 - Batch 5005/7702 - Train loss: 0.00193172  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.30 min) Epoch 41/300 -- Iteration 313162 - Batch 5082/7702 - Train loss: 0.00193189  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.32 min) Epoch 41/300 -- Iteration 313239 - Batch 5159/7702 - Train loss: 0.00193211  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.34 min) Epoch 41/300 -- Iteration 313316 - Batch 5236/7702 - Train loss: 0.00193198  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.37 min) Epoch 41/300 -- Iteration 313393 - Batch 5313/7702 - Train loss: 0.00193170  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.39 min) Epoch 41/300 -- Iteration 313470 - Batch 5390/7702 - Train loss: 0.00193143  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.41 min) Epoch 41/300 -- Iteration 313547 - Batch 5467/7702 - Train loss: 0.00193179  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.43 min) Epoch 41/300 -- Iteration 313624 - Batch 5544/7702 - Train loss: 0.00193222  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.45 min) Epoch 41/300 -- Iteration 313701 - Batch 5621/7702 - Train loss: 0.00193188  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.47 min) Epoch 41/300 -- Iteration 313778 - Batch 5698/7702 - Train loss: 0.00193208  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.49 min) Epoch 41/300 -- Iteration 313855 - Batch 5775/7702 - Train loss: 0.00193232  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.51 min) Epoch 41/300 -- Iteration 313932 - Batch 5852/7702 - Train loss: 0.00193249  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.53 min) Epoch 41/300 -- Iteration 314009 - Batch 5929/7702 - Train loss: 0.00193276  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.55 min) Epoch 41/300 -- Iteration 314086 - Batch 6006/7702 - Train loss: 0.00193255  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.57 min) Epoch 41/300 -- Iteration 314163 - Batch 6083/7702 - Train loss: 0.00193221  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.59 min) Epoch 41/300 -- Iteration 314240 - Batch 6160/7702 - Train loss: 0.00193152  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.61 min) Epoch 41/300 -- Iteration 314317 - Batch 6237/7702 - Train loss: 0.00193139  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.63 min) Epoch 41/300 -- Iteration 314394 - Batch 6314/7702 - Train loss: 0.00193085  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.66 min) Epoch 41/300 -- Iteration 314471 - Batch 6391/7702 - Train loss: 0.00193102  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.68 min) Epoch 41/300 -- Iteration 314548 - Batch 6468/7702 - Train loss: 0.00193081  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.70 min) Epoch 41/300 -- Iteration 314625 - Batch 6545/7702 - Train loss: 0.00193071  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.72 min) Epoch 41/300 -- Iteration 314702 - Batch 6622/7702 - Train loss: 0.00193059  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.74 min) Epoch 41/300 -- Iteration 314779 - Batch 6699/7702 - Train loss: 0.00193019  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.76 min) Epoch 41/300 -- Iteration 314856 - Batch 6776/7702 - Train loss: 0.00193018  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.78 min) Epoch 41/300 -- Iteration 314933 - Batch 6853/7702 - Train loss: 0.00193039  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.80 min) Epoch 41/300 -- Iteration 315010 - Batch 6930/7702 - Train loss: 0.00193009  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.82 min) Epoch 41/300 -- Iteration 315087 - Batch 7007/7702 - Train loss: 0.00192988  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.84 min) Epoch 41/300 -- Iteration 315164 - Batch 7084/7702 - Train loss: 0.00193012  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.86 min) Epoch 41/300 -- Iteration 315241 - Batch 7161/7702 - Train loss: 0.00192991  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.88 min) Epoch 41/300 -- Iteration 315318 - Batch 7238/7702 - Train loss: 0.00192987  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.90 min) Epoch 41/300 -- Iteration 315395 - Batch 7315/7702 - Train loss: 0.00193002  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.92 min) Epoch 41/300 -- Iteration 315472 - Batch 7392/7702 - Train loss: 0.00192989  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.95 min) Epoch 41/300 -- Iteration 315549 - Batch 7469/7702 - Train loss: 0.00192974  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.97 min) Epoch 41/300 -- Iteration 315626 - Batch 7546/7702 - Train loss: 0.00192970  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(84.99 min) Epoch 41/300 -- Iteration 315703 - Batch 7623/7702 - Train loss: 0.00192962  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(85.01 min) Epoch 41/300 -- Iteration 315780 - Batch 7700/7702 - Train loss: 0.00192985  - Train acc: -0.0000 - Val loss: 0.00047827\n",
      "(85.01 min) Epoch 41/300 -- Iteration 315782 - Batch 7701/7702 - Train loss: 0.00192985  - Train acc: -0.0000 - Val loss: 0.00042259 - Val acc: -0.0000\n",
      "(85.03 min) Epoch 42/300 -- Iteration 315859 - Batch 77/7702 - Train loss: 0.00191600  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.05 min) Epoch 42/300 -- Iteration 315936 - Batch 154/7702 - Train loss: 0.00190658  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.07 min) Epoch 42/300 -- Iteration 316013 - Batch 231/7702 - Train loss: 0.00191945  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.09 min) Epoch 42/300 -- Iteration 316090 - Batch 308/7702 - Train loss: 0.00192199  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.11 min) Epoch 42/300 -- Iteration 316167 - Batch 385/7702 - Train loss: 0.00192759  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.13 min) Epoch 42/300 -- Iteration 316244 - Batch 462/7702 - Train loss: 0.00192565  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.16 min) Epoch 42/300 -- Iteration 316321 - Batch 539/7702 - Train loss: 0.00192622  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.18 min) Epoch 42/300 -- Iteration 316398 - Batch 616/7702 - Train loss: 0.00192308  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.20 min) Epoch 42/300 -- Iteration 316475 - Batch 693/7702 - Train loss: 0.00192596  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.22 min) Epoch 42/300 -- Iteration 316552 - Batch 770/7702 - Train loss: 0.00192776  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.24 min) Epoch 42/300 -- Iteration 316629 - Batch 847/7702 - Train loss: 0.00192522  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.26 min) Epoch 42/300 -- Iteration 316706 - Batch 924/7702 - Train loss: 0.00192685  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.28 min) Epoch 42/300 -- Iteration 316783 - Batch 1001/7702 - Train loss: 0.00192685  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.30 min) Epoch 42/300 -- Iteration 316860 - Batch 1078/7702 - Train loss: 0.00192376  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.32 min) Epoch 42/300 -- Iteration 316937 - Batch 1155/7702 - Train loss: 0.00192362  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.34 min) Epoch 42/300 -- Iteration 317014 - Batch 1232/7702 - Train loss: 0.00192528  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.36 min) Epoch 42/300 -- Iteration 317091 - Batch 1309/7702 - Train loss: 0.00192254  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.38 min) Epoch 42/300 -- Iteration 317168 - Batch 1386/7702 - Train loss: 0.00192140  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.40 min) Epoch 42/300 -- Iteration 317245 - Batch 1463/7702 - Train loss: 0.00192158  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.43 min) Epoch 42/300 -- Iteration 317322 - Batch 1540/7702 - Train loss: 0.00192137  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.45 min) Epoch 42/300 -- Iteration 317399 - Batch 1617/7702 - Train loss: 0.00192116  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.47 min) Epoch 42/300 -- Iteration 317476 - Batch 1694/7702 - Train loss: 0.00192139  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.49 min) Epoch 42/300 -- Iteration 317553 - Batch 1771/7702 - Train loss: 0.00192173  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.51 min) Epoch 42/300 -- Iteration 317630 - Batch 1848/7702 - Train loss: 0.00192084  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.53 min) Epoch 42/300 -- Iteration 317707 - Batch 1925/7702 - Train loss: 0.00192198  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.55 min) Epoch 42/300 -- Iteration 317784 - Batch 2002/7702 - Train loss: 0.00192284  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.57 min) Epoch 42/300 -- Iteration 317861 - Batch 2079/7702 - Train loss: 0.00192347  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.59 min) Epoch 42/300 -- Iteration 317938 - Batch 2156/7702 - Train loss: 0.00192428  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.61 min) Epoch 42/300 -- Iteration 318015 - Batch 2233/7702 - Train loss: 0.00192523  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.63 min) Epoch 42/300 -- Iteration 318092 - Batch 2310/7702 - Train loss: 0.00192556  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.65 min) Epoch 42/300 -- Iteration 318169 - Batch 2387/7702 - Train loss: 0.00192559  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.67 min) Epoch 42/300 -- Iteration 318246 - Batch 2464/7702 - Train loss: 0.00192569  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.70 min) Epoch 42/300 -- Iteration 318323 - Batch 2541/7702 - Train loss: 0.00192636  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.72 min) Epoch 42/300 -- Iteration 318400 - Batch 2618/7702 - Train loss: 0.00192705  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.74 min) Epoch 42/300 -- Iteration 318477 - Batch 2695/7702 - Train loss: 0.00192713  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.76 min) Epoch 42/300 -- Iteration 318554 - Batch 2772/7702 - Train loss: 0.00192669  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.78 min) Epoch 42/300 -- Iteration 318631 - Batch 2849/7702 - Train loss: 0.00192781  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.80 min) Epoch 42/300 -- Iteration 318708 - Batch 2926/7702 - Train loss: 0.00192733  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.82 min) Epoch 42/300 -- Iteration 318785 - Batch 3003/7702 - Train loss: 0.00192780  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.84 min) Epoch 42/300 -- Iteration 318862 - Batch 3080/7702 - Train loss: 0.00192845  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.86 min) Epoch 42/300 -- Iteration 318939 - Batch 3157/7702 - Train loss: 0.00192872  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.88 min) Epoch 42/300 -- Iteration 319016 - Batch 3234/7702 - Train loss: 0.00192832  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.90 min) Epoch 42/300 -- Iteration 319093 - Batch 3311/7702 - Train loss: 0.00192842  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.92 min) Epoch 42/300 -- Iteration 319170 - Batch 3388/7702 - Train loss: 0.00192857  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.94 min) Epoch 42/300 -- Iteration 319247 - Batch 3465/7702 - Train loss: 0.00192842  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.96 min) Epoch 42/300 -- Iteration 319324 - Batch 3542/7702 - Train loss: 0.00192813  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(85.99 min) Epoch 42/300 -- Iteration 319401 - Batch 3619/7702 - Train loss: 0.00192916  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.01 min) Epoch 42/300 -- Iteration 319478 - Batch 3696/7702 - Train loss: 0.00192933  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.03 min) Epoch 42/300 -- Iteration 319555 - Batch 3773/7702 - Train loss: 0.00192909  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.05 min) Epoch 42/300 -- Iteration 319632 - Batch 3850/7702 - Train loss: 0.00192777  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.07 min) Epoch 42/300 -- Iteration 319709 - Batch 3927/7702 - Train loss: 0.00192790  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.09 min) Epoch 42/300 -- Iteration 319786 - Batch 4004/7702 - Train loss: 0.00192759  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.11 min) Epoch 42/300 -- Iteration 319863 - Batch 4081/7702 - Train loss: 0.00192782  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.13 min) Epoch 42/300 -- Iteration 319940 - Batch 4158/7702 - Train loss: 0.00192797  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.15 min) Epoch 42/300 -- Iteration 320017 - Batch 4235/7702 - Train loss: 0.00192754  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.17 min) Epoch 42/300 -- Iteration 320094 - Batch 4312/7702 - Train loss: 0.00192820  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.19 min) Epoch 42/300 -- Iteration 320171 - Batch 4389/7702 - Train loss: 0.00192810  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.21 min) Epoch 42/300 -- Iteration 320248 - Batch 4466/7702 - Train loss: 0.00192800  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.23 min) Epoch 42/300 -- Iteration 320325 - Batch 4543/7702 - Train loss: 0.00192818  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.25 min) Epoch 42/300 -- Iteration 320402 - Batch 4620/7702 - Train loss: 0.00192781  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.28 min) Epoch 42/300 -- Iteration 320479 - Batch 4697/7702 - Train loss: 0.00192790  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.30 min) Epoch 42/300 -- Iteration 320556 - Batch 4774/7702 - Train loss: 0.00192854  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.32 min) Epoch 42/300 -- Iteration 320633 - Batch 4851/7702 - Train loss: 0.00192867  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.34 min) Epoch 42/300 -- Iteration 320710 - Batch 4928/7702 - Train loss: 0.00192893  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.36 min) Epoch 42/300 -- Iteration 320787 - Batch 5005/7702 - Train loss: 0.00192838  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.38 min) Epoch 42/300 -- Iteration 320864 - Batch 5082/7702 - Train loss: 0.00192852  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.40 min) Epoch 42/300 -- Iteration 320941 - Batch 5159/7702 - Train loss: 0.00192819  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.42 min) Epoch 42/300 -- Iteration 321018 - Batch 5236/7702 - Train loss: 0.00192749  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.44 min) Epoch 42/300 -- Iteration 321095 - Batch 5313/7702 - Train loss: 0.00192720  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.46 min) Epoch 42/300 -- Iteration 321172 - Batch 5390/7702 - Train loss: 0.00192731  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.48 min) Epoch 42/300 -- Iteration 321249 - Batch 5467/7702 - Train loss: 0.00192730  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.50 min) Epoch 42/300 -- Iteration 321326 - Batch 5544/7702 - Train loss: 0.00192717  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.52 min) Epoch 42/300 -- Iteration 321403 - Batch 5621/7702 - Train loss: 0.00192674  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.55 min) Epoch 42/300 -- Iteration 321480 - Batch 5698/7702 - Train loss: 0.00192724  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.57 min) Epoch 42/300 -- Iteration 321557 - Batch 5775/7702 - Train loss: 0.00192683  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.59 min) Epoch 42/300 -- Iteration 321634 - Batch 5852/7702 - Train loss: 0.00192700  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.61 min) Epoch 42/300 -- Iteration 321711 - Batch 5929/7702 - Train loss: 0.00192709  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.63 min) Epoch 42/300 -- Iteration 321788 - Batch 6006/7702 - Train loss: 0.00192752  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.65 min) Epoch 42/300 -- Iteration 321865 - Batch 6083/7702 - Train loss: 0.00192790  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.67 min) Epoch 42/300 -- Iteration 321942 - Batch 6160/7702 - Train loss: 0.00192744  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.69 min) Epoch 42/300 -- Iteration 322019 - Batch 6237/7702 - Train loss: 0.00192724  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.71 min) Epoch 42/300 -- Iteration 322096 - Batch 6314/7702 - Train loss: 0.00192760  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.73 min) Epoch 42/300 -- Iteration 322173 - Batch 6391/7702 - Train loss: 0.00192729  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.75 min) Epoch 42/300 -- Iteration 322250 - Batch 6468/7702 - Train loss: 0.00192736  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.77 min) Epoch 42/300 -- Iteration 322327 - Batch 6545/7702 - Train loss: 0.00192735  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.79 min) Epoch 42/300 -- Iteration 322404 - Batch 6622/7702 - Train loss: 0.00192761  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.81 min) Epoch 42/300 -- Iteration 322481 - Batch 6699/7702 - Train loss: 0.00192792  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.84 min) Epoch 42/300 -- Iteration 322558 - Batch 6776/7702 - Train loss: 0.00192790  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.86 min) Epoch 42/300 -- Iteration 322635 - Batch 6853/7702 - Train loss: 0.00192833  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.88 min) Epoch 42/300 -- Iteration 322712 - Batch 6930/7702 - Train loss: 0.00192842  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.90 min) Epoch 42/300 -- Iteration 322789 - Batch 7007/7702 - Train loss: 0.00192876  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.92 min) Epoch 42/300 -- Iteration 322866 - Batch 7084/7702 - Train loss: 0.00192908  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.94 min) Epoch 42/300 -- Iteration 322943 - Batch 7161/7702 - Train loss: 0.00192908  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.96 min) Epoch 42/300 -- Iteration 323020 - Batch 7238/7702 - Train loss: 0.00192875  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(86.98 min) Epoch 42/300 -- Iteration 323097 - Batch 7315/7702 - Train loss: 0.00192869  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(87.00 min) Epoch 42/300 -- Iteration 323174 - Batch 7392/7702 - Train loss: 0.00192892  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(87.02 min) Epoch 42/300 -- Iteration 323251 - Batch 7469/7702 - Train loss: 0.00192917  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(87.04 min) Epoch 42/300 -- Iteration 323328 - Batch 7546/7702 - Train loss: 0.00192893  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(87.06 min) Epoch 42/300 -- Iteration 323405 - Batch 7623/7702 - Train loss: 0.00192856  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(87.08 min) Epoch 42/300 -- Iteration 323482 - Batch 7700/7702 - Train loss: 0.00192852  - Train acc: -0.0000 - Val loss: 0.00042259\n",
      "(87.08 min) Epoch 42/300 -- Iteration 323484 - Batch 7701/7702 - Train loss: 0.00192854  - Train acc: -0.0000 - Val loss: 0.00048683 - Val acc: -0.0000\n",
      "(87.11 min) Epoch 43/300 -- Iteration 323561 - Batch 77/7702 - Train loss: 0.00190985  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.13 min) Epoch 43/300 -- Iteration 323638 - Batch 154/7702 - Train loss: 0.00193027  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.15 min) Epoch 43/300 -- Iteration 323715 - Batch 231/7702 - Train loss: 0.00192926  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.17 min) Epoch 43/300 -- Iteration 323792 - Batch 308/7702 - Train loss: 0.00191769  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.19 min) Epoch 43/300 -- Iteration 323869 - Batch 385/7702 - Train loss: 0.00192152  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.21 min) Epoch 43/300 -- Iteration 323946 - Batch 462/7702 - Train loss: 0.00192585  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.23 min) Epoch 43/300 -- Iteration 324023 - Batch 539/7702 - Train loss: 0.00192628  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.25 min) Epoch 43/300 -- Iteration 324100 - Batch 616/7702 - Train loss: 0.00192516  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.27 min) Epoch 43/300 -- Iteration 324177 - Batch 693/7702 - Train loss: 0.00193033  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.29 min) Epoch 43/300 -- Iteration 324254 - Batch 770/7702 - Train loss: 0.00193165  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.31 min) Epoch 43/300 -- Iteration 324331 - Batch 847/7702 - Train loss: 0.00193031  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.33 min) Epoch 43/300 -- Iteration 324408 - Batch 924/7702 - Train loss: 0.00193139  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.36 min) Epoch 43/300 -- Iteration 324485 - Batch 1001/7702 - Train loss: 0.00193173  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.38 min) Epoch 43/300 -- Iteration 324562 - Batch 1078/7702 - Train loss: 0.00193129  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.40 min) Epoch 43/300 -- Iteration 324639 - Batch 1155/7702 - Train loss: 0.00193214  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.42 min) Epoch 43/300 -- Iteration 324716 - Batch 1232/7702 - Train loss: 0.00193117  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.44 min) Epoch 43/300 -- Iteration 324793 - Batch 1309/7702 - Train loss: 0.00193103  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.46 min) Epoch 43/300 -- Iteration 324870 - Batch 1386/7702 - Train loss: 0.00192993  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.48 min) Epoch 43/300 -- Iteration 324947 - Batch 1463/7702 - Train loss: 0.00192945  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.50 min) Epoch 43/300 -- Iteration 325024 - Batch 1540/7702 - Train loss: 0.00192711  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.52 min) Epoch 43/300 -- Iteration 325101 - Batch 1617/7702 - Train loss: 0.00192732  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.54 min) Epoch 43/300 -- Iteration 325178 - Batch 1694/7702 - Train loss: 0.00192739  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.56 min) Epoch 43/300 -- Iteration 325255 - Batch 1771/7702 - Train loss: 0.00192727  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.58 min) Epoch 43/300 -- Iteration 325332 - Batch 1848/7702 - Train loss: 0.00192735  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.60 min) Epoch 43/300 -- Iteration 325409 - Batch 1925/7702 - Train loss: 0.00192749  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.62 min) Epoch 43/300 -- Iteration 325486 - Batch 2002/7702 - Train loss: 0.00192645  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.65 min) Epoch 43/300 -- Iteration 325563 - Batch 2079/7702 - Train loss: 0.00192564  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.67 min) Epoch 43/300 -- Iteration 325640 - Batch 2156/7702 - Train loss: 0.00192657  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.69 min) Epoch 43/300 -- Iteration 325717 - Batch 2233/7702 - Train loss: 0.00192664  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.71 min) Epoch 43/300 -- Iteration 325794 - Batch 2310/7702 - Train loss: 0.00192567  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.73 min) Epoch 43/300 -- Iteration 325871 - Batch 2387/7702 - Train loss: 0.00192585  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.75 min) Epoch 43/300 -- Iteration 325948 - Batch 2464/7702 - Train loss: 0.00192697  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.77 min) Epoch 43/300 -- Iteration 326025 - Batch 2541/7702 - Train loss: 0.00192679  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.79 min) Epoch 43/300 -- Iteration 326102 - Batch 2618/7702 - Train loss: 0.00192715  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.81 min) Epoch 43/300 -- Iteration 326179 - Batch 2695/7702 - Train loss: 0.00192665  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.83 min) Epoch 43/300 -- Iteration 326256 - Batch 2772/7702 - Train loss: 0.00192607  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.85 min) Epoch 43/300 -- Iteration 326333 - Batch 2849/7702 - Train loss: 0.00192606  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.87 min) Epoch 43/300 -- Iteration 326410 - Batch 2926/7702 - Train loss: 0.00192684  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.89 min) Epoch 43/300 -- Iteration 326487 - Batch 3003/7702 - Train loss: 0.00192719  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.92 min) Epoch 43/300 -- Iteration 326564 - Batch 3080/7702 - Train loss: 0.00192696  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.94 min) Epoch 43/300 -- Iteration 326641 - Batch 3157/7702 - Train loss: 0.00192720  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.96 min) Epoch 43/300 -- Iteration 326718 - Batch 3234/7702 - Train loss: 0.00192754  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(87.98 min) Epoch 43/300 -- Iteration 326795 - Batch 3311/7702 - Train loss: 0.00192715  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.00 min) Epoch 43/300 -- Iteration 326872 - Batch 3388/7702 - Train loss: 0.00192636  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.02 min) Epoch 43/300 -- Iteration 326949 - Batch 3465/7702 - Train loss: 0.00192695  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.04 min) Epoch 43/300 -- Iteration 327026 - Batch 3542/7702 - Train loss: 0.00192756  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.06 min) Epoch 43/300 -- Iteration 327103 - Batch 3619/7702 - Train loss: 0.00193220  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.08 min) Epoch 43/300 -- Iteration 327180 - Batch 3696/7702 - Train loss: 0.00193276  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.10 min) Epoch 43/300 -- Iteration 327257 - Batch 3773/7702 - Train loss: 0.00193294  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.12 min) Epoch 43/300 -- Iteration 327334 - Batch 3850/7702 - Train loss: 0.00193303  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.14 min) Epoch 43/300 -- Iteration 327411 - Batch 3927/7702 - Train loss: 0.00193283  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.16 min) Epoch 43/300 -- Iteration 327488 - Batch 4004/7702 - Train loss: 0.00193268  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.18 min) Epoch 43/300 -- Iteration 327565 - Batch 4081/7702 - Train loss: 0.00193189  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.20 min) Epoch 43/300 -- Iteration 327642 - Batch 4158/7702 - Train loss: 0.00193205  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.23 min) Epoch 43/300 -- Iteration 327719 - Batch 4235/7702 - Train loss: 0.00193142  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.25 min) Epoch 43/300 -- Iteration 327796 - Batch 4312/7702 - Train loss: 0.00193080  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.27 min) Epoch 43/300 -- Iteration 327873 - Batch 4389/7702 - Train loss: 0.00193043  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.29 min) Epoch 43/300 -- Iteration 327950 - Batch 4466/7702 - Train loss: 0.00193035  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.31 min) Epoch 43/300 -- Iteration 328027 - Batch 4543/7702 - Train loss: 0.00193002  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.33 min) Epoch 43/300 -- Iteration 328104 - Batch 4620/7702 - Train loss: 0.00192947  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.35 min) Epoch 43/300 -- Iteration 328181 - Batch 4697/7702 - Train loss: 0.00192968  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.37 min) Epoch 43/300 -- Iteration 328258 - Batch 4774/7702 - Train loss: 0.00193027  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.39 min) Epoch 43/300 -- Iteration 328335 - Batch 4851/7702 - Train loss: 0.00193013  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.41 min) Epoch 43/300 -- Iteration 328412 - Batch 4928/7702 - Train loss: 0.00192965  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.43 min) Epoch 43/300 -- Iteration 328489 - Batch 5005/7702 - Train loss: 0.00192984  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.45 min) Epoch 43/300 -- Iteration 328566 - Batch 5082/7702 - Train loss: 0.00192968  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.47 min) Epoch 43/300 -- Iteration 328643 - Batch 5159/7702 - Train loss: 0.00192937  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.49 min) Epoch 43/300 -- Iteration 328720 - Batch 5236/7702 - Train loss: 0.00192906  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.51 min) Epoch 43/300 -- Iteration 328797 - Batch 5313/7702 - Train loss: 0.00192941  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.54 min) Epoch 43/300 -- Iteration 328874 - Batch 5390/7702 - Train loss: 0.00192910  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.56 min) Epoch 43/300 -- Iteration 328951 - Batch 5467/7702 - Train loss: 0.00192957  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.58 min) Epoch 43/300 -- Iteration 329028 - Batch 5544/7702 - Train loss: 0.00192953  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.60 min) Epoch 43/300 -- Iteration 329105 - Batch 5621/7702 - Train loss: 0.00192970  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.62 min) Epoch 43/300 -- Iteration 329182 - Batch 5698/7702 - Train loss: 0.00192961  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.64 min) Epoch 43/300 -- Iteration 329259 - Batch 5775/7702 - Train loss: 0.00192976  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.66 min) Epoch 43/300 -- Iteration 329336 - Batch 5852/7702 - Train loss: 0.00192979  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.68 min) Epoch 43/300 -- Iteration 329413 - Batch 5929/7702 - Train loss: 0.00192970  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.70 min) Epoch 43/300 -- Iteration 329490 - Batch 6006/7702 - Train loss: 0.00192973  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.72 min) Epoch 43/300 -- Iteration 329567 - Batch 6083/7702 - Train loss: 0.00192872  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.74 min) Epoch 43/300 -- Iteration 329644 - Batch 6160/7702 - Train loss: 0.00192860  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.76 min) Epoch 43/300 -- Iteration 329721 - Batch 6237/7702 - Train loss: 0.00192815  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.78 min) Epoch 43/300 -- Iteration 329798 - Batch 6314/7702 - Train loss: 0.00192836  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.80 min) Epoch 43/300 -- Iteration 329875 - Batch 6391/7702 - Train loss: 0.00192800  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.82 min) Epoch 43/300 -- Iteration 329952 - Batch 6468/7702 - Train loss: 0.00192769  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.85 min) Epoch 43/300 -- Iteration 330029 - Batch 6545/7702 - Train loss: 0.00192777  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.87 min) Epoch 43/300 -- Iteration 330106 - Batch 6622/7702 - Train loss: 0.00192823  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.89 min) Epoch 43/300 -- Iteration 330183 - Batch 6699/7702 - Train loss: 0.00192862  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.91 min) Epoch 43/300 -- Iteration 330260 - Batch 6776/7702 - Train loss: 0.00192818  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.93 min) Epoch 43/300 -- Iteration 330337 - Batch 6853/7702 - Train loss: 0.00192783  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.95 min) Epoch 43/300 -- Iteration 330414 - Batch 6930/7702 - Train loss: 0.00192805  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.97 min) Epoch 43/300 -- Iteration 330491 - Batch 7007/7702 - Train loss: 0.00192792  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(88.99 min) Epoch 43/300 -- Iteration 330568 - Batch 7084/7702 - Train loss: 0.00192829  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(89.01 min) Epoch 43/300 -- Iteration 330645 - Batch 7161/7702 - Train loss: 0.00192906  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(89.03 min) Epoch 43/300 -- Iteration 330722 - Batch 7238/7702 - Train loss: 0.00192903  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(89.05 min) Epoch 43/300 -- Iteration 330799 - Batch 7315/7702 - Train loss: 0.00192881  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(89.07 min) Epoch 43/300 -- Iteration 330876 - Batch 7392/7702 - Train loss: 0.00192858  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(89.09 min) Epoch 43/300 -- Iteration 330953 - Batch 7469/7702 - Train loss: 0.00192904  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(89.11 min) Epoch 43/300 -- Iteration 331030 - Batch 7546/7702 - Train loss: 0.00192928  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(89.14 min) Epoch 43/300 -- Iteration 331107 - Batch 7623/7702 - Train loss: 0.00192941  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(89.16 min) Epoch 43/300 -- Iteration 331184 - Batch 7700/7702 - Train loss: 0.00192961  - Train acc: -0.0000 - Val loss: 0.00048683\n",
      "(89.16 min) Epoch 43/300 -- Iteration 331186 - Batch 7701/7702 - Train loss: 0.00192958  - Train acc: -0.0000 - Val loss: 0.00050923 - Val acc: -0.0000\n",
      "(89.18 min) Epoch 44/300 -- Iteration 331263 - Batch 77/7702 - Train loss: 0.00196615  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.20 min) Epoch 44/300 -- Iteration 331340 - Batch 154/7702 - Train loss: 0.00195397  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.22 min) Epoch 44/300 -- Iteration 331417 - Batch 231/7702 - Train loss: 0.00195282  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.24 min) Epoch 44/300 -- Iteration 331494 - Batch 308/7702 - Train loss: 0.00194332  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.26 min) Epoch 44/300 -- Iteration 331571 - Batch 385/7702 - Train loss: 0.00193456  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.28 min) Epoch 44/300 -- Iteration 331648 - Batch 462/7702 - Train loss: 0.00193187  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.30 min) Epoch 44/300 -- Iteration 331725 - Batch 539/7702 - Train loss: 0.00192893  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.32 min) Epoch 44/300 -- Iteration 331802 - Batch 616/7702 - Train loss: 0.00192717  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.35 min) Epoch 44/300 -- Iteration 331879 - Batch 693/7702 - Train loss: 0.00192940  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.37 min) Epoch 44/300 -- Iteration 331956 - Batch 770/7702 - Train loss: 0.00193461  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.39 min) Epoch 44/300 -- Iteration 332033 - Batch 847/7702 - Train loss: 0.00193776  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.41 min) Epoch 44/300 -- Iteration 332110 - Batch 924/7702 - Train loss: 0.00193888  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.43 min) Epoch 44/300 -- Iteration 332187 - Batch 1001/7702 - Train loss: 0.00193869  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.45 min) Epoch 44/300 -- Iteration 332264 - Batch 1078/7702 - Train loss: 0.00193648  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.47 min) Epoch 44/300 -- Iteration 332341 - Batch 1155/7702 - Train loss: 0.00193635  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.49 min) Epoch 44/300 -- Iteration 332418 - Batch 1232/7702 - Train loss: 0.00193410  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.51 min) Epoch 44/300 -- Iteration 332495 - Batch 1309/7702 - Train loss: 0.00193400  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.53 min) Epoch 44/300 -- Iteration 332572 - Batch 1386/7702 - Train loss: 0.00193206  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.55 min) Epoch 44/300 -- Iteration 332649 - Batch 1463/7702 - Train loss: 0.00193102  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.57 min) Epoch 44/300 -- Iteration 332726 - Batch 1540/7702 - Train loss: 0.00193011  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.59 min) Epoch 44/300 -- Iteration 332803 - Batch 1617/7702 - Train loss: 0.00193007  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.61 min) Epoch 44/300 -- Iteration 332880 - Batch 1694/7702 - Train loss: 0.00192900  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.64 min) Epoch 44/300 -- Iteration 332957 - Batch 1771/7702 - Train loss: 0.00192865  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.66 min) Epoch 44/300 -- Iteration 333034 - Batch 1848/7702 - Train loss: 0.00192830  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.68 min) Epoch 44/300 -- Iteration 333111 - Batch 1925/7702 - Train loss: 0.00192734  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.70 min) Epoch 44/300 -- Iteration 333188 - Batch 2002/7702 - Train loss: 0.00192753  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.72 min) Epoch 44/300 -- Iteration 333265 - Batch 2079/7702 - Train loss: 0.00192697  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.74 min) Epoch 44/300 -- Iteration 333342 - Batch 2156/7702 - Train loss: 0.00192696  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.76 min) Epoch 44/300 -- Iteration 333419 - Batch 2233/7702 - Train loss: 0.00192664  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.78 min) Epoch 44/300 -- Iteration 333496 - Batch 2310/7702 - Train loss: 0.00192598  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.80 min) Epoch 44/300 -- Iteration 333573 - Batch 2387/7702 - Train loss: 0.00192623  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.82 min) Epoch 44/300 -- Iteration 333650 - Batch 2464/7702 - Train loss: 0.00192559  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.84 min) Epoch 44/300 -- Iteration 333727 - Batch 2541/7702 - Train loss: 0.00192475  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.86 min) Epoch 44/300 -- Iteration 333804 - Batch 2618/7702 - Train loss: 0.00192513  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.88 min) Epoch 44/300 -- Iteration 333881 - Batch 2695/7702 - Train loss: 0.00192419  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.91 min) Epoch 44/300 -- Iteration 333958 - Batch 2772/7702 - Train loss: 0.00192347  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.93 min) Epoch 44/300 -- Iteration 334035 - Batch 2849/7702 - Train loss: 0.00192273  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.95 min) Epoch 44/300 -- Iteration 334112 - Batch 2926/7702 - Train loss: 0.00192240  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.97 min) Epoch 44/300 -- Iteration 334189 - Batch 3003/7702 - Train loss: 0.00192286  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(89.99 min) Epoch 44/300 -- Iteration 334266 - Batch 3080/7702 - Train loss: 0.00192247  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.01 min) Epoch 44/300 -- Iteration 334343 - Batch 3157/7702 - Train loss: 0.00192226  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.03 min) Epoch 44/300 -- Iteration 334420 - Batch 3234/7702 - Train loss: 0.00192221  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.05 min) Epoch 44/300 -- Iteration 334497 - Batch 3311/7702 - Train loss: 0.00192155  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.07 min) Epoch 44/300 -- Iteration 334574 - Batch 3388/7702 - Train loss: 0.00192160  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.09 min) Epoch 44/300 -- Iteration 334651 - Batch 3465/7702 - Train loss: 0.00192129  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.11 min) Epoch 44/300 -- Iteration 334728 - Batch 3542/7702 - Train loss: 0.00192091  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.13 min) Epoch 44/300 -- Iteration 334805 - Batch 3619/7702 - Train loss: 0.00192121  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.15 min) Epoch 44/300 -- Iteration 334882 - Batch 3696/7702 - Train loss: 0.00192090  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.17 min) Epoch 44/300 -- Iteration 334959 - Batch 3773/7702 - Train loss: 0.00192157  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.20 min) Epoch 44/300 -- Iteration 335036 - Batch 3850/7702 - Train loss: 0.00192175  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.22 min) Epoch 44/300 -- Iteration 335113 - Batch 3927/7702 - Train loss: 0.00192138  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.24 min) Epoch 44/300 -- Iteration 335190 - Batch 4004/7702 - Train loss: 0.00192191  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.26 min) Epoch 44/300 -- Iteration 335267 - Batch 4081/7702 - Train loss: 0.00192224  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.28 min) Epoch 44/300 -- Iteration 335344 - Batch 4158/7702 - Train loss: 0.00192199  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.30 min) Epoch 44/300 -- Iteration 335421 - Batch 4235/7702 - Train loss: 0.00192249  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.32 min) Epoch 44/300 -- Iteration 335498 - Batch 4312/7702 - Train loss: 0.00192257  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.34 min) Epoch 44/300 -- Iteration 335575 - Batch 4389/7702 - Train loss: 0.00192283  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.36 min) Epoch 44/300 -- Iteration 335652 - Batch 4466/7702 - Train loss: 0.00192254  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.38 min) Epoch 44/300 -- Iteration 335729 - Batch 4543/7702 - Train loss: 0.00192211  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.40 min) Epoch 44/300 -- Iteration 335806 - Batch 4620/7702 - Train loss: 0.00192253  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.42 min) Epoch 44/300 -- Iteration 335883 - Batch 4697/7702 - Train loss: 0.00192283  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.45 min) Epoch 44/300 -- Iteration 335960 - Batch 4774/7702 - Train loss: 0.00192310  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.47 min) Epoch 44/300 -- Iteration 336037 - Batch 4851/7702 - Train loss: 0.00192312  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.49 min) Epoch 44/300 -- Iteration 336114 - Batch 4928/7702 - Train loss: 0.00192269  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.51 min) Epoch 44/300 -- Iteration 336191 - Batch 5005/7702 - Train loss: 0.00192299  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.53 min) Epoch 44/300 -- Iteration 336268 - Batch 5082/7702 - Train loss: 0.00192281  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.55 min) Epoch 44/300 -- Iteration 336345 - Batch 5159/7702 - Train loss: 0.00192260  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.57 min) Epoch 44/300 -- Iteration 336422 - Batch 5236/7702 - Train loss: 0.00192308  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.59 min) Epoch 44/300 -- Iteration 336499 - Batch 5313/7702 - Train loss: 0.00192337  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.61 min) Epoch 44/300 -- Iteration 336576 - Batch 5390/7702 - Train loss: 0.00192358  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.63 min) Epoch 44/300 -- Iteration 336653 - Batch 5467/7702 - Train loss: 0.00192347  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.65 min) Epoch 44/300 -- Iteration 336730 - Batch 5544/7702 - Train loss: 0.00192355  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.67 min) Epoch 44/300 -- Iteration 336807 - Batch 5621/7702 - Train loss: 0.00192367  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.69 min) Epoch 44/300 -- Iteration 336884 - Batch 5698/7702 - Train loss: 0.00192290  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.71 min) Epoch 44/300 -- Iteration 336961 - Batch 5775/7702 - Train loss: 0.00192319  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.74 min) Epoch 44/300 -- Iteration 337038 - Batch 5852/7702 - Train loss: 0.00192294  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.76 min) Epoch 44/300 -- Iteration 337115 - Batch 5929/7702 - Train loss: 0.00192294  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.78 min) Epoch 44/300 -- Iteration 337192 - Batch 6006/7702 - Train loss: 0.00192320  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.80 min) Epoch 44/300 -- Iteration 337269 - Batch 6083/7702 - Train loss: 0.00192316  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.82 min) Epoch 44/300 -- Iteration 337346 - Batch 6160/7702 - Train loss: 0.00192295  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.84 min) Epoch 44/300 -- Iteration 337423 - Batch 6237/7702 - Train loss: 0.00192307  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.86 min) Epoch 44/300 -- Iteration 337500 - Batch 6314/7702 - Train loss: 0.00192304  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.88 min) Epoch 44/300 -- Iteration 337577 - Batch 6391/7702 - Train loss: 0.00192315  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.90 min) Epoch 44/300 -- Iteration 337654 - Batch 6468/7702 - Train loss: 0.00192320  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.92 min) Epoch 44/300 -- Iteration 337731 - Batch 6545/7702 - Train loss: 0.00192311  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.94 min) Epoch 44/300 -- Iteration 337808 - Batch 6622/7702 - Train loss: 0.00192300  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.96 min) Epoch 44/300 -- Iteration 337885 - Batch 6699/7702 - Train loss: 0.00192368  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(90.98 min) Epoch 44/300 -- Iteration 337962 - Batch 6776/7702 - Train loss: 0.00192408  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.00 min) Epoch 44/300 -- Iteration 338039 - Batch 6853/7702 - Train loss: 0.00192395  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.03 min) Epoch 44/300 -- Iteration 338116 - Batch 6930/7702 - Train loss: 0.00192388  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.05 min) Epoch 44/300 -- Iteration 338193 - Batch 7007/7702 - Train loss: 0.00192394  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.07 min) Epoch 44/300 -- Iteration 338270 - Batch 7084/7702 - Train loss: 0.00192413  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.09 min) Epoch 44/300 -- Iteration 338347 - Batch 7161/7702 - Train loss: 0.00192376  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.11 min) Epoch 44/300 -- Iteration 338424 - Batch 7238/7702 - Train loss: 0.00192350  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.13 min) Epoch 44/300 -- Iteration 338501 - Batch 7315/7702 - Train loss: 0.00192335  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.15 min) Epoch 44/300 -- Iteration 338578 - Batch 7392/7702 - Train loss: 0.00192360  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.17 min) Epoch 44/300 -- Iteration 338655 - Batch 7469/7702 - Train loss: 0.00192327  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.19 min) Epoch 44/300 -- Iteration 338732 - Batch 7546/7702 - Train loss: 0.00192350  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.21 min) Epoch 44/300 -- Iteration 338809 - Batch 7623/7702 - Train loss: 0.00192377  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.23 min) Epoch 44/300 -- Iteration 338886 - Batch 7700/7702 - Train loss: 0.00192364  - Train acc: -0.0000 - Val loss: 0.00050923\n",
      "(91.23 min) Epoch 44/300 -- Iteration 338888 - Batch 7701/7702 - Train loss: 0.00192368  - Train acc: -0.0000 - Val loss: 0.00042050 - Val acc: -0.0000\n",
      "(91.26 min) Epoch 45/300 -- Iteration 338965 - Batch 77/7702 - Train loss: 0.00194068  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.28 min) Epoch 45/300 -- Iteration 339042 - Batch 154/7702 - Train loss: 0.00193027  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.30 min) Epoch 45/300 -- Iteration 339119 - Batch 231/7702 - Train loss: 0.00191753  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.32 min) Epoch 45/300 -- Iteration 339196 - Batch 308/7702 - Train loss: 0.00192685  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.34 min) Epoch 45/300 -- Iteration 339273 - Batch 385/7702 - Train loss: 0.00193128  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.36 min) Epoch 45/300 -- Iteration 339350 - Batch 462/7702 - Train loss: 0.00192859  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.38 min) Epoch 45/300 -- Iteration 339427 - Batch 539/7702 - Train loss: 0.00193004  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.40 min) Epoch 45/300 -- Iteration 339504 - Batch 616/7702 - Train loss: 0.00192523  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.42 min) Epoch 45/300 -- Iteration 339581 - Batch 693/7702 - Train loss: 0.00192653  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.44 min) Epoch 45/300 -- Iteration 339658 - Batch 770/7702 - Train loss: 0.00192526  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.46 min) Epoch 45/300 -- Iteration 339735 - Batch 847/7702 - Train loss: 0.00192229  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.49 min) Epoch 45/300 -- Iteration 339812 - Batch 924/7702 - Train loss: 0.00191922  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.51 min) Epoch 45/300 -- Iteration 339889 - Batch 1001/7702 - Train loss: 0.00191891  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.53 min) Epoch 45/300 -- Iteration 339966 - Batch 1078/7702 - Train loss: 0.00192150  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.55 min) Epoch 45/300 -- Iteration 340043 - Batch 1155/7702 - Train loss: 0.00192062  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.57 min) Epoch 45/300 -- Iteration 340120 - Batch 1232/7702 - Train loss: 0.00192133  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.59 min) Epoch 45/300 -- Iteration 340197 - Batch 1309/7702 - Train loss: 0.00192075  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.61 min) Epoch 45/300 -- Iteration 340274 - Batch 1386/7702 - Train loss: 0.00192186  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.63 min) Epoch 45/300 -- Iteration 340351 - Batch 1463/7702 - Train loss: 0.00192304  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.65 min) Epoch 45/300 -- Iteration 340428 - Batch 1540/7702 - Train loss: 0.00192419  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.67 min) Epoch 45/300 -- Iteration 340505 - Batch 1617/7702 - Train loss: 0.00192516  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.69 min) Epoch 45/300 -- Iteration 340582 - Batch 1694/7702 - Train loss: 0.00192638  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.71 min) Epoch 45/300 -- Iteration 340659 - Batch 1771/7702 - Train loss: 0.00192562  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.73 min) Epoch 45/300 -- Iteration 340736 - Batch 1848/7702 - Train loss: 0.00192616  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.76 min) Epoch 45/300 -- Iteration 340813 - Batch 1925/7702 - Train loss: 0.00192676  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.78 min) Epoch 45/300 -- Iteration 340890 - Batch 2002/7702 - Train loss: 0.00192843  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.80 min) Epoch 45/300 -- Iteration 340967 - Batch 2079/7702 - Train loss: 0.00192999  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.82 min) Epoch 45/300 -- Iteration 341044 - Batch 2156/7702 - Train loss: 0.00192863  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.84 min) Epoch 45/300 -- Iteration 341121 - Batch 2233/7702 - Train loss: 0.00192796  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.86 min) Epoch 45/300 -- Iteration 341198 - Batch 2310/7702 - Train loss: 0.00192654  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.88 min) Epoch 45/300 -- Iteration 341275 - Batch 2387/7702 - Train loss: 0.00192768  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.90 min) Epoch 45/300 -- Iteration 341352 - Batch 2464/7702 - Train loss: 0.00192754  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.92 min) Epoch 45/300 -- Iteration 341429 - Batch 2541/7702 - Train loss: 0.00192524  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.94 min) Epoch 45/300 -- Iteration 341506 - Batch 2618/7702 - Train loss: 0.00192529  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.96 min) Epoch 45/300 -- Iteration 341583 - Batch 2695/7702 - Train loss: 0.00192517  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(91.98 min) Epoch 45/300 -- Iteration 341660 - Batch 2772/7702 - Train loss: 0.00192344  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.00 min) Epoch 45/300 -- Iteration 341737 - Batch 2849/7702 - Train loss: 0.00192357  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.02 min) Epoch 45/300 -- Iteration 341814 - Batch 2926/7702 - Train loss: 0.00192366  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.05 min) Epoch 45/300 -- Iteration 341891 - Batch 3003/7702 - Train loss: 0.00192417  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.07 min) Epoch 45/300 -- Iteration 341968 - Batch 3080/7702 - Train loss: 0.00192449  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.09 min) Epoch 45/300 -- Iteration 342045 - Batch 3157/7702 - Train loss: 0.00192308  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.11 min) Epoch 45/300 -- Iteration 342122 - Batch 3234/7702 - Train loss: 0.00192260  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.13 min) Epoch 45/300 -- Iteration 342199 - Batch 3311/7702 - Train loss: 0.00192259  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.15 min) Epoch 45/300 -- Iteration 342276 - Batch 3388/7702 - Train loss: 0.00192263  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.17 min) Epoch 45/300 -- Iteration 342353 - Batch 3465/7702 - Train loss: 0.00192287  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.19 min) Epoch 45/300 -- Iteration 342430 - Batch 3542/7702 - Train loss: 0.00192288  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.21 min) Epoch 45/300 -- Iteration 342507 - Batch 3619/7702 - Train loss: 0.00192184  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.23 min) Epoch 45/300 -- Iteration 342584 - Batch 3696/7702 - Train loss: 0.00192180  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.25 min) Epoch 45/300 -- Iteration 342661 - Batch 3773/7702 - Train loss: 0.00192201  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.27 min) Epoch 45/300 -- Iteration 342738 - Batch 3850/7702 - Train loss: 0.00192278  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.29 min) Epoch 45/300 -- Iteration 342815 - Batch 3927/7702 - Train loss: 0.00192266  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.31 min) Epoch 45/300 -- Iteration 342892 - Batch 4004/7702 - Train loss: 0.00192241  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.34 min) Epoch 45/300 -- Iteration 342969 - Batch 4081/7702 - Train loss: 0.00192174  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.36 min) Epoch 45/300 -- Iteration 343046 - Batch 4158/7702 - Train loss: 0.00192294  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.38 min) Epoch 45/300 -- Iteration 343123 - Batch 4235/7702 - Train loss: 0.00192288  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.40 min) Epoch 45/300 -- Iteration 343200 - Batch 4312/7702 - Train loss: 0.00192370  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.42 min) Epoch 45/300 -- Iteration 343277 - Batch 4389/7702 - Train loss: 0.00192423  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.44 min) Epoch 45/300 -- Iteration 343354 - Batch 4466/7702 - Train loss: 0.00192471  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.46 min) Epoch 45/300 -- Iteration 343431 - Batch 4543/7702 - Train loss: 0.00192481  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.48 min) Epoch 45/300 -- Iteration 343508 - Batch 4620/7702 - Train loss: 0.00192449  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.50 min) Epoch 45/300 -- Iteration 343585 - Batch 4697/7702 - Train loss: 0.00192407  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.52 min) Epoch 45/300 -- Iteration 343662 - Batch 4774/7702 - Train loss: 0.00192472  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.54 min) Epoch 45/300 -- Iteration 343739 - Batch 4851/7702 - Train loss: 0.00192422  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.56 min) Epoch 45/300 -- Iteration 343816 - Batch 4928/7702 - Train loss: 0.00192459  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.58 min) Epoch 45/300 -- Iteration 343893 - Batch 5005/7702 - Train loss: 0.00192458  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.61 min) Epoch 45/300 -- Iteration 343970 - Batch 5082/7702 - Train loss: 0.00192401  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.63 min) Epoch 45/300 -- Iteration 344047 - Batch 5159/7702 - Train loss: 0.00192442  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.65 min) Epoch 45/300 -- Iteration 344124 - Batch 5236/7702 - Train loss: 0.00192398  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.67 min) Epoch 45/300 -- Iteration 344201 - Batch 5313/7702 - Train loss: 0.00192429  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.69 min) Epoch 45/300 -- Iteration 344278 - Batch 5390/7702 - Train loss: 0.00192402  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.71 min) Epoch 45/300 -- Iteration 344355 - Batch 5467/7702 - Train loss: 0.00192415  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.73 min) Epoch 45/300 -- Iteration 344432 - Batch 5544/7702 - Train loss: 0.00192393  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.75 min) Epoch 45/300 -- Iteration 344509 - Batch 5621/7702 - Train loss: 0.00192379  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.77 min) Epoch 45/300 -- Iteration 344586 - Batch 5698/7702 - Train loss: 0.00192378  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.79 min) Epoch 45/300 -- Iteration 344663 - Batch 5775/7702 - Train loss: 0.00192342  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.81 min) Epoch 45/300 -- Iteration 344740 - Batch 5852/7702 - Train loss: 0.00192306  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.83 min) Epoch 45/300 -- Iteration 344817 - Batch 5929/7702 - Train loss: 0.00192318  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.85 min) Epoch 45/300 -- Iteration 344894 - Batch 6006/7702 - Train loss: 0.00192326  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.88 min) Epoch 45/300 -- Iteration 344971 - Batch 6083/7702 - Train loss: 0.00192332  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.90 min) Epoch 45/300 -- Iteration 345048 - Batch 6160/7702 - Train loss: 0.00192283  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.92 min) Epoch 45/300 -- Iteration 345125 - Batch 6237/7702 - Train loss: 0.00192295  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.94 min) Epoch 45/300 -- Iteration 345202 - Batch 6314/7702 - Train loss: 0.00192247  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.96 min) Epoch 45/300 -- Iteration 345279 - Batch 6391/7702 - Train loss: 0.00192239  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(92.98 min) Epoch 45/300 -- Iteration 345356 - Batch 6468/7702 - Train loss: 0.00192259  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.00 min) Epoch 45/300 -- Iteration 345433 - Batch 6545/7702 - Train loss: 0.00192242  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.02 min) Epoch 45/300 -- Iteration 345510 - Batch 6622/7702 - Train loss: 0.00192259  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.04 min) Epoch 45/300 -- Iteration 345587 - Batch 6699/7702 - Train loss: 0.00192229  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.06 min) Epoch 45/300 -- Iteration 345664 - Batch 6776/7702 - Train loss: 0.00192256  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.08 min) Epoch 45/300 -- Iteration 345741 - Batch 6853/7702 - Train loss: 0.00192257  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.10 min) Epoch 45/300 -- Iteration 345818 - Batch 6930/7702 - Train loss: 0.00192220  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.12 min) Epoch 45/300 -- Iteration 345895 - Batch 7007/7702 - Train loss: 0.00192247  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.14 min) Epoch 45/300 -- Iteration 345972 - Batch 7084/7702 - Train loss: 0.00192226  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.17 min) Epoch 45/300 -- Iteration 346049 - Batch 7161/7702 - Train loss: 0.00192262  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.19 min) Epoch 45/300 -- Iteration 346126 - Batch 7238/7702 - Train loss: 0.00192267  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.21 min) Epoch 45/300 -- Iteration 346203 - Batch 7315/7702 - Train loss: 0.00192226  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.23 min) Epoch 45/300 -- Iteration 346280 - Batch 7392/7702 - Train loss: 0.00192252  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.25 min) Epoch 45/300 -- Iteration 346357 - Batch 7469/7702 - Train loss: 0.00192249  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.27 min) Epoch 45/300 -- Iteration 346434 - Batch 7546/7702 - Train loss: 0.00192290  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.29 min) Epoch 45/300 -- Iteration 346511 - Batch 7623/7702 - Train loss: 0.00192260  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.31 min) Epoch 45/300 -- Iteration 346588 - Batch 7700/7702 - Train loss: 0.00192254  - Train acc: -0.0000 - Val loss: 0.00042050\n",
      "(93.31 min) Epoch 45/300 -- Iteration 346590 - Batch 7701/7702 - Train loss: 0.00192253  - Train acc: -0.0000 - Val loss: 0.00050845 - Val acc: -0.0000\n",
      "(93.33 min) Epoch 46/300 -- Iteration 346667 - Batch 77/7702 - Train loss: 0.00193629  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.35 min) Epoch 46/300 -- Iteration 346744 - Batch 154/7702 - Train loss: 0.00192373  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.38 min) Epoch 46/300 -- Iteration 346821 - Batch 231/7702 - Train loss: 0.00193110  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.40 min) Epoch 46/300 -- Iteration 346898 - Batch 308/7702 - Train loss: 0.00193546  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.42 min) Epoch 46/300 -- Iteration 346975 - Batch 385/7702 - Train loss: 0.00193425  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.44 min) Epoch 46/300 -- Iteration 347052 - Batch 462/7702 - Train loss: 0.00192474  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.46 min) Epoch 46/300 -- Iteration 347129 - Batch 539/7702 - Train loss: 0.00192924  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.48 min) Epoch 46/300 -- Iteration 347206 - Batch 616/7702 - Train loss: 0.00192786  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.50 min) Epoch 46/300 -- Iteration 347283 - Batch 693/7702 - Train loss: 0.00192336  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.52 min) Epoch 46/300 -- Iteration 347360 - Batch 770/7702 - Train loss: 0.00192673  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.54 min) Epoch 46/300 -- Iteration 347437 - Batch 847/7702 - Train loss: 0.00192783  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.56 min) Epoch 46/300 -- Iteration 347514 - Batch 924/7702 - Train loss: 0.00192860  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.58 min) Epoch 46/300 -- Iteration 347591 - Batch 1001/7702 - Train loss: 0.00192562  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.60 min) Epoch 46/300 -- Iteration 347668 - Batch 1078/7702 - Train loss: 0.00192667  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.62 min) Epoch 46/300 -- Iteration 347745 - Batch 1155/7702 - Train loss: 0.00192750  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.64 min) Epoch 46/300 -- Iteration 347822 - Batch 1232/7702 - Train loss: 0.00192613  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.67 min) Epoch 46/300 -- Iteration 347899 - Batch 1309/7702 - Train loss: 0.00192788  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.69 min) Epoch 46/300 -- Iteration 347976 - Batch 1386/7702 - Train loss: 0.00192988  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.71 min) Epoch 46/300 -- Iteration 348053 - Batch 1463/7702 - Train loss: 0.00192865  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.73 min) Epoch 46/300 -- Iteration 348130 - Batch 1540/7702 - Train loss: 0.00192766  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.75 min) Epoch 46/300 -- Iteration 348207 - Batch 1617/7702 - Train loss: 0.00193003  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.77 min) Epoch 46/300 -- Iteration 348284 - Batch 1694/7702 - Train loss: 0.00193005  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.79 min) Epoch 46/300 -- Iteration 348361 - Batch 1771/7702 - Train loss: 0.00193599  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.81 min) Epoch 46/300 -- Iteration 348438 - Batch 1848/7702 - Train loss: 0.00193661  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.83 min) Epoch 46/300 -- Iteration 348515 - Batch 1925/7702 - Train loss: 0.00193493  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.85 min) Epoch 46/300 -- Iteration 348592 - Batch 2002/7702 - Train loss: 0.00193357  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.87 min) Epoch 46/300 -- Iteration 348669 - Batch 2079/7702 - Train loss: 0.00193282  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.89 min) Epoch 46/300 -- Iteration 348746 - Batch 2156/7702 - Train loss: 0.00193174  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.91 min) Epoch 46/300 -- Iteration 348823 - Batch 2233/7702 - Train loss: 0.00193190  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.93 min) Epoch 46/300 -- Iteration 348900 - Batch 2310/7702 - Train loss: 0.00193213  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.96 min) Epoch 46/300 -- Iteration 348977 - Batch 2387/7702 - Train loss: 0.00193209  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(93.98 min) Epoch 46/300 -- Iteration 349054 - Batch 2464/7702 - Train loss: 0.00193153  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.00 min) Epoch 46/300 -- Iteration 349131 - Batch 2541/7702 - Train loss: 0.00193142  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.02 min) Epoch 46/300 -- Iteration 349208 - Batch 2618/7702 - Train loss: 0.00193178  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.04 min) Epoch 46/300 -- Iteration 349285 - Batch 2695/7702 - Train loss: 0.00193202  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.06 min) Epoch 46/300 -- Iteration 349362 - Batch 2772/7702 - Train loss: 0.00193174  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.08 min) Epoch 46/300 -- Iteration 349439 - Batch 2849/7702 - Train loss: 0.00193164  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.10 min) Epoch 46/300 -- Iteration 349516 - Batch 2926/7702 - Train loss: 0.00193079  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.12 min) Epoch 46/300 -- Iteration 349593 - Batch 3003/7702 - Train loss: 0.00193124  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.14 min) Epoch 46/300 -- Iteration 349670 - Batch 3080/7702 - Train loss: 0.00193103  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.16 min) Epoch 46/300 -- Iteration 349747 - Batch 3157/7702 - Train loss: 0.00193043  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.18 min) Epoch 46/300 -- Iteration 349824 - Batch 3234/7702 - Train loss: 0.00193080  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.20 min) Epoch 46/300 -- Iteration 349901 - Batch 3311/7702 - Train loss: 0.00193045  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.22 min) Epoch 46/300 -- Iteration 349978 - Batch 3388/7702 - Train loss: 0.00193005  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.25 min) Epoch 46/300 -- Iteration 350055 - Batch 3465/7702 - Train loss: 0.00193015  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.27 min) Epoch 46/300 -- Iteration 350132 - Batch 3542/7702 - Train loss: 0.00192975  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.29 min) Epoch 46/300 -- Iteration 350209 - Batch 3619/7702 - Train loss: 0.00192943  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.31 min) Epoch 46/300 -- Iteration 350286 - Batch 3696/7702 - Train loss: 0.00192871  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.33 min) Epoch 46/300 -- Iteration 350363 - Batch 3773/7702 - Train loss: 0.00192825  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.35 min) Epoch 46/300 -- Iteration 350440 - Batch 3850/7702 - Train loss: 0.00192890  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.37 min) Epoch 46/300 -- Iteration 350517 - Batch 3927/7702 - Train loss: 0.00192841  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.39 min) Epoch 46/300 -- Iteration 350594 - Batch 4004/7702 - Train loss: 0.00192860  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.41 min) Epoch 46/300 -- Iteration 350671 - Batch 4081/7702 - Train loss: 0.00192862  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.43 min) Epoch 46/300 -- Iteration 350748 - Batch 4158/7702 - Train loss: 0.00192838  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.45 min) Epoch 46/300 -- Iteration 350825 - Batch 4235/7702 - Train loss: 0.00192789  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.47 min) Epoch 46/300 -- Iteration 350902 - Batch 4312/7702 - Train loss: 0.00192779  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.49 min) Epoch 46/300 -- Iteration 350979 - Batch 4389/7702 - Train loss: 0.00192821  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.52 min) Epoch 46/300 -- Iteration 351056 - Batch 4466/7702 - Train loss: 0.00192800  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.54 min) Epoch 46/300 -- Iteration 351133 - Batch 4543/7702 - Train loss: 0.00192887  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.56 min) Epoch 46/300 -- Iteration 351210 - Batch 4620/7702 - Train loss: 0.00192848  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.58 min) Epoch 46/300 -- Iteration 351287 - Batch 4697/7702 - Train loss: 0.00192899  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.60 min) Epoch 46/300 -- Iteration 351364 - Batch 4774/7702 - Train loss: 0.00192913  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.62 min) Epoch 46/300 -- Iteration 351441 - Batch 4851/7702 - Train loss: 0.00192946  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.64 min) Epoch 46/300 -- Iteration 351518 - Batch 4928/7702 - Train loss: 0.00192948  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.66 min) Epoch 46/300 -- Iteration 351595 - Batch 5005/7702 - Train loss: 0.00192930  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.68 min) Epoch 46/300 -- Iteration 351672 - Batch 5082/7702 - Train loss: 0.00192956  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.70 min) Epoch 46/300 -- Iteration 351749 - Batch 5159/7702 - Train loss: 0.00192898  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.72 min) Epoch 46/300 -- Iteration 351826 - Batch 5236/7702 - Train loss: 0.00192873  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.74 min) Epoch 46/300 -- Iteration 351903 - Batch 5313/7702 - Train loss: 0.00192830  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.76 min) Epoch 46/300 -- Iteration 351980 - Batch 5390/7702 - Train loss: 0.00192784  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.78 min) Epoch 46/300 -- Iteration 352057 - Batch 5467/7702 - Train loss: 0.00192727  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.81 min) Epoch 46/300 -- Iteration 352134 - Batch 5544/7702 - Train loss: 0.00192677  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.83 min) Epoch 46/300 -- Iteration 352211 - Batch 5621/7702 - Train loss: 0.00192684  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.85 min) Epoch 46/300 -- Iteration 352288 - Batch 5698/7702 - Train loss: 0.00192609  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.87 min) Epoch 46/300 -- Iteration 352365 - Batch 5775/7702 - Train loss: 0.00192627  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.89 min) Epoch 46/300 -- Iteration 352442 - Batch 5852/7702 - Train loss: 0.00192575  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.91 min) Epoch 46/300 -- Iteration 352519 - Batch 5929/7702 - Train loss: 0.00192590  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.93 min) Epoch 46/300 -- Iteration 352596 - Batch 6006/7702 - Train loss: 0.00192572  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.95 min) Epoch 46/300 -- Iteration 352673 - Batch 6083/7702 - Train loss: 0.00192511  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.97 min) Epoch 46/300 -- Iteration 352750 - Batch 6160/7702 - Train loss: 0.00192572  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(94.99 min) Epoch 46/300 -- Iteration 352827 - Batch 6237/7702 - Train loss: 0.00192589  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.01 min) Epoch 46/300 -- Iteration 352904 - Batch 6314/7702 - Train loss: 0.00192606  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.03 min) Epoch 46/300 -- Iteration 352981 - Batch 6391/7702 - Train loss: 0.00192582  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.05 min) Epoch 46/300 -- Iteration 353058 - Batch 6468/7702 - Train loss: 0.00192554  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.08 min) Epoch 46/300 -- Iteration 353135 - Batch 6545/7702 - Train loss: 0.00192579  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.10 min) Epoch 46/300 -- Iteration 353212 - Batch 6622/7702 - Train loss: 0.00192595  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.12 min) Epoch 46/300 -- Iteration 353289 - Batch 6699/7702 - Train loss: 0.00192617  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.14 min) Epoch 46/300 -- Iteration 353366 - Batch 6776/7702 - Train loss: 0.00192627  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.16 min) Epoch 46/300 -- Iteration 353443 - Batch 6853/7702 - Train loss: 0.00192611  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.18 min) Epoch 46/300 -- Iteration 353520 - Batch 6930/7702 - Train loss: 0.00192643  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.20 min) Epoch 46/300 -- Iteration 353597 - Batch 7007/7702 - Train loss: 0.00192613  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.22 min) Epoch 46/300 -- Iteration 353674 - Batch 7084/7702 - Train loss: 0.00192625  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.24 min) Epoch 46/300 -- Iteration 353751 - Batch 7161/7702 - Train loss: 0.00192607  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.26 min) Epoch 46/300 -- Iteration 353828 - Batch 7238/7702 - Train loss: 0.00192626  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.28 min) Epoch 46/300 -- Iteration 353905 - Batch 7315/7702 - Train loss: 0.00192604  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.30 min) Epoch 46/300 -- Iteration 353982 - Batch 7392/7702 - Train loss: 0.00192586  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.32 min) Epoch 46/300 -- Iteration 354059 - Batch 7469/7702 - Train loss: 0.00192577  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.34 min) Epoch 46/300 -- Iteration 354136 - Batch 7546/7702 - Train loss: 0.00192580  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.37 min) Epoch 46/300 -- Iteration 354213 - Batch 7623/7702 - Train loss: 0.00192565  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.39 min) Epoch 46/300 -- Iteration 354290 - Batch 7700/7702 - Train loss: 0.00192534  - Train acc: -0.0000 - Val loss: 0.00050845\n",
      "(95.39 min) Epoch 46/300 -- Iteration 354292 - Batch 7701/7702 - Train loss: 0.00192531  - Train acc: -0.0000 - Val loss: 0.00049196 - Val acc: -0.0000\n",
      "(95.41 min) Epoch 47/300 -- Iteration 354369 - Batch 77/7702 - Train loss: 0.00189913  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.43 min) Epoch 47/300 -- Iteration 354446 - Batch 154/7702 - Train loss: 0.00191273  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.45 min) Epoch 47/300 -- Iteration 354523 - Batch 231/7702 - Train loss: 0.00190891  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.47 min) Epoch 47/300 -- Iteration 354600 - Batch 308/7702 - Train loss: 0.00191374  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.49 min) Epoch 47/300 -- Iteration 354677 - Batch 385/7702 - Train loss: 0.00191420  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.51 min) Epoch 47/300 -- Iteration 354754 - Batch 462/7702 - Train loss: 0.00191396  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.53 min) Epoch 47/300 -- Iteration 354831 - Batch 539/7702 - Train loss: 0.00191491  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.55 min) Epoch 47/300 -- Iteration 354908 - Batch 616/7702 - Train loss: 0.00191587  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.58 min) Epoch 47/300 -- Iteration 354985 - Batch 693/7702 - Train loss: 0.00192065  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.60 min) Epoch 47/300 -- Iteration 355062 - Batch 770/7702 - Train loss: 0.00192409  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.62 min) Epoch 47/300 -- Iteration 355139 - Batch 847/7702 - Train loss: 0.00192534  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.64 min) Epoch 47/300 -- Iteration 355216 - Batch 924/7702 - Train loss: 0.00192466  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.66 min) Epoch 47/300 -- Iteration 355293 - Batch 1001/7702 - Train loss: 0.00192454  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.68 min) Epoch 47/300 -- Iteration 355370 - Batch 1078/7702 - Train loss: 0.00192265  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.70 min) Epoch 47/300 -- Iteration 355447 - Batch 1155/7702 - Train loss: 0.00192302  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.72 min) Epoch 47/300 -- Iteration 355524 - Batch 1232/7702 - Train loss: 0.00192204  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.74 min) Epoch 47/300 -- Iteration 355601 - Batch 1309/7702 - Train loss: 0.00192238  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.76 min) Epoch 47/300 -- Iteration 355678 - Batch 1386/7702 - Train loss: 0.00192279  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.78 min) Epoch 47/300 -- Iteration 355755 - Batch 1463/7702 - Train loss: 0.00192272  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.80 min) Epoch 47/300 -- Iteration 355832 - Batch 1540/7702 - Train loss: 0.00192366  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.82 min) Epoch 47/300 -- Iteration 355909 - Batch 1617/7702 - Train loss: 0.00192322  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.84 min) Epoch 47/300 -- Iteration 355986 - Batch 1694/7702 - Train loss: 0.00192210  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.87 min) Epoch 47/300 -- Iteration 356063 - Batch 1771/7702 - Train loss: 0.00192220  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.89 min) Epoch 47/300 -- Iteration 356140 - Batch 1848/7702 - Train loss: 0.00192301  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.91 min) Epoch 47/300 -- Iteration 356217 - Batch 1925/7702 - Train loss: 0.00192244  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.93 min) Epoch 47/300 -- Iteration 356294 - Batch 2002/7702 - Train loss: 0.00192221  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.95 min) Epoch 47/300 -- Iteration 356371 - Batch 2079/7702 - Train loss: 0.00192123  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.97 min) Epoch 47/300 -- Iteration 356448 - Batch 2156/7702 - Train loss: 0.00192171  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(95.99 min) Epoch 47/300 -- Iteration 356525 - Batch 2233/7702 - Train loss: 0.00192057  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.01 min) Epoch 47/300 -- Iteration 356602 - Batch 2310/7702 - Train loss: 0.00192038  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.03 min) Epoch 47/300 -- Iteration 356679 - Batch 2387/7702 - Train loss: 0.00191899  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.05 min) Epoch 47/300 -- Iteration 356756 - Batch 2464/7702 - Train loss: 0.00191897  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.07 min) Epoch 47/300 -- Iteration 356833 - Batch 2541/7702 - Train loss: 0.00191748  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.09 min) Epoch 47/300 -- Iteration 356910 - Batch 2618/7702 - Train loss: 0.00191619  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.11 min) Epoch 47/300 -- Iteration 356987 - Batch 2695/7702 - Train loss: 0.00191596  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.13 min) Epoch 47/300 -- Iteration 357064 - Batch 2772/7702 - Train loss: 0.00191597  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.16 min) Epoch 47/300 -- Iteration 357141 - Batch 2849/7702 - Train loss: 0.00191569  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.18 min) Epoch 47/300 -- Iteration 357218 - Batch 2926/7702 - Train loss: 0.00191549  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.20 min) Epoch 47/300 -- Iteration 357295 - Batch 3003/7702 - Train loss: 0.00191439  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.22 min) Epoch 47/300 -- Iteration 357372 - Batch 3080/7702 - Train loss: 0.00191408  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.24 min) Epoch 47/300 -- Iteration 357449 - Batch 3157/7702 - Train loss: 0.00191477  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.26 min) Epoch 47/300 -- Iteration 357526 - Batch 3234/7702 - Train loss: 0.00191545  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.28 min) Epoch 47/300 -- Iteration 357603 - Batch 3311/7702 - Train loss: 0.00191614  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.30 min) Epoch 47/300 -- Iteration 357680 - Batch 3388/7702 - Train loss: 0.00191574  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.32 min) Epoch 47/300 -- Iteration 357757 - Batch 3465/7702 - Train loss: 0.00191590  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.34 min) Epoch 47/300 -- Iteration 357834 - Batch 3542/7702 - Train loss: 0.00191685  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.36 min) Epoch 47/300 -- Iteration 357911 - Batch 3619/7702 - Train loss: 0.00191661  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.38 min) Epoch 47/300 -- Iteration 357988 - Batch 3696/7702 - Train loss: 0.00191664  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.40 min) Epoch 47/300 -- Iteration 358065 - Batch 3773/7702 - Train loss: 0.00191610  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.43 min) Epoch 47/300 -- Iteration 358142 - Batch 3850/7702 - Train loss: 0.00191626  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.45 min) Epoch 47/300 -- Iteration 358219 - Batch 3927/7702 - Train loss: 0.00191696  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.47 min) Epoch 47/300 -- Iteration 358296 - Batch 4004/7702 - Train loss: 0.00191732  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.49 min) Epoch 47/300 -- Iteration 358373 - Batch 4081/7702 - Train loss: 0.00191711  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.51 min) Epoch 47/300 -- Iteration 358450 - Batch 4158/7702 - Train loss: 0.00191782  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.53 min) Epoch 47/300 -- Iteration 358527 - Batch 4235/7702 - Train loss: 0.00191764  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.55 min) Epoch 47/300 -- Iteration 358604 - Batch 4312/7702 - Train loss: 0.00191784  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.57 min) Epoch 47/300 -- Iteration 358681 - Batch 4389/7702 - Train loss: 0.00191828  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.59 min) Epoch 47/300 -- Iteration 358758 - Batch 4466/7702 - Train loss: 0.00191810  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.61 min) Epoch 47/300 -- Iteration 358835 - Batch 4543/7702 - Train loss: 0.00191817  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.63 min) Epoch 47/300 -- Iteration 358912 - Batch 4620/7702 - Train loss: 0.00191844  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.65 min) Epoch 47/300 -- Iteration 358989 - Batch 4697/7702 - Train loss: 0.00191772  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.67 min) Epoch 47/300 -- Iteration 359066 - Batch 4774/7702 - Train loss: 0.00191735  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.69 min) Epoch 47/300 -- Iteration 359143 - Batch 4851/7702 - Train loss: 0.00191754  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.72 min) Epoch 47/300 -- Iteration 359220 - Batch 4928/7702 - Train loss: 0.00191777  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.74 min) Epoch 47/300 -- Iteration 359297 - Batch 5005/7702 - Train loss: 0.00191779  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.76 min) Epoch 47/300 -- Iteration 359374 - Batch 5082/7702 - Train loss: 0.00191783  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.78 min) Epoch 47/300 -- Iteration 359451 - Batch 5159/7702 - Train loss: 0.00191746  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.80 min) Epoch 47/300 -- Iteration 359528 - Batch 5236/7702 - Train loss: 0.00191747  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.82 min) Epoch 47/300 -- Iteration 359605 - Batch 5313/7702 - Train loss: 0.00191740  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.84 min) Epoch 47/300 -- Iteration 359682 - Batch 5390/7702 - Train loss: 0.00191741  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.86 min) Epoch 47/300 -- Iteration 359759 - Batch 5467/7702 - Train loss: 0.00191796  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.88 min) Epoch 47/300 -- Iteration 359836 - Batch 5544/7702 - Train loss: 0.00191824  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.90 min) Epoch 47/300 -- Iteration 359913 - Batch 5621/7702 - Train loss: 0.00191853  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.92 min) Epoch 47/300 -- Iteration 359990 - Batch 5698/7702 - Train loss: 0.00191852  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.94 min) Epoch 47/300 -- Iteration 360067 - Batch 5775/7702 - Train loss: 0.00191866  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.96 min) Epoch 47/300 -- Iteration 360144 - Batch 5852/7702 - Train loss: 0.00191846  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(96.98 min) Epoch 47/300 -- Iteration 360221 - Batch 5929/7702 - Train loss: 0.00191806  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.01 min) Epoch 47/300 -- Iteration 360298 - Batch 6006/7702 - Train loss: 0.00191791  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.03 min) Epoch 47/300 -- Iteration 360375 - Batch 6083/7702 - Train loss: 0.00191790  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.05 min) Epoch 47/300 -- Iteration 360452 - Batch 6160/7702 - Train loss: 0.00191776  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.07 min) Epoch 47/300 -- Iteration 360529 - Batch 6237/7702 - Train loss: 0.00191752  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.09 min) Epoch 47/300 -- Iteration 360606 - Batch 6314/7702 - Train loss: 0.00191817  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.11 min) Epoch 47/300 -- Iteration 360683 - Batch 6391/7702 - Train loss: 0.00191833  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.13 min) Epoch 47/300 -- Iteration 360760 - Batch 6468/7702 - Train loss: 0.00191849  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.15 min) Epoch 47/300 -- Iteration 360837 - Batch 6545/7702 - Train loss: 0.00191815  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.17 min) Epoch 47/300 -- Iteration 360914 - Batch 6622/7702 - Train loss: 0.00191808  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.19 min) Epoch 47/300 -- Iteration 360991 - Batch 6699/7702 - Train loss: 0.00191774  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.21 min) Epoch 47/300 -- Iteration 361068 - Batch 6776/7702 - Train loss: 0.00191754  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.23 min) Epoch 47/300 -- Iteration 361145 - Batch 6853/7702 - Train loss: 0.00191787  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.25 min) Epoch 47/300 -- Iteration 361222 - Batch 6930/7702 - Train loss: 0.00191751  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.27 min) Epoch 47/300 -- Iteration 361299 - Batch 7007/7702 - Train loss: 0.00191725  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.30 min) Epoch 47/300 -- Iteration 361376 - Batch 7084/7702 - Train loss: 0.00191738  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.32 min) Epoch 47/300 -- Iteration 361453 - Batch 7161/7702 - Train loss: 0.00191761  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.34 min) Epoch 47/300 -- Iteration 361530 - Batch 7238/7702 - Train loss: 0.00191796  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.36 min) Epoch 47/300 -- Iteration 361607 - Batch 7315/7702 - Train loss: 0.00191775  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.38 min) Epoch 47/300 -- Iteration 361684 - Batch 7392/7702 - Train loss: 0.00191854  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.40 min) Epoch 47/300 -- Iteration 361761 - Batch 7469/7702 - Train loss: 0.00191880  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.42 min) Epoch 47/300 -- Iteration 361838 - Batch 7546/7702 - Train loss: 0.00191847  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.44 min) Epoch 47/300 -- Iteration 361915 - Batch 7623/7702 - Train loss: 0.00191813  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.46 min) Epoch 47/300 -- Iteration 361992 - Batch 7700/7702 - Train loss: 0.00191863  - Train acc: -0.0000 - Val loss: 0.00049196\n",
      "(97.46 min) Epoch 47/300 -- Iteration 361994 - Batch 7701/7702 - Train loss: 0.00191868  - Train acc: -0.0000 - Val loss: 0.00041135 - Val acc: -0.0000\n",
      "(97.49 min) Epoch 48/300 -- Iteration 362071 - Batch 77/7702 - Train loss: 0.00195554  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.51 min) Epoch 48/300 -- Iteration 362148 - Batch 154/7702 - Train loss: 0.00194885  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.53 min) Epoch 48/300 -- Iteration 362225 - Batch 231/7702 - Train loss: 0.00193071  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.55 min) Epoch 48/300 -- Iteration 362302 - Batch 308/7702 - Train loss: 0.00193608  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.57 min) Epoch 48/300 -- Iteration 362379 - Batch 385/7702 - Train loss: 0.00193457  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.59 min) Epoch 48/300 -- Iteration 362456 - Batch 462/7702 - Train loss: 0.00193465  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.61 min) Epoch 48/300 -- Iteration 362533 - Batch 539/7702 - Train loss: 0.00193548  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.63 min) Epoch 48/300 -- Iteration 362610 - Batch 616/7702 - Train loss: 0.00193160  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.65 min) Epoch 48/300 -- Iteration 362687 - Batch 693/7702 - Train loss: 0.00193033  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.67 min) Epoch 48/300 -- Iteration 362764 - Batch 770/7702 - Train loss: 0.00192626  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.69 min) Epoch 48/300 -- Iteration 362841 - Batch 847/7702 - Train loss: 0.00192680  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.71 min) Epoch 48/300 -- Iteration 362918 - Batch 924/7702 - Train loss: 0.00192677  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.73 min) Epoch 48/300 -- Iteration 362995 - Batch 1001/7702 - Train loss: 0.00192767  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.76 min) Epoch 48/300 -- Iteration 363072 - Batch 1078/7702 - Train loss: 0.00192592  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.78 min) Epoch 48/300 -- Iteration 363149 - Batch 1155/7702 - Train loss: 0.00192383  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.80 min) Epoch 48/300 -- Iteration 363226 - Batch 1232/7702 - Train loss: 0.00192544  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.82 min) Epoch 48/300 -- Iteration 363303 - Batch 1309/7702 - Train loss: 0.00192372  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.84 min) Epoch 48/300 -- Iteration 363380 - Batch 1386/7702 - Train loss: 0.00192481  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.86 min) Epoch 48/300 -- Iteration 363457 - Batch 1463/7702 - Train loss: 0.00192467  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.88 min) Epoch 48/300 -- Iteration 363534 - Batch 1540/7702 - Train loss: 0.00192590  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.90 min) Epoch 48/300 -- Iteration 363611 - Batch 1617/7702 - Train loss: 0.00192597  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.92 min) Epoch 48/300 -- Iteration 363688 - Batch 1694/7702 - Train loss: 0.00192744  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.94 min) Epoch 48/300 -- Iteration 363765 - Batch 1771/7702 - Train loss: 0.00192847  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.96 min) Epoch 48/300 -- Iteration 363842 - Batch 1848/7702 - Train loss: 0.00192794  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(97.98 min) Epoch 48/300 -- Iteration 363919 - Batch 1925/7702 - Train loss: 0.00192698  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.00 min) Epoch 48/300 -- Iteration 363996 - Batch 2002/7702 - Train loss: 0.00192651  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.02 min) Epoch 48/300 -- Iteration 364073 - Batch 2079/7702 - Train loss: 0.00192690  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.05 min) Epoch 48/300 -- Iteration 364150 - Batch 2156/7702 - Train loss: 0.00192683  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.07 min) Epoch 48/300 -- Iteration 364227 - Batch 2233/7702 - Train loss: 0.00192725  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.09 min) Epoch 48/300 -- Iteration 364304 - Batch 2310/7702 - Train loss: 0.00192705  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.11 min) Epoch 48/300 -- Iteration 364381 - Batch 2387/7702 - Train loss: 0.00192874  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.13 min) Epoch 48/300 -- Iteration 364458 - Batch 2464/7702 - Train loss: 0.00192880  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.15 min) Epoch 48/300 -- Iteration 364535 - Batch 2541/7702 - Train loss: 0.00192760  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.17 min) Epoch 48/300 -- Iteration 364612 - Batch 2618/7702 - Train loss: 0.00192658  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.19 min) Epoch 48/300 -- Iteration 364689 - Batch 2695/7702 - Train loss: 0.00192621  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.21 min) Epoch 48/300 -- Iteration 364766 - Batch 2772/7702 - Train loss: 0.00192558  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.23 min) Epoch 48/300 -- Iteration 364843 - Batch 2849/7702 - Train loss: 0.00192654  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.25 min) Epoch 48/300 -- Iteration 364920 - Batch 2926/7702 - Train loss: 0.00192652  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.27 min) Epoch 48/300 -- Iteration 364997 - Batch 3003/7702 - Train loss: 0.00192676  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.29 min) Epoch 48/300 -- Iteration 365074 - Batch 3080/7702 - Train loss: 0.00192675  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.31 min) Epoch 48/300 -- Iteration 365151 - Batch 3157/7702 - Train loss: 0.00192682  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.34 min) Epoch 48/300 -- Iteration 365228 - Batch 3234/7702 - Train loss: 0.00192566  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.36 min) Epoch 48/300 -- Iteration 365305 - Batch 3311/7702 - Train loss: 0.00192511  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.38 min) Epoch 48/300 -- Iteration 365382 - Batch 3388/7702 - Train loss: 0.00192484  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.40 min) Epoch 48/300 -- Iteration 365459 - Batch 3465/7702 - Train loss: 0.00192540  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.42 min) Epoch 48/300 -- Iteration 365536 - Batch 3542/7702 - Train loss: 0.00192563  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.44 min) Epoch 48/300 -- Iteration 365613 - Batch 3619/7702 - Train loss: 0.00192586  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.46 min) Epoch 48/300 -- Iteration 365690 - Batch 3696/7702 - Train loss: 0.00192674  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.48 min) Epoch 48/300 -- Iteration 365767 - Batch 3773/7702 - Train loss: 0.00192682  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.50 min) Epoch 48/300 -- Iteration 365844 - Batch 3850/7702 - Train loss: 0.00192715  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.52 min) Epoch 48/300 -- Iteration 365921 - Batch 3927/7702 - Train loss: 0.00192692  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.54 min) Epoch 48/300 -- Iteration 365998 - Batch 4004/7702 - Train loss: 0.00192672  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.56 min) Epoch 48/300 -- Iteration 366075 - Batch 4081/7702 - Train loss: 0.00192617  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.58 min) Epoch 48/300 -- Iteration 366152 - Batch 4158/7702 - Train loss: 0.00192596  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.61 min) Epoch 48/300 -- Iteration 366229 - Batch 4235/7702 - Train loss: 0.00192580  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.63 min) Epoch 48/300 -- Iteration 366306 - Batch 4312/7702 - Train loss: 0.00192612  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.65 min) Epoch 48/300 -- Iteration 366383 - Batch 4389/7702 - Train loss: 0.00192599  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.67 min) Epoch 48/300 -- Iteration 366460 - Batch 4466/7702 - Train loss: 0.00192587  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.69 min) Epoch 48/300 -- Iteration 366537 - Batch 4543/7702 - Train loss: 0.00192521  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.71 min) Epoch 48/300 -- Iteration 366614 - Batch 4620/7702 - Train loss: 0.00192506  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.73 min) Epoch 48/300 -- Iteration 366691 - Batch 4697/7702 - Train loss: 0.00192472  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.75 min) Epoch 48/300 -- Iteration 366768 - Batch 4774/7702 - Train loss: 0.00192426  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.77 min) Epoch 48/300 -- Iteration 366845 - Batch 4851/7702 - Train loss: 0.00192407  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.79 min) Epoch 48/300 -- Iteration 366922 - Batch 4928/7702 - Train loss: 0.00192393  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.81 min) Epoch 48/300 -- Iteration 366999 - Batch 5005/7702 - Train loss: 0.00192422  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.83 min) Epoch 48/300 -- Iteration 367076 - Batch 5082/7702 - Train loss: 0.00192358  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.85 min) Epoch 48/300 -- Iteration 367153 - Batch 5159/7702 - Train loss: 0.00192366  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.87 min) Epoch 48/300 -- Iteration 367230 - Batch 5236/7702 - Train loss: 0.00192389  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.90 min) Epoch 48/300 -- Iteration 367307 - Batch 5313/7702 - Train loss: 0.00192387  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.92 min) Epoch 48/300 -- Iteration 367384 - Batch 5390/7702 - Train loss: 0.00192383  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.94 min) Epoch 48/300 -- Iteration 367461 - Batch 5467/7702 - Train loss: 0.00192343  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.96 min) Epoch 48/300 -- Iteration 367538 - Batch 5544/7702 - Train loss: 0.00192337  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(98.98 min) Epoch 48/300 -- Iteration 367615 - Batch 5621/7702 - Train loss: 0.00192298  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.00 min) Epoch 48/300 -- Iteration 367692 - Batch 5698/7702 - Train loss: 0.00192347  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.02 min) Epoch 48/300 -- Iteration 367769 - Batch 5775/7702 - Train loss: 0.00192336  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.04 min) Epoch 48/300 -- Iteration 367846 - Batch 5852/7702 - Train loss: 0.00192292  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.06 min) Epoch 48/300 -- Iteration 367923 - Batch 5929/7702 - Train loss: 0.00192272  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.08 min) Epoch 48/300 -- Iteration 368000 - Batch 6006/7702 - Train loss: 0.00192240  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.10 min) Epoch 48/300 -- Iteration 368077 - Batch 6083/7702 - Train loss: 0.00192293  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.12 min) Epoch 48/300 -- Iteration 368154 - Batch 6160/7702 - Train loss: 0.00192315  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.14 min) Epoch 48/300 -- Iteration 368231 - Batch 6237/7702 - Train loss: 0.00192285  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.16 min) Epoch 48/300 -- Iteration 368308 - Batch 6314/7702 - Train loss: 0.00192270  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.19 min) Epoch 48/300 -- Iteration 368385 - Batch 6391/7702 - Train loss: 0.00192331  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.21 min) Epoch 48/300 -- Iteration 368462 - Batch 6468/7702 - Train loss: 0.00192321  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.23 min) Epoch 48/300 -- Iteration 368539 - Batch 6545/7702 - Train loss: 0.00192314  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.25 min) Epoch 48/300 -- Iteration 368616 - Batch 6622/7702 - Train loss: 0.00192309  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.27 min) Epoch 48/300 -- Iteration 368693 - Batch 6699/7702 - Train loss: 0.00192324  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.29 min) Epoch 48/300 -- Iteration 368770 - Batch 6776/7702 - Train loss: 0.00192321  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.31 min) Epoch 48/300 -- Iteration 368847 - Batch 6853/7702 - Train loss: 0.00192375  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.33 min) Epoch 48/300 -- Iteration 368924 - Batch 6930/7702 - Train loss: 0.00192380  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.35 min) Epoch 48/300 -- Iteration 369001 - Batch 7007/7702 - Train loss: 0.00192369  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.37 min) Epoch 48/300 -- Iteration 369078 - Batch 7084/7702 - Train loss: 0.00192391  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.39 min) Epoch 48/300 -- Iteration 369155 - Batch 7161/7702 - Train loss: 0.00192407  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.41 min) Epoch 48/300 -- Iteration 369232 - Batch 7238/7702 - Train loss: 0.00192405  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.43 min) Epoch 48/300 -- Iteration 369309 - Batch 7315/7702 - Train loss: 0.00192378  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.45 min) Epoch 48/300 -- Iteration 369386 - Batch 7392/7702 - Train loss: 0.00192398  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.48 min) Epoch 48/300 -- Iteration 369463 - Batch 7469/7702 - Train loss: 0.00192430  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.50 min) Epoch 48/300 -- Iteration 369540 - Batch 7546/7702 - Train loss: 0.00192394  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.52 min) Epoch 48/300 -- Iteration 369617 - Batch 7623/7702 - Train loss: 0.00192403  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.54 min) Epoch 48/300 -- Iteration 369694 - Batch 7700/7702 - Train loss: 0.00192381  - Train acc: -0.0000 - Val loss: 0.00041135\n",
      "(99.54 min) Epoch 48/300 -- Iteration 369696 - Batch 7701/7702 - Train loss: 0.00192380  - Train acc: -0.0000 - Val loss: 0.00048959 - Val acc: -0.0000\n",
      "(99.56 min) Epoch 49/300 -- Iteration 369773 - Batch 77/7702 - Train loss: 0.00191289  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.58 min) Epoch 49/300 -- Iteration 369850 - Batch 154/7702 - Train loss: 0.00190953  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.60 min) Epoch 49/300 -- Iteration 369927 - Batch 231/7702 - Train loss: 0.00191734  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.62 min) Epoch 49/300 -- Iteration 370004 - Batch 308/7702 - Train loss: 0.00190934  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.64 min) Epoch 49/300 -- Iteration 370081 - Batch 385/7702 - Train loss: 0.00191287  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.66 min) Epoch 49/300 -- Iteration 370158 - Batch 462/7702 - Train loss: 0.00190984  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.69 min) Epoch 49/300 -- Iteration 370235 - Batch 539/7702 - Train loss: 0.00191136  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.71 min) Epoch 49/300 -- Iteration 370312 - Batch 616/7702 - Train loss: 0.00191448  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.73 min) Epoch 49/300 -- Iteration 370389 - Batch 693/7702 - Train loss: 0.00190915  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.75 min) Epoch 49/300 -- Iteration 370466 - Batch 770/7702 - Train loss: 0.00191836  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.77 min) Epoch 49/300 -- Iteration 370543 - Batch 847/7702 - Train loss: 0.00191561  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.79 min) Epoch 49/300 -- Iteration 370620 - Batch 924/7702 - Train loss: 0.00191640  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.81 min) Epoch 49/300 -- Iteration 370697 - Batch 1001/7702 - Train loss: 0.00191624  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.83 min) Epoch 49/300 -- Iteration 370774 - Batch 1078/7702 - Train loss: 0.00191380  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.85 min) Epoch 49/300 -- Iteration 370851 - Batch 1155/7702 - Train loss: 0.00191377  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.87 min) Epoch 49/300 -- Iteration 370928 - Batch 1232/7702 - Train loss: 0.00191396  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.89 min) Epoch 49/300 -- Iteration 371005 - Batch 1309/7702 - Train loss: 0.00191331  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.91 min) Epoch 49/300 -- Iteration 371082 - Batch 1386/7702 - Train loss: 0.00191353  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.93 min) Epoch 49/300 -- Iteration 371159 - Batch 1463/7702 - Train loss: 0.00191245  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.95 min) Epoch 49/300 -- Iteration 371236 - Batch 1540/7702 - Train loss: 0.00191077  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(99.98 min) Epoch 49/300 -- Iteration 371313 - Batch 1617/7702 - Train loss: 0.00191212  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.00 min) Epoch 49/300 -- Iteration 371390 - Batch 1694/7702 - Train loss: 0.00191277  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.02 min) Epoch 49/300 -- Iteration 371467 - Batch 1771/7702 - Train loss: 0.00191124  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.04 min) Epoch 49/300 -- Iteration 371544 - Batch 1848/7702 - Train loss: 0.00191071  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.06 min) Epoch 49/300 -- Iteration 371621 - Batch 1925/7702 - Train loss: 0.00191078  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.08 min) Epoch 49/300 -- Iteration 371698 - Batch 2002/7702 - Train loss: 0.00191254  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.10 min) Epoch 49/300 -- Iteration 371775 - Batch 2079/7702 - Train loss: 0.00191147  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.12 min) Epoch 49/300 -- Iteration 371852 - Batch 2156/7702 - Train loss: 0.00191111  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.14 min) Epoch 49/300 -- Iteration 371929 - Batch 2233/7702 - Train loss: 0.00191050  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.16 min) Epoch 49/300 -- Iteration 372006 - Batch 2310/7702 - Train loss: 0.00191139  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.18 min) Epoch 49/300 -- Iteration 372083 - Batch 2387/7702 - Train loss: 0.00191212  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.20 min) Epoch 49/300 -- Iteration 372160 - Batch 2464/7702 - Train loss: 0.00191314  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.22 min) Epoch 49/300 -- Iteration 372237 - Batch 2541/7702 - Train loss: 0.00191307  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.24 min) Epoch 49/300 -- Iteration 372314 - Batch 2618/7702 - Train loss: 0.00191353  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.26 min) Epoch 49/300 -- Iteration 372391 - Batch 2695/7702 - Train loss: 0.00191406  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.29 min) Epoch 49/300 -- Iteration 372468 - Batch 2772/7702 - Train loss: 0.00191337  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.31 min) Epoch 49/300 -- Iteration 372545 - Batch 2849/7702 - Train loss: 0.00191418  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.33 min) Epoch 49/300 -- Iteration 372622 - Batch 2926/7702 - Train loss: 0.00191571  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.35 min) Epoch 49/300 -- Iteration 372699 - Batch 3003/7702 - Train loss: 0.00191591  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.37 min) Epoch 49/300 -- Iteration 372776 - Batch 3080/7702 - Train loss: 0.00191573  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.39 min) Epoch 49/300 -- Iteration 372853 - Batch 3157/7702 - Train loss: 0.00191530  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.41 min) Epoch 49/300 -- Iteration 372930 - Batch 3234/7702 - Train loss: 0.00191543  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.43 min) Epoch 49/300 -- Iteration 373007 - Batch 3311/7702 - Train loss: 0.00191594  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.45 min) Epoch 49/300 -- Iteration 373084 - Batch 3388/7702 - Train loss: 0.00191616  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.47 min) Epoch 49/300 -- Iteration 373161 - Batch 3465/7702 - Train loss: 0.00191642  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.49 min) Epoch 49/300 -- Iteration 373238 - Batch 3542/7702 - Train loss: 0.00191617  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.51 min) Epoch 49/300 -- Iteration 373315 - Batch 3619/7702 - Train loss: 0.00191614  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.53 min) Epoch 49/300 -- Iteration 373392 - Batch 3696/7702 - Train loss: 0.00191576  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.55 min) Epoch 49/300 -- Iteration 373469 - Batch 3773/7702 - Train loss: 0.00191591  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.58 min) Epoch 49/300 -- Iteration 373546 - Batch 3850/7702 - Train loss: 0.00191577  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.60 min) Epoch 49/300 -- Iteration 373623 - Batch 3927/7702 - Train loss: 0.00191589  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.62 min) Epoch 49/300 -- Iteration 373700 - Batch 4004/7702 - Train loss: 0.00191475  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.64 min) Epoch 49/300 -- Iteration 373777 - Batch 4081/7702 - Train loss: 0.00191451  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.66 min) Epoch 49/300 -- Iteration 373854 - Batch 4158/7702 - Train loss: 0.00191433  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.68 min) Epoch 49/300 -- Iteration 373931 - Batch 4235/7702 - Train loss: 0.00191480  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.70 min) Epoch 49/300 -- Iteration 374008 - Batch 4312/7702 - Train loss: 0.00191509  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.72 min) Epoch 49/300 -- Iteration 374085 - Batch 4389/7702 - Train loss: 0.00191527  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.74 min) Epoch 49/300 -- Iteration 374162 - Batch 4466/7702 - Train loss: 0.00191488  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.76 min) Epoch 49/300 -- Iteration 374239 - Batch 4543/7702 - Train loss: 0.00191511  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.78 min) Epoch 49/300 -- Iteration 374316 - Batch 4620/7702 - Train loss: 0.00191554  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.80 min) Epoch 49/300 -- Iteration 374393 - Batch 4697/7702 - Train loss: 0.00191536  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.82 min) Epoch 49/300 -- Iteration 374470 - Batch 4774/7702 - Train loss: 0.00191516  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.84 min) Epoch 49/300 -- Iteration 374547 - Batch 4851/7702 - Train loss: 0.00191525  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.87 min) Epoch 49/300 -- Iteration 374624 - Batch 4928/7702 - Train loss: 0.00191533  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.89 min) Epoch 49/300 -- Iteration 374701 - Batch 5005/7702 - Train loss: 0.00191469  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.91 min) Epoch 49/300 -- Iteration 374778 - Batch 5082/7702 - Train loss: 0.00191499  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.93 min) Epoch 49/300 -- Iteration 374855 - Batch 5159/7702 - Train loss: 0.00191457  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.95 min) Epoch 49/300 -- Iteration 374932 - Batch 5236/7702 - Train loss: 0.00191484  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.97 min) Epoch 49/300 -- Iteration 375009 - Batch 5313/7702 - Train loss: 0.00191472  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(100.99 min) Epoch 49/300 -- Iteration 375086 - Batch 5390/7702 - Train loss: 0.00191447  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.01 min) Epoch 49/300 -- Iteration 375163 - Batch 5467/7702 - Train loss: 0.00191423  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.03 min) Epoch 49/300 -- Iteration 375240 - Batch 5544/7702 - Train loss: 0.00191433  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.05 min) Epoch 49/300 -- Iteration 375317 - Batch 5621/7702 - Train loss: 0.00191474  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.07 min) Epoch 49/300 -- Iteration 375394 - Batch 5698/7702 - Train loss: 0.00191454  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.09 min) Epoch 49/300 -- Iteration 375471 - Batch 5775/7702 - Train loss: 0.00191473  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.11 min) Epoch 49/300 -- Iteration 375548 - Batch 5852/7702 - Train loss: 0.00191492  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.13 min) Epoch 49/300 -- Iteration 375625 - Batch 5929/7702 - Train loss: 0.00191427  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.16 min) Epoch 49/300 -- Iteration 375702 - Batch 6006/7702 - Train loss: 0.00191335  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.18 min) Epoch 49/300 -- Iteration 375779 - Batch 6083/7702 - Train loss: 0.00191307  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.20 min) Epoch 49/300 -- Iteration 375856 - Batch 6160/7702 - Train loss: 0.00191303  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.22 min) Epoch 49/300 -- Iteration 375933 - Batch 6237/7702 - Train loss: 0.00191345  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.24 min) Epoch 49/300 -- Iteration 376010 - Batch 6314/7702 - Train loss: 0.00191306  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.26 min) Epoch 49/300 -- Iteration 376087 - Batch 6391/7702 - Train loss: 0.00191348  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.28 min) Epoch 49/300 -- Iteration 376164 - Batch 6468/7702 - Train loss: 0.00191354  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.30 min) Epoch 49/300 -- Iteration 376241 - Batch 6545/7702 - Train loss: 0.00191368  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.32 min) Epoch 49/300 -- Iteration 376318 - Batch 6622/7702 - Train loss: 0.00191368  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.34 min) Epoch 49/300 -- Iteration 376395 - Batch 6699/7702 - Train loss: 0.00191360  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.36 min) Epoch 49/300 -- Iteration 376472 - Batch 6776/7702 - Train loss: 0.00191354  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.38 min) Epoch 49/300 -- Iteration 376549 - Batch 6853/7702 - Train loss: 0.00191366  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.40 min) Epoch 49/300 -- Iteration 376626 - Batch 6930/7702 - Train loss: 0.00191392  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.42 min) Epoch 49/300 -- Iteration 376703 - Batch 7007/7702 - Train loss: 0.00191388  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.45 min) Epoch 49/300 -- Iteration 376780 - Batch 7084/7702 - Train loss: 0.00191404  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.47 min) Epoch 49/300 -- Iteration 376857 - Batch 7161/7702 - Train loss: 0.00191440  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.49 min) Epoch 49/300 -- Iteration 376934 - Batch 7238/7702 - Train loss: 0.00191437  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.51 min) Epoch 49/300 -- Iteration 377011 - Batch 7315/7702 - Train loss: 0.00191415  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.53 min) Epoch 49/300 -- Iteration 377088 - Batch 7392/7702 - Train loss: 0.00191423  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.55 min) Epoch 49/300 -- Iteration 377165 - Batch 7469/7702 - Train loss: 0.00191439  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.57 min) Epoch 49/300 -- Iteration 377242 - Batch 7546/7702 - Train loss: 0.00191428  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.59 min) Epoch 49/300 -- Iteration 377319 - Batch 7623/7702 - Train loss: 0.00191404  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.61 min) Epoch 49/300 -- Iteration 377396 - Batch 7700/7702 - Train loss: 0.00191407  - Train acc: -0.0000 - Val loss: 0.00048959\n",
      "(101.61 min) Epoch 49/300 -- Iteration 377398 - Batch 7701/7702 - Train loss: 0.00191407  - Train acc: -0.0000 - Val loss: 0.00041799 - Val acc: -0.0000\n",
      "(101.63 min) Epoch 50/300 -- Iteration 377475 - Batch 77/7702 - Train loss: 0.00195510  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.66 min) Epoch 50/300 -- Iteration 377552 - Batch 154/7702 - Train loss: 0.00194125  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.68 min) Epoch 50/300 -- Iteration 377629 - Batch 231/7702 - Train loss: 0.00192762  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.70 min) Epoch 50/300 -- Iteration 377706 - Batch 308/7702 - Train loss: 0.00191445  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.72 min) Epoch 50/300 -- Iteration 377783 - Batch 385/7702 - Train loss: 0.00191036  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.74 min) Epoch 50/300 -- Iteration 377860 - Batch 462/7702 - Train loss: 0.00191183  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.76 min) Epoch 50/300 -- Iteration 377937 - Batch 539/7702 - Train loss: 0.00191757  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.78 min) Epoch 50/300 -- Iteration 378014 - Batch 616/7702 - Train loss: 0.00192106  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.80 min) Epoch 50/300 -- Iteration 378091 - Batch 693/7702 - Train loss: 0.00192171  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.82 min) Epoch 50/300 -- Iteration 378168 - Batch 770/7702 - Train loss: 0.00192588  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.84 min) Epoch 50/300 -- Iteration 378245 - Batch 847/7702 - Train loss: 0.00192613  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.86 min) Epoch 50/300 -- Iteration 378322 - Batch 924/7702 - Train loss: 0.00192849  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.88 min) Epoch 50/300 -- Iteration 378399 - Batch 1001/7702 - Train loss: 0.00192612  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.90 min) Epoch 50/300 -- Iteration 378476 - Batch 1078/7702 - Train loss: 0.00192405  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.92 min) Epoch 50/300 -- Iteration 378553 - Batch 1155/7702 - Train loss: 0.00191964  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.94 min) Epoch 50/300 -- Iteration 378630 - Batch 1232/7702 - Train loss: 0.00191543  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.97 min) Epoch 50/300 -- Iteration 378707 - Batch 1309/7702 - Train loss: 0.00191600  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(101.99 min) Epoch 50/300 -- Iteration 378784 - Batch 1386/7702 - Train loss: 0.00191650  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.01 min) Epoch 50/300 -- Iteration 378861 - Batch 1463/7702 - Train loss: 0.00191665  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.03 min) Epoch 50/300 -- Iteration 378938 - Batch 1540/7702 - Train loss: 0.00191615  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.05 min) Epoch 50/300 -- Iteration 379015 - Batch 1617/7702 - Train loss: 0.00191791  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.07 min) Epoch 50/300 -- Iteration 379092 - Batch 1694/7702 - Train loss: 0.00191670  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.09 min) Epoch 50/300 -- Iteration 379169 - Batch 1771/7702 - Train loss: 0.00191626  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.11 min) Epoch 50/300 -- Iteration 379246 - Batch 1848/7702 - Train loss: 0.00191615  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.13 min) Epoch 50/300 -- Iteration 379323 - Batch 1925/7702 - Train loss: 0.00191674  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.15 min) Epoch 50/300 -- Iteration 379400 - Batch 2002/7702 - Train loss: 0.00191721  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.17 min) Epoch 50/300 -- Iteration 379477 - Batch 2079/7702 - Train loss: 0.00191558  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.19 min) Epoch 50/300 -- Iteration 379554 - Batch 2156/7702 - Train loss: 0.00191552  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.22 min) Epoch 50/300 -- Iteration 379631 - Batch 2233/7702 - Train loss: 0.00191459  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.24 min) Epoch 50/300 -- Iteration 379708 - Batch 2310/7702 - Train loss: 0.00191558  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.26 min) Epoch 50/300 -- Iteration 379785 - Batch 2387/7702 - Train loss: 0.00191486  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.28 min) Epoch 50/300 -- Iteration 379862 - Batch 2464/7702 - Train loss: 0.00191523  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.30 min) Epoch 50/300 -- Iteration 379939 - Batch 2541/7702 - Train loss: 0.00191658  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.32 min) Epoch 50/300 -- Iteration 380016 - Batch 2618/7702 - Train loss: 0.00191659  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.34 min) Epoch 50/300 -- Iteration 380093 - Batch 2695/7702 - Train loss: 0.00191689  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.36 min) Epoch 50/300 -- Iteration 380170 - Batch 2772/7702 - Train loss: 0.00191680  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.38 min) Epoch 50/300 -- Iteration 380247 - Batch 2849/7702 - Train loss: 0.00191717  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.40 min) Epoch 50/300 -- Iteration 380324 - Batch 2926/7702 - Train loss: 0.00191598  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.42 min) Epoch 50/300 -- Iteration 380401 - Batch 3003/7702 - Train loss: 0.00191726  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.44 min) Epoch 50/300 -- Iteration 380478 - Batch 3080/7702 - Train loss: 0.00191754  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.46 min) Epoch 50/300 -- Iteration 380555 - Batch 3157/7702 - Train loss: 0.00191769  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.49 min) Epoch 50/300 -- Iteration 380632 - Batch 3234/7702 - Train loss: 0.00191723  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.51 min) Epoch 50/300 -- Iteration 380709 - Batch 3311/7702 - Train loss: 0.00191672  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.53 min) Epoch 50/300 -- Iteration 380786 - Batch 3388/7702 - Train loss: 0.00191645  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.55 min) Epoch 50/300 -- Iteration 380863 - Batch 3465/7702 - Train loss: 0.00191598  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.57 min) Epoch 50/300 -- Iteration 380940 - Batch 3542/7702 - Train loss: 0.00191525  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.59 min) Epoch 50/300 -- Iteration 381017 - Batch 3619/7702 - Train loss: 0.00191522  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.61 min) Epoch 50/300 -- Iteration 381094 - Batch 3696/7702 - Train loss: 0.00191512  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.63 min) Epoch 50/300 -- Iteration 381171 - Batch 3773/7702 - Train loss: 0.00191486  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.65 min) Epoch 50/300 -- Iteration 381248 - Batch 3850/7702 - Train loss: 0.00191489  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.67 min) Epoch 50/300 -- Iteration 381325 - Batch 3927/7702 - Train loss: 0.00191495  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.69 min) Epoch 50/300 -- Iteration 381402 - Batch 4004/7702 - Train loss: 0.00191518  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.71 min) Epoch 50/300 -- Iteration 381479 - Batch 4081/7702 - Train loss: 0.00191536  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.73 min) Epoch 50/300 -- Iteration 381556 - Batch 4158/7702 - Train loss: 0.00191589  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.75 min) Epoch 50/300 -- Iteration 381633 - Batch 4235/7702 - Train loss: 0.00191565  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.78 min) Epoch 50/300 -- Iteration 381710 - Batch 4312/7702 - Train loss: 0.00191604  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.80 min) Epoch 50/300 -- Iteration 381787 - Batch 4389/7702 - Train loss: 0.00191592  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.82 min) Epoch 50/300 -- Iteration 381864 - Batch 4466/7702 - Train loss: 0.00191576  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.84 min) Epoch 50/300 -- Iteration 381941 - Batch 4543/7702 - Train loss: 0.00191600  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.86 min) Epoch 50/300 -- Iteration 382018 - Batch 4620/7702 - Train loss: 0.00191658  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.88 min) Epoch 50/300 -- Iteration 382095 - Batch 4697/7702 - Train loss: 0.00191648  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.90 min) Epoch 50/300 -- Iteration 382172 - Batch 4774/7702 - Train loss: 0.00191673  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.92 min) Epoch 50/300 -- Iteration 382249 - Batch 4851/7702 - Train loss: 0.00191702  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.94 min) Epoch 50/300 -- Iteration 382326 - Batch 4928/7702 - Train loss: 0.00191701  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.96 min) Epoch 50/300 -- Iteration 382403 - Batch 5005/7702 - Train loss: 0.00191693  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(102.98 min) Epoch 50/300 -- Iteration 382480 - Batch 5082/7702 - Train loss: 0.00191723  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.00 min) Epoch 50/300 -- Iteration 382557 - Batch 5159/7702 - Train loss: 0.00191678  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.02 min) Epoch 50/300 -- Iteration 382634 - Batch 5236/7702 - Train loss: 0.00191627  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.04 min) Epoch 50/300 -- Iteration 382711 - Batch 5313/7702 - Train loss: 0.00191616  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.06 min) Epoch 50/300 -- Iteration 382788 - Batch 5390/7702 - Train loss: 0.00191610  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.09 min) Epoch 50/300 -- Iteration 382865 - Batch 5467/7702 - Train loss: 0.00191630  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.11 min) Epoch 50/300 -- Iteration 382942 - Batch 5544/7702 - Train loss: 0.00191626  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.13 min) Epoch 50/300 -- Iteration 383019 - Batch 5621/7702 - Train loss: 0.00191637  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.15 min) Epoch 50/300 -- Iteration 383096 - Batch 5698/7702 - Train loss: 0.00191628  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.17 min) Epoch 50/300 -- Iteration 383173 - Batch 5775/7702 - Train loss: 0.00191634  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.19 min) Epoch 50/300 -- Iteration 383250 - Batch 5852/7702 - Train loss: 0.00191656  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.21 min) Epoch 50/300 -- Iteration 383327 - Batch 5929/7702 - Train loss: 0.00191661  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.23 min) Epoch 50/300 -- Iteration 383404 - Batch 6006/7702 - Train loss: 0.00191644  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.25 min) Epoch 50/300 -- Iteration 383481 - Batch 6083/7702 - Train loss: 0.00191639  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.27 min) Epoch 50/300 -- Iteration 383558 - Batch 6160/7702 - Train loss: 0.00191582  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.29 min) Epoch 50/300 -- Iteration 383635 - Batch 6237/7702 - Train loss: 0.00191591  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.31 min) Epoch 50/300 -- Iteration 383712 - Batch 6314/7702 - Train loss: 0.00191625  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.33 min) Epoch 50/300 -- Iteration 383789 - Batch 6391/7702 - Train loss: 0.00191631  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.35 min) Epoch 50/300 -- Iteration 383866 - Batch 6468/7702 - Train loss: 0.00191624  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.38 min) Epoch 50/300 -- Iteration 383943 - Batch 6545/7702 - Train loss: 0.00191601  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.40 min) Epoch 50/300 -- Iteration 384020 - Batch 6622/7702 - Train loss: 0.00191586  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.42 min) Epoch 50/300 -- Iteration 384097 - Batch 6699/7702 - Train loss: 0.00191594  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.44 min) Epoch 50/300 -- Iteration 384174 - Batch 6776/7702 - Train loss: 0.00191624  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.46 min) Epoch 50/300 -- Iteration 384251 - Batch 6853/7702 - Train loss: 0.00191618  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.48 min) Epoch 50/300 -- Iteration 384328 - Batch 6930/7702 - Train loss: 0.00191644  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.50 min) Epoch 50/300 -- Iteration 384405 - Batch 7007/7702 - Train loss: 0.00191616  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.52 min) Epoch 50/300 -- Iteration 384482 - Batch 7084/7702 - Train loss: 0.00191826  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.54 min) Epoch 50/300 -- Iteration 384559 - Batch 7161/7702 - Train loss: 0.00191852  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.56 min) Epoch 50/300 -- Iteration 384636 - Batch 7238/7702 - Train loss: 0.00191914  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.58 min) Epoch 50/300 -- Iteration 384713 - Batch 7315/7702 - Train loss: 0.00191916  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.60 min) Epoch 50/300 -- Iteration 384790 - Batch 7392/7702 - Train loss: 0.00191936  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.62 min) Epoch 50/300 -- Iteration 384867 - Batch 7469/7702 - Train loss: 0.00191966  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.64 min) Epoch 50/300 -- Iteration 384944 - Batch 7546/7702 - Train loss: 0.00191983  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.67 min) Epoch 50/300 -- Iteration 385021 - Batch 7623/7702 - Train loss: 0.00192021  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.69 min) Epoch 50/300 -- Iteration 385098 - Batch 7700/7702 - Train loss: 0.00192000  - Train acc: -0.0000 - Val loss: 0.00041799\n",
      "(103.69 min) Epoch 50/300 -- Iteration 385100 - Batch 7701/7702 - Train loss: 0.00191997  - Train acc: -0.0000 - Val loss: 0.00043781 - Val acc: -0.0000\n",
      "(103.71 min) Epoch 51/300 -- Iteration 385177 - Batch 77/7702 - Train loss: 0.00193075  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.73 min) Epoch 51/300 -- Iteration 385254 - Batch 154/7702 - Train loss: 0.00192972  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.75 min) Epoch 51/300 -- Iteration 385331 - Batch 231/7702 - Train loss: 0.00191240  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.77 min) Epoch 51/300 -- Iteration 385408 - Batch 308/7702 - Train loss: 0.00190806  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.79 min) Epoch 51/300 -- Iteration 385485 - Batch 385/7702 - Train loss: 0.00190622  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.81 min) Epoch 51/300 -- Iteration 385562 - Batch 462/7702 - Train loss: 0.00189865  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.83 min) Epoch 51/300 -- Iteration 385639 - Batch 539/7702 - Train loss: 0.00190218  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.85 min) Epoch 51/300 -- Iteration 385716 - Batch 616/7702 - Train loss: 0.00189705  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.88 min) Epoch 51/300 -- Iteration 385793 - Batch 693/7702 - Train loss: 0.00190056  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.90 min) Epoch 51/300 -- Iteration 385870 - Batch 770/7702 - Train loss: 0.00190027  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.92 min) Epoch 51/300 -- Iteration 385947 - Batch 847/7702 - Train loss: 0.00190057  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.94 min) Epoch 51/300 -- Iteration 386024 - Batch 924/7702 - Train loss: 0.00190150  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.96 min) Epoch 51/300 -- Iteration 386101 - Batch 1001/7702 - Train loss: 0.00190428  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(103.98 min) Epoch 51/300 -- Iteration 386178 - Batch 1078/7702 - Train loss: 0.00190672  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.00 min) Epoch 51/300 -- Iteration 386255 - Batch 1155/7702 - Train loss: 0.00190479  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.02 min) Epoch 51/300 -- Iteration 386332 - Batch 1232/7702 - Train loss: 0.00190658  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.04 min) Epoch 51/300 -- Iteration 386409 - Batch 1309/7702 - Train loss: 0.00190678  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.06 min) Epoch 51/300 -- Iteration 386486 - Batch 1386/7702 - Train loss: 0.00190670  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.08 min) Epoch 51/300 -- Iteration 386563 - Batch 1463/7702 - Train loss: 0.00190695  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.10 min) Epoch 51/300 -- Iteration 386640 - Batch 1540/7702 - Train loss: 0.00190914  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.12 min) Epoch 51/300 -- Iteration 386717 - Batch 1617/7702 - Train loss: 0.00190934  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.14 min) Epoch 51/300 -- Iteration 386794 - Batch 1694/7702 - Train loss: 0.00190869  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.17 min) Epoch 51/300 -- Iteration 386871 - Batch 1771/7702 - Train loss: 0.00190967  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.19 min) Epoch 51/300 -- Iteration 386948 - Batch 1848/7702 - Train loss: 0.00191100  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.21 min) Epoch 51/300 -- Iteration 387025 - Batch 1925/7702 - Train loss: 0.00191092  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.23 min) Epoch 51/300 -- Iteration 387102 - Batch 2002/7702 - Train loss: 0.00191073  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.25 min) Epoch 51/300 -- Iteration 387179 - Batch 2079/7702 - Train loss: 0.00191018  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.27 min) Epoch 51/300 -- Iteration 387256 - Batch 2156/7702 - Train loss: 0.00191081  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.29 min) Epoch 51/300 -- Iteration 387333 - Batch 2233/7702 - Train loss: 0.00191076  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.31 min) Epoch 51/300 -- Iteration 387410 - Batch 2310/7702 - Train loss: 0.00190962  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.33 min) Epoch 51/300 -- Iteration 387487 - Batch 2387/7702 - Train loss: 0.00190957  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.35 min) Epoch 51/300 -- Iteration 387564 - Batch 2464/7702 - Train loss: 0.00191023  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.37 min) Epoch 51/300 -- Iteration 387641 - Batch 2541/7702 - Train loss: 0.00190991  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.39 min) Epoch 51/300 -- Iteration 387718 - Batch 2618/7702 - Train loss: 0.00191094  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.41 min) Epoch 51/300 -- Iteration 387795 - Batch 2695/7702 - Train loss: 0.00191096  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.44 min) Epoch 51/300 -- Iteration 387872 - Batch 2772/7702 - Train loss: 0.00191056  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.46 min) Epoch 51/300 -- Iteration 387949 - Batch 2849/7702 - Train loss: 0.00191059  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.48 min) Epoch 51/300 -- Iteration 388026 - Batch 2926/7702 - Train loss: 0.00191031  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.50 min) Epoch 51/300 -- Iteration 388103 - Batch 3003/7702 - Train loss: 0.00190989  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.52 min) Epoch 51/300 -- Iteration 388180 - Batch 3080/7702 - Train loss: 0.00191060  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.54 min) Epoch 51/300 -- Iteration 388257 - Batch 3157/7702 - Train loss: 0.00190989  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.56 min) Epoch 51/300 -- Iteration 388334 - Batch 3234/7702 - Train loss: 0.00191023  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.58 min) Epoch 51/300 -- Iteration 388411 - Batch 3311/7702 - Train loss: 0.00191089  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.60 min) Epoch 51/300 -- Iteration 388488 - Batch 3388/7702 - Train loss: 0.00191077  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.62 min) Epoch 51/300 -- Iteration 388565 - Batch 3465/7702 - Train loss: 0.00191099  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.64 min) Epoch 51/300 -- Iteration 388642 - Batch 3542/7702 - Train loss: 0.00191142  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.66 min) Epoch 51/300 -- Iteration 388719 - Batch 3619/7702 - Train loss: 0.00191244  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.68 min) Epoch 51/300 -- Iteration 388796 - Batch 3696/7702 - Train loss: 0.00191213  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.70 min) Epoch 51/300 -- Iteration 388873 - Batch 3773/7702 - Train loss: 0.00191296  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.72 min) Epoch 51/300 -- Iteration 388950 - Batch 3850/7702 - Train loss: 0.00191287  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.75 min) Epoch 51/300 -- Iteration 389027 - Batch 3927/7702 - Train loss: 0.00191274  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.77 min) Epoch 51/300 -- Iteration 389104 - Batch 4004/7702 - Train loss: 0.00191280  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.79 min) Epoch 51/300 -- Iteration 389181 - Batch 4081/7702 - Train loss: 0.00191257  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.81 min) Epoch 51/300 -- Iteration 389258 - Batch 4158/7702 - Train loss: 0.00191161  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.83 min) Epoch 51/300 -- Iteration 389335 - Batch 4235/7702 - Train loss: 0.00191107  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.85 min) Epoch 51/300 -- Iteration 389412 - Batch 4312/7702 - Train loss: 0.00191121  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.87 min) Epoch 51/300 -- Iteration 389489 - Batch 4389/7702 - Train loss: 0.00191118  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.89 min) Epoch 51/300 -- Iteration 389566 - Batch 4466/7702 - Train loss: 0.00191223  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.91 min) Epoch 51/300 -- Iteration 389643 - Batch 4543/7702 - Train loss: 0.00191289  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.93 min) Epoch 51/300 -- Iteration 389720 - Batch 4620/7702 - Train loss: 0.00191240  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.95 min) Epoch 51/300 -- Iteration 389797 - Batch 4697/7702 - Train loss: 0.00191255  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.97 min) Epoch 51/300 -- Iteration 389874 - Batch 4774/7702 - Train loss: 0.00191196  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(104.99 min) Epoch 51/300 -- Iteration 389951 - Batch 4851/7702 - Train loss: 0.00191206  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.01 min) Epoch 51/300 -- Iteration 390028 - Batch 4928/7702 - Train loss: 0.00191185  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.04 min) Epoch 51/300 -- Iteration 390105 - Batch 5005/7702 - Train loss: 0.00191208  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.06 min) Epoch 51/300 -- Iteration 390182 - Batch 5082/7702 - Train loss: 0.00191220  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.08 min) Epoch 51/300 -- Iteration 390259 - Batch 5159/7702 - Train loss: 0.00191236  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.10 min) Epoch 51/300 -- Iteration 390336 - Batch 5236/7702 - Train loss: 0.00191267  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.12 min) Epoch 51/300 -- Iteration 390413 - Batch 5313/7702 - Train loss: 0.00191268  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.14 min) Epoch 51/300 -- Iteration 390490 - Batch 5390/7702 - Train loss: 0.00191288  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.16 min) Epoch 51/300 -- Iteration 390567 - Batch 5467/7702 - Train loss: 0.00191292  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.18 min) Epoch 51/300 -- Iteration 390644 - Batch 5544/7702 - Train loss: 0.00191275  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.20 min) Epoch 51/300 -- Iteration 390721 - Batch 5621/7702 - Train loss: 0.00191276  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.22 min) Epoch 51/300 -- Iteration 390798 - Batch 5698/7702 - Train loss: 0.00191307  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.24 min) Epoch 51/300 -- Iteration 390875 - Batch 5775/7702 - Train loss: 0.00191288  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.26 min) Epoch 51/300 -- Iteration 390952 - Batch 5852/7702 - Train loss: 0.00191299  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.28 min) Epoch 51/300 -- Iteration 391029 - Batch 5929/7702 - Train loss: 0.00191325  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.30 min) Epoch 51/300 -- Iteration 391106 - Batch 6006/7702 - Train loss: 0.00191345  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.33 min) Epoch 51/300 -- Iteration 391183 - Batch 6083/7702 - Train loss: 0.00191361  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.35 min) Epoch 51/300 -- Iteration 391260 - Batch 6160/7702 - Train loss: 0.00191410  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.37 min) Epoch 51/300 -- Iteration 391337 - Batch 6237/7702 - Train loss: 0.00191412  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.39 min) Epoch 51/300 -- Iteration 391414 - Batch 6314/7702 - Train loss: 0.00191371  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.41 min) Epoch 51/300 -- Iteration 391491 - Batch 6391/7702 - Train loss: 0.00191357  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.43 min) Epoch 51/300 -- Iteration 391568 - Batch 6468/7702 - Train loss: 0.00191390  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.45 min) Epoch 51/300 -- Iteration 391645 - Batch 6545/7702 - Train loss: 0.00191399  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.47 min) Epoch 51/300 -- Iteration 391722 - Batch 6622/7702 - Train loss: 0.00191387  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.49 min) Epoch 51/300 -- Iteration 391799 - Batch 6699/7702 - Train loss: 0.00191383  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.51 min) Epoch 51/300 -- Iteration 391876 - Batch 6776/7702 - Train loss: 0.00191425  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.53 min) Epoch 51/300 -- Iteration 391953 - Batch 6853/7702 - Train loss: 0.00191430  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.55 min) Epoch 51/300 -- Iteration 392030 - Batch 6930/7702 - Train loss: 0.00191446  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.57 min) Epoch 51/300 -- Iteration 392107 - Batch 7007/7702 - Train loss: 0.00191467  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.59 min) Epoch 51/300 -- Iteration 392184 - Batch 7084/7702 - Train loss: 0.00191504  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.62 min) Epoch 51/300 -- Iteration 392261 - Batch 7161/7702 - Train loss: 0.00191487  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.64 min) Epoch 51/300 -- Iteration 392338 - Batch 7238/7702 - Train loss: 0.00191491  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.66 min) Epoch 51/300 -- Iteration 392415 - Batch 7315/7702 - Train loss: 0.00191487  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.68 min) Epoch 51/300 -- Iteration 392492 - Batch 7392/7702 - Train loss: 0.00191465  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.70 min) Epoch 51/300 -- Iteration 392569 - Batch 7469/7702 - Train loss: 0.00191484  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.72 min) Epoch 51/300 -- Iteration 392646 - Batch 7546/7702 - Train loss: 0.00191517  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.74 min) Epoch 51/300 -- Iteration 392723 - Batch 7623/7702 - Train loss: 0.00191513  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.76 min) Epoch 51/300 -- Iteration 392800 - Batch 7700/7702 - Train loss: 0.00191514  - Train acc: -0.0000 - Val loss: 0.00043781\n",
      "(105.76 min) Epoch 51/300 -- Iteration 392802 - Batch 7701/7702 - Train loss: 0.00191508  - Train acc: -0.0000 - Val loss: 0.00045030 - Val acc: -0.0000\n",
      "(105.78 min) Epoch 52/300 -- Iteration 392879 - Batch 77/7702 - Train loss: 0.00188603  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.80 min) Epoch 52/300 -- Iteration 392956 - Batch 154/7702 - Train loss: 0.00191277  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.83 min) Epoch 52/300 -- Iteration 393033 - Batch 231/7702 - Train loss: 0.00191188  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.85 min) Epoch 52/300 -- Iteration 393110 - Batch 308/7702 - Train loss: 0.00190729  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.87 min) Epoch 52/300 -- Iteration 393187 - Batch 385/7702 - Train loss: 0.00190756  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.89 min) Epoch 52/300 -- Iteration 393264 - Batch 462/7702 - Train loss: 0.00190808  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.91 min) Epoch 52/300 -- Iteration 393341 - Batch 539/7702 - Train loss: 0.00190707  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.93 min) Epoch 52/300 -- Iteration 393418 - Batch 616/7702 - Train loss: 0.00190457  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.95 min) Epoch 52/300 -- Iteration 393495 - Batch 693/7702 - Train loss: 0.00190454  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.97 min) Epoch 52/300 -- Iteration 393572 - Batch 770/7702 - Train loss: 0.00190959  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(105.99 min) Epoch 52/300 -- Iteration 393649 - Batch 847/7702 - Train loss: 0.00190850  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.01 min) Epoch 52/300 -- Iteration 393726 - Batch 924/7702 - Train loss: 0.00190953  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.03 min) Epoch 52/300 -- Iteration 393803 - Batch 1001/7702 - Train loss: 0.00191003  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.05 min) Epoch 52/300 -- Iteration 393880 - Batch 1078/7702 - Train loss: 0.00191048  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.07 min) Epoch 52/300 -- Iteration 393957 - Batch 1155/7702 - Train loss: 0.00191126  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.09 min) Epoch 52/300 -- Iteration 394034 - Batch 1232/7702 - Train loss: 0.00191160  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.12 min) Epoch 52/300 -- Iteration 394111 - Batch 1309/7702 - Train loss: 0.00191231  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.14 min) Epoch 52/300 -- Iteration 394188 - Batch 1386/7702 - Train loss: 0.00191017  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.16 min) Epoch 52/300 -- Iteration 394265 - Batch 1463/7702 - Train loss: 0.00190944  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.18 min) Epoch 52/300 -- Iteration 394342 - Batch 1540/7702 - Train loss: 0.00191050  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.20 min) Epoch 52/300 -- Iteration 394419 - Batch 1617/7702 - Train loss: 0.00191091  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.22 min) Epoch 52/300 -- Iteration 394496 - Batch 1694/7702 - Train loss: 0.00191096  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.24 min) Epoch 52/300 -- Iteration 394573 - Batch 1771/7702 - Train loss: 0.00191022  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.26 min) Epoch 52/300 -- Iteration 394650 - Batch 1848/7702 - Train loss: 0.00191054  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.28 min) Epoch 52/300 -- Iteration 394727 - Batch 1925/7702 - Train loss: 0.00191108  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.30 min) Epoch 52/300 -- Iteration 394804 - Batch 2002/7702 - Train loss: 0.00191137  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.32 min) Epoch 52/300 -- Iteration 394881 - Batch 2079/7702 - Train loss: 0.00191159  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.34 min) Epoch 52/300 -- Iteration 394958 - Batch 2156/7702 - Train loss: 0.00191067  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.36 min) Epoch 52/300 -- Iteration 395035 - Batch 2233/7702 - Train loss: 0.00191118  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.39 min) Epoch 52/300 -- Iteration 395112 - Batch 2310/7702 - Train loss: 0.00191187  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.41 min) Epoch 52/300 -- Iteration 395189 - Batch 2387/7702 - Train loss: 0.00191206  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.43 min) Epoch 52/300 -- Iteration 395266 - Batch 2464/7702 - Train loss: 0.00191308  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.45 min) Epoch 52/300 -- Iteration 395343 - Batch 2541/7702 - Train loss: 0.00191325  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.47 min) Epoch 52/300 -- Iteration 395420 - Batch 2618/7702 - Train loss: 0.00191278  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.49 min) Epoch 52/300 -- Iteration 395497 - Batch 2695/7702 - Train loss: 0.00191258  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.51 min) Epoch 52/300 -- Iteration 395574 - Batch 2772/7702 - Train loss: 0.00191274  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.53 min) Epoch 52/300 -- Iteration 395651 - Batch 2849/7702 - Train loss: 0.00191325  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.55 min) Epoch 52/300 -- Iteration 395728 - Batch 2926/7702 - Train loss: 0.00191336  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.57 min) Epoch 52/300 -- Iteration 395805 - Batch 3003/7702 - Train loss: 0.00191327  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.59 min) Epoch 52/300 -- Iteration 395882 - Batch 3080/7702 - Train loss: 0.00191322  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.61 min) Epoch 52/300 -- Iteration 395959 - Batch 3157/7702 - Train loss: 0.00191385  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.63 min) Epoch 52/300 -- Iteration 396036 - Batch 3234/7702 - Train loss: 0.00191446  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.65 min) Epoch 52/300 -- Iteration 396113 - Batch 3311/7702 - Train loss: 0.00191488  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.68 min) Epoch 52/300 -- Iteration 396190 - Batch 3388/7702 - Train loss: 0.00191540  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.70 min) Epoch 52/300 -- Iteration 396267 - Batch 3465/7702 - Train loss: 0.00191455  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.72 min) Epoch 52/300 -- Iteration 396344 - Batch 3542/7702 - Train loss: 0.00191483  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.74 min) Epoch 52/300 -- Iteration 396421 - Batch 3619/7702 - Train loss: 0.00191487  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.76 min) Epoch 52/300 -- Iteration 396498 - Batch 3696/7702 - Train loss: 0.00191448  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.78 min) Epoch 52/300 -- Iteration 396575 - Batch 3773/7702 - Train loss: 0.00191422  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.80 min) Epoch 52/300 -- Iteration 396652 - Batch 3850/7702 - Train loss: 0.00191338  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.82 min) Epoch 52/300 -- Iteration 396729 - Batch 3927/7702 - Train loss: 0.00191389  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.84 min) Epoch 52/300 -- Iteration 396806 - Batch 4004/7702 - Train loss: 0.00191404  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.86 min) Epoch 52/300 -- Iteration 396883 - Batch 4081/7702 - Train loss: 0.00191388  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.88 min) Epoch 52/300 -- Iteration 396960 - Batch 4158/7702 - Train loss: 0.00191302  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.90 min) Epoch 52/300 -- Iteration 397037 - Batch 4235/7702 - Train loss: 0.00191248  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.92 min) Epoch 52/300 -- Iteration 397114 - Batch 4312/7702 - Train loss: 0.00191257  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.94 min) Epoch 52/300 -- Iteration 397191 - Batch 4389/7702 - Train loss: 0.00191242  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.97 min) Epoch 52/300 -- Iteration 397268 - Batch 4466/7702 - Train loss: 0.00191265  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(106.99 min) Epoch 52/300 -- Iteration 397345 - Batch 4543/7702 - Train loss: 0.00191183  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.01 min) Epoch 52/300 -- Iteration 397422 - Batch 4620/7702 - Train loss: 0.00191160  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.03 min) Epoch 52/300 -- Iteration 397499 - Batch 4697/7702 - Train loss: 0.00191227  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.05 min) Epoch 52/300 -- Iteration 397576 - Batch 4774/7702 - Train loss: 0.00191260  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.07 min) Epoch 52/300 -- Iteration 397653 - Batch 4851/7702 - Train loss: 0.00191223  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.09 min) Epoch 52/300 -- Iteration 397730 - Batch 4928/7702 - Train loss: 0.00191249  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.11 min) Epoch 52/300 -- Iteration 397807 - Batch 5005/7702 - Train loss: 0.00191308  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.13 min) Epoch 52/300 -- Iteration 397884 - Batch 5082/7702 - Train loss: 0.00191345  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.15 min) Epoch 52/300 -- Iteration 397961 - Batch 5159/7702 - Train loss: 0.00191344  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.17 min) Epoch 52/300 -- Iteration 398038 - Batch 5236/7702 - Train loss: 0.00191352  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.19 min) Epoch 52/300 -- Iteration 398115 - Batch 5313/7702 - Train loss: 0.00191352  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.21 min) Epoch 52/300 -- Iteration 398192 - Batch 5390/7702 - Train loss: 0.00191328  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.23 min) Epoch 52/300 -- Iteration 398269 - Batch 5467/7702 - Train loss: 0.00191240  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.26 min) Epoch 52/300 -- Iteration 398346 - Batch 5544/7702 - Train loss: 0.00191258  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.28 min) Epoch 52/300 -- Iteration 398423 - Batch 5621/7702 - Train loss: 0.00191242  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.30 min) Epoch 52/300 -- Iteration 398500 - Batch 5698/7702 - Train loss: 0.00191237  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.32 min) Epoch 52/300 -- Iteration 398577 - Batch 5775/7702 - Train loss: 0.00191191  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.34 min) Epoch 52/300 -- Iteration 398654 - Batch 5852/7702 - Train loss: 0.00191165  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.36 min) Epoch 52/300 -- Iteration 398731 - Batch 5929/7702 - Train loss: 0.00191125  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.38 min) Epoch 52/300 -- Iteration 398808 - Batch 6006/7702 - Train loss: 0.00191157  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.40 min) Epoch 52/300 -- Iteration 398885 - Batch 6083/7702 - Train loss: 0.00191190  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.42 min) Epoch 52/300 -- Iteration 398962 - Batch 6160/7702 - Train loss: 0.00191147  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.44 min) Epoch 52/300 -- Iteration 399039 - Batch 6237/7702 - Train loss: 0.00191131  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.46 min) Epoch 52/300 -- Iteration 399116 - Batch 6314/7702 - Train loss: 0.00191087  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.48 min) Epoch 52/300 -- Iteration 399193 - Batch 6391/7702 - Train loss: 0.00191110  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.50 min) Epoch 52/300 -- Iteration 399270 - Batch 6468/7702 - Train loss: 0.00191141  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.53 min) Epoch 52/300 -- Iteration 399347 - Batch 6545/7702 - Train loss: 0.00191155  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.55 min) Epoch 52/300 -- Iteration 399424 - Batch 6622/7702 - Train loss: 0.00191130  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.57 min) Epoch 52/300 -- Iteration 399501 - Batch 6699/7702 - Train loss: 0.00191125  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.59 min) Epoch 52/300 -- Iteration 399578 - Batch 6776/7702 - Train loss: 0.00191115  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.61 min) Epoch 52/300 -- Iteration 399655 - Batch 6853/7702 - Train loss: 0.00191118  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.63 min) Epoch 52/300 -- Iteration 399732 - Batch 6930/7702 - Train loss: 0.00191093  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.65 min) Epoch 52/300 -- Iteration 399809 - Batch 7007/7702 - Train loss: 0.00191130  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.67 min) Epoch 52/300 -- Iteration 399886 - Batch 7084/7702 - Train loss: 0.00191149  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.69 min) Epoch 52/300 -- Iteration 399963 - Batch 7161/7702 - Train loss: 0.00191146  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.71 min) Epoch 52/300 -- Iteration 400040 - Batch 7238/7702 - Train loss: 0.00191163  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.73 min) Epoch 52/300 -- Iteration 400117 - Batch 7315/7702 - Train loss: 0.00191154  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.75 min) Epoch 52/300 -- Iteration 400194 - Batch 7392/7702 - Train loss: 0.00191181  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.78 min) Epoch 52/300 -- Iteration 400271 - Batch 7469/7702 - Train loss: 0.00191219  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.80 min) Epoch 52/300 -- Iteration 400348 - Batch 7546/7702 - Train loss: 0.00191229  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.82 min) Epoch 52/300 -- Iteration 400425 - Batch 7623/7702 - Train loss: 0.00191253  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.84 min) Epoch 52/300 -- Iteration 400502 - Batch 7700/7702 - Train loss: 0.00191260  - Train acc: -0.0000 - Val loss: 0.00045030\n",
      "(107.84 min) Epoch 52/300 -- Iteration 400504 - Batch 7701/7702 - Train loss: 0.00191258  - Train acc: -0.0000 - Val loss: 0.00045714 - Val acc: -0.0000\n",
      "(107.86 min) Epoch 53/300 -- Iteration 400581 - Batch 77/7702 - Train loss: 0.00193045  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(107.88 min) Epoch 53/300 -- Iteration 400658 - Batch 154/7702 - Train loss: 0.00192051  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(107.90 min) Epoch 53/300 -- Iteration 400735 - Batch 231/7702 - Train loss: 0.00190987  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(107.92 min) Epoch 53/300 -- Iteration 400812 - Batch 308/7702 - Train loss: 0.00190788  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(107.94 min) Epoch 53/300 -- Iteration 400889 - Batch 385/7702 - Train loss: 0.00190685  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(107.96 min) Epoch 53/300 -- Iteration 400966 - Batch 462/7702 - Train loss: 0.00190261  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(107.99 min) Epoch 53/300 -- Iteration 401043 - Batch 539/7702 - Train loss: 0.00191011  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.01 min) Epoch 53/300 -- Iteration 401120 - Batch 616/7702 - Train loss: 0.00190774  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.03 min) Epoch 53/300 -- Iteration 401197 - Batch 693/7702 - Train loss: 0.00190786  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.05 min) Epoch 53/300 -- Iteration 401274 - Batch 770/7702 - Train loss: 0.00190627  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.07 min) Epoch 53/300 -- Iteration 401351 - Batch 847/7702 - Train loss: 0.00190602  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.09 min) Epoch 53/300 -- Iteration 401428 - Batch 924/7702 - Train loss: 0.00190469  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.11 min) Epoch 53/300 -- Iteration 401505 - Batch 1001/7702 - Train loss: 0.00190710  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.13 min) Epoch 53/300 -- Iteration 401582 - Batch 1078/7702 - Train loss: 0.00190674  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.15 min) Epoch 53/300 -- Iteration 401659 - Batch 1155/7702 - Train loss: 0.00190716  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.17 min) Epoch 53/300 -- Iteration 401736 - Batch 1232/7702 - Train loss: 0.00190594  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.19 min) Epoch 53/300 -- Iteration 401813 - Batch 1309/7702 - Train loss: 0.00190553  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.21 min) Epoch 53/300 -- Iteration 401890 - Batch 1386/7702 - Train loss: 0.00190514  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.23 min) Epoch 53/300 -- Iteration 401967 - Batch 1463/7702 - Train loss: 0.00190445  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.25 min) Epoch 53/300 -- Iteration 402044 - Batch 1540/7702 - Train loss: 0.00190611  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.28 min) Epoch 53/300 -- Iteration 402121 - Batch 1617/7702 - Train loss: 0.00190644  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.30 min) Epoch 53/300 -- Iteration 402198 - Batch 1694/7702 - Train loss: 0.00190728  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.32 min) Epoch 53/300 -- Iteration 402275 - Batch 1771/7702 - Train loss: 0.00190913  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.34 min) Epoch 53/300 -- Iteration 402352 - Batch 1848/7702 - Train loss: 0.00190912  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.36 min) Epoch 53/300 -- Iteration 402429 - Batch 1925/7702 - Train loss: 0.00191025  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.38 min) Epoch 53/300 -- Iteration 402506 - Batch 2002/7702 - Train loss: 0.00191107  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.40 min) Epoch 53/300 -- Iteration 402583 - Batch 2079/7702 - Train loss: 0.00191037  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.42 min) Epoch 53/300 -- Iteration 402660 - Batch 2156/7702 - Train loss: 0.00191136  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.44 min) Epoch 53/300 -- Iteration 402737 - Batch 2233/7702 - Train loss: 0.00190965  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.46 min) Epoch 53/300 -- Iteration 402814 - Batch 2310/7702 - Train loss: 0.00190877  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.48 min) Epoch 53/300 -- Iteration 402891 - Batch 2387/7702 - Train loss: 0.00190892  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.50 min) Epoch 53/300 -- Iteration 402968 - Batch 2464/7702 - Train loss: 0.00190897  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.52 min) Epoch 53/300 -- Iteration 403045 - Batch 2541/7702 - Train loss: 0.00190937  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.55 min) Epoch 53/300 -- Iteration 403122 - Batch 2618/7702 - Train loss: 0.00190858  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.57 min) Epoch 53/300 -- Iteration 403199 - Batch 2695/7702 - Train loss: 0.00190792  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.59 min) Epoch 53/300 -- Iteration 403276 - Batch 2772/7702 - Train loss: 0.00190746  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.61 min) Epoch 53/300 -- Iteration 403353 - Batch 2849/7702 - Train loss: 0.00190859  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.63 min) Epoch 53/300 -- Iteration 403430 - Batch 2926/7702 - Train loss: 0.00190850  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.65 min) Epoch 53/300 -- Iteration 403507 - Batch 3003/7702 - Train loss: 0.00190974  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.67 min) Epoch 53/300 -- Iteration 403584 - Batch 3080/7702 - Train loss: 0.00191074  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.69 min) Epoch 53/300 -- Iteration 403661 - Batch 3157/7702 - Train loss: 0.00191023  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.71 min) Epoch 53/300 -- Iteration 403738 - Batch 3234/7702 - Train loss: 0.00191117  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.73 min) Epoch 53/300 -- Iteration 403815 - Batch 3311/7702 - Train loss: 0.00191051  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.75 min) Epoch 53/300 -- Iteration 403892 - Batch 3388/7702 - Train loss: 0.00191049  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.77 min) Epoch 53/300 -- Iteration 403969 - Batch 3465/7702 - Train loss: 0.00191050  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.79 min) Epoch 53/300 -- Iteration 404046 - Batch 3542/7702 - Train loss: 0.00190970  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.81 min) Epoch 53/300 -- Iteration 404123 - Batch 3619/7702 - Train loss: 0.00190951  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.84 min) Epoch 53/300 -- Iteration 404200 - Batch 3696/7702 - Train loss: 0.00190996  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.86 min) Epoch 53/300 -- Iteration 404277 - Batch 3773/7702 - Train loss: 0.00190947  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.88 min) Epoch 53/300 -- Iteration 404354 - Batch 3850/7702 - Train loss: 0.00190988  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.90 min) Epoch 53/300 -- Iteration 404431 - Batch 3927/7702 - Train loss: 0.00190946  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.92 min) Epoch 53/300 -- Iteration 404508 - Batch 4004/7702 - Train loss: 0.00191026  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.94 min) Epoch 53/300 -- Iteration 404585 - Batch 4081/7702 - Train loss: 0.00191043  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.96 min) Epoch 53/300 -- Iteration 404662 - Batch 4158/7702 - Train loss: 0.00190984  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(108.98 min) Epoch 53/300 -- Iteration 404739 - Batch 4235/7702 - Train loss: 0.00190931  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.00 min) Epoch 53/300 -- Iteration 404816 - Batch 4312/7702 - Train loss: 0.00190857  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.02 min) Epoch 53/300 -- Iteration 404893 - Batch 4389/7702 - Train loss: 0.00190885  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.04 min) Epoch 53/300 -- Iteration 404970 - Batch 4466/7702 - Train loss: 0.00190840  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.06 min) Epoch 53/300 -- Iteration 405047 - Batch 4543/7702 - Train loss: 0.00190852  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.08 min) Epoch 53/300 -- Iteration 405124 - Batch 4620/7702 - Train loss: 0.00190791  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.10 min) Epoch 53/300 -- Iteration 405201 - Batch 4697/7702 - Train loss: 0.00190833  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.13 min) Epoch 53/300 -- Iteration 405278 - Batch 4774/7702 - Train loss: 0.00190820  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.15 min) Epoch 53/300 -- Iteration 405355 - Batch 4851/7702 - Train loss: 0.00190807  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.17 min) Epoch 53/300 -- Iteration 405432 - Batch 4928/7702 - Train loss: 0.00190812  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.19 min) Epoch 53/300 -- Iteration 405509 - Batch 5005/7702 - Train loss: 0.00190776  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.21 min) Epoch 53/300 -- Iteration 405586 - Batch 5082/7702 - Train loss: 0.00190823  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.23 min) Epoch 53/300 -- Iteration 405663 - Batch 5159/7702 - Train loss: 0.00190854  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.25 min) Epoch 53/300 -- Iteration 405740 - Batch 5236/7702 - Train loss: 0.00190860  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.27 min) Epoch 53/300 -- Iteration 405817 - Batch 5313/7702 - Train loss: 0.00190814  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.29 min) Epoch 53/300 -- Iteration 405894 - Batch 5390/7702 - Train loss: 0.00190776  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.31 min) Epoch 53/300 -- Iteration 405971 - Batch 5467/7702 - Train loss: 0.00190837  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.33 min) Epoch 53/300 -- Iteration 406048 - Batch 5544/7702 - Train loss: 0.00190775  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.35 min) Epoch 53/300 -- Iteration 406125 - Batch 5621/7702 - Train loss: 0.00190824  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.37 min) Epoch 53/300 -- Iteration 406202 - Batch 5698/7702 - Train loss: 0.00190871  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.40 min) Epoch 53/300 -- Iteration 406279 - Batch 5775/7702 - Train loss: 0.00190894  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.42 min) Epoch 53/300 -- Iteration 406356 - Batch 5852/7702 - Train loss: 0.00190909  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.44 min) Epoch 53/300 -- Iteration 406433 - Batch 5929/7702 - Train loss: 0.00190890  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.46 min) Epoch 53/300 -- Iteration 406510 - Batch 6006/7702 - Train loss: 0.00190903  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.48 min) Epoch 53/300 -- Iteration 406587 - Batch 6083/7702 - Train loss: 0.00190923  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.50 min) Epoch 53/300 -- Iteration 406664 - Batch 6160/7702 - Train loss: 0.00190923  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.52 min) Epoch 53/300 -- Iteration 406741 - Batch 6237/7702 - Train loss: 0.00190912  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.54 min) Epoch 53/300 -- Iteration 406818 - Batch 6314/7702 - Train loss: 0.00190939  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.56 min) Epoch 53/300 -- Iteration 406895 - Batch 6391/7702 - Train loss: 0.00190951  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.58 min) Epoch 53/300 -- Iteration 406972 - Batch 6468/7702 - Train loss: 0.00190914  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.60 min) Epoch 53/300 -- Iteration 407049 - Batch 6545/7702 - Train loss: 0.00190918  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.62 min) Epoch 53/300 -- Iteration 407126 - Batch 6622/7702 - Train loss: 0.00190909  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.64 min) Epoch 53/300 -- Iteration 407203 - Batch 6699/7702 - Train loss: 0.00190940  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.66 min) Epoch 53/300 -- Iteration 407280 - Batch 6776/7702 - Train loss: 0.00190990  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.69 min) Epoch 53/300 -- Iteration 407357 - Batch 6853/7702 - Train loss: 0.00190996  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.71 min) Epoch 53/300 -- Iteration 407434 - Batch 6930/7702 - Train loss: 0.00190957  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.73 min) Epoch 53/300 -- Iteration 407511 - Batch 7007/7702 - Train loss: 0.00190925  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.75 min) Epoch 53/300 -- Iteration 407588 - Batch 7084/7702 - Train loss: 0.00190941  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.77 min) Epoch 53/300 -- Iteration 407665 - Batch 7161/7702 - Train loss: 0.00190925  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.79 min) Epoch 53/300 -- Iteration 407742 - Batch 7238/7702 - Train loss: 0.00190957  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.81 min) Epoch 53/300 -- Iteration 407819 - Batch 7315/7702 - Train loss: 0.00190960  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.83 min) Epoch 53/300 -- Iteration 407896 - Batch 7392/7702 - Train loss: 0.00190970  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.85 min) Epoch 53/300 -- Iteration 407973 - Batch 7469/7702 - Train loss: 0.00190979  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.87 min) Epoch 53/300 -- Iteration 408050 - Batch 7546/7702 - Train loss: 0.00190980  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.89 min) Epoch 53/300 -- Iteration 408127 - Batch 7623/7702 - Train loss: 0.00191005  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.91 min) Epoch 53/300 -- Iteration 408204 - Batch 7700/7702 - Train loss: 0.00190963  - Train acc: -0.0000 - Val loss: 0.00045714\n",
      "(109.91 min) Epoch 53/300 -- Iteration 408206 - Batch 7701/7702 - Train loss: 0.00190966  - Train acc: -0.0000 - Val loss: 0.00037786 - Val acc: -0.0000\n",
      "(109.94 min) Epoch 54/300 -- Iteration 408283 - Batch 77/7702 - Train loss: 0.00192912  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(109.96 min) Epoch 54/300 -- Iteration 408360 - Batch 154/7702 - Train loss: 0.00191874  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(109.98 min) Epoch 54/300 -- Iteration 408437 - Batch 231/7702 - Train loss: 0.00189613  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.00 min) Epoch 54/300 -- Iteration 408514 - Batch 308/7702 - Train loss: 0.00189967  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.02 min) Epoch 54/300 -- Iteration 408591 - Batch 385/7702 - Train loss: 0.00189829  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.04 min) Epoch 54/300 -- Iteration 408668 - Batch 462/7702 - Train loss: 0.00190145  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.06 min) Epoch 54/300 -- Iteration 408745 - Batch 539/7702 - Train loss: 0.00190094  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.08 min) Epoch 54/300 -- Iteration 408822 - Batch 616/7702 - Train loss: 0.00190216  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.10 min) Epoch 54/300 -- Iteration 408899 - Batch 693/7702 - Train loss: 0.00190247  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.12 min) Epoch 54/300 -- Iteration 408976 - Batch 770/7702 - Train loss: 0.00190553  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.14 min) Epoch 54/300 -- Iteration 409053 - Batch 847/7702 - Train loss: 0.00190843  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.16 min) Epoch 54/300 -- Iteration 409130 - Batch 924/7702 - Train loss: 0.00190955  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.19 min) Epoch 54/300 -- Iteration 409207 - Batch 1001/7702 - Train loss: 0.00190823  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.21 min) Epoch 54/300 -- Iteration 409284 - Batch 1078/7702 - Train loss: 0.00191000  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.23 min) Epoch 54/300 -- Iteration 409361 - Batch 1155/7702 - Train loss: 0.00191138  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.25 min) Epoch 54/300 -- Iteration 409438 - Batch 1232/7702 - Train loss: 0.00190992  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.27 min) Epoch 54/300 -- Iteration 409515 - Batch 1309/7702 - Train loss: 0.00191061  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.29 min) Epoch 54/300 -- Iteration 409592 - Batch 1386/7702 - Train loss: 0.00191142  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.31 min) Epoch 54/300 -- Iteration 409669 - Batch 1463/7702 - Train loss: 0.00191032  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.33 min) Epoch 54/300 -- Iteration 409746 - Batch 1540/7702 - Train loss: 0.00190908  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.35 min) Epoch 54/300 -- Iteration 409823 - Batch 1617/7702 - Train loss: 0.00190995  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.37 min) Epoch 54/300 -- Iteration 409900 - Batch 1694/7702 - Train loss: 0.00190979  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.39 min) Epoch 54/300 -- Iteration 409977 - Batch 1771/7702 - Train loss: 0.00191084  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.41 min) Epoch 54/300 -- Iteration 410054 - Batch 1848/7702 - Train loss: 0.00191099  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.43 min) Epoch 54/300 -- Iteration 410131 - Batch 1925/7702 - Train loss: 0.00190973  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.45 min) Epoch 54/300 -- Iteration 410208 - Batch 2002/7702 - Train loss: 0.00191004  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.48 min) Epoch 54/300 -- Iteration 410285 - Batch 2079/7702 - Train loss: 0.00191032  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.50 min) Epoch 54/300 -- Iteration 410362 - Batch 2156/7702 - Train loss: 0.00191082  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.52 min) Epoch 54/300 -- Iteration 410439 - Batch 2233/7702 - Train loss: 0.00191006  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.54 min) Epoch 54/300 -- Iteration 410516 - Batch 2310/7702 - Train loss: 0.00191066  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.56 min) Epoch 54/300 -- Iteration 410593 - Batch 2387/7702 - Train loss: 0.00191058  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.58 min) Epoch 54/300 -- Iteration 410670 - Batch 2464/7702 - Train loss: 0.00191027  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.60 min) Epoch 54/300 -- Iteration 410747 - Batch 2541/7702 - Train loss: 0.00191086  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.62 min) Epoch 54/300 -- Iteration 410824 - Batch 2618/7702 - Train loss: 0.00191034  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.64 min) Epoch 54/300 -- Iteration 410901 - Batch 2695/7702 - Train loss: 0.00191055  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.66 min) Epoch 54/300 -- Iteration 410978 - Batch 2772/7702 - Train loss: 0.00191063  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.68 min) Epoch 54/300 -- Iteration 411055 - Batch 2849/7702 - Train loss: 0.00190964  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.70 min) Epoch 54/300 -- Iteration 411132 - Batch 2926/7702 - Train loss: 0.00190989  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.72 min) Epoch 54/300 -- Iteration 411209 - Batch 3003/7702 - Train loss: 0.00190946  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.74 min) Epoch 54/300 -- Iteration 411286 - Batch 3080/7702 - Train loss: 0.00190916  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.77 min) Epoch 54/300 -- Iteration 411363 - Batch 3157/7702 - Train loss: 0.00190971  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.79 min) Epoch 54/300 -- Iteration 411440 - Batch 3234/7702 - Train loss: 0.00190958  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.81 min) Epoch 54/300 -- Iteration 411517 - Batch 3311/7702 - Train loss: 0.00190944  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.83 min) Epoch 54/300 -- Iteration 411594 - Batch 3388/7702 - Train loss: 0.00190939  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.85 min) Epoch 54/300 -- Iteration 411671 - Batch 3465/7702 - Train loss: 0.00190959  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.87 min) Epoch 54/300 -- Iteration 411748 - Batch 3542/7702 - Train loss: 0.00190923  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.89 min) Epoch 54/300 -- Iteration 411825 - Batch 3619/7702 - Train loss: 0.00190868  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.91 min) Epoch 54/300 -- Iteration 411902 - Batch 3696/7702 - Train loss: 0.00190861  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.93 min) Epoch 54/300 -- Iteration 411979 - Batch 3773/7702 - Train loss: 0.00190852  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.95 min) Epoch 54/300 -- Iteration 412056 - Batch 3850/7702 - Train loss: 0.00190829  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.97 min) Epoch 54/300 -- Iteration 412133 - Batch 3927/7702 - Train loss: 0.00190744  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(110.99 min) Epoch 54/300 -- Iteration 412210 - Batch 4004/7702 - Train loss: 0.00190749  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.01 min) Epoch 54/300 -- Iteration 412287 - Batch 4081/7702 - Train loss: 0.00190729  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.03 min) Epoch 54/300 -- Iteration 412364 - Batch 4158/7702 - Train loss: 0.00190750  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.06 min) Epoch 54/300 -- Iteration 412441 - Batch 4235/7702 - Train loss: 0.00190800  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.08 min) Epoch 54/300 -- Iteration 412518 - Batch 4312/7702 - Train loss: 0.00190770  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.10 min) Epoch 54/300 -- Iteration 412595 - Batch 4389/7702 - Train loss: 0.00190769  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.12 min) Epoch 54/300 -- Iteration 412672 - Batch 4466/7702 - Train loss: 0.00190687  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.14 min) Epoch 54/300 -- Iteration 412749 - Batch 4543/7702 - Train loss: 0.00190699  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.16 min) Epoch 54/300 -- Iteration 412826 - Batch 4620/7702 - Train loss: 0.00190681  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.18 min) Epoch 54/300 -- Iteration 412903 - Batch 4697/7702 - Train loss: 0.00190714  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.20 min) Epoch 54/300 -- Iteration 412980 - Batch 4774/7702 - Train loss: 0.00190756  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.22 min) Epoch 54/300 -- Iteration 413057 - Batch 4851/7702 - Train loss: 0.00190768  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.24 min) Epoch 54/300 -- Iteration 413134 - Batch 4928/7702 - Train loss: 0.00190763  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.26 min) Epoch 54/300 -- Iteration 413211 - Batch 5005/7702 - Train loss: 0.00190802  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.28 min) Epoch 54/300 -- Iteration 413288 - Batch 5082/7702 - Train loss: 0.00190751  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.30 min) Epoch 54/300 -- Iteration 413365 - Batch 5159/7702 - Train loss: 0.00190780  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.32 min) Epoch 54/300 -- Iteration 413442 - Batch 5236/7702 - Train loss: 0.00190851  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.34 min) Epoch 54/300 -- Iteration 413519 - Batch 5313/7702 - Train loss: 0.00190826  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.37 min) Epoch 54/300 -- Iteration 413596 - Batch 5390/7702 - Train loss: 0.00190842  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.39 min) Epoch 54/300 -- Iteration 413673 - Batch 5467/7702 - Train loss: 0.00190885  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.41 min) Epoch 54/300 -- Iteration 413750 - Batch 5544/7702 - Train loss: 0.00190840  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.43 min) Epoch 54/300 -- Iteration 413827 - Batch 5621/7702 - Train loss: 0.00190890  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.45 min) Epoch 54/300 -- Iteration 413904 - Batch 5698/7702 - Train loss: 0.00190874  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.47 min) Epoch 54/300 -- Iteration 413981 - Batch 5775/7702 - Train loss: 0.00190895  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.49 min) Epoch 54/300 -- Iteration 414058 - Batch 5852/7702 - Train loss: 0.00190879  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.51 min) Epoch 54/300 -- Iteration 414135 - Batch 5929/7702 - Train loss: 0.00190855  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.53 min) Epoch 54/300 -- Iteration 414212 - Batch 6006/7702 - Train loss: 0.00190890  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.55 min) Epoch 54/300 -- Iteration 414289 - Batch 6083/7702 - Train loss: 0.00190914  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.57 min) Epoch 54/300 -- Iteration 414366 - Batch 6160/7702 - Train loss: 0.00190926  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.59 min) Epoch 54/300 -- Iteration 414443 - Batch 6237/7702 - Train loss: 0.00190889  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.61 min) Epoch 54/300 -- Iteration 414520 - Batch 6314/7702 - Train loss: 0.00190902  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.63 min) Epoch 54/300 -- Iteration 414597 - Batch 6391/7702 - Train loss: 0.00190886  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.66 min) Epoch 54/300 -- Iteration 414674 - Batch 6468/7702 - Train loss: 0.00190861  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.68 min) Epoch 54/300 -- Iteration 414751 - Batch 6545/7702 - Train loss: 0.00190839  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.70 min) Epoch 54/300 -- Iteration 414828 - Batch 6622/7702 - Train loss: 0.00190825  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.72 min) Epoch 54/300 -- Iteration 414905 - Batch 6699/7702 - Train loss: 0.00190829  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.74 min) Epoch 54/300 -- Iteration 414982 - Batch 6776/7702 - Train loss: 0.00190806  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.76 min) Epoch 54/300 -- Iteration 415059 - Batch 6853/7702 - Train loss: 0.00190814  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.78 min) Epoch 54/300 -- Iteration 415136 - Batch 6930/7702 - Train loss: 0.00190762  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.80 min) Epoch 54/300 -- Iteration 415213 - Batch 7007/7702 - Train loss: 0.00190777  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.82 min) Epoch 54/300 -- Iteration 415290 - Batch 7084/7702 - Train loss: 0.00190752  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.84 min) Epoch 54/300 -- Iteration 415367 - Batch 7161/7702 - Train loss: 0.00190769  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.86 min) Epoch 54/300 -- Iteration 415444 - Batch 7238/7702 - Train loss: 0.00190747  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.88 min) Epoch 54/300 -- Iteration 415521 - Batch 7315/7702 - Train loss: 0.00190697  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.90 min) Epoch 54/300 -- Iteration 415598 - Batch 7392/7702 - Train loss: 0.00190708  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.92 min) Epoch 54/300 -- Iteration 415675 - Batch 7469/7702 - Train loss: 0.00190716  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.95 min) Epoch 54/300 -- Iteration 415752 - Batch 7546/7702 - Train loss: 0.00190737  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.97 min) Epoch 54/300 -- Iteration 415829 - Batch 7623/7702 - Train loss: 0.00190753  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.99 min) Epoch 54/300 -- Iteration 415906 - Batch 7700/7702 - Train loss: 0.00190723  - Train acc: -0.0000 - Val loss: 0.00037786\n",
      "(111.99 min) Epoch 54/300 -- Iteration 415908 - Batch 7701/7702 - Train loss: 0.00190722  - Train acc: -0.0000 - Val loss: 0.00045854 - Val acc: -0.0000\n",
      "(112.01 min) Epoch 55/300 -- Iteration 415985 - Batch 77/7702 - Train loss: 0.00192004  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.03 min) Epoch 55/300 -- Iteration 416062 - Batch 154/7702 - Train loss: 0.00192034  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.05 min) Epoch 55/300 -- Iteration 416139 - Batch 231/7702 - Train loss: 0.00190603  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.07 min) Epoch 55/300 -- Iteration 416216 - Batch 308/7702 - Train loss: 0.00190931  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.09 min) Epoch 55/300 -- Iteration 416293 - Batch 385/7702 - Train loss: 0.00191486  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.11 min) Epoch 55/300 -- Iteration 416370 - Batch 462/7702 - Train loss: 0.00191589  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.13 min) Epoch 55/300 -- Iteration 416447 - Batch 539/7702 - Train loss: 0.00191721  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.15 min) Epoch 55/300 -- Iteration 416524 - Batch 616/7702 - Train loss: 0.00191532  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.18 min) Epoch 55/300 -- Iteration 416601 - Batch 693/7702 - Train loss: 0.00191440  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.20 min) Epoch 55/300 -- Iteration 416678 - Batch 770/7702 - Train loss: 0.00190964  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.22 min) Epoch 55/300 -- Iteration 416755 - Batch 847/7702 - Train loss: 0.00191047  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.24 min) Epoch 55/300 -- Iteration 416832 - Batch 924/7702 - Train loss: 0.00191238  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.26 min) Epoch 55/300 -- Iteration 416909 - Batch 1001/7702 - Train loss: 0.00191181  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.28 min) Epoch 55/300 -- Iteration 416986 - Batch 1078/7702 - Train loss: 0.00191378  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.30 min) Epoch 55/300 -- Iteration 417063 - Batch 1155/7702 - Train loss: 0.00191215  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.32 min) Epoch 55/300 -- Iteration 417140 - Batch 1232/7702 - Train loss: 0.00191076  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.34 min) Epoch 55/300 -- Iteration 417217 - Batch 1309/7702 - Train loss: 0.00190881  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.36 min) Epoch 55/300 -- Iteration 417294 - Batch 1386/7702 - Train loss: 0.00190948  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.38 min) Epoch 55/300 -- Iteration 417371 - Batch 1463/7702 - Train loss: 0.00190832  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.40 min) Epoch 55/300 -- Iteration 417448 - Batch 1540/7702 - Train loss: 0.00190719  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.42 min) Epoch 55/300 -- Iteration 417525 - Batch 1617/7702 - Train loss: 0.00190565  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.44 min) Epoch 55/300 -- Iteration 417602 - Batch 1694/7702 - Train loss: 0.00190759  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.47 min) Epoch 55/300 -- Iteration 417679 - Batch 1771/7702 - Train loss: 0.00190777  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.49 min) Epoch 55/300 -- Iteration 417756 - Batch 1848/7702 - Train loss: 0.00190781  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.51 min) Epoch 55/300 -- Iteration 417833 - Batch 1925/7702 - Train loss: 0.00190864  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.53 min) Epoch 55/300 -- Iteration 417910 - Batch 2002/7702 - Train loss: 0.00190737  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.55 min) Epoch 55/300 -- Iteration 417987 - Batch 2079/7702 - Train loss: 0.00190611  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.57 min) Epoch 55/300 -- Iteration 418064 - Batch 2156/7702 - Train loss: 0.00190676  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.59 min) Epoch 55/300 -- Iteration 418141 - Batch 2233/7702 - Train loss: 0.00190767  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.61 min) Epoch 55/300 -- Iteration 418218 - Batch 2310/7702 - Train loss: 0.00190801  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.63 min) Epoch 55/300 -- Iteration 418295 - Batch 2387/7702 - Train loss: 0.00190889  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.65 min) Epoch 55/300 -- Iteration 418372 - Batch 2464/7702 - Train loss: 0.00190883  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.67 min) Epoch 55/300 -- Iteration 418449 - Batch 2541/7702 - Train loss: 0.00190808  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.69 min) Epoch 55/300 -- Iteration 418526 - Batch 2618/7702 - Train loss: 0.00190842  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.71 min) Epoch 55/300 -- Iteration 418603 - Batch 2695/7702 - Train loss: 0.00190830  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.74 min) Epoch 55/300 -- Iteration 418680 - Batch 2772/7702 - Train loss: 0.00190788  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.76 min) Epoch 55/300 -- Iteration 418757 - Batch 2849/7702 - Train loss: 0.00190791  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.78 min) Epoch 55/300 -- Iteration 418834 - Batch 2926/7702 - Train loss: 0.00190778  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.80 min) Epoch 55/300 -- Iteration 418911 - Batch 3003/7702 - Train loss: 0.00190825  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.82 min) Epoch 55/300 -- Iteration 418988 - Batch 3080/7702 - Train loss: 0.00190789  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.84 min) Epoch 55/300 -- Iteration 419065 - Batch 3157/7702 - Train loss: 0.00190769  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.86 min) Epoch 55/300 -- Iteration 419142 - Batch 3234/7702 - Train loss: 0.00190884  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.88 min) Epoch 55/300 -- Iteration 419219 - Batch 3311/7702 - Train loss: 0.00190797  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.90 min) Epoch 55/300 -- Iteration 419296 - Batch 3388/7702 - Train loss: 0.00190748  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.92 min) Epoch 55/300 -- Iteration 419373 - Batch 3465/7702 - Train loss: 0.00190703  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.94 min) Epoch 55/300 -- Iteration 419450 - Batch 3542/7702 - Train loss: 0.00190684  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.96 min) Epoch 55/300 -- Iteration 419527 - Batch 3619/7702 - Train loss: 0.00190615  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(112.98 min) Epoch 55/300 -- Iteration 419604 - Batch 3696/7702 - Train loss: 0.00190641  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.00 min) Epoch 55/300 -- Iteration 419681 - Batch 3773/7702 - Train loss: 0.00190678  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.03 min) Epoch 55/300 -- Iteration 419758 - Batch 3850/7702 - Train loss: 0.00190708  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.05 min) Epoch 55/300 -- Iteration 419835 - Batch 3927/7702 - Train loss: 0.00190726  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.07 min) Epoch 55/300 -- Iteration 419912 - Batch 4004/7702 - Train loss: 0.00190801  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.09 min) Epoch 55/300 -- Iteration 419989 - Batch 4081/7702 - Train loss: 0.00190805  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.11 min) Epoch 55/300 -- Iteration 420066 - Batch 4158/7702 - Train loss: 0.00190703  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.13 min) Epoch 55/300 -- Iteration 420143 - Batch 4235/7702 - Train loss: 0.00190701  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.15 min) Epoch 55/300 -- Iteration 420220 - Batch 4312/7702 - Train loss: 0.00190691  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.17 min) Epoch 55/300 -- Iteration 420297 - Batch 4389/7702 - Train loss: 0.00190750  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.19 min) Epoch 55/300 -- Iteration 420374 - Batch 4466/7702 - Train loss: 0.00190743  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.21 min) Epoch 55/300 -- Iteration 420451 - Batch 4543/7702 - Train loss: 0.00190732  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.23 min) Epoch 55/300 -- Iteration 420528 - Batch 4620/7702 - Train loss: 0.00190716  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.25 min) Epoch 55/300 -- Iteration 420605 - Batch 4697/7702 - Train loss: 0.00190718  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.27 min) Epoch 55/300 -- Iteration 420682 - Batch 4774/7702 - Train loss: 0.00190746  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.29 min) Epoch 55/300 -- Iteration 420759 - Batch 4851/7702 - Train loss: 0.00190769  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.32 min) Epoch 55/300 -- Iteration 420836 - Batch 4928/7702 - Train loss: 0.00190865  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.34 min) Epoch 55/300 -- Iteration 420913 - Batch 5005/7702 - Train loss: 0.00190811  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.36 min) Epoch 55/300 -- Iteration 420990 - Batch 5082/7702 - Train loss: 0.00190770  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.38 min) Epoch 55/300 -- Iteration 421067 - Batch 5159/7702 - Train loss: 0.00190764  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.40 min) Epoch 55/300 -- Iteration 421144 - Batch 5236/7702 - Train loss: 0.00190746  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.42 min) Epoch 55/300 -- Iteration 421221 - Batch 5313/7702 - Train loss: 0.00190746  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.44 min) Epoch 55/300 -- Iteration 421298 - Batch 5390/7702 - Train loss: 0.00190711  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.46 min) Epoch 55/300 -- Iteration 421375 - Batch 5467/7702 - Train loss: 0.00190662  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.48 min) Epoch 55/300 -- Iteration 421452 - Batch 5544/7702 - Train loss: 0.00190877  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.50 min) Epoch 55/300 -- Iteration 421529 - Batch 5621/7702 - Train loss: 0.00190988  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.52 min) Epoch 55/300 -- Iteration 421606 - Batch 5698/7702 - Train loss: 0.00191001  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.54 min) Epoch 55/300 -- Iteration 421683 - Batch 5775/7702 - Train loss: 0.00191015  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.56 min) Epoch 55/300 -- Iteration 421760 - Batch 5852/7702 - Train loss: 0.00190977  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.59 min) Epoch 55/300 -- Iteration 421837 - Batch 5929/7702 - Train loss: 0.00190960  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.61 min) Epoch 55/300 -- Iteration 421914 - Batch 6006/7702 - Train loss: 0.00190979  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.63 min) Epoch 55/300 -- Iteration 421991 - Batch 6083/7702 - Train loss: 0.00190963  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.65 min) Epoch 55/300 -- Iteration 422068 - Batch 6160/7702 - Train loss: 0.00190974  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.67 min) Epoch 55/300 -- Iteration 422145 - Batch 6237/7702 - Train loss: 0.00190978  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.69 min) Epoch 55/300 -- Iteration 422222 - Batch 6314/7702 - Train loss: 0.00190929  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.71 min) Epoch 55/300 -- Iteration 422299 - Batch 6391/7702 - Train loss: 0.00190902  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.73 min) Epoch 55/300 -- Iteration 422376 - Batch 6468/7702 - Train loss: 0.00190888  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.75 min) Epoch 55/300 -- Iteration 422453 - Batch 6545/7702 - Train loss: 0.00190883  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.77 min) Epoch 55/300 -- Iteration 422530 - Batch 6622/7702 - Train loss: 0.00190871  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.79 min) Epoch 55/300 -- Iteration 422607 - Batch 6699/7702 - Train loss: 0.00190865  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.81 min) Epoch 55/300 -- Iteration 422684 - Batch 6776/7702 - Train loss: 0.00190835  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.83 min) Epoch 55/300 -- Iteration 422761 - Batch 6853/7702 - Train loss: 0.00190817  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.85 min) Epoch 55/300 -- Iteration 422838 - Batch 6930/7702 - Train loss: 0.00190825  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.88 min) Epoch 55/300 -- Iteration 422915 - Batch 7007/7702 - Train loss: 0.00190806  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.90 min) Epoch 55/300 -- Iteration 422992 - Batch 7084/7702 - Train loss: 0.00190839  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.92 min) Epoch 55/300 -- Iteration 423069 - Batch 7161/7702 - Train loss: 0.00190837  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.94 min) Epoch 55/300 -- Iteration 423146 - Batch 7238/7702 - Train loss: 0.00190868  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.96 min) Epoch 55/300 -- Iteration 423223 - Batch 7315/7702 - Train loss: 0.00190877  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(113.98 min) Epoch 55/300 -- Iteration 423300 - Batch 7392/7702 - Train loss: 0.00190870  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(114.00 min) Epoch 55/300 -- Iteration 423377 - Batch 7469/7702 - Train loss: 0.00190877  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(114.02 min) Epoch 55/300 -- Iteration 423454 - Batch 7546/7702 - Train loss: 0.00190871  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(114.04 min) Epoch 55/300 -- Iteration 423531 - Batch 7623/7702 - Train loss: 0.00190875  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(114.06 min) Epoch 55/300 -- Iteration 423608 - Batch 7700/7702 - Train loss: 0.00190875  - Train acc: -0.0000 - Val loss: 0.00045854\n",
      "(114.06 min) Epoch 55/300 -- Iteration 423610 - Batch 7701/7702 - Train loss: 0.00190874  - Train acc: -0.0000 - Val loss: 0.00038125 - Val acc: -0.0000\n",
      "(114.09 min) Epoch 56/300 -- Iteration 423687 - Batch 77/7702 - Train loss: 0.00187769  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.11 min) Epoch 56/300 -- Iteration 423764 - Batch 154/7702 - Train loss: 0.00187684  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.13 min) Epoch 56/300 -- Iteration 423841 - Batch 231/7702 - Train loss: 0.00188118  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.15 min) Epoch 56/300 -- Iteration 423918 - Batch 308/7702 - Train loss: 0.00188613  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.17 min) Epoch 56/300 -- Iteration 423995 - Batch 385/7702 - Train loss: 0.00188595  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.19 min) Epoch 56/300 -- Iteration 424072 - Batch 462/7702 - Train loss: 0.00189307  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.21 min) Epoch 56/300 -- Iteration 424149 - Batch 539/7702 - Train loss: 0.00189412  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.23 min) Epoch 56/300 -- Iteration 424226 - Batch 616/7702 - Train loss: 0.00189546  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.25 min) Epoch 56/300 -- Iteration 424303 - Batch 693/7702 - Train loss: 0.00189423  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.27 min) Epoch 56/300 -- Iteration 424380 - Batch 770/7702 - Train loss: 0.00189511  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.29 min) Epoch 56/300 -- Iteration 424457 - Batch 847/7702 - Train loss: 0.00189946  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.31 min) Epoch 56/300 -- Iteration 424534 - Batch 924/7702 - Train loss: 0.00189898  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.33 min) Epoch 56/300 -- Iteration 424611 - Batch 1001/7702 - Train loss: 0.00189850  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.35 min) Epoch 56/300 -- Iteration 424688 - Batch 1078/7702 - Train loss: 0.00190125  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.38 min) Epoch 56/300 -- Iteration 424765 - Batch 1155/7702 - Train loss: 0.00190294  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.40 min) Epoch 56/300 -- Iteration 424842 - Batch 1232/7702 - Train loss: 0.00190310  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.42 min) Epoch 56/300 -- Iteration 424919 - Batch 1309/7702 - Train loss: 0.00190264  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.44 min) Epoch 56/300 -- Iteration 424996 - Batch 1386/7702 - Train loss: 0.00190193  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.46 min) Epoch 56/300 -- Iteration 425073 - Batch 1463/7702 - Train loss: 0.00190155  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.48 min) Epoch 56/300 -- Iteration 425150 - Batch 1540/7702 - Train loss: 0.00190266  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.50 min) Epoch 56/300 -- Iteration 425227 - Batch 1617/7702 - Train loss: 0.00190152  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.52 min) Epoch 56/300 -- Iteration 425304 - Batch 1694/7702 - Train loss: 0.00190031  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.54 min) Epoch 56/300 -- Iteration 425381 - Batch 1771/7702 - Train loss: 0.00190058  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.56 min) Epoch 56/300 -- Iteration 425458 - Batch 1848/7702 - Train loss: 0.00190182  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.58 min) Epoch 56/300 -- Iteration 425535 - Batch 1925/7702 - Train loss: 0.00190152  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.60 min) Epoch 56/300 -- Iteration 425612 - Batch 2002/7702 - Train loss: 0.00190229  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.62 min) Epoch 56/300 -- Iteration 425689 - Batch 2079/7702 - Train loss: 0.00190223  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.65 min) Epoch 56/300 -- Iteration 425766 - Batch 2156/7702 - Train loss: 0.00190247  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.67 min) Epoch 56/300 -- Iteration 425843 - Batch 2233/7702 - Train loss: 0.00190242  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.69 min) Epoch 56/300 -- Iteration 425920 - Batch 2310/7702 - Train loss: 0.00190339  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.71 min) Epoch 56/300 -- Iteration 425997 - Batch 2387/7702 - Train loss: 0.00190364  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.73 min) Epoch 56/300 -- Iteration 426074 - Batch 2464/7702 - Train loss: 0.00190422  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.75 min) Epoch 56/300 -- Iteration 426151 - Batch 2541/7702 - Train loss: 0.00190468  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.77 min) Epoch 56/300 -- Iteration 426228 - Batch 2618/7702 - Train loss: 0.00190484  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.79 min) Epoch 56/300 -- Iteration 426305 - Batch 2695/7702 - Train loss: 0.00190564  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.81 min) Epoch 56/300 -- Iteration 426382 - Batch 2772/7702 - Train loss: 0.00190524  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.83 min) Epoch 56/300 -- Iteration 426459 - Batch 2849/7702 - Train loss: 0.00190542  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.85 min) Epoch 56/300 -- Iteration 426536 - Batch 2926/7702 - Train loss: 0.00190557  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.87 min) Epoch 56/300 -- Iteration 426613 - Batch 3003/7702 - Train loss: 0.00190582  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.89 min) Epoch 56/300 -- Iteration 426690 - Batch 3080/7702 - Train loss: 0.00190594  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.91 min) Epoch 56/300 -- Iteration 426767 - Batch 3157/7702 - Train loss: 0.00190560  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.93 min) Epoch 56/300 -- Iteration 426844 - Batch 3234/7702 - Train loss: 0.00190609  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.96 min) Epoch 56/300 -- Iteration 426921 - Batch 3311/7702 - Train loss: 0.00190525  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(114.98 min) Epoch 56/300 -- Iteration 426998 - Batch 3388/7702 - Train loss: 0.00190521  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.00 min) Epoch 56/300 -- Iteration 427075 - Batch 3465/7702 - Train loss: 0.00190497  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.02 min) Epoch 56/300 -- Iteration 427152 - Batch 3542/7702 - Train loss: 0.00190499  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.04 min) Epoch 56/300 -- Iteration 427229 - Batch 3619/7702 - Train loss: 0.00190471  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.06 min) Epoch 56/300 -- Iteration 427306 - Batch 3696/7702 - Train loss: 0.00190441  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.08 min) Epoch 56/300 -- Iteration 427383 - Batch 3773/7702 - Train loss: 0.00190419  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.10 min) Epoch 56/300 -- Iteration 427460 - Batch 3850/7702 - Train loss: 0.00190434  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.12 min) Epoch 56/300 -- Iteration 427537 - Batch 3927/7702 - Train loss: 0.00190407  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.14 min) Epoch 56/300 -- Iteration 427614 - Batch 4004/7702 - Train loss: 0.00190465  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.16 min) Epoch 56/300 -- Iteration 427691 - Batch 4081/7702 - Train loss: 0.00190492  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.18 min) Epoch 56/300 -- Iteration 427768 - Batch 4158/7702 - Train loss: 0.00190436  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.20 min) Epoch 56/300 -- Iteration 427845 - Batch 4235/7702 - Train loss: 0.00190340  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.23 min) Epoch 56/300 -- Iteration 427922 - Batch 4312/7702 - Train loss: 0.00190304  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.25 min) Epoch 56/300 -- Iteration 427999 - Batch 4389/7702 - Train loss: 0.00190299  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.27 min) Epoch 56/300 -- Iteration 428076 - Batch 4466/7702 - Train loss: 0.00190231  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.29 min) Epoch 56/300 -- Iteration 428153 - Batch 4543/7702 - Train loss: 0.00190243  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.31 min) Epoch 56/300 -- Iteration 428230 - Batch 4620/7702 - Train loss: 0.00190271  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.33 min) Epoch 56/300 -- Iteration 428307 - Batch 4697/7702 - Train loss: 0.00190292  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.35 min) Epoch 56/300 -- Iteration 428384 - Batch 4774/7702 - Train loss: 0.00190312  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.37 min) Epoch 56/300 -- Iteration 428461 - Batch 4851/7702 - Train loss: 0.00190330  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.39 min) Epoch 56/300 -- Iteration 428538 - Batch 4928/7702 - Train loss: 0.00190343  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.41 min) Epoch 56/300 -- Iteration 428615 - Batch 5005/7702 - Train loss: 0.00190266  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.43 min) Epoch 56/300 -- Iteration 428692 - Batch 5082/7702 - Train loss: 0.00190251  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.45 min) Epoch 56/300 -- Iteration 428769 - Batch 5159/7702 - Train loss: 0.00190252  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.47 min) Epoch 56/300 -- Iteration 428846 - Batch 5236/7702 - Train loss: 0.00190385  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.50 min) Epoch 56/300 -- Iteration 428923 - Batch 5313/7702 - Train loss: 0.00190397  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.52 min) Epoch 56/300 -- Iteration 429000 - Batch 5390/7702 - Train loss: 0.00190404  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.54 min) Epoch 56/300 -- Iteration 429077 - Batch 5467/7702 - Train loss: 0.00190413  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.56 min) Epoch 56/300 -- Iteration 429154 - Batch 5544/7702 - Train loss: 0.00190426  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.58 min) Epoch 56/300 -- Iteration 429231 - Batch 5621/7702 - Train loss: 0.00190400  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.60 min) Epoch 56/300 -- Iteration 429308 - Batch 5698/7702 - Train loss: 0.00190391  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.62 min) Epoch 56/300 -- Iteration 429385 - Batch 5775/7702 - Train loss: 0.00190430  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.64 min) Epoch 56/300 -- Iteration 429462 - Batch 5852/7702 - Train loss: 0.00190460  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.66 min) Epoch 56/300 -- Iteration 429539 - Batch 5929/7702 - Train loss: 0.00190497  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.68 min) Epoch 56/300 -- Iteration 429616 - Batch 6006/7702 - Train loss: 0.00190522  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.70 min) Epoch 56/300 -- Iteration 429693 - Batch 6083/7702 - Train loss: 0.00190571  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.72 min) Epoch 56/300 -- Iteration 429770 - Batch 6160/7702 - Train loss: 0.00190583  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.74 min) Epoch 56/300 -- Iteration 429847 - Batch 6237/7702 - Train loss: 0.00190572  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.76 min) Epoch 56/300 -- Iteration 429924 - Batch 6314/7702 - Train loss: 0.00190590  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.79 min) Epoch 56/300 -- Iteration 430001 - Batch 6391/7702 - Train loss: 0.00190560  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.81 min) Epoch 56/300 -- Iteration 430078 - Batch 6468/7702 - Train loss: 0.00190543  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.83 min) Epoch 56/300 -- Iteration 430155 - Batch 6545/7702 - Train loss: 0.00190528  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.85 min) Epoch 56/300 -- Iteration 430232 - Batch 6622/7702 - Train loss: 0.00190495  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.87 min) Epoch 56/300 -- Iteration 430309 - Batch 6699/7702 - Train loss: 0.00190477  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.89 min) Epoch 56/300 -- Iteration 430386 - Batch 6776/7702 - Train loss: 0.00190436  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.91 min) Epoch 56/300 -- Iteration 430463 - Batch 6853/7702 - Train loss: 0.00190421  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.93 min) Epoch 56/300 -- Iteration 430540 - Batch 6930/7702 - Train loss: 0.00190415  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.95 min) Epoch 56/300 -- Iteration 430617 - Batch 7007/7702 - Train loss: 0.00190422  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.97 min) Epoch 56/300 -- Iteration 430694 - Batch 7084/7702 - Train loss: 0.00190398  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(115.99 min) Epoch 56/300 -- Iteration 430771 - Batch 7161/7702 - Train loss: 0.00190377  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(116.01 min) Epoch 56/300 -- Iteration 430848 - Batch 7238/7702 - Train loss: 0.00190392  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(116.03 min) Epoch 56/300 -- Iteration 430925 - Batch 7315/7702 - Train loss: 0.00190403  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(116.05 min) Epoch 56/300 -- Iteration 431002 - Batch 7392/7702 - Train loss: 0.00190416  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(116.08 min) Epoch 56/300 -- Iteration 431079 - Batch 7469/7702 - Train loss: 0.00190417  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(116.10 min) Epoch 56/300 -- Iteration 431156 - Batch 7546/7702 - Train loss: 0.00190413  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(116.12 min) Epoch 56/300 -- Iteration 431233 - Batch 7623/7702 - Train loss: 0.00190372  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(116.14 min) Epoch 56/300 -- Iteration 431310 - Batch 7700/7702 - Train loss: 0.00190415  - Train acc: -0.0000 - Val loss: 0.00038125\n",
      "(116.14 min) Epoch 56/300 -- Iteration 431312 - Batch 7701/7702 - Train loss: 0.00190418  - Train acc: -0.0000 - Val loss: 0.00042376 - Val acc: -0.0000\n",
      "(116.16 min) Epoch 57/300 -- Iteration 431389 - Batch 77/7702 - Train loss: 0.00190949  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.18 min) Epoch 57/300 -- Iteration 431466 - Batch 154/7702 - Train loss: 0.00187742  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.20 min) Epoch 57/300 -- Iteration 431543 - Batch 231/7702 - Train loss: 0.00189218  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.22 min) Epoch 57/300 -- Iteration 431620 - Batch 308/7702 - Train loss: 0.00188796  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.24 min) Epoch 57/300 -- Iteration 431697 - Batch 385/7702 - Train loss: 0.00189161  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.26 min) Epoch 57/300 -- Iteration 431774 - Batch 462/7702 - Train loss: 0.00189207  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.29 min) Epoch 57/300 -- Iteration 431851 - Batch 539/7702 - Train loss: 0.00188932  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.31 min) Epoch 57/300 -- Iteration 431928 - Batch 616/7702 - Train loss: 0.00189327  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.33 min) Epoch 57/300 -- Iteration 432005 - Batch 693/7702 - Train loss: 0.00189435  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.35 min) Epoch 57/300 -- Iteration 432082 - Batch 770/7702 - Train loss: 0.00189647  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.37 min) Epoch 57/300 -- Iteration 432159 - Batch 847/7702 - Train loss: 0.00189883  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.39 min) Epoch 57/300 -- Iteration 432236 - Batch 924/7702 - Train loss: 0.00190072  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.41 min) Epoch 57/300 -- Iteration 432313 - Batch 1001/7702 - Train loss: 0.00189919  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.43 min) Epoch 57/300 -- Iteration 432390 - Batch 1078/7702 - Train loss: 0.00189834  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.45 min) Epoch 57/300 -- Iteration 432467 - Batch 1155/7702 - Train loss: 0.00189672  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.47 min) Epoch 57/300 -- Iteration 432544 - Batch 1232/7702 - Train loss: 0.00189499  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.49 min) Epoch 57/300 -- Iteration 432621 - Batch 1309/7702 - Train loss: 0.00189647  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.51 min) Epoch 57/300 -- Iteration 432698 - Batch 1386/7702 - Train loss: 0.00189596  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.53 min) Epoch 57/300 -- Iteration 432775 - Batch 1463/7702 - Train loss: 0.00189622  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.56 min) Epoch 57/300 -- Iteration 432852 - Batch 1540/7702 - Train loss: 0.00189437  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.58 min) Epoch 57/300 -- Iteration 432929 - Batch 1617/7702 - Train loss: 0.00189422  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.60 min) Epoch 57/300 -- Iteration 433006 - Batch 1694/7702 - Train loss: 0.00189351  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.62 min) Epoch 57/300 -- Iteration 433083 - Batch 1771/7702 - Train loss: 0.00189465  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.64 min) Epoch 57/300 -- Iteration 433160 - Batch 1848/7702 - Train loss: 0.00189534  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.66 min) Epoch 57/300 -- Iteration 433237 - Batch 1925/7702 - Train loss: 0.00189531  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.68 min) Epoch 57/300 -- Iteration 433314 - Batch 2002/7702 - Train loss: 0.00189578  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.70 min) Epoch 57/300 -- Iteration 433391 - Batch 2079/7702 - Train loss: 0.00189607  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.72 min) Epoch 57/300 -- Iteration 433468 - Batch 2156/7702 - Train loss: 0.00189657  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.74 min) Epoch 57/300 -- Iteration 433545 - Batch 2233/7702 - Train loss: 0.00189696  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.76 min) Epoch 57/300 -- Iteration 433622 - Batch 2310/7702 - Train loss: 0.00189684  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.78 min) Epoch 57/300 -- Iteration 433699 - Batch 2387/7702 - Train loss: 0.00189751  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.80 min) Epoch 57/300 -- Iteration 433776 - Batch 2464/7702 - Train loss: 0.00189725  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.83 min) Epoch 57/300 -- Iteration 433853 - Batch 2541/7702 - Train loss: 0.00189657  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.85 min) Epoch 57/300 -- Iteration 433930 - Batch 2618/7702 - Train loss: 0.00189734  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.87 min) Epoch 57/300 -- Iteration 434007 - Batch 2695/7702 - Train loss: 0.00189863  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.89 min) Epoch 57/300 -- Iteration 434084 - Batch 2772/7702 - Train loss: 0.00189966  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.91 min) Epoch 57/300 -- Iteration 434161 - Batch 2849/7702 - Train loss: 0.00190034  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.93 min) Epoch 57/300 -- Iteration 434238 - Batch 2926/7702 - Train loss: 0.00189986  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.95 min) Epoch 57/300 -- Iteration 434315 - Batch 3003/7702 - Train loss: 0.00189961  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.97 min) Epoch 57/300 -- Iteration 434392 - Batch 3080/7702 - Train loss: 0.00189974  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(116.99 min) Epoch 57/300 -- Iteration 434469 - Batch 3157/7702 - Train loss: 0.00189971  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.01 min) Epoch 57/300 -- Iteration 434546 - Batch 3234/7702 - Train loss: 0.00190016  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.03 min) Epoch 57/300 -- Iteration 434623 - Batch 3311/7702 - Train loss: 0.00190031  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.05 min) Epoch 57/300 -- Iteration 434700 - Batch 3388/7702 - Train loss: 0.00190044  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.07 min) Epoch 57/300 -- Iteration 434777 - Batch 3465/7702 - Train loss: 0.00189974  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.10 min) Epoch 57/300 -- Iteration 434854 - Batch 3542/7702 - Train loss: 0.00189976  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.12 min) Epoch 57/300 -- Iteration 434931 - Batch 3619/7702 - Train loss: 0.00189936  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.14 min) Epoch 57/300 -- Iteration 435008 - Batch 3696/7702 - Train loss: 0.00189975  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.16 min) Epoch 57/300 -- Iteration 435085 - Batch 3773/7702 - Train loss: 0.00189980  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.18 min) Epoch 57/300 -- Iteration 435162 - Batch 3850/7702 - Train loss: 0.00190025  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.20 min) Epoch 57/300 -- Iteration 435239 - Batch 3927/7702 - Train loss: 0.00190104  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.22 min) Epoch 57/300 -- Iteration 435316 - Batch 4004/7702 - Train loss: 0.00190095  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.24 min) Epoch 57/300 -- Iteration 435393 - Batch 4081/7702 - Train loss: 0.00190110  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.26 min) Epoch 57/300 -- Iteration 435470 - Batch 4158/7702 - Train loss: 0.00190146  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.28 min) Epoch 57/300 -- Iteration 435547 - Batch 4235/7702 - Train loss: 0.00190158  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.30 min) Epoch 57/300 -- Iteration 435624 - Batch 4312/7702 - Train loss: 0.00190212  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.32 min) Epoch 57/300 -- Iteration 435701 - Batch 4389/7702 - Train loss: 0.00190189  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.34 min) Epoch 57/300 -- Iteration 435778 - Batch 4466/7702 - Train loss: 0.00190230  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.36 min) Epoch 57/300 -- Iteration 435855 - Batch 4543/7702 - Train loss: 0.00190206  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.39 min) Epoch 57/300 -- Iteration 435932 - Batch 4620/7702 - Train loss: 0.00190233  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.41 min) Epoch 57/300 -- Iteration 436009 - Batch 4697/7702 - Train loss: 0.00190292  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.43 min) Epoch 57/300 -- Iteration 436086 - Batch 4774/7702 - Train loss: 0.00190309  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.45 min) Epoch 57/300 -- Iteration 436163 - Batch 4851/7702 - Train loss: 0.00190349  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.47 min) Epoch 57/300 -- Iteration 436240 - Batch 4928/7702 - Train loss: 0.00190387  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.49 min) Epoch 57/300 -- Iteration 436317 - Batch 5005/7702 - Train loss: 0.00190399  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.51 min) Epoch 57/300 -- Iteration 436394 - Batch 5082/7702 - Train loss: 0.00190412  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.53 min) Epoch 57/300 -- Iteration 436471 - Batch 5159/7702 - Train loss: 0.00190412  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.55 min) Epoch 57/300 -- Iteration 436548 - Batch 5236/7702 - Train loss: 0.00190416  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.57 min) Epoch 57/300 -- Iteration 436625 - Batch 5313/7702 - Train loss: 0.00190433  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.59 min) Epoch 57/300 -- Iteration 436702 - Batch 5390/7702 - Train loss: 0.00190421  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.61 min) Epoch 57/300 -- Iteration 436779 - Batch 5467/7702 - Train loss: 0.00190377  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.63 min) Epoch 57/300 -- Iteration 436856 - Batch 5544/7702 - Train loss: 0.00190362  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.65 min) Epoch 57/300 -- Iteration 436933 - Batch 5621/7702 - Train loss: 0.00190411  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.67 min) Epoch 57/300 -- Iteration 437010 - Batch 5698/7702 - Train loss: 0.00190405  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.70 min) Epoch 57/300 -- Iteration 437087 - Batch 5775/7702 - Train loss: 0.00190410  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.72 min) Epoch 57/300 -- Iteration 437164 - Batch 5852/7702 - Train loss: 0.00190352  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.74 min) Epoch 57/300 -- Iteration 437241 - Batch 5929/7702 - Train loss: 0.00190376  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.76 min) Epoch 57/300 -- Iteration 437318 - Batch 6006/7702 - Train loss: 0.00190364  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.78 min) Epoch 57/300 -- Iteration 437395 - Batch 6083/7702 - Train loss: 0.00190414  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.80 min) Epoch 57/300 -- Iteration 437472 - Batch 6160/7702 - Train loss: 0.00190432  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.82 min) Epoch 57/300 -- Iteration 437549 - Batch 6237/7702 - Train loss: 0.00190433  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.84 min) Epoch 57/300 -- Iteration 437626 - Batch 6314/7702 - Train loss: 0.00190395  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.86 min) Epoch 57/300 -- Iteration 437703 - Batch 6391/7702 - Train loss: 0.00190348  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.88 min) Epoch 57/300 -- Iteration 437780 - Batch 6468/7702 - Train loss: 0.00190343  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.90 min) Epoch 57/300 -- Iteration 437857 - Batch 6545/7702 - Train loss: 0.00190301  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.92 min) Epoch 57/300 -- Iteration 437934 - Batch 6622/7702 - Train loss: 0.00190288  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.95 min) Epoch 57/300 -- Iteration 438011 - Batch 6699/7702 - Train loss: 0.00190313  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.97 min) Epoch 57/300 -- Iteration 438088 - Batch 6776/7702 - Train loss: 0.00190336  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(117.99 min) Epoch 57/300 -- Iteration 438165 - Batch 6853/7702 - Train loss: 0.00190327  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.01 min) Epoch 57/300 -- Iteration 438242 - Batch 6930/7702 - Train loss: 0.00190359  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.03 min) Epoch 57/300 -- Iteration 438319 - Batch 7007/7702 - Train loss: 0.00190374  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.05 min) Epoch 57/300 -- Iteration 438396 - Batch 7084/7702 - Train loss: 0.00190398  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.07 min) Epoch 57/300 -- Iteration 438473 - Batch 7161/7702 - Train loss: 0.00190418  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.09 min) Epoch 57/300 -- Iteration 438550 - Batch 7238/7702 - Train loss: 0.00190452  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.11 min) Epoch 57/300 -- Iteration 438627 - Batch 7315/7702 - Train loss: 0.00190450  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.13 min) Epoch 57/300 -- Iteration 438704 - Batch 7392/7702 - Train loss: 0.00190427  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.15 min) Epoch 57/300 -- Iteration 438781 - Batch 7469/7702 - Train loss: 0.00190426  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.17 min) Epoch 57/300 -- Iteration 438858 - Batch 7546/7702 - Train loss: 0.00190454  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.19 min) Epoch 57/300 -- Iteration 438935 - Batch 7623/7702 - Train loss: 0.00190451  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.21 min) Epoch 57/300 -- Iteration 439012 - Batch 7700/7702 - Train loss: 0.00190416  - Train acc: -0.0000 - Val loss: 0.00042376\n",
      "(118.22 min) Epoch 57/300 -- Iteration 439014 - Batch 7701/7702 - Train loss: 0.00190413  - Train acc: -0.0000 - Val loss: 0.00046361 - Val acc: -0.0000\n",
      "(118.24 min) Epoch 58/300 -- Iteration 439091 - Batch 77/7702 - Train loss: 0.00193593  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.26 min) Epoch 58/300 -- Iteration 439168 - Batch 154/7702 - Train loss: 0.00191108  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.28 min) Epoch 58/300 -- Iteration 439245 - Batch 231/7702 - Train loss: 0.00191550  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.30 min) Epoch 58/300 -- Iteration 439322 - Batch 308/7702 - Train loss: 0.00191282  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.32 min) Epoch 58/300 -- Iteration 439399 - Batch 385/7702 - Train loss: 0.00191219  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.34 min) Epoch 58/300 -- Iteration 439476 - Batch 462/7702 - Train loss: 0.00190881  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.36 min) Epoch 58/300 -- Iteration 439553 - Batch 539/7702 - Train loss: 0.00191121  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.38 min) Epoch 58/300 -- Iteration 439630 - Batch 616/7702 - Train loss: 0.00190802  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.40 min) Epoch 58/300 -- Iteration 439707 - Batch 693/7702 - Train loss: 0.00190622  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.42 min) Epoch 58/300 -- Iteration 439784 - Batch 770/7702 - Train loss: 0.00190701  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.45 min) Epoch 58/300 -- Iteration 439861 - Batch 847/7702 - Train loss: 0.00190600  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.47 min) Epoch 58/300 -- Iteration 439938 - Batch 924/7702 - Train loss: 0.00190370  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.49 min) Epoch 58/300 -- Iteration 440015 - Batch 1001/7702 - Train loss: 0.00190291  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.51 min) Epoch 58/300 -- Iteration 440092 - Batch 1078/7702 - Train loss: 0.00190486  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.53 min) Epoch 58/300 -- Iteration 440169 - Batch 1155/7702 - Train loss: 0.00190550  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.55 min) Epoch 58/300 -- Iteration 440246 - Batch 1232/7702 - Train loss: 0.00190461  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.57 min) Epoch 58/300 -- Iteration 440323 - Batch 1309/7702 - Train loss: 0.00190414  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.59 min) Epoch 58/300 -- Iteration 440400 - Batch 1386/7702 - Train loss: 0.00190299  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.61 min) Epoch 58/300 -- Iteration 440477 - Batch 1463/7702 - Train loss: 0.00190572  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.63 min) Epoch 58/300 -- Iteration 440554 - Batch 1540/7702 - Train loss: 0.00190617  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.65 min) Epoch 58/300 -- Iteration 440631 - Batch 1617/7702 - Train loss: 0.00190499  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.67 min) Epoch 58/300 -- Iteration 440708 - Batch 1694/7702 - Train loss: 0.00190473  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.70 min) Epoch 58/300 -- Iteration 440785 - Batch 1771/7702 - Train loss: 0.00190493  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.72 min) Epoch 58/300 -- Iteration 440862 - Batch 1848/7702 - Train loss: 0.00190588  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.74 min) Epoch 58/300 -- Iteration 440939 - Batch 1925/7702 - Train loss: 0.00190618  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.76 min) Epoch 58/300 -- Iteration 441016 - Batch 2002/7702 - Train loss: 0.00190531  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.78 min) Epoch 58/300 -- Iteration 441093 - Batch 2079/7702 - Train loss: 0.00190316  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.80 min) Epoch 58/300 -- Iteration 441170 - Batch 2156/7702 - Train loss: 0.00190346  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.82 min) Epoch 58/300 -- Iteration 441247 - Batch 2233/7702 - Train loss: 0.00190237  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.84 min) Epoch 58/300 -- Iteration 441324 - Batch 2310/7702 - Train loss: 0.00190171  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.86 min) Epoch 58/300 -- Iteration 441401 - Batch 2387/7702 - Train loss: 0.00190257  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.88 min) Epoch 58/300 -- Iteration 441478 - Batch 2464/7702 - Train loss: 0.00190181  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.90 min) Epoch 58/300 -- Iteration 441555 - Batch 2541/7702 - Train loss: 0.00190145  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.92 min) Epoch 58/300 -- Iteration 441632 - Batch 2618/7702 - Train loss: 0.00190006  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.94 min) Epoch 58/300 -- Iteration 441709 - Batch 2695/7702 - Train loss: 0.00190111  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.97 min) Epoch 58/300 -- Iteration 441786 - Batch 2772/7702 - Train loss: 0.00190104  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(118.99 min) Epoch 58/300 -- Iteration 441863 - Batch 2849/7702 - Train loss: 0.00190068  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.01 min) Epoch 58/300 -- Iteration 441940 - Batch 2926/7702 - Train loss: 0.00190093  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.03 min) Epoch 58/300 -- Iteration 442017 - Batch 3003/7702 - Train loss: 0.00190070  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.05 min) Epoch 58/300 -- Iteration 442094 - Batch 3080/7702 - Train loss: 0.00190036  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.07 min) Epoch 58/300 -- Iteration 442171 - Batch 3157/7702 - Train loss: 0.00190079  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.09 min) Epoch 58/300 -- Iteration 442248 - Batch 3234/7702 - Train loss: 0.00190111  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.11 min) Epoch 58/300 -- Iteration 442325 - Batch 3311/7702 - Train loss: 0.00190235  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.13 min) Epoch 58/300 -- Iteration 442402 - Batch 3388/7702 - Train loss: 0.00190287  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.15 min) Epoch 58/300 -- Iteration 442479 - Batch 3465/7702 - Train loss: 0.00190359  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.17 min) Epoch 58/300 -- Iteration 442556 - Batch 3542/7702 - Train loss: 0.00190335  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.19 min) Epoch 58/300 -- Iteration 442633 - Batch 3619/7702 - Train loss: 0.00190269  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.21 min) Epoch 58/300 -- Iteration 442710 - Batch 3696/7702 - Train loss: 0.00190225  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.23 min) Epoch 58/300 -- Iteration 442787 - Batch 3773/7702 - Train loss: 0.00190244  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.26 min) Epoch 58/300 -- Iteration 442864 - Batch 3850/7702 - Train loss: 0.00190238  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.28 min) Epoch 58/300 -- Iteration 442941 - Batch 3927/7702 - Train loss: 0.00190201  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.30 min) Epoch 58/300 -- Iteration 443018 - Batch 4004/7702 - Train loss: 0.00190134  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.32 min) Epoch 58/300 -- Iteration 443095 - Batch 4081/7702 - Train loss: 0.00190168  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.34 min) Epoch 58/300 -- Iteration 443172 - Batch 4158/7702 - Train loss: 0.00190190  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.36 min) Epoch 58/300 -- Iteration 443249 - Batch 4235/7702 - Train loss: 0.00190140  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.38 min) Epoch 58/300 -- Iteration 443326 - Batch 4312/7702 - Train loss: 0.00190153  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.40 min) Epoch 58/300 -- Iteration 443403 - Batch 4389/7702 - Train loss: 0.00190123  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.42 min) Epoch 58/300 -- Iteration 443480 - Batch 4466/7702 - Train loss: 0.00190101  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.44 min) Epoch 58/300 -- Iteration 443557 - Batch 4543/7702 - Train loss: 0.00190111  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.46 min) Epoch 58/300 -- Iteration 443634 - Batch 4620/7702 - Train loss: 0.00190193  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.48 min) Epoch 58/300 -- Iteration 443711 - Batch 4697/7702 - Train loss: 0.00190220  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.50 min) Epoch 58/300 -- Iteration 443788 - Batch 4774/7702 - Train loss: 0.00190208  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.53 min) Epoch 58/300 -- Iteration 443865 - Batch 4851/7702 - Train loss: 0.00190212  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.55 min) Epoch 58/300 -- Iteration 443942 - Batch 4928/7702 - Train loss: 0.00190231  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.57 min) Epoch 58/300 -- Iteration 444019 - Batch 5005/7702 - Train loss: 0.00190267  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.59 min) Epoch 58/300 -- Iteration 444096 - Batch 5082/7702 - Train loss: 0.00190256  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.61 min) Epoch 58/300 -- Iteration 444173 - Batch 5159/7702 - Train loss: 0.00190249  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.63 min) Epoch 58/300 -- Iteration 444250 - Batch 5236/7702 - Train loss: 0.00190207  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.65 min) Epoch 58/300 -- Iteration 444327 - Batch 5313/7702 - Train loss: 0.00190249  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.67 min) Epoch 58/300 -- Iteration 444404 - Batch 5390/7702 - Train loss: 0.00190273  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.69 min) Epoch 58/300 -- Iteration 444481 - Batch 5467/7702 - Train loss: 0.00190271  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.71 min) Epoch 58/300 -- Iteration 444558 - Batch 5544/7702 - Train loss: 0.00190243  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.73 min) Epoch 58/300 -- Iteration 444635 - Batch 5621/7702 - Train loss: 0.00190256  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.75 min) Epoch 58/300 -- Iteration 444712 - Batch 5698/7702 - Train loss: 0.00190253  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.77 min) Epoch 58/300 -- Iteration 444789 - Batch 5775/7702 - Train loss: 0.00190271  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.79 min) Epoch 58/300 -- Iteration 444866 - Batch 5852/7702 - Train loss: 0.00190303  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.82 min) Epoch 58/300 -- Iteration 444943 - Batch 5929/7702 - Train loss: 0.00190297  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.84 min) Epoch 58/300 -- Iteration 445020 - Batch 6006/7702 - Train loss: 0.00190272  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.86 min) Epoch 58/300 -- Iteration 445097 - Batch 6083/7702 - Train loss: 0.00190274  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.88 min) Epoch 58/300 -- Iteration 445174 - Batch 6160/7702 - Train loss: 0.00190293  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.90 min) Epoch 58/300 -- Iteration 445251 - Batch 6237/7702 - Train loss: 0.00190361  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.92 min) Epoch 58/300 -- Iteration 445328 - Batch 6314/7702 - Train loss: 0.00190378  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.94 min) Epoch 58/300 -- Iteration 445405 - Batch 6391/7702 - Train loss: 0.00190413  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.96 min) Epoch 58/300 -- Iteration 445482 - Batch 6468/7702 - Train loss: 0.00190412  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(119.98 min) Epoch 58/300 -- Iteration 445559 - Batch 6545/7702 - Train loss: 0.00190379  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.00 min) Epoch 58/300 -- Iteration 445636 - Batch 6622/7702 - Train loss: 0.00190418  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.02 min) Epoch 58/300 -- Iteration 445713 - Batch 6699/7702 - Train loss: 0.00190386  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.04 min) Epoch 58/300 -- Iteration 445790 - Batch 6776/7702 - Train loss: 0.00190402  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.06 min) Epoch 58/300 -- Iteration 445867 - Batch 6853/7702 - Train loss: 0.00190344  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.08 min) Epoch 58/300 -- Iteration 445944 - Batch 6930/7702 - Train loss: 0.00190397  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.11 min) Epoch 58/300 -- Iteration 446021 - Batch 7007/7702 - Train loss: 0.00190386  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.13 min) Epoch 58/300 -- Iteration 446098 - Batch 7084/7702 - Train loss: 0.00190414  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.15 min) Epoch 58/300 -- Iteration 446175 - Batch 7161/7702 - Train loss: 0.00190444  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.17 min) Epoch 58/300 -- Iteration 446252 - Batch 7238/7702 - Train loss: 0.00190423  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.19 min) Epoch 58/300 -- Iteration 446329 - Batch 7315/7702 - Train loss: 0.00190462  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.21 min) Epoch 58/300 -- Iteration 446406 - Batch 7392/7702 - Train loss: 0.00190499  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.23 min) Epoch 58/300 -- Iteration 446483 - Batch 7469/7702 - Train loss: 0.00190497  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.25 min) Epoch 58/300 -- Iteration 446560 - Batch 7546/7702 - Train loss: 0.00190475  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.27 min) Epoch 58/300 -- Iteration 446637 - Batch 7623/7702 - Train loss: 0.00190444  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.29 min) Epoch 58/300 -- Iteration 446714 - Batch 7700/7702 - Train loss: 0.00190465  - Train acc: -0.0000 - Val loss: 0.00046361\n",
      "(120.29 min) Epoch 58/300 -- Iteration 446716 - Batch 7701/7702 - Train loss: 0.00190465  - Train acc: -0.0000 - Val loss: 0.00035830 - Val acc: -0.0000\n",
      "(120.32 min) Epoch 59/300 -- Iteration 446793 - Batch 77/7702 - Train loss: 0.00185297  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.34 min) Epoch 59/300 -- Iteration 446870 - Batch 154/7702 - Train loss: 0.00188749  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.36 min) Epoch 59/300 -- Iteration 446947 - Batch 231/7702 - Train loss: 0.00188829  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.38 min) Epoch 59/300 -- Iteration 447024 - Batch 308/7702 - Train loss: 0.00189078  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.40 min) Epoch 59/300 -- Iteration 447101 - Batch 385/7702 - Train loss: 0.00189176  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.42 min) Epoch 59/300 -- Iteration 447178 - Batch 462/7702 - Train loss: 0.00189328  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.44 min) Epoch 59/300 -- Iteration 447255 - Batch 539/7702 - Train loss: 0.00189730  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.46 min) Epoch 59/300 -- Iteration 447332 - Batch 616/7702 - Train loss: 0.00189914  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.48 min) Epoch 59/300 -- Iteration 447409 - Batch 693/7702 - Train loss: 0.00189686  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.50 min) Epoch 59/300 -- Iteration 447486 - Batch 770/7702 - Train loss: 0.00189684  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.52 min) Epoch 59/300 -- Iteration 447563 - Batch 847/7702 - Train loss: 0.00189844  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.54 min) Epoch 59/300 -- Iteration 447640 - Batch 924/7702 - Train loss: 0.00189881  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.57 min) Epoch 59/300 -- Iteration 447717 - Batch 1001/7702 - Train loss: 0.00189891  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.59 min) Epoch 59/300 -- Iteration 447794 - Batch 1078/7702 - Train loss: 0.00189748  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.61 min) Epoch 59/300 -- Iteration 447871 - Batch 1155/7702 - Train loss: 0.00189750  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.63 min) Epoch 59/300 -- Iteration 447948 - Batch 1232/7702 - Train loss: 0.00189770  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.65 min) Epoch 59/300 -- Iteration 448025 - Batch 1309/7702 - Train loss: 0.00189820  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.67 min) Epoch 59/300 -- Iteration 448102 - Batch 1386/7702 - Train loss: 0.00190058  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.69 min) Epoch 59/300 -- Iteration 448179 - Batch 1463/7702 - Train loss: 0.00190100  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.71 min) Epoch 59/300 -- Iteration 448256 - Batch 1540/7702 - Train loss: 0.00190152  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.73 min) Epoch 59/300 -- Iteration 448333 - Batch 1617/7702 - Train loss: 0.00190097  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.75 min) Epoch 59/300 -- Iteration 448410 - Batch 1694/7702 - Train loss: 0.00190008  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.77 min) Epoch 59/300 -- Iteration 448487 - Batch 1771/7702 - Train loss: 0.00190020  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.79 min) Epoch 59/300 -- Iteration 448564 - Batch 1848/7702 - Train loss: 0.00190128  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.81 min) Epoch 59/300 -- Iteration 448641 - Batch 1925/7702 - Train loss: 0.00190210  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.84 min) Epoch 59/300 -- Iteration 448718 - Batch 2002/7702 - Train loss: 0.00190255  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.86 min) Epoch 59/300 -- Iteration 448795 - Batch 2079/7702 - Train loss: 0.00190270  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.88 min) Epoch 59/300 -- Iteration 448872 - Batch 2156/7702 - Train loss: 0.00190191  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.90 min) Epoch 59/300 -- Iteration 448949 - Batch 2233/7702 - Train loss: 0.00190071  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.92 min) Epoch 59/300 -- Iteration 449026 - Batch 2310/7702 - Train loss: 0.00190014  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.94 min) Epoch 59/300 -- Iteration 449103 - Batch 2387/7702 - Train loss: 0.00189890  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.96 min) Epoch 59/300 -- Iteration 449180 - Batch 2464/7702 - Train loss: 0.00189945  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(120.98 min) Epoch 59/300 -- Iteration 449257 - Batch 2541/7702 - Train loss: 0.00189991  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.00 min) Epoch 59/300 -- Iteration 449334 - Batch 2618/7702 - Train loss: 0.00189974  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.02 min) Epoch 59/300 -- Iteration 449411 - Batch 2695/7702 - Train loss: 0.00189998  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.04 min) Epoch 59/300 -- Iteration 449488 - Batch 2772/7702 - Train loss: 0.00189888  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.06 min) Epoch 59/300 -- Iteration 449565 - Batch 2849/7702 - Train loss: 0.00189904  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.08 min) Epoch 59/300 -- Iteration 449642 - Batch 2926/7702 - Train loss: 0.00189827  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.10 min) Epoch 59/300 -- Iteration 449719 - Batch 3003/7702 - Train loss: 0.00189845  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.13 min) Epoch 59/300 -- Iteration 449796 - Batch 3080/7702 - Train loss: 0.00189948  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.15 min) Epoch 59/300 -- Iteration 449873 - Batch 3157/7702 - Train loss: 0.00190015  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.17 min) Epoch 59/300 -- Iteration 449950 - Batch 3234/7702 - Train loss: 0.00190040  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.19 min) Epoch 59/300 -- Iteration 450027 - Batch 3311/7702 - Train loss: 0.00190100  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.21 min) Epoch 59/300 -- Iteration 450104 - Batch 3388/7702 - Train loss: 0.00190107  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.23 min) Epoch 59/300 -- Iteration 450181 - Batch 3465/7702 - Train loss: 0.00190161  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.25 min) Epoch 59/300 -- Iteration 450258 - Batch 3542/7702 - Train loss: 0.00190149  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.27 min) Epoch 59/300 -- Iteration 450335 - Batch 3619/7702 - Train loss: 0.00190040  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.29 min) Epoch 59/300 -- Iteration 450412 - Batch 3696/7702 - Train loss: 0.00190056  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.31 min) Epoch 59/300 -- Iteration 450489 - Batch 3773/7702 - Train loss: 0.00190088  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.33 min) Epoch 59/300 -- Iteration 450566 - Batch 3850/7702 - Train loss: 0.00190182  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.35 min) Epoch 59/300 -- Iteration 450643 - Batch 3927/7702 - Train loss: 0.00190229  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.37 min) Epoch 59/300 -- Iteration 450720 - Batch 4004/7702 - Train loss: 0.00190200  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.39 min) Epoch 59/300 -- Iteration 450797 - Batch 4081/7702 - Train loss: 0.00190214  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.41 min) Epoch 59/300 -- Iteration 450874 - Batch 4158/7702 - Train loss: 0.00190265  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.44 min) Epoch 59/300 -- Iteration 450951 - Batch 4235/7702 - Train loss: 0.00190281  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.46 min) Epoch 59/300 -- Iteration 451028 - Batch 4312/7702 - Train loss: 0.00190188  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.48 min) Epoch 59/300 -- Iteration 451105 - Batch 4389/7702 - Train loss: 0.00190246  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.50 min) Epoch 59/300 -- Iteration 451182 - Batch 4466/7702 - Train loss: 0.00190217  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.52 min) Epoch 59/300 -- Iteration 451259 - Batch 4543/7702 - Train loss: 0.00190226  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.54 min) Epoch 59/300 -- Iteration 451336 - Batch 4620/7702 - Train loss: 0.00190256  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.56 min) Epoch 59/300 -- Iteration 451413 - Batch 4697/7702 - Train loss: 0.00190223  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.58 min) Epoch 59/300 -- Iteration 451490 - Batch 4774/7702 - Train loss: 0.00190243  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.60 min) Epoch 59/300 -- Iteration 451567 - Batch 4851/7702 - Train loss: 0.00190300  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.62 min) Epoch 59/300 -- Iteration 451644 - Batch 4928/7702 - Train loss: 0.00190297  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.64 min) Epoch 59/300 -- Iteration 451721 - Batch 5005/7702 - Train loss: 0.00190288  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.66 min) Epoch 59/300 -- Iteration 451798 - Batch 5082/7702 - Train loss: 0.00190239  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.68 min) Epoch 59/300 -- Iteration 451875 - Batch 5159/7702 - Train loss: 0.00190214  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.71 min) Epoch 59/300 -- Iteration 451952 - Batch 5236/7702 - Train loss: 0.00190233  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.73 min) Epoch 59/300 -- Iteration 452029 - Batch 5313/7702 - Train loss: 0.00190225  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.75 min) Epoch 59/300 -- Iteration 452106 - Batch 5390/7702 - Train loss: 0.00190210  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.77 min) Epoch 59/300 -- Iteration 452183 - Batch 5467/7702 - Train loss: 0.00190237  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.79 min) Epoch 59/300 -- Iteration 452260 - Batch 5544/7702 - Train loss: 0.00190280  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.81 min) Epoch 59/300 -- Iteration 452337 - Batch 5621/7702 - Train loss: 0.00190310  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.83 min) Epoch 59/300 -- Iteration 452414 - Batch 5698/7702 - Train loss: 0.00190355  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.85 min) Epoch 59/300 -- Iteration 452491 - Batch 5775/7702 - Train loss: 0.00190330  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.87 min) Epoch 59/300 -- Iteration 452568 - Batch 5852/7702 - Train loss: 0.00190338  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.89 min) Epoch 59/300 -- Iteration 452645 - Batch 5929/7702 - Train loss: 0.00190332  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.91 min) Epoch 59/300 -- Iteration 452722 - Batch 6006/7702 - Train loss: 0.00190385  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.93 min) Epoch 59/300 -- Iteration 452799 - Batch 6083/7702 - Train loss: 0.00190336  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.95 min) Epoch 59/300 -- Iteration 452876 - Batch 6160/7702 - Train loss: 0.00190353  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(121.98 min) Epoch 59/300 -- Iteration 452953 - Batch 6237/7702 - Train loss: 0.00190322  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.00 min) Epoch 59/300 -- Iteration 453030 - Batch 6314/7702 - Train loss: 0.00190323  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.02 min) Epoch 59/300 -- Iteration 453107 - Batch 6391/7702 - Train loss: 0.00190354  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.04 min) Epoch 59/300 -- Iteration 453184 - Batch 6468/7702 - Train loss: 0.00190332  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.06 min) Epoch 59/300 -- Iteration 453261 - Batch 6545/7702 - Train loss: 0.00190288  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.08 min) Epoch 59/300 -- Iteration 453338 - Batch 6622/7702 - Train loss: 0.00190233  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.10 min) Epoch 59/300 -- Iteration 453415 - Batch 6699/7702 - Train loss: 0.00190226  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.12 min) Epoch 59/300 -- Iteration 453492 - Batch 6776/7702 - Train loss: 0.00190240  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.14 min) Epoch 59/300 -- Iteration 453569 - Batch 6853/7702 - Train loss: 0.00190225  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.16 min) Epoch 59/300 -- Iteration 453646 - Batch 6930/7702 - Train loss: 0.00190220  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.18 min) Epoch 59/300 -- Iteration 453723 - Batch 7007/7702 - Train loss: 0.00190220  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.20 min) Epoch 59/300 -- Iteration 453800 - Batch 7084/7702 - Train loss: 0.00190212  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.22 min) Epoch 59/300 -- Iteration 453877 - Batch 7161/7702 - Train loss: 0.00190188  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.24 min) Epoch 59/300 -- Iteration 453954 - Batch 7238/7702 - Train loss: 0.00190176  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.27 min) Epoch 59/300 -- Iteration 454031 - Batch 7315/7702 - Train loss: 0.00190167  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.29 min) Epoch 59/300 -- Iteration 454108 - Batch 7392/7702 - Train loss: 0.00190171  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.31 min) Epoch 59/300 -- Iteration 454185 - Batch 7469/7702 - Train loss: 0.00190252  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.33 min) Epoch 59/300 -- Iteration 454262 - Batch 7546/7702 - Train loss: 0.00190304  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.35 min) Epoch 59/300 -- Iteration 454339 - Batch 7623/7702 - Train loss: 0.00190275  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.37 min) Epoch 59/300 -- Iteration 454416 - Batch 7700/7702 - Train loss: 0.00190267  - Train acc: -0.0000 - Val loss: 0.00035830\n",
      "(122.37 min) Epoch 59/300 -- Iteration 454418 - Batch 7701/7702 - Train loss: 0.00190264  - Train acc: -0.0000 - Val loss: 0.00045442 - Val acc: -0.0000\n",
      "(122.39 min) Epoch 60/300 -- Iteration 454495 - Batch 77/7702 - Train loss: 0.00191567  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.41 min) Epoch 60/300 -- Iteration 454572 - Batch 154/7702 - Train loss: 0.00190908  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.43 min) Epoch 60/300 -- Iteration 454649 - Batch 231/7702 - Train loss: 0.00190394  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.45 min) Epoch 60/300 -- Iteration 454726 - Batch 308/7702 - Train loss: 0.00189388  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.48 min) Epoch 60/300 -- Iteration 454803 - Batch 385/7702 - Train loss: 0.00189896  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.50 min) Epoch 60/300 -- Iteration 454880 - Batch 462/7702 - Train loss: 0.00189659  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.52 min) Epoch 60/300 -- Iteration 454957 - Batch 539/7702 - Train loss: 0.00189498  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.54 min) Epoch 60/300 -- Iteration 455034 - Batch 616/7702 - Train loss: 0.00189221  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.56 min) Epoch 60/300 -- Iteration 455111 - Batch 693/7702 - Train loss: 0.00189017  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.58 min) Epoch 60/300 -- Iteration 455188 - Batch 770/7702 - Train loss: 0.00189438  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.60 min) Epoch 60/300 -- Iteration 455265 - Batch 847/7702 - Train loss: 0.00189563  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.62 min) Epoch 60/300 -- Iteration 455342 - Batch 924/7702 - Train loss: 0.00189404  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.64 min) Epoch 60/300 -- Iteration 455419 - Batch 1001/7702 - Train loss: 0.00189728  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.66 min) Epoch 60/300 -- Iteration 455496 - Batch 1078/7702 - Train loss: 0.00189744  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.68 min) Epoch 60/300 -- Iteration 455573 - Batch 1155/7702 - Train loss: 0.00189894  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.70 min) Epoch 60/300 -- Iteration 455650 - Batch 1232/7702 - Train loss: 0.00190047  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.72 min) Epoch 60/300 -- Iteration 455727 - Batch 1309/7702 - Train loss: 0.00190221  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.74 min) Epoch 60/300 -- Iteration 455804 - Batch 1386/7702 - Train loss: 0.00190301  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.76 min) Epoch 60/300 -- Iteration 455881 - Batch 1463/7702 - Train loss: 0.00190298  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.79 min) Epoch 60/300 -- Iteration 455958 - Batch 1540/7702 - Train loss: 0.00190226  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.81 min) Epoch 60/300 -- Iteration 456035 - Batch 1617/7702 - Train loss: 0.00190271  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.83 min) Epoch 60/300 -- Iteration 456112 - Batch 1694/7702 - Train loss: 0.00190316  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.85 min) Epoch 60/300 -- Iteration 456189 - Batch 1771/7702 - Train loss: 0.00190400  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.87 min) Epoch 60/300 -- Iteration 456266 - Batch 1848/7702 - Train loss: 0.00190200  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.89 min) Epoch 60/300 -- Iteration 456343 - Batch 1925/7702 - Train loss: 0.00190176  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.91 min) Epoch 60/300 -- Iteration 456420 - Batch 2002/7702 - Train loss: 0.00190093  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.93 min) Epoch 60/300 -- Iteration 456497 - Batch 2079/7702 - Train loss: 0.00190093  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.95 min) Epoch 60/300 -- Iteration 456574 - Batch 2156/7702 - Train loss: 0.00190176  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.97 min) Epoch 60/300 -- Iteration 456651 - Batch 2233/7702 - Train loss: 0.00190200  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(122.99 min) Epoch 60/300 -- Iteration 456728 - Batch 2310/7702 - Train loss: 0.00190037  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.01 min) Epoch 60/300 -- Iteration 456805 - Batch 2387/7702 - Train loss: 0.00189917  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.03 min) Epoch 60/300 -- Iteration 456882 - Batch 2464/7702 - Train loss: 0.00189930  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.05 min) Epoch 60/300 -- Iteration 456959 - Batch 2541/7702 - Train loss: 0.00189911  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.07 min) Epoch 60/300 -- Iteration 457036 - Batch 2618/7702 - Train loss: 0.00189837  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.10 min) Epoch 60/300 -- Iteration 457113 - Batch 2695/7702 - Train loss: 0.00189785  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.12 min) Epoch 60/300 -- Iteration 457190 - Batch 2772/7702 - Train loss: 0.00189816  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.14 min) Epoch 60/300 -- Iteration 457267 - Batch 2849/7702 - Train loss: 0.00189833  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.16 min) Epoch 60/300 -- Iteration 457344 - Batch 2926/7702 - Train loss: 0.00189911  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.18 min) Epoch 60/300 -- Iteration 457421 - Batch 3003/7702 - Train loss: 0.00189934  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.20 min) Epoch 60/300 -- Iteration 457498 - Batch 3080/7702 - Train loss: 0.00190033  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.22 min) Epoch 60/300 -- Iteration 457575 - Batch 3157/7702 - Train loss: 0.00190046  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.24 min) Epoch 60/300 -- Iteration 457652 - Batch 3234/7702 - Train loss: 0.00190020  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.26 min) Epoch 60/300 -- Iteration 457729 - Batch 3311/7702 - Train loss: 0.00189988  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.28 min) Epoch 60/300 -- Iteration 457806 - Batch 3388/7702 - Train loss: 0.00189906  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.30 min) Epoch 60/300 -- Iteration 457883 - Batch 3465/7702 - Train loss: 0.00189874  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.32 min) Epoch 60/300 -- Iteration 457960 - Batch 3542/7702 - Train loss: 0.00189889  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.34 min) Epoch 60/300 -- Iteration 458037 - Batch 3619/7702 - Train loss: 0.00189852  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.36 min) Epoch 60/300 -- Iteration 458114 - Batch 3696/7702 - Train loss: 0.00189851  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.39 min) Epoch 60/300 -- Iteration 458191 - Batch 3773/7702 - Train loss: 0.00189826  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.41 min) Epoch 60/300 -- Iteration 458268 - Batch 3850/7702 - Train loss: 0.00189889  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.43 min) Epoch 60/300 -- Iteration 458345 - Batch 3927/7702 - Train loss: 0.00189873  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.45 min) Epoch 60/300 -- Iteration 458422 - Batch 4004/7702 - Train loss: 0.00189790  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.47 min) Epoch 60/300 -- Iteration 458499 - Batch 4081/7702 - Train loss: 0.00189787  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.49 min) Epoch 60/300 -- Iteration 458576 - Batch 4158/7702 - Train loss: 0.00189789  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.51 min) Epoch 60/300 -- Iteration 458653 - Batch 4235/7702 - Train loss: 0.00189744  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.53 min) Epoch 60/300 -- Iteration 458730 - Batch 4312/7702 - Train loss: 0.00189708  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.55 min) Epoch 60/300 -- Iteration 458807 - Batch 4389/7702 - Train loss: 0.00189666  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.57 min) Epoch 60/300 -- Iteration 458884 - Batch 4466/7702 - Train loss: 0.00189758  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.59 min) Epoch 60/300 -- Iteration 458961 - Batch 4543/7702 - Train loss: 0.00189774  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.61 min) Epoch 60/300 -- Iteration 459038 - Batch 4620/7702 - Train loss: 0.00189710  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.63 min) Epoch 60/300 -- Iteration 459115 - Batch 4697/7702 - Train loss: 0.00189737  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.66 min) Epoch 60/300 -- Iteration 459192 - Batch 4774/7702 - Train loss: 0.00189764  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.68 min) Epoch 60/300 -- Iteration 459269 - Batch 4851/7702 - Train loss: 0.00189795  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.70 min) Epoch 60/300 -- Iteration 459346 - Batch 4928/7702 - Train loss: 0.00189816  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.72 min) Epoch 60/300 -- Iteration 459423 - Batch 5005/7702 - Train loss: 0.00189809  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.74 min) Epoch 60/300 -- Iteration 459500 - Batch 5082/7702 - Train loss: 0.00189790  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.76 min) Epoch 60/300 -- Iteration 459577 - Batch 5159/7702 - Train loss: 0.00189780  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.78 min) Epoch 60/300 -- Iteration 459654 - Batch 5236/7702 - Train loss: 0.00189731  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.80 min) Epoch 60/300 -- Iteration 459731 - Batch 5313/7702 - Train loss: 0.00189711  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.82 min) Epoch 60/300 -- Iteration 459808 - Batch 5390/7702 - Train loss: 0.00189722  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.84 min) Epoch 60/300 -- Iteration 459885 - Batch 5467/7702 - Train loss: 0.00189721  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.86 min) Epoch 60/300 -- Iteration 459962 - Batch 5544/7702 - Train loss: 0.00189705  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.88 min) Epoch 60/300 -- Iteration 460039 - Batch 5621/7702 - Train loss: 0.00189684  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.90 min) Epoch 60/300 -- Iteration 460116 - Batch 5698/7702 - Train loss: 0.00189695  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.92 min) Epoch 60/300 -- Iteration 460193 - Batch 5775/7702 - Train loss: 0.00189662  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.95 min) Epoch 60/300 -- Iteration 460270 - Batch 5852/7702 - Train loss: 0.00189635  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.97 min) Epoch 60/300 -- Iteration 460347 - Batch 5929/7702 - Train loss: 0.00189623  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(123.99 min) Epoch 60/300 -- Iteration 460424 - Batch 6006/7702 - Train loss: 0.00189634  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.01 min) Epoch 60/300 -- Iteration 460501 - Batch 6083/7702 - Train loss: 0.00189619  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.03 min) Epoch 60/300 -- Iteration 460578 - Batch 6160/7702 - Train loss: 0.00189622  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.05 min) Epoch 60/300 -- Iteration 460655 - Batch 6237/7702 - Train loss: 0.00189608  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.07 min) Epoch 60/300 -- Iteration 460732 - Batch 6314/7702 - Train loss: 0.00189604  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.09 min) Epoch 60/300 -- Iteration 460809 - Batch 6391/7702 - Train loss: 0.00189661  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.11 min) Epoch 60/300 -- Iteration 460886 - Batch 6468/7702 - Train loss: 0.00189647  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.13 min) Epoch 60/300 -- Iteration 460963 - Batch 6545/7702 - Train loss: 0.00189636  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.15 min) Epoch 60/300 -- Iteration 461040 - Batch 6622/7702 - Train loss: 0.00189641  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.17 min) Epoch 60/300 -- Iteration 461117 - Batch 6699/7702 - Train loss: 0.00189648  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.19 min) Epoch 60/300 -- Iteration 461194 - Batch 6776/7702 - Train loss: 0.00189589  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.21 min) Epoch 60/300 -- Iteration 461271 - Batch 6853/7702 - Train loss: 0.00189598  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.23 min) Epoch 60/300 -- Iteration 461348 - Batch 6930/7702 - Train loss: 0.00189603  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.26 min) Epoch 60/300 -- Iteration 461425 - Batch 7007/7702 - Train loss: 0.00189593  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.28 min) Epoch 60/300 -- Iteration 461502 - Batch 7084/7702 - Train loss: 0.00189579  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.30 min) Epoch 60/300 -- Iteration 461579 - Batch 7161/7702 - Train loss: 0.00189566  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.32 min) Epoch 60/300 -- Iteration 461656 - Batch 7238/7702 - Train loss: 0.00189534  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.34 min) Epoch 60/300 -- Iteration 461733 - Batch 7315/7702 - Train loss: 0.00189554  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.36 min) Epoch 60/300 -- Iteration 461810 - Batch 7392/7702 - Train loss: 0.00189509  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.38 min) Epoch 60/300 -- Iteration 461887 - Batch 7469/7702 - Train loss: 0.00189505  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.40 min) Epoch 60/300 -- Iteration 461964 - Batch 7546/7702 - Train loss: 0.00189508  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.42 min) Epoch 60/300 -- Iteration 462041 - Batch 7623/7702 - Train loss: 0.00189520  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.44 min) Epoch 60/300 -- Iteration 462118 - Batch 7700/7702 - Train loss: 0.00189528  - Train acc: -0.0000 - Val loss: 0.00045442\n",
      "(124.44 min) Epoch 60/300 -- Iteration 462120 - Batch 7701/7702 - Train loss: 0.00189527  - Train acc: -0.0000 - Val loss: 0.00046514 - Val acc: -0.0000\n",
      "(124.47 min) Epoch 61/300 -- Iteration 462197 - Batch 77/7702 - Train loss: 0.00192673  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.49 min) Epoch 61/300 -- Iteration 462274 - Batch 154/7702 - Train loss: 0.00191540  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.51 min) Epoch 61/300 -- Iteration 462351 - Batch 231/7702 - Train loss: 0.00190956  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.53 min) Epoch 61/300 -- Iteration 462428 - Batch 308/7702 - Train loss: 0.00190800  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.55 min) Epoch 61/300 -- Iteration 462505 - Batch 385/7702 - Train loss: 0.00190834  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.57 min) Epoch 61/300 -- Iteration 462582 - Batch 462/7702 - Train loss: 0.00190931  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.59 min) Epoch 61/300 -- Iteration 462659 - Batch 539/7702 - Train loss: 0.00190854  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.61 min) Epoch 61/300 -- Iteration 462736 - Batch 616/7702 - Train loss: 0.00190671  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.63 min) Epoch 61/300 -- Iteration 462813 - Batch 693/7702 - Train loss: 0.00190919  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.65 min) Epoch 61/300 -- Iteration 462890 - Batch 770/7702 - Train loss: 0.00190743  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.67 min) Epoch 61/300 -- Iteration 462967 - Batch 847/7702 - Train loss: 0.00190678  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.69 min) Epoch 61/300 -- Iteration 463044 - Batch 924/7702 - Train loss: 0.00190497  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.71 min) Epoch 61/300 -- Iteration 463121 - Batch 1001/7702 - Train loss: 0.00190583  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.74 min) Epoch 61/300 -- Iteration 463198 - Batch 1078/7702 - Train loss: 0.00190555  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.76 min) Epoch 61/300 -- Iteration 463275 - Batch 1155/7702 - Train loss: 0.00190353  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.78 min) Epoch 61/300 -- Iteration 463352 - Batch 1232/7702 - Train loss: 0.00190609  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.80 min) Epoch 61/300 -- Iteration 463429 - Batch 1309/7702 - Train loss: 0.00190596  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.82 min) Epoch 61/300 -- Iteration 463506 - Batch 1386/7702 - Train loss: 0.00190561  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.84 min) Epoch 61/300 -- Iteration 463583 - Batch 1463/7702 - Train loss: 0.00190745  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.86 min) Epoch 61/300 -- Iteration 463660 - Batch 1540/7702 - Train loss: 0.00190621  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.88 min) Epoch 61/300 -- Iteration 463737 - Batch 1617/7702 - Train loss: 0.00190682  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.90 min) Epoch 61/300 -- Iteration 463814 - Batch 1694/7702 - Train loss: 0.00190677  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.92 min) Epoch 61/300 -- Iteration 463891 - Batch 1771/7702 - Train loss: 0.00190601  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.94 min) Epoch 61/300 -- Iteration 463968 - Batch 1848/7702 - Train loss: 0.00190577  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.96 min) Epoch 61/300 -- Iteration 464045 - Batch 1925/7702 - Train loss: 0.00190642  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(124.98 min) Epoch 61/300 -- Iteration 464122 - Batch 2002/7702 - Train loss: 0.00190576  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.01 min) Epoch 61/300 -- Iteration 464199 - Batch 2079/7702 - Train loss: 0.00190664  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.03 min) Epoch 61/300 -- Iteration 464276 - Batch 2156/7702 - Train loss: 0.00190737  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.05 min) Epoch 61/300 -- Iteration 464353 - Batch 2233/7702 - Train loss: 0.00190752  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.07 min) Epoch 61/300 -- Iteration 464430 - Batch 2310/7702 - Train loss: 0.00190613  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.09 min) Epoch 61/300 -- Iteration 464507 - Batch 2387/7702 - Train loss: 0.00190603  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.11 min) Epoch 61/300 -- Iteration 464584 - Batch 2464/7702 - Train loss: 0.00190473  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.13 min) Epoch 61/300 -- Iteration 464661 - Batch 2541/7702 - Train loss: 0.00190500  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.15 min) Epoch 61/300 -- Iteration 464738 - Batch 2618/7702 - Train loss: 0.00190432  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.17 min) Epoch 61/300 -- Iteration 464815 - Batch 2695/7702 - Train loss: 0.00190403  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.19 min) Epoch 61/300 -- Iteration 464892 - Batch 2772/7702 - Train loss: 0.00190452  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.21 min) Epoch 61/300 -- Iteration 464969 - Batch 2849/7702 - Train loss: 0.00190502  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.23 min) Epoch 61/300 -- Iteration 465046 - Batch 2926/7702 - Train loss: 0.00190472  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.25 min) Epoch 61/300 -- Iteration 465123 - Batch 3003/7702 - Train loss: 0.00190441  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.27 min) Epoch 61/300 -- Iteration 465200 - Batch 3080/7702 - Train loss: 0.00190384  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.30 min) Epoch 61/300 -- Iteration 465277 - Batch 3157/7702 - Train loss: 0.00190390  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.32 min) Epoch 61/300 -- Iteration 465354 - Batch 3234/7702 - Train loss: 0.00190261  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.34 min) Epoch 61/300 -- Iteration 465431 - Batch 3311/7702 - Train loss: 0.00190284  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.36 min) Epoch 61/300 -- Iteration 465508 - Batch 3388/7702 - Train loss: 0.00190220  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.38 min) Epoch 61/300 -- Iteration 465585 - Batch 3465/7702 - Train loss: 0.00190180  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.40 min) Epoch 61/300 -- Iteration 465662 - Batch 3542/7702 - Train loss: 0.00190198  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.42 min) Epoch 61/300 -- Iteration 465739 - Batch 3619/7702 - Train loss: 0.00190256  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.44 min) Epoch 61/300 -- Iteration 465816 - Batch 3696/7702 - Train loss: 0.00190198  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.46 min) Epoch 61/300 -- Iteration 465893 - Batch 3773/7702 - Train loss: 0.00190220  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.48 min) Epoch 61/300 -- Iteration 465970 - Batch 3850/7702 - Train loss: 0.00190260  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.50 min) Epoch 61/300 -- Iteration 466047 - Batch 3927/7702 - Train loss: 0.00190204  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.52 min) Epoch 61/300 -- Iteration 466124 - Batch 4004/7702 - Train loss: 0.00190251  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.54 min) Epoch 61/300 -- Iteration 466201 - Batch 4081/7702 - Train loss: 0.00190210  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.57 min) Epoch 61/300 -- Iteration 466278 - Batch 4158/7702 - Train loss: 0.00190213  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.59 min) Epoch 61/300 -- Iteration 466355 - Batch 4235/7702 - Train loss: 0.00190251  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.61 min) Epoch 61/300 -- Iteration 466432 - Batch 4312/7702 - Train loss: 0.00190193  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.63 min) Epoch 61/300 -- Iteration 466509 - Batch 4389/7702 - Train loss: 0.00190193  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.65 min) Epoch 61/300 -- Iteration 466586 - Batch 4466/7702 - Train loss: 0.00190242  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.67 min) Epoch 61/300 -- Iteration 466663 - Batch 4543/7702 - Train loss: 0.00190193  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.69 min) Epoch 61/300 -- Iteration 466740 - Batch 4620/7702 - Train loss: 0.00190201  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.71 min) Epoch 61/300 -- Iteration 466817 - Batch 4697/7702 - Train loss: 0.00190150  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.73 min) Epoch 61/300 -- Iteration 466894 - Batch 4774/7702 - Train loss: 0.00190138  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.75 min) Epoch 61/300 -- Iteration 466971 - Batch 4851/7702 - Train loss: 0.00190141  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.77 min) Epoch 61/300 -- Iteration 467048 - Batch 4928/7702 - Train loss: 0.00190123  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.79 min) Epoch 61/300 -- Iteration 467125 - Batch 5005/7702 - Train loss: 0.00190093  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.81 min) Epoch 61/300 -- Iteration 467202 - Batch 5082/7702 - Train loss: 0.00190154  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.83 min) Epoch 61/300 -- Iteration 467279 - Batch 5159/7702 - Train loss: 0.00190208  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.86 min) Epoch 61/300 -- Iteration 467356 - Batch 5236/7702 - Train loss: 0.00190244  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.88 min) Epoch 61/300 -- Iteration 467433 - Batch 5313/7702 - Train loss: 0.00190278  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.90 min) Epoch 61/300 -- Iteration 467510 - Batch 5390/7702 - Train loss: 0.00190289  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.92 min) Epoch 61/300 -- Iteration 467587 - Batch 5467/7702 - Train loss: 0.00190296  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.94 min) Epoch 61/300 -- Iteration 467664 - Batch 5544/7702 - Train loss: 0.00190331  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.96 min) Epoch 61/300 -- Iteration 467741 - Batch 5621/7702 - Train loss: 0.00190358  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(125.98 min) Epoch 61/300 -- Iteration 467818 - Batch 5698/7702 - Train loss: 0.00190345  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.00 min) Epoch 61/300 -- Iteration 467895 - Batch 5775/7702 - Train loss: 0.00190362  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.02 min) Epoch 61/300 -- Iteration 467972 - Batch 5852/7702 - Train loss: 0.00190289  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.04 min) Epoch 61/300 -- Iteration 468049 - Batch 5929/7702 - Train loss: 0.00190337  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.06 min) Epoch 61/300 -- Iteration 468126 - Batch 6006/7702 - Train loss: 0.00190309  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.08 min) Epoch 61/300 -- Iteration 468203 - Batch 6083/7702 - Train loss: 0.00190260  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.10 min) Epoch 61/300 -- Iteration 468280 - Batch 6160/7702 - Train loss: 0.00190260  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.12 min) Epoch 61/300 -- Iteration 468357 - Batch 6237/7702 - Train loss: 0.00190279  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.15 min) Epoch 61/300 -- Iteration 468434 - Batch 6314/7702 - Train loss: 0.00190271  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.17 min) Epoch 61/300 -- Iteration 468511 - Batch 6391/7702 - Train loss: 0.00190187  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.19 min) Epoch 61/300 -- Iteration 468588 - Batch 6468/7702 - Train loss: 0.00190177  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.21 min) Epoch 61/300 -- Iteration 468665 - Batch 6545/7702 - Train loss: 0.00190136  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.23 min) Epoch 61/300 -- Iteration 468742 - Batch 6622/7702 - Train loss: 0.00190094  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.25 min) Epoch 61/300 -- Iteration 468819 - Batch 6699/7702 - Train loss: 0.00190094  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.27 min) Epoch 61/300 -- Iteration 468896 - Batch 6776/7702 - Train loss: 0.00190108  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.29 min) Epoch 61/300 -- Iteration 468973 - Batch 6853/7702 - Train loss: 0.00190073  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.31 min) Epoch 61/300 -- Iteration 469050 - Batch 6930/7702 - Train loss: 0.00190089  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.33 min) Epoch 61/300 -- Iteration 469127 - Batch 7007/7702 - Train loss: 0.00190033  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.35 min) Epoch 61/300 -- Iteration 469204 - Batch 7084/7702 - Train loss: 0.00190009  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.37 min) Epoch 61/300 -- Iteration 469281 - Batch 7161/7702 - Train loss: 0.00190044  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.40 min) Epoch 61/300 -- Iteration 469358 - Batch 7238/7702 - Train loss: 0.00190004  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.42 min) Epoch 61/300 -- Iteration 469435 - Batch 7315/7702 - Train loss: 0.00189995  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.44 min) Epoch 61/300 -- Iteration 469512 - Batch 7392/7702 - Train loss: 0.00190013  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.46 min) Epoch 61/300 -- Iteration 469589 - Batch 7469/7702 - Train loss: 0.00189999  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.48 min) Epoch 61/300 -- Iteration 469666 - Batch 7546/7702 - Train loss: 0.00189989  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.50 min) Epoch 61/300 -- Iteration 469743 - Batch 7623/7702 - Train loss: 0.00189969  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.52 min) Epoch 61/300 -- Iteration 469820 - Batch 7700/7702 - Train loss: 0.00190010  - Train acc: -0.0000 - Val loss: 0.00046514\n",
      "(126.52 min) Epoch 61/300 -- Iteration 469822 - Batch 7701/7702 - Train loss: 0.00190008  - Train acc: -0.0000 - Val loss: 0.00042585 - Val acc: -0.0000\n",
      "(126.54 min) Epoch 62/300 -- Iteration 469899 - Batch 77/7702 - Train loss: 0.00189096  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.56 min) Epoch 62/300 -- Iteration 469976 - Batch 154/7702 - Train loss: 0.00189628  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.58 min) Epoch 62/300 -- Iteration 470053 - Batch 231/7702 - Train loss: 0.00190537  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.61 min) Epoch 62/300 -- Iteration 470130 - Batch 308/7702 - Train loss: 0.00190744  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.63 min) Epoch 62/300 -- Iteration 470207 - Batch 385/7702 - Train loss: 0.00190925  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.65 min) Epoch 62/300 -- Iteration 470284 - Batch 462/7702 - Train loss: 0.00190200  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.67 min) Epoch 62/300 -- Iteration 470361 - Batch 539/7702 - Train loss: 0.00190225  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.69 min) Epoch 62/300 -- Iteration 470438 - Batch 616/7702 - Train loss: 0.00190834  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.71 min) Epoch 62/300 -- Iteration 470515 - Batch 693/7702 - Train loss: 0.00190573  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.73 min) Epoch 62/300 -- Iteration 470592 - Batch 770/7702 - Train loss: 0.00190195  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.75 min) Epoch 62/300 -- Iteration 470669 - Batch 847/7702 - Train loss: 0.00190540  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.77 min) Epoch 62/300 -- Iteration 470746 - Batch 924/7702 - Train loss: 0.00190540  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.79 min) Epoch 62/300 -- Iteration 470823 - Batch 1001/7702 - Train loss: 0.00190396  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.81 min) Epoch 62/300 -- Iteration 470900 - Batch 1078/7702 - Train loss: 0.00190457  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.83 min) Epoch 62/300 -- Iteration 470977 - Batch 1155/7702 - Train loss: 0.00190247  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.85 min) Epoch 62/300 -- Iteration 471054 - Batch 1232/7702 - Train loss: 0.00190247  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.87 min) Epoch 62/300 -- Iteration 471131 - Batch 1309/7702 - Train loss: 0.00190108  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.90 min) Epoch 62/300 -- Iteration 471208 - Batch 1386/7702 - Train loss: 0.00189999  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.92 min) Epoch 62/300 -- Iteration 471285 - Batch 1463/7702 - Train loss: 0.00189788  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.94 min) Epoch 62/300 -- Iteration 471362 - Batch 1540/7702 - Train loss: 0.00189847  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.96 min) Epoch 62/300 -- Iteration 471439 - Batch 1617/7702 - Train loss: 0.00190004  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(126.98 min) Epoch 62/300 -- Iteration 471516 - Batch 1694/7702 - Train loss: 0.00189857  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.00 min) Epoch 62/300 -- Iteration 471593 - Batch 1771/7702 - Train loss: 0.00189910  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.02 min) Epoch 62/300 -- Iteration 471670 - Batch 1848/7702 - Train loss: 0.00189851  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.04 min) Epoch 62/300 -- Iteration 471747 - Batch 1925/7702 - Train loss: 0.00189956  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.06 min) Epoch 62/300 -- Iteration 471824 - Batch 2002/7702 - Train loss: 0.00189892  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.08 min) Epoch 62/300 -- Iteration 471901 - Batch 2079/7702 - Train loss: 0.00189889  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.10 min) Epoch 62/300 -- Iteration 471978 - Batch 2156/7702 - Train loss: 0.00189817  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.12 min) Epoch 62/300 -- Iteration 472055 - Batch 2233/7702 - Train loss: 0.00189829  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.14 min) Epoch 62/300 -- Iteration 472132 - Batch 2310/7702 - Train loss: 0.00189813  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.16 min) Epoch 62/300 -- Iteration 472209 - Batch 2387/7702 - Train loss: 0.00189781  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.18 min) Epoch 62/300 -- Iteration 472286 - Batch 2464/7702 - Train loss: 0.00189763  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.21 min) Epoch 62/300 -- Iteration 472363 - Batch 2541/7702 - Train loss: 0.00189839  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.23 min) Epoch 62/300 -- Iteration 472440 - Batch 2618/7702 - Train loss: 0.00189823  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.25 min) Epoch 62/300 -- Iteration 472517 - Batch 2695/7702 - Train loss: 0.00189786  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.27 min) Epoch 62/300 -- Iteration 472594 - Batch 2772/7702 - Train loss: 0.00189722  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.29 min) Epoch 62/300 -- Iteration 472671 - Batch 2849/7702 - Train loss: 0.00189676  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.31 min) Epoch 62/300 -- Iteration 472748 - Batch 2926/7702 - Train loss: 0.00189713  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.33 min) Epoch 62/300 -- Iteration 472825 - Batch 3003/7702 - Train loss: 0.00189544  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.35 min) Epoch 62/300 -- Iteration 472902 - Batch 3080/7702 - Train loss: 0.00189499  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.37 min) Epoch 62/300 -- Iteration 472979 - Batch 3157/7702 - Train loss: 0.00189559  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.39 min) Epoch 62/300 -- Iteration 473056 - Batch 3234/7702 - Train loss: 0.00189556  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.41 min) Epoch 62/300 -- Iteration 473133 - Batch 3311/7702 - Train loss: 0.00189486  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.43 min) Epoch 62/300 -- Iteration 473210 - Batch 3388/7702 - Train loss: 0.00189379  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.45 min) Epoch 62/300 -- Iteration 473287 - Batch 3465/7702 - Train loss: 0.00189377  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.47 min) Epoch 62/300 -- Iteration 473364 - Batch 3542/7702 - Train loss: 0.00189420  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.50 min) Epoch 62/300 -- Iteration 473441 - Batch 3619/7702 - Train loss: 0.00189405  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.52 min) Epoch 62/300 -- Iteration 473518 - Batch 3696/7702 - Train loss: 0.00189422  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.54 min) Epoch 62/300 -- Iteration 473595 - Batch 3773/7702 - Train loss: 0.00189490  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.56 min) Epoch 62/300 -- Iteration 473672 - Batch 3850/7702 - Train loss: 0.00189438  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.58 min) Epoch 62/300 -- Iteration 473749 - Batch 3927/7702 - Train loss: 0.00189515  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.60 min) Epoch 62/300 -- Iteration 473826 - Batch 4004/7702 - Train loss: 0.00189554  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.62 min) Epoch 62/300 -- Iteration 473903 - Batch 4081/7702 - Train loss: 0.00189527  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.64 min) Epoch 62/300 -- Iteration 473980 - Batch 4158/7702 - Train loss: 0.00189528  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.66 min) Epoch 62/300 -- Iteration 474057 - Batch 4235/7702 - Train loss: 0.00189540  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.68 min) Epoch 62/300 -- Iteration 474134 - Batch 4312/7702 - Train loss: 0.00189575  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.70 min) Epoch 62/300 -- Iteration 474211 - Batch 4389/7702 - Train loss: 0.00189585  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.72 min) Epoch 62/300 -- Iteration 474288 - Batch 4466/7702 - Train loss: 0.00189550  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.74 min) Epoch 62/300 -- Iteration 474365 - Batch 4543/7702 - Train loss: 0.00189531  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.76 min) Epoch 62/300 -- Iteration 474442 - Batch 4620/7702 - Train loss: 0.00189519  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.79 min) Epoch 62/300 -- Iteration 474519 - Batch 4697/7702 - Train loss: 0.00189544  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.81 min) Epoch 62/300 -- Iteration 474596 - Batch 4774/7702 - Train loss: 0.00189540  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.83 min) Epoch 62/300 -- Iteration 474673 - Batch 4851/7702 - Train loss: 0.00189562  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.85 min) Epoch 62/300 -- Iteration 474750 - Batch 4928/7702 - Train loss: 0.00189558  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.87 min) Epoch 62/300 -- Iteration 474827 - Batch 5005/7702 - Train loss: 0.00189562  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.89 min) Epoch 62/300 -- Iteration 474904 - Batch 5082/7702 - Train loss: 0.00189556  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.91 min) Epoch 62/300 -- Iteration 474981 - Batch 5159/7702 - Train loss: 0.00189644  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.93 min) Epoch 62/300 -- Iteration 475058 - Batch 5236/7702 - Train loss: 0.00189680  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.95 min) Epoch 62/300 -- Iteration 475135 - Batch 5313/7702 - Train loss: 0.00189703  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.97 min) Epoch 62/300 -- Iteration 475212 - Batch 5390/7702 - Train loss: 0.00189675  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(127.99 min) Epoch 62/300 -- Iteration 475289 - Batch 5467/7702 - Train loss: 0.00189667  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.01 min) Epoch 62/300 -- Iteration 475366 - Batch 5544/7702 - Train loss: 0.00189659  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.03 min) Epoch 62/300 -- Iteration 475443 - Batch 5621/7702 - Train loss: 0.00189689  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.05 min) Epoch 62/300 -- Iteration 475520 - Batch 5698/7702 - Train loss: 0.00189737  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.08 min) Epoch 62/300 -- Iteration 475597 - Batch 5775/7702 - Train loss: 0.00189718  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.10 min) Epoch 62/300 -- Iteration 475674 - Batch 5852/7702 - Train loss: 0.00189710  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.12 min) Epoch 62/300 -- Iteration 475751 - Batch 5929/7702 - Train loss: 0.00189713  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.14 min) Epoch 62/300 -- Iteration 475828 - Batch 6006/7702 - Train loss: 0.00189735  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.16 min) Epoch 62/300 -- Iteration 475905 - Batch 6083/7702 - Train loss: 0.00189733  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.18 min) Epoch 62/300 -- Iteration 475982 - Batch 6160/7702 - Train loss: 0.00189735  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.20 min) Epoch 62/300 -- Iteration 476059 - Batch 6237/7702 - Train loss: 0.00189745  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.22 min) Epoch 62/300 -- Iteration 476136 - Batch 6314/7702 - Train loss: 0.00189743  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.24 min) Epoch 62/300 -- Iteration 476213 - Batch 6391/7702 - Train loss: 0.00189760  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.26 min) Epoch 62/300 -- Iteration 476290 - Batch 6468/7702 - Train loss: 0.00189824  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.28 min) Epoch 62/300 -- Iteration 476367 - Batch 6545/7702 - Train loss: 0.00189847  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.30 min) Epoch 62/300 -- Iteration 476444 - Batch 6622/7702 - Train loss: 0.00189830  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.32 min) Epoch 62/300 -- Iteration 476521 - Batch 6699/7702 - Train loss: 0.00189820  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.34 min) Epoch 62/300 -- Iteration 476598 - Batch 6776/7702 - Train loss: 0.00189834  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.37 min) Epoch 62/300 -- Iteration 476675 - Batch 6853/7702 - Train loss: 0.00189831  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.39 min) Epoch 62/300 -- Iteration 476752 - Batch 6930/7702 - Train loss: 0.00189834  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.41 min) Epoch 62/300 -- Iteration 476829 - Batch 7007/7702 - Train loss: 0.00189762  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.43 min) Epoch 62/300 -- Iteration 476906 - Batch 7084/7702 - Train loss: 0.00189763  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.45 min) Epoch 62/300 -- Iteration 476983 - Batch 7161/7702 - Train loss: 0.00189738  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.47 min) Epoch 62/300 -- Iteration 477060 - Batch 7238/7702 - Train loss: 0.00189766  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.49 min) Epoch 62/300 -- Iteration 477137 - Batch 7315/7702 - Train loss: 0.00189750  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.51 min) Epoch 62/300 -- Iteration 477214 - Batch 7392/7702 - Train loss: 0.00189738  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.53 min) Epoch 62/300 -- Iteration 477291 - Batch 7469/7702 - Train loss: 0.00189717  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.55 min) Epoch 62/300 -- Iteration 477368 - Batch 7546/7702 - Train loss: 0.00189723  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.57 min) Epoch 62/300 -- Iteration 477445 - Batch 7623/7702 - Train loss: 0.00189699  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.59 min) Epoch 62/300 -- Iteration 477522 - Batch 7700/7702 - Train loss: 0.00189695  - Train acc: -0.0000 - Val loss: 0.00042585\n",
      "(128.59 min) Epoch 62/300 -- Iteration 477524 - Batch 7701/7702 - Train loss: 0.00189692  - Train acc: -0.0000 - Val loss: 0.00036896 - Val acc: -0.0000\n",
      "(128.62 min) Epoch 63/300 -- Iteration 477601 - Batch 77/7702 - Train loss: 0.00186152  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.64 min) Epoch 63/300 -- Iteration 477678 - Batch 154/7702 - Train loss: 0.00186064  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.66 min) Epoch 63/300 -- Iteration 477755 - Batch 231/7702 - Train loss: 0.00188226  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.68 min) Epoch 63/300 -- Iteration 477832 - Batch 308/7702 - Train loss: 0.00188540  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.70 min) Epoch 63/300 -- Iteration 477909 - Batch 385/7702 - Train loss: 0.00188609  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.72 min) Epoch 63/300 -- Iteration 477986 - Batch 462/7702 - Train loss: 0.00188268  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.74 min) Epoch 63/300 -- Iteration 478063 - Batch 539/7702 - Train loss: 0.00188934  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.76 min) Epoch 63/300 -- Iteration 478140 - Batch 616/7702 - Train loss: 0.00189026  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.78 min) Epoch 63/300 -- Iteration 478217 - Batch 693/7702 - Train loss: 0.00188940  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.80 min) Epoch 63/300 -- Iteration 478294 - Batch 770/7702 - Train loss: 0.00189155  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.82 min) Epoch 63/300 -- Iteration 478371 - Batch 847/7702 - Train loss: 0.00189396  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.84 min) Epoch 63/300 -- Iteration 478448 - Batch 924/7702 - Train loss: 0.00189611  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.86 min) Epoch 63/300 -- Iteration 478525 - Batch 1001/7702 - Train loss: 0.00189738  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.89 min) Epoch 63/300 -- Iteration 478602 - Batch 1078/7702 - Train loss: 0.00189681  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.91 min) Epoch 63/300 -- Iteration 478679 - Batch 1155/7702 - Train loss: 0.00189693  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.93 min) Epoch 63/300 -- Iteration 478756 - Batch 1232/7702 - Train loss: 0.00189479  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.95 min) Epoch 63/300 -- Iteration 478833 - Batch 1309/7702 - Train loss: 0.00189611  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.97 min) Epoch 63/300 -- Iteration 478910 - Batch 1386/7702 - Train loss: 0.00189536  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(128.99 min) Epoch 63/300 -- Iteration 478987 - Batch 1463/7702 - Train loss: 0.00189747  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.01 min) Epoch 63/300 -- Iteration 479064 - Batch 1540/7702 - Train loss: 0.00189898  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.03 min) Epoch 63/300 -- Iteration 479141 - Batch 1617/7702 - Train loss: 0.00189901  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.05 min) Epoch 63/300 -- Iteration 479218 - Batch 1694/7702 - Train loss: 0.00189740  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.07 min) Epoch 63/300 -- Iteration 479295 - Batch 1771/7702 - Train loss: 0.00189576  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.09 min) Epoch 63/300 -- Iteration 479372 - Batch 1848/7702 - Train loss: 0.00189602  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.11 min) Epoch 63/300 -- Iteration 479449 - Batch 1925/7702 - Train loss: 0.00189936  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.13 min) Epoch 63/300 -- Iteration 479526 - Batch 2002/7702 - Train loss: 0.00189936  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.15 min) Epoch 63/300 -- Iteration 479603 - Batch 2079/7702 - Train loss: 0.00190045  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.18 min) Epoch 63/300 -- Iteration 479680 - Batch 2156/7702 - Train loss: 0.00190154  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.20 min) Epoch 63/300 -- Iteration 479757 - Batch 2233/7702 - Train loss: 0.00190072  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.22 min) Epoch 63/300 -- Iteration 479834 - Batch 2310/7702 - Train loss: 0.00190129  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.24 min) Epoch 63/300 -- Iteration 479911 - Batch 2387/7702 - Train loss: 0.00190038  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.26 min) Epoch 63/300 -- Iteration 479988 - Batch 2464/7702 - Train loss: 0.00190027  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.28 min) Epoch 63/300 -- Iteration 480065 - Batch 2541/7702 - Train loss: 0.00190128  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.30 min) Epoch 63/300 -- Iteration 480142 - Batch 2618/7702 - Train loss: 0.00190090  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.32 min) Epoch 63/300 -- Iteration 480219 - Batch 2695/7702 - Train loss: 0.00189986  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.34 min) Epoch 63/300 -- Iteration 480296 - Batch 2772/7702 - Train loss: 0.00189998  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.36 min) Epoch 63/300 -- Iteration 480373 - Batch 2849/7702 - Train loss: 0.00190020  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.38 min) Epoch 63/300 -- Iteration 480450 - Batch 2926/7702 - Train loss: 0.00189937  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.40 min) Epoch 63/300 -- Iteration 480527 - Batch 3003/7702 - Train loss: 0.00189990  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.42 min) Epoch 63/300 -- Iteration 480604 - Batch 3080/7702 - Train loss: 0.00190034  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.44 min) Epoch 63/300 -- Iteration 480681 - Batch 3157/7702 - Train loss: 0.00190088  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.47 min) Epoch 63/300 -- Iteration 480758 - Batch 3234/7702 - Train loss: 0.00190089  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.49 min) Epoch 63/300 -- Iteration 480835 - Batch 3311/7702 - Train loss: 0.00190067  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.51 min) Epoch 63/300 -- Iteration 480912 - Batch 3388/7702 - Train loss: 0.00190007  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.53 min) Epoch 63/300 -- Iteration 480989 - Batch 3465/7702 - Train loss: 0.00190031  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.55 min) Epoch 63/300 -- Iteration 481066 - Batch 3542/7702 - Train loss: 0.00190067  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.57 min) Epoch 63/300 -- Iteration 481143 - Batch 3619/7702 - Train loss: 0.00190133  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.59 min) Epoch 63/300 -- Iteration 481220 - Batch 3696/7702 - Train loss: 0.00190095  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.61 min) Epoch 63/300 -- Iteration 481297 - Batch 3773/7702 - Train loss: 0.00190095  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.63 min) Epoch 63/300 -- Iteration 481374 - Batch 3850/7702 - Train loss: 0.00190077  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.65 min) Epoch 63/300 -- Iteration 481451 - Batch 3927/7702 - Train loss: 0.00190073  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.67 min) Epoch 63/300 -- Iteration 481528 - Batch 4004/7702 - Train loss: 0.00190120  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.69 min) Epoch 63/300 -- Iteration 481605 - Batch 4081/7702 - Train loss: 0.00190091  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.72 min) Epoch 63/300 -- Iteration 481682 - Batch 4158/7702 - Train loss: 0.00190090  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.74 min) Epoch 63/300 -- Iteration 481759 - Batch 4235/7702 - Train loss: 0.00190042  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.76 min) Epoch 63/300 -- Iteration 481836 - Batch 4312/7702 - Train loss: 0.00190095  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.78 min) Epoch 63/300 -- Iteration 481913 - Batch 4389/7702 - Train loss: 0.00190095  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.80 min) Epoch 63/300 -- Iteration 481990 - Batch 4466/7702 - Train loss: 0.00190075  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.82 min) Epoch 63/300 -- Iteration 482067 - Batch 4543/7702 - Train loss: 0.00190072  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.84 min) Epoch 63/300 -- Iteration 482144 - Batch 4620/7702 - Train loss: 0.00190066  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.86 min) Epoch 63/300 -- Iteration 482221 - Batch 4697/7702 - Train loss: 0.00190081  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.88 min) Epoch 63/300 -- Iteration 482298 - Batch 4774/7702 - Train loss: 0.00190043  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.90 min) Epoch 63/300 -- Iteration 482375 - Batch 4851/7702 - Train loss: 0.00189997  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.92 min) Epoch 63/300 -- Iteration 482452 - Batch 4928/7702 - Train loss: 0.00190007  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.94 min) Epoch 63/300 -- Iteration 482529 - Batch 5005/7702 - Train loss: 0.00190037  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.96 min) Epoch 63/300 -- Iteration 482606 - Batch 5082/7702 - Train loss: 0.00190092  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(129.98 min) Epoch 63/300 -- Iteration 482683 - Batch 5159/7702 - Train loss: 0.00190031  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.01 min) Epoch 63/300 -- Iteration 482760 - Batch 5236/7702 - Train loss: 0.00189971  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.03 min) Epoch 63/300 -- Iteration 482837 - Batch 5313/7702 - Train loss: 0.00189976  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.05 min) Epoch 63/300 -- Iteration 482914 - Batch 5390/7702 - Train loss: 0.00189934  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.07 min) Epoch 63/300 -- Iteration 482991 - Batch 5467/7702 - Train loss: 0.00189979  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.09 min) Epoch 63/300 -- Iteration 483068 - Batch 5544/7702 - Train loss: 0.00189933  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.11 min) Epoch 63/300 -- Iteration 483145 - Batch 5621/7702 - Train loss: 0.00189901  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.13 min) Epoch 63/300 -- Iteration 483222 - Batch 5698/7702 - Train loss: 0.00190036  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.15 min) Epoch 63/300 -- Iteration 483299 - Batch 5775/7702 - Train loss: 0.00190030  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.17 min) Epoch 63/300 -- Iteration 483376 - Batch 5852/7702 - Train loss: 0.00190007  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.19 min) Epoch 63/300 -- Iteration 483453 - Batch 5929/7702 - Train loss: 0.00189977  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.21 min) Epoch 63/300 -- Iteration 483530 - Batch 6006/7702 - Train loss: 0.00189958  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.23 min) Epoch 63/300 -- Iteration 483607 - Batch 6083/7702 - Train loss: 0.00189984  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.25 min) Epoch 63/300 -- Iteration 483684 - Batch 6160/7702 - Train loss: 0.00189982  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.27 min) Epoch 63/300 -- Iteration 483761 - Batch 6237/7702 - Train loss: 0.00189978  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.30 min) Epoch 63/300 -- Iteration 483838 - Batch 6314/7702 - Train loss: 0.00190003  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.32 min) Epoch 63/300 -- Iteration 483915 - Batch 6391/7702 - Train loss: 0.00190010  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.34 min) Epoch 63/300 -- Iteration 483992 - Batch 6468/7702 - Train loss: 0.00189948  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.36 min) Epoch 63/300 -- Iteration 484069 - Batch 6545/7702 - Train loss: 0.00189939  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.38 min) Epoch 63/300 -- Iteration 484146 - Batch 6622/7702 - Train loss: 0.00189895  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.40 min) Epoch 63/300 -- Iteration 484223 - Batch 6699/7702 - Train loss: 0.00189880  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.42 min) Epoch 63/300 -- Iteration 484300 - Batch 6776/7702 - Train loss: 0.00189887  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.44 min) Epoch 63/300 -- Iteration 484377 - Batch 6853/7702 - Train loss: 0.00189857  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.46 min) Epoch 63/300 -- Iteration 484454 - Batch 6930/7702 - Train loss: 0.00189867  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.48 min) Epoch 63/300 -- Iteration 484531 - Batch 7007/7702 - Train loss: 0.00189846  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.50 min) Epoch 63/300 -- Iteration 484608 - Batch 7084/7702 - Train loss: 0.00189841  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.52 min) Epoch 63/300 -- Iteration 484685 - Batch 7161/7702 - Train loss: 0.00189871  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.55 min) Epoch 63/300 -- Iteration 484762 - Batch 7238/7702 - Train loss: 0.00189872  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.57 min) Epoch 63/300 -- Iteration 484839 - Batch 7315/7702 - Train loss: 0.00189808  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.59 min) Epoch 63/300 -- Iteration 484916 - Batch 7392/7702 - Train loss: 0.00189804  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.61 min) Epoch 63/300 -- Iteration 484993 - Batch 7469/7702 - Train loss: 0.00189804  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.63 min) Epoch 63/300 -- Iteration 485070 - Batch 7546/7702 - Train loss: 0.00189788  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.65 min) Epoch 63/300 -- Iteration 485147 - Batch 7623/7702 - Train loss: 0.00189758  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.67 min) Epoch 63/300 -- Iteration 485224 - Batch 7700/7702 - Train loss: 0.00189754  - Train acc: -0.0000 - Val loss: 0.00036896\n",
      "(130.67 min) Epoch 63/300 -- Iteration 485226 - Batch 7701/7702 - Train loss: 0.00189748  - Train acc: -0.0000 - Val loss: 0.00039623 - Val acc: -0.0000\n",
      "(130.69 min) Epoch 64/300 -- Iteration 485303 - Batch 77/7702 - Train loss: 0.00189647  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.71 min) Epoch 64/300 -- Iteration 485380 - Batch 154/7702 - Train loss: 0.00190413  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.73 min) Epoch 64/300 -- Iteration 485457 - Batch 231/7702 - Train loss: 0.00190366  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.76 min) Epoch 64/300 -- Iteration 485534 - Batch 308/7702 - Train loss: 0.00190230  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.78 min) Epoch 64/300 -- Iteration 485611 - Batch 385/7702 - Train loss: 0.00190584  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.80 min) Epoch 64/300 -- Iteration 485688 - Batch 462/7702 - Train loss: 0.00189614  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.82 min) Epoch 64/300 -- Iteration 485765 - Batch 539/7702 - Train loss: 0.00189463  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.84 min) Epoch 64/300 -- Iteration 485842 - Batch 616/7702 - Train loss: 0.00189552  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.86 min) Epoch 64/300 -- Iteration 485919 - Batch 693/7702 - Train loss: 0.00189405  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.88 min) Epoch 64/300 -- Iteration 485996 - Batch 770/7702 - Train loss: 0.00189572  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.90 min) Epoch 64/300 -- Iteration 486073 - Batch 847/7702 - Train loss: 0.00189417  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.92 min) Epoch 64/300 -- Iteration 486150 - Batch 924/7702 - Train loss: 0.00189464  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.94 min) Epoch 64/300 -- Iteration 486227 - Batch 1001/7702 - Train loss: 0.00189581  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.96 min) Epoch 64/300 -- Iteration 486304 - Batch 1078/7702 - Train loss: 0.00189632  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(130.98 min) Epoch 64/300 -- Iteration 486381 - Batch 1155/7702 - Train loss: 0.00189623  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.00 min) Epoch 64/300 -- Iteration 486458 - Batch 1232/7702 - Train loss: 0.00189739  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.03 min) Epoch 64/300 -- Iteration 486535 - Batch 1309/7702 - Train loss: 0.00189676  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.05 min) Epoch 64/300 -- Iteration 486612 - Batch 1386/7702 - Train loss: 0.00189681  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.07 min) Epoch 64/300 -- Iteration 486689 - Batch 1463/7702 - Train loss: 0.00189829  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.09 min) Epoch 64/300 -- Iteration 486766 - Batch 1540/7702 - Train loss: 0.00189792  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.11 min) Epoch 64/300 -- Iteration 486843 - Batch 1617/7702 - Train loss: 0.00189732  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.13 min) Epoch 64/300 -- Iteration 486920 - Batch 1694/7702 - Train loss: 0.00189698  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.15 min) Epoch 64/300 -- Iteration 486997 - Batch 1771/7702 - Train loss: 0.00189810  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.17 min) Epoch 64/300 -- Iteration 487074 - Batch 1848/7702 - Train loss: 0.00189619  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.19 min) Epoch 64/300 -- Iteration 487151 - Batch 1925/7702 - Train loss: 0.00189558  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.21 min) Epoch 64/300 -- Iteration 487228 - Batch 2002/7702 - Train loss: 0.00189550  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.23 min) Epoch 64/300 -- Iteration 487305 - Batch 2079/7702 - Train loss: 0.00189604  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.25 min) Epoch 64/300 -- Iteration 487382 - Batch 2156/7702 - Train loss: 0.00189544  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.27 min) Epoch 64/300 -- Iteration 487459 - Batch 2233/7702 - Train loss: 0.00189529  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.30 min) Epoch 64/300 -- Iteration 487536 - Batch 2310/7702 - Train loss: 0.00189665  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.32 min) Epoch 64/300 -- Iteration 487613 - Batch 2387/7702 - Train loss: 0.00189694  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.34 min) Epoch 64/300 -- Iteration 487690 - Batch 2464/7702 - Train loss: 0.00189701  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.36 min) Epoch 64/300 -- Iteration 487767 - Batch 2541/7702 - Train loss: 0.00189681  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.38 min) Epoch 64/300 -- Iteration 487844 - Batch 2618/7702 - Train loss: 0.00189676  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.40 min) Epoch 64/300 -- Iteration 487921 - Batch 2695/7702 - Train loss: 0.00189763  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.42 min) Epoch 64/300 -- Iteration 487998 - Batch 2772/7702 - Train loss: 0.00189896  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.44 min) Epoch 64/300 -- Iteration 488075 - Batch 2849/7702 - Train loss: 0.00189852  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.46 min) Epoch 64/300 -- Iteration 488152 - Batch 2926/7702 - Train loss: 0.00189858  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.48 min) Epoch 64/300 -- Iteration 488229 - Batch 3003/7702 - Train loss: 0.00189842  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.50 min) Epoch 64/300 -- Iteration 488306 - Batch 3080/7702 - Train loss: 0.00189793  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.52 min) Epoch 64/300 -- Iteration 488383 - Batch 3157/7702 - Train loss: 0.00189791  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.54 min) Epoch 64/300 -- Iteration 488460 - Batch 3234/7702 - Train loss: 0.00189799  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.57 min) Epoch 64/300 -- Iteration 488537 - Batch 3311/7702 - Train loss: 0.00189881  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.59 min) Epoch 64/300 -- Iteration 488614 - Batch 3388/7702 - Train loss: 0.00189898  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.61 min) Epoch 64/300 -- Iteration 488691 - Batch 3465/7702 - Train loss: 0.00189891  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.63 min) Epoch 64/300 -- Iteration 488768 - Batch 3542/7702 - Train loss: 0.00189908  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.65 min) Epoch 64/300 -- Iteration 488845 - Batch 3619/7702 - Train loss: 0.00189930  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.67 min) Epoch 64/300 -- Iteration 488922 - Batch 3696/7702 - Train loss: 0.00189968  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.69 min) Epoch 64/300 -- Iteration 488999 - Batch 3773/7702 - Train loss: 0.00189961  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.71 min) Epoch 64/300 -- Iteration 489076 - Batch 3850/7702 - Train loss: 0.00189926  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.73 min) Epoch 64/300 -- Iteration 489153 - Batch 3927/7702 - Train loss: 0.00189898  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.75 min) Epoch 64/300 -- Iteration 489230 - Batch 4004/7702 - Train loss: 0.00189904  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.77 min) Epoch 64/300 -- Iteration 489307 - Batch 4081/7702 - Train loss: 0.00189895  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.79 min) Epoch 64/300 -- Iteration 489384 - Batch 4158/7702 - Train loss: 0.00189827  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.81 min) Epoch 64/300 -- Iteration 489461 - Batch 4235/7702 - Train loss: 0.00189781  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.84 min) Epoch 64/300 -- Iteration 489538 - Batch 4312/7702 - Train loss: 0.00189825  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.86 min) Epoch 64/300 -- Iteration 489615 - Batch 4389/7702 - Train loss: 0.00189771  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.88 min) Epoch 64/300 -- Iteration 489692 - Batch 4466/7702 - Train loss: 0.00189764  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.90 min) Epoch 64/300 -- Iteration 489769 - Batch 4543/7702 - Train loss: 0.00189749  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.92 min) Epoch 64/300 -- Iteration 489846 - Batch 4620/7702 - Train loss: 0.00189765  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.94 min) Epoch 64/300 -- Iteration 489923 - Batch 4697/7702 - Train loss: 0.00189737  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.96 min) Epoch 64/300 -- Iteration 490000 - Batch 4774/7702 - Train loss: 0.00189710  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(131.98 min) Epoch 64/300 -- Iteration 490077 - Batch 4851/7702 - Train loss: 0.00189739  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.00 min) Epoch 64/300 -- Iteration 490154 - Batch 4928/7702 - Train loss: 0.00189741  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.02 min) Epoch 64/300 -- Iteration 490231 - Batch 5005/7702 - Train loss: 0.00189783  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.04 min) Epoch 64/300 -- Iteration 490308 - Batch 5082/7702 - Train loss: 0.00189821  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.06 min) Epoch 64/300 -- Iteration 490385 - Batch 5159/7702 - Train loss: 0.00189815  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.08 min) Epoch 64/300 -- Iteration 490462 - Batch 5236/7702 - Train loss: 0.00189784  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.10 min) Epoch 64/300 -- Iteration 490539 - Batch 5313/7702 - Train loss: 0.00189764  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.12 min) Epoch 64/300 -- Iteration 490616 - Batch 5390/7702 - Train loss: 0.00189754  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.14 min) Epoch 64/300 -- Iteration 490693 - Batch 5467/7702 - Train loss: 0.00189678  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.17 min) Epoch 64/300 -- Iteration 490770 - Batch 5544/7702 - Train loss: 0.00189697  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.19 min) Epoch 64/300 -- Iteration 490847 - Batch 5621/7702 - Train loss: 0.00189699  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.21 min) Epoch 64/300 -- Iteration 490924 - Batch 5698/7702 - Train loss: 0.00189743  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.23 min) Epoch 64/300 -- Iteration 491001 - Batch 5775/7702 - Train loss: 0.00189721  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.25 min) Epoch 64/300 -- Iteration 491078 - Batch 5852/7702 - Train loss: 0.00189730  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.27 min) Epoch 64/300 -- Iteration 491155 - Batch 5929/7702 - Train loss: 0.00189727  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.29 min) Epoch 64/300 -- Iteration 491232 - Batch 6006/7702 - Train loss: 0.00189741  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.31 min) Epoch 64/300 -- Iteration 491309 - Batch 6083/7702 - Train loss: 0.00189721  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.33 min) Epoch 64/300 -- Iteration 491386 - Batch 6160/7702 - Train loss: 0.00189711  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.35 min) Epoch 64/300 -- Iteration 491463 - Batch 6237/7702 - Train loss: 0.00189737  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.37 min) Epoch 64/300 -- Iteration 491540 - Batch 6314/7702 - Train loss: 0.00189744  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.39 min) Epoch 64/300 -- Iteration 491617 - Batch 6391/7702 - Train loss: 0.00189752  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.41 min) Epoch 64/300 -- Iteration 491694 - Batch 6468/7702 - Train loss: 0.00189715  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.43 min) Epoch 64/300 -- Iteration 491771 - Batch 6545/7702 - Train loss: 0.00189746  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.46 min) Epoch 64/300 -- Iteration 491848 - Batch 6622/7702 - Train loss: 0.00189698  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.48 min) Epoch 64/300 -- Iteration 491925 - Batch 6699/7702 - Train loss: 0.00189692  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.50 min) Epoch 64/300 -- Iteration 492002 - Batch 6776/7702 - Train loss: 0.00189688  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.52 min) Epoch 64/300 -- Iteration 492079 - Batch 6853/7702 - Train loss: 0.00189707  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.54 min) Epoch 64/300 -- Iteration 492156 - Batch 6930/7702 - Train loss: 0.00189669  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.56 min) Epoch 64/300 -- Iteration 492233 - Batch 7007/7702 - Train loss: 0.00189659  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.58 min) Epoch 64/300 -- Iteration 492310 - Batch 7084/7702 - Train loss: 0.00189634  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.60 min) Epoch 64/300 -- Iteration 492387 - Batch 7161/7702 - Train loss: 0.00189620  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.62 min) Epoch 64/300 -- Iteration 492464 - Batch 7238/7702 - Train loss: 0.00189614  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.64 min) Epoch 64/300 -- Iteration 492541 - Batch 7315/7702 - Train loss: 0.00189632  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.66 min) Epoch 64/300 -- Iteration 492618 - Batch 7392/7702 - Train loss: 0.00189617  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.68 min) Epoch 64/300 -- Iteration 492695 - Batch 7469/7702 - Train loss: 0.00189586  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.70 min) Epoch 64/300 -- Iteration 492772 - Batch 7546/7702 - Train loss: 0.00189593  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.72 min) Epoch 64/300 -- Iteration 492849 - Batch 7623/7702 - Train loss: 0.00189572  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.75 min) Epoch 64/300 -- Iteration 492926 - Batch 7700/7702 - Train loss: 0.00189564  - Train acc: -0.0000 - Val loss: 0.00039623\n",
      "(132.75 min) Epoch 64/300 -- Iteration 492928 - Batch 7701/7702 - Train loss: 0.00189568  - Train acc: -0.0000 - Val loss: 0.00041423 - Val acc: -0.0000\n",
      "(132.77 min) Epoch 65/300 -- Iteration 493005 - Batch 77/7702 - Train loss: 0.00195570  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.79 min) Epoch 65/300 -- Iteration 493082 - Batch 154/7702 - Train loss: 0.00193320  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.81 min) Epoch 65/300 -- Iteration 493159 - Batch 231/7702 - Train loss: 0.00192443  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.83 min) Epoch 65/300 -- Iteration 493236 - Batch 308/7702 - Train loss: 0.00191073  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.85 min) Epoch 65/300 -- Iteration 493313 - Batch 385/7702 - Train loss: 0.00190464  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.87 min) Epoch 65/300 -- Iteration 493390 - Batch 462/7702 - Train loss: 0.00190147  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.89 min) Epoch 65/300 -- Iteration 493467 - Batch 539/7702 - Train loss: 0.00189908  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.91 min) Epoch 65/300 -- Iteration 493544 - Batch 616/7702 - Train loss: 0.00189591  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.94 min) Epoch 65/300 -- Iteration 493621 - Batch 693/7702 - Train loss: 0.00189685  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.96 min) Epoch 65/300 -- Iteration 493698 - Batch 770/7702 - Train loss: 0.00189519  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(132.98 min) Epoch 65/300 -- Iteration 493775 - Batch 847/7702 - Train loss: 0.00189825  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.00 min) Epoch 65/300 -- Iteration 493852 - Batch 924/7702 - Train loss: 0.00189814  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.02 min) Epoch 65/300 -- Iteration 493929 - Batch 1001/7702 - Train loss: 0.00189999  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.04 min) Epoch 65/300 -- Iteration 494006 - Batch 1078/7702 - Train loss: 0.00189856  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.06 min) Epoch 65/300 -- Iteration 494083 - Batch 1155/7702 - Train loss: 0.00189867  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.08 min) Epoch 65/300 -- Iteration 494160 - Batch 1232/7702 - Train loss: 0.00189923  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.10 min) Epoch 65/300 -- Iteration 494237 - Batch 1309/7702 - Train loss: 0.00189732  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.12 min) Epoch 65/300 -- Iteration 494314 - Batch 1386/7702 - Train loss: 0.00189671  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.14 min) Epoch 65/300 -- Iteration 494391 - Batch 1463/7702 - Train loss: 0.00189675  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.16 min) Epoch 65/300 -- Iteration 494468 - Batch 1540/7702 - Train loss: 0.00189766  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.18 min) Epoch 65/300 -- Iteration 494545 - Batch 1617/7702 - Train loss: 0.00189684  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.21 min) Epoch 65/300 -- Iteration 494622 - Batch 1694/7702 - Train loss: 0.00189566  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.23 min) Epoch 65/300 -- Iteration 494699 - Batch 1771/7702 - Train loss: 0.00189552  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.25 min) Epoch 65/300 -- Iteration 494776 - Batch 1848/7702 - Train loss: 0.00189559  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.27 min) Epoch 65/300 -- Iteration 494853 - Batch 1925/7702 - Train loss: 0.00189568  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.29 min) Epoch 65/300 -- Iteration 494930 - Batch 2002/7702 - Train loss: 0.00189414  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.31 min) Epoch 65/300 -- Iteration 495007 - Batch 2079/7702 - Train loss: 0.00189445  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.33 min) Epoch 65/300 -- Iteration 495084 - Batch 2156/7702 - Train loss: 0.00189341  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.35 min) Epoch 65/300 -- Iteration 495161 - Batch 2233/7702 - Train loss: 0.00189291  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.37 min) Epoch 65/300 -- Iteration 495238 - Batch 2310/7702 - Train loss: 0.00189346  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.39 min) Epoch 65/300 -- Iteration 495315 - Batch 2387/7702 - Train loss: 0.00189429  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.41 min) Epoch 65/300 -- Iteration 495392 - Batch 2464/7702 - Train loss: 0.00189375  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.43 min) Epoch 65/300 -- Iteration 495469 - Batch 2541/7702 - Train loss: 0.00189412  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.45 min) Epoch 65/300 -- Iteration 495546 - Batch 2618/7702 - Train loss: 0.00189384  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.47 min) Epoch 65/300 -- Iteration 495623 - Batch 2695/7702 - Train loss: 0.00189478  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.50 min) Epoch 65/300 -- Iteration 495700 - Batch 2772/7702 - Train loss: 0.00189507  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.52 min) Epoch 65/300 -- Iteration 495777 - Batch 2849/7702 - Train loss: 0.00189381  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.54 min) Epoch 65/300 -- Iteration 495854 - Batch 2926/7702 - Train loss: 0.00189414  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.56 min) Epoch 65/300 -- Iteration 495931 - Batch 3003/7702 - Train loss: 0.00189506  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.58 min) Epoch 65/300 -- Iteration 496008 - Batch 3080/7702 - Train loss: 0.00189515  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.60 min) Epoch 65/300 -- Iteration 496085 - Batch 3157/7702 - Train loss: 0.00189578  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.62 min) Epoch 65/300 -- Iteration 496162 - Batch 3234/7702 - Train loss: 0.00189592  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.64 min) Epoch 65/300 -- Iteration 496239 - Batch 3311/7702 - Train loss: 0.00189569  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.66 min) Epoch 65/300 -- Iteration 496316 - Batch 3388/7702 - Train loss: 0.00189590  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.68 min) Epoch 65/300 -- Iteration 496393 - Batch 3465/7702 - Train loss: 0.00189553  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.70 min) Epoch 65/300 -- Iteration 496470 - Batch 3542/7702 - Train loss: 0.00189493  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.72 min) Epoch 65/300 -- Iteration 496547 - Batch 3619/7702 - Train loss: 0.00189503  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.74 min) Epoch 65/300 -- Iteration 496624 - Batch 3696/7702 - Train loss: 0.00189507  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.76 min) Epoch 65/300 -- Iteration 496701 - Batch 3773/7702 - Train loss: 0.00189494  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.79 min) Epoch 65/300 -- Iteration 496778 - Batch 3850/7702 - Train loss: 0.00189496  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.81 min) Epoch 65/300 -- Iteration 496855 - Batch 3927/7702 - Train loss: 0.00189508  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.83 min) Epoch 65/300 -- Iteration 496932 - Batch 4004/7702 - Train loss: 0.00189438  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.85 min) Epoch 65/300 -- Iteration 497009 - Batch 4081/7702 - Train loss: 0.00189404  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.87 min) Epoch 65/300 -- Iteration 497086 - Batch 4158/7702 - Train loss: 0.00189392  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.89 min) Epoch 65/300 -- Iteration 497163 - Batch 4235/7702 - Train loss: 0.00189401  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.91 min) Epoch 65/300 -- Iteration 497240 - Batch 4312/7702 - Train loss: 0.00189369  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.93 min) Epoch 65/300 -- Iteration 497317 - Batch 4389/7702 - Train loss: 0.00189318  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.95 min) Epoch 65/300 -- Iteration 497394 - Batch 4466/7702 - Train loss: 0.00189305  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.97 min) Epoch 65/300 -- Iteration 497471 - Batch 4543/7702 - Train loss: 0.00189308  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(133.99 min) Epoch 65/300 -- Iteration 497548 - Batch 4620/7702 - Train loss: 0.00189273  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.01 min) Epoch 65/300 -- Iteration 497625 - Batch 4697/7702 - Train loss: 0.00189268  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.03 min) Epoch 65/300 -- Iteration 497702 - Batch 4774/7702 - Train loss: 0.00189260  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.06 min) Epoch 65/300 -- Iteration 497779 - Batch 4851/7702 - Train loss: 0.00189275  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.08 min) Epoch 65/300 -- Iteration 497856 - Batch 4928/7702 - Train loss: 0.00189212  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.10 min) Epoch 65/300 -- Iteration 497933 - Batch 5005/7702 - Train loss: 0.00189161  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.12 min) Epoch 65/300 -- Iteration 498010 - Batch 5082/7702 - Train loss: 0.00189179  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.14 min) Epoch 65/300 -- Iteration 498087 - Batch 5159/7702 - Train loss: 0.00189123  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.16 min) Epoch 65/300 -- Iteration 498164 - Batch 5236/7702 - Train loss: 0.00189137  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.18 min) Epoch 65/300 -- Iteration 498241 - Batch 5313/7702 - Train loss: 0.00189161  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.20 min) Epoch 65/300 -- Iteration 498318 - Batch 5390/7702 - Train loss: 0.00189209  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.22 min) Epoch 65/300 -- Iteration 498395 - Batch 5467/7702 - Train loss: 0.00189224  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.24 min) Epoch 65/300 -- Iteration 498472 - Batch 5544/7702 - Train loss: 0.00189196  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.26 min) Epoch 65/300 -- Iteration 498549 - Batch 5621/7702 - Train loss: 0.00189155  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.28 min) Epoch 65/300 -- Iteration 498626 - Batch 5698/7702 - Train loss: 0.00189147  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.30 min) Epoch 65/300 -- Iteration 498703 - Batch 5775/7702 - Train loss: 0.00189127  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.32 min) Epoch 65/300 -- Iteration 498780 - Batch 5852/7702 - Train loss: 0.00189175  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.35 min) Epoch 65/300 -- Iteration 498857 - Batch 5929/7702 - Train loss: 0.00189212  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.37 min) Epoch 65/300 -- Iteration 498934 - Batch 6006/7702 - Train loss: 0.00189249  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.39 min) Epoch 65/300 -- Iteration 499011 - Batch 6083/7702 - Train loss: 0.00189244  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.41 min) Epoch 65/300 -- Iteration 499088 - Batch 6160/7702 - Train loss: 0.00189212  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.43 min) Epoch 65/300 -- Iteration 499165 - Batch 6237/7702 - Train loss: 0.00189251  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.45 min) Epoch 65/300 -- Iteration 499242 - Batch 6314/7702 - Train loss: 0.00189280  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.47 min) Epoch 65/300 -- Iteration 499319 - Batch 6391/7702 - Train loss: 0.00189278  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.49 min) Epoch 65/300 -- Iteration 499396 - Batch 6468/7702 - Train loss: 0.00189339  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.51 min) Epoch 65/300 -- Iteration 499473 - Batch 6545/7702 - Train loss: 0.00189356  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.53 min) Epoch 65/300 -- Iteration 499550 - Batch 6622/7702 - Train loss: 0.00189326  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.55 min) Epoch 65/300 -- Iteration 499627 - Batch 6699/7702 - Train loss: 0.00189325  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.57 min) Epoch 65/300 -- Iteration 499704 - Batch 6776/7702 - Train loss: 0.00189324  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.60 min) Epoch 65/300 -- Iteration 499781 - Batch 6853/7702 - Train loss: 0.00189329  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.62 min) Epoch 65/300 -- Iteration 499858 - Batch 6930/7702 - Train loss: 0.00189340  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.64 min) Epoch 65/300 -- Iteration 499935 - Batch 7007/7702 - Train loss: 0.00189338  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.66 min) Epoch 65/300 -- Iteration 500012 - Batch 7084/7702 - Train loss: 0.00189307  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.68 min) Epoch 65/300 -- Iteration 500089 - Batch 7161/7702 - Train loss: 0.00189300  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.70 min) Epoch 65/300 -- Iteration 500166 - Batch 7238/7702 - Train loss: 0.00189313  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.72 min) Epoch 65/300 -- Iteration 500243 - Batch 7315/7702 - Train loss: 0.00189309  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.74 min) Epoch 65/300 -- Iteration 500320 - Batch 7392/7702 - Train loss: 0.00189274  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.76 min) Epoch 65/300 -- Iteration 500397 - Batch 7469/7702 - Train loss: 0.00189269  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.78 min) Epoch 65/300 -- Iteration 500474 - Batch 7546/7702 - Train loss: 0.00189293  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.80 min) Epoch 65/300 -- Iteration 500551 - Batch 7623/7702 - Train loss: 0.00189262  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.82 min) Epoch 65/300 -- Iteration 500628 - Batch 7700/7702 - Train loss: 0.00189255  - Train acc: -0.0000 - Val loss: 0.00041423\n",
      "(134.82 min) Epoch 65/300 -- Iteration 500630 - Batch 7701/7702 - Train loss: 0.00189256  - Train acc: -0.0000 - Val loss: 0.00053413 - Val acc: -0.0000\n",
      "(134.85 min) Epoch 66/300 -- Iteration 500707 - Batch 77/7702 - Train loss: 0.00193918  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(134.87 min) Epoch 66/300 -- Iteration 500784 - Batch 154/7702 - Train loss: 0.00191173  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(134.89 min) Epoch 66/300 -- Iteration 500861 - Batch 231/7702 - Train loss: 0.00190737  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(134.91 min) Epoch 66/300 -- Iteration 500938 - Batch 308/7702 - Train loss: 0.00190902  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(134.93 min) Epoch 66/300 -- Iteration 501015 - Batch 385/7702 - Train loss: 0.00190244  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(134.95 min) Epoch 66/300 -- Iteration 501092 - Batch 462/7702 - Train loss: 0.00189667  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(134.97 min) Epoch 66/300 -- Iteration 501169 - Batch 539/7702 - Train loss: 0.00189220  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(134.99 min) Epoch 66/300 -- Iteration 501246 - Batch 616/7702 - Train loss: 0.00189090  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.01 min) Epoch 66/300 -- Iteration 501323 - Batch 693/7702 - Train loss: 0.00188692  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.03 min) Epoch 66/300 -- Iteration 501400 - Batch 770/7702 - Train loss: 0.00188814  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.06 min) Epoch 66/300 -- Iteration 501477 - Batch 847/7702 - Train loss: 0.00188755  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.08 min) Epoch 66/300 -- Iteration 501554 - Batch 924/7702 - Train loss: 0.00189072  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.10 min) Epoch 66/300 -- Iteration 501631 - Batch 1001/7702 - Train loss: 0.00189367  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.12 min) Epoch 66/300 -- Iteration 501708 - Batch 1078/7702 - Train loss: 0.00189255  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.14 min) Epoch 66/300 -- Iteration 501785 - Batch 1155/7702 - Train loss: 0.00189253  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.16 min) Epoch 66/300 -- Iteration 501862 - Batch 1232/7702 - Train loss: 0.00189277  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.18 min) Epoch 66/300 -- Iteration 501939 - Batch 1309/7702 - Train loss: 0.00189545  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.20 min) Epoch 66/300 -- Iteration 502016 - Batch 1386/7702 - Train loss: 0.00189476  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.22 min) Epoch 66/300 -- Iteration 502093 - Batch 1463/7702 - Train loss: 0.00189432  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.24 min) Epoch 66/300 -- Iteration 502170 - Batch 1540/7702 - Train loss: 0.00189343  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.26 min) Epoch 66/300 -- Iteration 502247 - Batch 1617/7702 - Train loss: 0.00189503  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.28 min) Epoch 66/300 -- Iteration 502324 - Batch 1694/7702 - Train loss: 0.00189617  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.30 min) Epoch 66/300 -- Iteration 502401 - Batch 1771/7702 - Train loss: 0.00189595  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.32 min) Epoch 66/300 -- Iteration 502478 - Batch 1848/7702 - Train loss: 0.00189615  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.35 min) Epoch 66/300 -- Iteration 502555 - Batch 1925/7702 - Train loss: 0.00189597  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.37 min) Epoch 66/300 -- Iteration 502632 - Batch 2002/7702 - Train loss: 0.00189656  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.39 min) Epoch 66/300 -- Iteration 502709 - Batch 2079/7702 - Train loss: 0.00189624  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.41 min) Epoch 66/300 -- Iteration 502786 - Batch 2156/7702 - Train loss: 0.00189607  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.43 min) Epoch 66/300 -- Iteration 502863 - Batch 2233/7702 - Train loss: 0.00189534  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.45 min) Epoch 66/300 -- Iteration 502940 - Batch 2310/7702 - Train loss: 0.00189658  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.47 min) Epoch 66/300 -- Iteration 503017 - Batch 2387/7702 - Train loss: 0.00189582  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.49 min) Epoch 66/300 -- Iteration 503094 - Batch 2464/7702 - Train loss: 0.00189544  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.51 min) Epoch 66/300 -- Iteration 503171 - Batch 2541/7702 - Train loss: 0.00189513  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.53 min) Epoch 66/300 -- Iteration 503248 - Batch 2618/7702 - Train loss: 0.00189456  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.55 min) Epoch 66/300 -- Iteration 503325 - Batch 2695/7702 - Train loss: 0.00189408  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.57 min) Epoch 66/300 -- Iteration 503402 - Batch 2772/7702 - Train loss: 0.00189493  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.59 min) Epoch 66/300 -- Iteration 503479 - Batch 2849/7702 - Train loss: 0.00189501  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.61 min) Epoch 66/300 -- Iteration 503556 - Batch 2926/7702 - Train loss: 0.00189533  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.64 min) Epoch 66/300 -- Iteration 503633 - Batch 3003/7702 - Train loss: 0.00189483  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.66 min) Epoch 66/300 -- Iteration 503710 - Batch 3080/7702 - Train loss: 0.00189529  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.68 min) Epoch 66/300 -- Iteration 503787 - Batch 3157/7702 - Train loss: 0.00189620  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.70 min) Epoch 66/300 -- Iteration 503864 - Batch 3234/7702 - Train loss: 0.00189653  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.72 min) Epoch 66/300 -- Iteration 503941 - Batch 3311/7702 - Train loss: 0.00189576  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.74 min) Epoch 66/300 -- Iteration 504018 - Batch 3388/7702 - Train loss: 0.00189581  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.76 min) Epoch 66/300 -- Iteration 504095 - Batch 3465/7702 - Train loss: 0.00189592  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.78 min) Epoch 66/300 -- Iteration 504172 - Batch 3542/7702 - Train loss: 0.00189645  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.80 min) Epoch 66/300 -- Iteration 504249 - Batch 3619/7702 - Train loss: 0.00189734  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.82 min) Epoch 66/300 -- Iteration 504326 - Batch 3696/7702 - Train loss: 0.00189653  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.84 min) Epoch 66/300 -- Iteration 504403 - Batch 3773/7702 - Train loss: 0.00189605  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.86 min) Epoch 66/300 -- Iteration 504480 - Batch 3850/7702 - Train loss: 0.00189556  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.88 min) Epoch 66/300 -- Iteration 504557 - Batch 3927/7702 - Train loss: 0.00189554  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.90 min) Epoch 66/300 -- Iteration 504634 - Batch 4004/7702 - Train loss: 0.00189557  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.92 min) Epoch 66/300 -- Iteration 504711 - Batch 4081/7702 - Train loss: 0.00189578  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.95 min) Epoch 66/300 -- Iteration 504788 - Batch 4158/7702 - Train loss: 0.00189588  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.97 min) Epoch 66/300 -- Iteration 504865 - Batch 4235/7702 - Train loss: 0.00189633  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(135.99 min) Epoch 66/300 -- Iteration 504942 - Batch 4312/7702 - Train loss: 0.00189557  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.01 min) Epoch 66/300 -- Iteration 505019 - Batch 4389/7702 - Train loss: 0.00189566  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.03 min) Epoch 66/300 -- Iteration 505096 - Batch 4466/7702 - Train loss: 0.00189509  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.05 min) Epoch 66/300 -- Iteration 505173 - Batch 4543/7702 - Train loss: 0.00189519  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.07 min) Epoch 66/300 -- Iteration 505250 - Batch 4620/7702 - Train loss: 0.00189496  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.09 min) Epoch 66/300 -- Iteration 505327 - Batch 4697/7702 - Train loss: 0.00189461  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.11 min) Epoch 66/300 -- Iteration 505404 - Batch 4774/7702 - Train loss: 0.00189381  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.13 min) Epoch 66/300 -- Iteration 505481 - Batch 4851/7702 - Train loss: 0.00189418  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.15 min) Epoch 66/300 -- Iteration 505558 - Batch 4928/7702 - Train loss: 0.00189437  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.17 min) Epoch 66/300 -- Iteration 505635 - Batch 5005/7702 - Train loss: 0.00189491  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.19 min) Epoch 66/300 -- Iteration 505712 - Batch 5082/7702 - Train loss: 0.00189484  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.22 min) Epoch 66/300 -- Iteration 505789 - Batch 5159/7702 - Train loss: 0.00189430  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.24 min) Epoch 66/300 -- Iteration 505866 - Batch 5236/7702 - Train loss: 0.00189440  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.26 min) Epoch 66/300 -- Iteration 505943 - Batch 5313/7702 - Train loss: 0.00189426  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.28 min) Epoch 66/300 -- Iteration 506020 - Batch 5390/7702 - Train loss: 0.00189414  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.30 min) Epoch 66/300 -- Iteration 506097 - Batch 5467/7702 - Train loss: 0.00189422  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.32 min) Epoch 66/300 -- Iteration 506174 - Batch 5544/7702 - Train loss: 0.00189394  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.34 min) Epoch 66/300 -- Iteration 506251 - Batch 5621/7702 - Train loss: 0.00189387  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.36 min) Epoch 66/300 -- Iteration 506328 - Batch 5698/7702 - Train loss: 0.00189386  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.38 min) Epoch 66/300 -- Iteration 506405 - Batch 5775/7702 - Train loss: 0.00189383  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.40 min) Epoch 66/300 -- Iteration 506482 - Batch 5852/7702 - Train loss: 0.00189423  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.42 min) Epoch 66/300 -- Iteration 506559 - Batch 5929/7702 - Train loss: 0.00189421  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.44 min) Epoch 66/300 -- Iteration 506636 - Batch 6006/7702 - Train loss: 0.00189362  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.46 min) Epoch 66/300 -- Iteration 506713 - Batch 6083/7702 - Train loss: 0.00189353  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.49 min) Epoch 66/300 -- Iteration 506790 - Batch 6160/7702 - Train loss: 0.00189341  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.51 min) Epoch 66/300 -- Iteration 506867 - Batch 6237/7702 - Train loss: 0.00189334  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.53 min) Epoch 66/300 -- Iteration 506944 - Batch 6314/7702 - Train loss: 0.00189344  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.55 min) Epoch 66/300 -- Iteration 507021 - Batch 6391/7702 - Train loss: 0.00189374  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.57 min) Epoch 66/300 -- Iteration 507098 - Batch 6468/7702 - Train loss: 0.00189390  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.59 min) Epoch 66/300 -- Iteration 507175 - Batch 6545/7702 - Train loss: 0.00189377  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.61 min) Epoch 66/300 -- Iteration 507252 - Batch 6622/7702 - Train loss: 0.00189361  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.63 min) Epoch 66/300 -- Iteration 507329 - Batch 6699/7702 - Train loss: 0.00189354  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.65 min) Epoch 66/300 -- Iteration 507406 - Batch 6776/7702 - Train loss: 0.00189363  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.67 min) Epoch 66/300 -- Iteration 507483 - Batch 6853/7702 - Train loss: 0.00189384  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.69 min) Epoch 66/300 -- Iteration 507560 - Batch 6930/7702 - Train loss: 0.00189381  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.71 min) Epoch 66/300 -- Iteration 507637 - Batch 7007/7702 - Train loss: 0.00189368  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.74 min) Epoch 66/300 -- Iteration 507714 - Batch 7084/7702 - Train loss: 0.00189372  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.76 min) Epoch 66/300 -- Iteration 507791 - Batch 7161/7702 - Train loss: 0.00189438  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.78 min) Epoch 66/300 -- Iteration 507868 - Batch 7238/7702 - Train loss: 0.00189493  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.80 min) Epoch 66/300 -- Iteration 507945 - Batch 7315/7702 - Train loss: 0.00189481  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.82 min) Epoch 66/300 -- Iteration 508022 - Batch 7392/7702 - Train loss: 0.00189448  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.84 min) Epoch 66/300 -- Iteration 508099 - Batch 7469/7702 - Train loss: 0.00189423  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.86 min) Epoch 66/300 -- Iteration 508176 - Batch 7546/7702 - Train loss: 0.00189427  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.88 min) Epoch 66/300 -- Iteration 508253 - Batch 7623/7702 - Train loss: 0.00189429  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.90 min) Epoch 66/300 -- Iteration 508330 - Batch 7700/7702 - Train loss: 0.00189426  - Train acc: -0.0000 - Val loss: 0.00053413\n",
      "(136.90 min) Epoch 66/300 -- Iteration 508332 - Batch 7701/7702 - Train loss: 0.00189424  - Train acc: -0.0000 - Val loss: 0.00044952 - Val acc: -0.0000\n",
      "(136.93 min) Epoch 67/300 -- Iteration 508409 - Batch 77/7702 - Train loss: 0.00190583  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(136.95 min) Epoch 67/300 -- Iteration 508486 - Batch 154/7702 - Train loss: 0.00191317  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(136.97 min) Epoch 67/300 -- Iteration 508563 - Batch 231/7702 - Train loss: 0.00190414  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(136.99 min) Epoch 67/300 -- Iteration 508640 - Batch 308/7702 - Train loss: 0.00190297  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.01 min) Epoch 67/300 -- Iteration 508717 - Batch 385/7702 - Train loss: 0.00190275  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.03 min) Epoch 67/300 -- Iteration 508794 - Batch 462/7702 - Train loss: 0.00190242  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.05 min) Epoch 67/300 -- Iteration 508871 - Batch 539/7702 - Train loss: 0.00190239  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.07 min) Epoch 67/300 -- Iteration 508948 - Batch 616/7702 - Train loss: 0.00190681  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.09 min) Epoch 67/300 -- Iteration 509025 - Batch 693/7702 - Train loss: 0.00190520  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.11 min) Epoch 67/300 -- Iteration 509102 - Batch 770/7702 - Train loss: 0.00190247  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.13 min) Epoch 67/300 -- Iteration 509179 - Batch 847/7702 - Train loss: 0.00190126  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.15 min) Epoch 67/300 -- Iteration 509256 - Batch 924/7702 - Train loss: 0.00190256  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.17 min) Epoch 67/300 -- Iteration 509333 - Batch 1001/7702 - Train loss: 0.00190406  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.19 min) Epoch 67/300 -- Iteration 509410 - Batch 1078/7702 - Train loss: 0.00190355  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.22 min) Epoch 67/300 -- Iteration 509487 - Batch 1155/7702 - Train loss: 0.00190157  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.24 min) Epoch 67/300 -- Iteration 509564 - Batch 1232/7702 - Train loss: 0.00189910  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.26 min) Epoch 67/300 -- Iteration 509641 - Batch 1309/7702 - Train loss: 0.00189712  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.28 min) Epoch 67/300 -- Iteration 509718 - Batch 1386/7702 - Train loss: 0.00189640  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.30 min) Epoch 67/300 -- Iteration 509795 - Batch 1463/7702 - Train loss: 0.00189549  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.32 min) Epoch 67/300 -- Iteration 509872 - Batch 1540/7702 - Train loss: 0.00189614  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.34 min) Epoch 67/300 -- Iteration 509949 - Batch 1617/7702 - Train loss: 0.00189600  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.36 min) Epoch 67/300 -- Iteration 510026 - Batch 1694/7702 - Train loss: 0.00189531  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.38 min) Epoch 67/300 -- Iteration 510103 - Batch 1771/7702 - Train loss: 0.00189505  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.40 min) Epoch 67/300 -- Iteration 510180 - Batch 1848/7702 - Train loss: 0.00189502  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.42 min) Epoch 67/300 -- Iteration 510257 - Batch 1925/7702 - Train loss: 0.00189374  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.44 min) Epoch 67/300 -- Iteration 510334 - Batch 2002/7702 - Train loss: 0.00189305  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.46 min) Epoch 67/300 -- Iteration 510411 - Batch 2079/7702 - Train loss: 0.00189315  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.48 min) Epoch 67/300 -- Iteration 510488 - Batch 2156/7702 - Train loss: 0.00189363  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.50 min) Epoch 67/300 -- Iteration 510565 - Batch 2233/7702 - Train loss: 0.00189351  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.53 min) Epoch 67/300 -- Iteration 510642 - Batch 2310/7702 - Train loss: 0.00189477  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.55 min) Epoch 67/300 -- Iteration 510719 - Batch 2387/7702 - Train loss: 0.00189433  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.57 min) Epoch 67/300 -- Iteration 510796 - Batch 2464/7702 - Train loss: 0.00189315  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.59 min) Epoch 67/300 -- Iteration 510873 - Batch 2541/7702 - Train loss: 0.00189298  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.61 min) Epoch 67/300 -- Iteration 510950 - Batch 2618/7702 - Train loss: 0.00189277  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.63 min) Epoch 67/300 -- Iteration 511027 - Batch 2695/7702 - Train loss: 0.00189301  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.65 min) Epoch 67/300 -- Iteration 511104 - Batch 2772/7702 - Train loss: 0.00189346  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.67 min) Epoch 67/300 -- Iteration 511181 - Batch 2849/7702 - Train loss: 0.00189415  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.69 min) Epoch 67/300 -- Iteration 511258 - Batch 2926/7702 - Train loss: 0.00189462  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.71 min) Epoch 67/300 -- Iteration 511335 - Batch 3003/7702 - Train loss: 0.00189530  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.73 min) Epoch 67/300 -- Iteration 511412 - Batch 3080/7702 - Train loss: 0.00189452  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.75 min) Epoch 67/300 -- Iteration 511489 - Batch 3157/7702 - Train loss: 0.00189353  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.77 min) Epoch 67/300 -- Iteration 511566 - Batch 3234/7702 - Train loss: 0.00189294  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.80 min) Epoch 67/300 -- Iteration 511643 - Batch 3311/7702 - Train loss: 0.00189404  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.82 min) Epoch 67/300 -- Iteration 511720 - Batch 3388/7702 - Train loss: 0.00189440  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.84 min) Epoch 67/300 -- Iteration 511797 - Batch 3465/7702 - Train loss: 0.00189502  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.86 min) Epoch 67/300 -- Iteration 511874 - Batch 3542/7702 - Train loss: 0.00189498  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.88 min) Epoch 67/300 -- Iteration 511951 - Batch 3619/7702 - Train loss: 0.00189464  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.90 min) Epoch 67/300 -- Iteration 512028 - Batch 3696/7702 - Train loss: 0.00189390  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.92 min) Epoch 67/300 -- Iteration 512105 - Batch 3773/7702 - Train loss: 0.00189385  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.94 min) Epoch 67/300 -- Iteration 512182 - Batch 3850/7702 - Train loss: 0.00189398  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.96 min) Epoch 67/300 -- Iteration 512259 - Batch 3927/7702 - Train loss: 0.00189417  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(137.98 min) Epoch 67/300 -- Iteration 512336 - Batch 4004/7702 - Train loss: 0.00189456  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.00 min) Epoch 67/300 -- Iteration 512413 - Batch 4081/7702 - Train loss: 0.00189418  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.02 min) Epoch 67/300 -- Iteration 512490 - Batch 4158/7702 - Train loss: 0.00189406  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.04 min) Epoch 67/300 -- Iteration 512567 - Batch 4235/7702 - Train loss: 0.00189411  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.06 min) Epoch 67/300 -- Iteration 512644 - Batch 4312/7702 - Train loss: 0.00189339  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.09 min) Epoch 67/300 -- Iteration 512721 - Batch 4389/7702 - Train loss: 0.00189267  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.11 min) Epoch 67/300 -- Iteration 512798 - Batch 4466/7702 - Train loss: 0.00189258  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.13 min) Epoch 67/300 -- Iteration 512875 - Batch 4543/7702 - Train loss: 0.00189188  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.15 min) Epoch 67/300 -- Iteration 512952 - Batch 4620/7702 - Train loss: 0.00189228  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.17 min) Epoch 67/300 -- Iteration 513029 - Batch 4697/7702 - Train loss: 0.00189218  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.19 min) Epoch 67/300 -- Iteration 513106 - Batch 4774/7702 - Train loss: 0.00189252  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.21 min) Epoch 67/300 -- Iteration 513183 - Batch 4851/7702 - Train loss: 0.00189317  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.23 min) Epoch 67/300 -- Iteration 513260 - Batch 4928/7702 - Train loss: 0.00189286  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.25 min) Epoch 67/300 -- Iteration 513337 - Batch 5005/7702 - Train loss: 0.00189270  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.27 min) Epoch 67/300 -- Iteration 513414 - Batch 5082/7702 - Train loss: 0.00189302  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.29 min) Epoch 67/300 -- Iteration 513491 - Batch 5159/7702 - Train loss: 0.00189370  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.31 min) Epoch 67/300 -- Iteration 513568 - Batch 5236/7702 - Train loss: 0.00189388  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.33 min) Epoch 67/300 -- Iteration 513645 - Batch 5313/7702 - Train loss: 0.00189329  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.35 min) Epoch 67/300 -- Iteration 513722 - Batch 5390/7702 - Train loss: 0.00189333  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.38 min) Epoch 67/300 -- Iteration 513799 - Batch 5467/7702 - Train loss: 0.00189290  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.40 min) Epoch 67/300 -- Iteration 513876 - Batch 5544/7702 - Train loss: 0.00189262  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.42 min) Epoch 67/300 -- Iteration 513953 - Batch 5621/7702 - Train loss: 0.00189302  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.44 min) Epoch 67/300 -- Iteration 514030 - Batch 5698/7702 - Train loss: 0.00189313  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.46 min) Epoch 67/300 -- Iteration 514107 - Batch 5775/7702 - Train loss: 0.00189293  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.48 min) Epoch 67/300 -- Iteration 514184 - Batch 5852/7702 - Train loss: 0.00189273  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.50 min) Epoch 67/300 -- Iteration 514261 - Batch 5929/7702 - Train loss: 0.00189264  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.52 min) Epoch 67/300 -- Iteration 514338 - Batch 6006/7702 - Train loss: 0.00189285  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.54 min) Epoch 67/300 -- Iteration 514415 - Batch 6083/7702 - Train loss: 0.00189266  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.56 min) Epoch 67/300 -- Iteration 514492 - Batch 6160/7702 - Train loss: 0.00189235  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.58 min) Epoch 67/300 -- Iteration 514569 - Batch 6237/7702 - Train loss: 0.00189228  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.60 min) Epoch 67/300 -- Iteration 514646 - Batch 6314/7702 - Train loss: 0.00189238  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.62 min) Epoch 67/300 -- Iteration 514723 - Batch 6391/7702 - Train loss: 0.00189239  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.65 min) Epoch 67/300 -- Iteration 514800 - Batch 6468/7702 - Train loss: 0.00189231  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.67 min) Epoch 67/300 -- Iteration 514877 - Batch 6545/7702 - Train loss: 0.00189217  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.69 min) Epoch 67/300 -- Iteration 514954 - Batch 6622/7702 - Train loss: 0.00189181  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.71 min) Epoch 67/300 -- Iteration 515031 - Batch 6699/7702 - Train loss: 0.00189177  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.73 min) Epoch 67/300 -- Iteration 515108 - Batch 6776/7702 - Train loss: 0.00189139  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.75 min) Epoch 67/300 -- Iteration 515185 - Batch 6853/7702 - Train loss: 0.00189171  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.77 min) Epoch 67/300 -- Iteration 515262 - Batch 6930/7702 - Train loss: 0.00189115  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.79 min) Epoch 67/300 -- Iteration 515339 - Batch 7007/7702 - Train loss: 0.00189099  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.81 min) Epoch 67/300 -- Iteration 515416 - Batch 7084/7702 - Train loss: 0.00189073  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.83 min) Epoch 67/300 -- Iteration 515493 - Batch 7161/7702 - Train loss: 0.00189081  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.85 min) Epoch 67/300 -- Iteration 515570 - Batch 7238/7702 - Train loss: 0.00189079  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.87 min) Epoch 67/300 -- Iteration 515647 - Batch 7315/7702 - Train loss: 0.00189065  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.89 min) Epoch 67/300 -- Iteration 515724 - Batch 7392/7702 - Train loss: 0.00189083  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.91 min) Epoch 67/300 -- Iteration 515801 - Batch 7469/7702 - Train loss: 0.00189070  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.93 min) Epoch 67/300 -- Iteration 515878 - Batch 7546/7702 - Train loss: 0.00189065  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.96 min) Epoch 67/300 -- Iteration 515955 - Batch 7623/7702 - Train loss: 0.00189013  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.98 min) Epoch 67/300 -- Iteration 516032 - Batch 7700/7702 - Train loss: 0.00189021  - Train acc: -0.0000 - Val loss: 0.00044952\n",
      "(138.98 min) Epoch 67/300 -- Iteration 516034 - Batch 7701/7702 - Train loss: 0.00189021  - Train acc: -0.0000 - Val loss: 0.00039193 - Val acc: -0.0000\n",
      "(139.00 min) Epoch 68/300 -- Iteration 516111 - Batch 77/7702 - Train loss: 0.00188748  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.02 min) Epoch 68/300 -- Iteration 516188 - Batch 154/7702 - Train loss: 0.00187819  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.04 min) Epoch 68/300 -- Iteration 516265 - Batch 231/7702 - Train loss: 0.00187170  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.06 min) Epoch 68/300 -- Iteration 516342 - Batch 308/7702 - Train loss: 0.00187966  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.08 min) Epoch 68/300 -- Iteration 516419 - Batch 385/7702 - Train loss: 0.00188827  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.10 min) Epoch 68/300 -- Iteration 516496 - Batch 462/7702 - Train loss: 0.00189083  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.12 min) Epoch 68/300 -- Iteration 516573 - Batch 539/7702 - Train loss: 0.00189223  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.14 min) Epoch 68/300 -- Iteration 516650 - Batch 616/7702 - Train loss: 0.00189604  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.17 min) Epoch 68/300 -- Iteration 516727 - Batch 693/7702 - Train loss: 0.00189787  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.19 min) Epoch 68/300 -- Iteration 516804 - Batch 770/7702 - Train loss: 0.00189646  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.21 min) Epoch 68/300 -- Iteration 516881 - Batch 847/7702 - Train loss: 0.00189743  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.23 min) Epoch 68/300 -- Iteration 516958 - Batch 924/7702 - Train loss: 0.00189605  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.25 min) Epoch 68/300 -- Iteration 517035 - Batch 1001/7702 - Train loss: 0.00189324  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.27 min) Epoch 68/300 -- Iteration 517112 - Batch 1078/7702 - Train loss: 0.00188828  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.29 min) Epoch 68/300 -- Iteration 517189 - Batch 1155/7702 - Train loss: 0.00188942  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.31 min) Epoch 68/300 -- Iteration 517266 - Batch 1232/7702 - Train loss: 0.00188890  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.33 min) Epoch 68/300 -- Iteration 517343 - Batch 1309/7702 - Train loss: 0.00188841  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.35 min) Epoch 68/300 -- Iteration 517420 - Batch 1386/7702 - Train loss: 0.00189088  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.37 min) Epoch 68/300 -- Iteration 517497 - Batch 1463/7702 - Train loss: 0.00189052  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.39 min) Epoch 68/300 -- Iteration 517574 - Batch 1540/7702 - Train loss: 0.00189082  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.41 min) Epoch 68/300 -- Iteration 517651 - Batch 1617/7702 - Train loss: 0.00189252  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.43 min) Epoch 68/300 -- Iteration 517728 - Batch 1694/7702 - Train loss: 0.00189148  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.45 min) Epoch 68/300 -- Iteration 517805 - Batch 1771/7702 - Train loss: 0.00189241  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.48 min) Epoch 68/300 -- Iteration 517882 - Batch 1848/7702 - Train loss: 0.00189247  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.50 min) Epoch 68/300 -- Iteration 517959 - Batch 1925/7702 - Train loss: 0.00189274  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.52 min) Epoch 68/300 -- Iteration 518036 - Batch 2002/7702 - Train loss: 0.00189339  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.54 min) Epoch 68/300 -- Iteration 518113 - Batch 2079/7702 - Train loss: 0.00189341  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.56 min) Epoch 68/300 -- Iteration 518190 - Batch 2156/7702 - Train loss: 0.00189418  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.58 min) Epoch 68/300 -- Iteration 518267 - Batch 2233/7702 - Train loss: 0.00189405  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.60 min) Epoch 68/300 -- Iteration 518344 - Batch 2310/7702 - Train loss: 0.00189320  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.62 min) Epoch 68/300 -- Iteration 518421 - Batch 2387/7702 - Train loss: 0.00189381  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.64 min) Epoch 68/300 -- Iteration 518498 - Batch 2464/7702 - Train loss: 0.00189395  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.66 min) Epoch 68/300 -- Iteration 518575 - Batch 2541/7702 - Train loss: 0.00189298  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.68 min) Epoch 68/300 -- Iteration 518652 - Batch 2618/7702 - Train loss: 0.00189250  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.70 min) Epoch 68/300 -- Iteration 518729 - Batch 2695/7702 - Train loss: 0.00189202  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.72 min) Epoch 68/300 -- Iteration 518806 - Batch 2772/7702 - Train loss: 0.00189181  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.75 min) Epoch 68/300 -- Iteration 518883 - Batch 2849/7702 - Train loss: 0.00189263  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.77 min) Epoch 68/300 -- Iteration 518960 - Batch 2926/7702 - Train loss: 0.00189199  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.79 min) Epoch 68/300 -- Iteration 519037 - Batch 3003/7702 - Train loss: 0.00189190  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.81 min) Epoch 68/300 -- Iteration 519114 - Batch 3080/7702 - Train loss: 0.00189148  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.83 min) Epoch 68/300 -- Iteration 519191 - Batch 3157/7702 - Train loss: 0.00189107  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.85 min) Epoch 68/300 -- Iteration 519268 - Batch 3234/7702 - Train loss: 0.00189091  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.87 min) Epoch 68/300 -- Iteration 519345 - Batch 3311/7702 - Train loss: 0.00189154  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.89 min) Epoch 68/300 -- Iteration 519422 - Batch 3388/7702 - Train loss: 0.00189148  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.91 min) Epoch 68/300 -- Iteration 519499 - Batch 3465/7702 - Train loss: 0.00189207  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.93 min) Epoch 68/300 -- Iteration 519576 - Batch 3542/7702 - Train loss: 0.00189307  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.95 min) Epoch 68/300 -- Iteration 519653 - Batch 3619/7702 - Train loss: 0.00189340  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.97 min) Epoch 68/300 -- Iteration 519730 - Batch 3696/7702 - Train loss: 0.00189349  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(139.99 min) Epoch 68/300 -- Iteration 519807 - Batch 3773/7702 - Train loss: 0.00189275  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.02 min) Epoch 68/300 -- Iteration 519884 - Batch 3850/7702 - Train loss: 0.00189301  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.04 min) Epoch 68/300 -- Iteration 519961 - Batch 3927/7702 - Train loss: 0.00189302  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.06 min) Epoch 68/300 -- Iteration 520038 - Batch 4004/7702 - Train loss: 0.00189369  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.08 min) Epoch 68/300 -- Iteration 520115 - Batch 4081/7702 - Train loss: 0.00189385  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.10 min) Epoch 68/300 -- Iteration 520192 - Batch 4158/7702 - Train loss: 0.00189397  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.12 min) Epoch 68/300 -- Iteration 520269 - Batch 4235/7702 - Train loss: 0.00189402  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.14 min) Epoch 68/300 -- Iteration 520346 - Batch 4312/7702 - Train loss: 0.00189408  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.16 min) Epoch 68/300 -- Iteration 520423 - Batch 4389/7702 - Train loss: 0.00189406  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.18 min) Epoch 68/300 -- Iteration 520500 - Batch 4466/7702 - Train loss: 0.00189328  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.20 min) Epoch 68/300 -- Iteration 520577 - Batch 4543/7702 - Train loss: 0.00189275  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.22 min) Epoch 68/300 -- Iteration 520654 - Batch 4620/7702 - Train loss: 0.00189268  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.24 min) Epoch 68/300 -- Iteration 520731 - Batch 4697/7702 - Train loss: 0.00189248  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.26 min) Epoch 68/300 -- Iteration 520808 - Batch 4774/7702 - Train loss: 0.00189220  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.29 min) Epoch 68/300 -- Iteration 520885 - Batch 4851/7702 - Train loss: 0.00189197  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.31 min) Epoch 68/300 -- Iteration 520962 - Batch 4928/7702 - Train loss: 0.00189187  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.33 min) Epoch 68/300 -- Iteration 521039 - Batch 5005/7702 - Train loss: 0.00189203  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.35 min) Epoch 68/300 -- Iteration 521116 - Batch 5082/7702 - Train loss: 0.00189257  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.37 min) Epoch 68/300 -- Iteration 521193 - Batch 5159/7702 - Train loss: 0.00189275  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.39 min) Epoch 68/300 -- Iteration 521270 - Batch 5236/7702 - Train loss: 0.00189300  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.41 min) Epoch 68/300 -- Iteration 521347 - Batch 5313/7702 - Train loss: 0.00189272  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.43 min) Epoch 68/300 -- Iteration 521424 - Batch 5390/7702 - Train loss: 0.00189281  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.45 min) Epoch 68/300 -- Iteration 521501 - Batch 5467/7702 - Train loss: 0.00189304  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.47 min) Epoch 68/300 -- Iteration 521578 - Batch 5544/7702 - Train loss: 0.00189314  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.49 min) Epoch 68/300 -- Iteration 521655 - Batch 5621/7702 - Train loss: 0.00189267  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.51 min) Epoch 68/300 -- Iteration 521732 - Batch 5698/7702 - Train loss: 0.00189268  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.53 min) Epoch 68/300 -- Iteration 521809 - Batch 5775/7702 - Train loss: 0.00189289  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.55 min) Epoch 68/300 -- Iteration 521886 - Batch 5852/7702 - Train loss: 0.00189269  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.58 min) Epoch 68/300 -- Iteration 521963 - Batch 5929/7702 - Train loss: 0.00189239  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.60 min) Epoch 68/300 -- Iteration 522040 - Batch 6006/7702 - Train loss: 0.00189220  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.62 min) Epoch 68/300 -- Iteration 522117 - Batch 6083/7702 - Train loss: 0.00189217  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.64 min) Epoch 68/300 -- Iteration 522194 - Batch 6160/7702 - Train loss: 0.00189210  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.66 min) Epoch 68/300 -- Iteration 522271 - Batch 6237/7702 - Train loss: 0.00189235  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.68 min) Epoch 68/300 -- Iteration 522348 - Batch 6314/7702 - Train loss: 0.00189225  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.70 min) Epoch 68/300 -- Iteration 522425 - Batch 6391/7702 - Train loss: 0.00189225  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.72 min) Epoch 68/300 -- Iteration 522502 - Batch 6468/7702 - Train loss: 0.00189191  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.74 min) Epoch 68/300 -- Iteration 522579 - Batch 6545/7702 - Train loss: 0.00189218  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.76 min) Epoch 68/300 -- Iteration 522656 - Batch 6622/7702 - Train loss: 0.00189240  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.78 min) Epoch 68/300 -- Iteration 522733 - Batch 6699/7702 - Train loss: 0.00189219  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.80 min) Epoch 68/300 -- Iteration 522810 - Batch 6776/7702 - Train loss: 0.00189202  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.82 min) Epoch 68/300 -- Iteration 522887 - Batch 6853/7702 - Train loss: 0.00189185  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.85 min) Epoch 68/300 -- Iteration 522964 - Batch 6930/7702 - Train loss: 0.00189168  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.87 min) Epoch 68/300 -- Iteration 523041 - Batch 7007/7702 - Train loss: 0.00189187  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.89 min) Epoch 68/300 -- Iteration 523118 - Batch 7084/7702 - Train loss: 0.00189174  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.91 min) Epoch 68/300 -- Iteration 523195 - Batch 7161/7702 - Train loss: 0.00189177  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.93 min) Epoch 68/300 -- Iteration 523272 - Batch 7238/7702 - Train loss: 0.00189142  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.95 min) Epoch 68/300 -- Iteration 523349 - Batch 7315/7702 - Train loss: 0.00189172  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.97 min) Epoch 68/300 -- Iteration 523426 - Batch 7392/7702 - Train loss: 0.00189174  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(140.99 min) Epoch 68/300 -- Iteration 523503 - Batch 7469/7702 - Train loss: 0.00189126  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(141.01 min) Epoch 68/300 -- Iteration 523580 - Batch 7546/7702 - Train loss: 0.00189148  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(141.03 min) Epoch 68/300 -- Iteration 523657 - Batch 7623/7702 - Train loss: 0.00189159  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(141.05 min) Epoch 68/300 -- Iteration 523734 - Batch 7700/7702 - Train loss: 0.00189134  - Train acc: -0.0000 - Val loss: 0.00039193\n",
      "(141.05 min) Epoch 68/300 -- Iteration 523736 - Batch 7701/7702 - Train loss: 0.00189136  - Train acc: -0.0000 - Val loss: 0.00037382 - Val acc: -0.0000\n",
      "(141.08 min) Epoch 69/300 -- Iteration 523813 - Batch 77/7702 - Train loss: 0.00195929  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.10 min) Epoch 69/300 -- Iteration 523890 - Batch 154/7702 - Train loss: 0.00190615  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.12 min) Epoch 69/300 -- Iteration 523967 - Batch 231/7702 - Train loss: 0.00190730  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.14 min) Epoch 69/300 -- Iteration 524044 - Batch 308/7702 - Train loss: 0.00190064  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.16 min) Epoch 69/300 -- Iteration 524121 - Batch 385/7702 - Train loss: 0.00189429  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.18 min) Epoch 69/300 -- Iteration 524198 - Batch 462/7702 - Train loss: 0.00189344  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.20 min) Epoch 69/300 -- Iteration 524275 - Batch 539/7702 - Train loss: 0.00189076  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.22 min) Epoch 69/300 -- Iteration 524352 - Batch 616/7702 - Train loss: 0.00188867  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.24 min) Epoch 69/300 -- Iteration 524429 - Batch 693/7702 - Train loss: 0.00188656  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.26 min) Epoch 69/300 -- Iteration 524506 - Batch 770/7702 - Train loss: 0.00188528  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.28 min) Epoch 69/300 -- Iteration 524583 - Batch 847/7702 - Train loss: 0.00188485  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.30 min) Epoch 69/300 -- Iteration 524660 - Batch 924/7702 - Train loss: 0.00188490  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.32 min) Epoch 69/300 -- Iteration 524737 - Batch 1001/7702 - Train loss: 0.00188474  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.34 min) Epoch 69/300 -- Iteration 524814 - Batch 1078/7702 - Train loss: 0.00188390  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.37 min) Epoch 69/300 -- Iteration 524891 - Batch 1155/7702 - Train loss: 0.00188550  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.39 min) Epoch 69/300 -- Iteration 524968 - Batch 1232/7702 - Train loss: 0.00188605  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.41 min) Epoch 69/300 -- Iteration 525045 - Batch 1309/7702 - Train loss: 0.00188614  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.43 min) Epoch 69/300 -- Iteration 525122 - Batch 1386/7702 - Train loss: 0.00188351  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.45 min) Epoch 69/300 -- Iteration 525199 - Batch 1463/7702 - Train loss: 0.00188340  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.47 min) Epoch 69/300 -- Iteration 525276 - Batch 1540/7702 - Train loss: 0.00188468  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.49 min) Epoch 69/300 -- Iteration 525353 - Batch 1617/7702 - Train loss: 0.00188478  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.51 min) Epoch 69/300 -- Iteration 525430 - Batch 1694/7702 - Train loss: 0.00188508  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.53 min) Epoch 69/300 -- Iteration 525507 - Batch 1771/7702 - Train loss: 0.00188585  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.55 min) Epoch 69/300 -- Iteration 525584 - Batch 1848/7702 - Train loss: 0.00188652  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.57 min) Epoch 69/300 -- Iteration 525661 - Batch 1925/7702 - Train loss: 0.00188614  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.59 min) Epoch 69/300 -- Iteration 525738 - Batch 2002/7702 - Train loss: 0.00188686  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.61 min) Epoch 69/300 -- Iteration 525815 - Batch 2079/7702 - Train loss: 0.00188610  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.64 min) Epoch 69/300 -- Iteration 525892 - Batch 2156/7702 - Train loss: 0.00188724  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.66 min) Epoch 69/300 -- Iteration 525969 - Batch 2233/7702 - Train loss: 0.00188793  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.68 min) Epoch 69/300 -- Iteration 526046 - Batch 2310/7702 - Train loss: 0.00188810  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.70 min) Epoch 69/300 -- Iteration 526123 - Batch 2387/7702 - Train loss: 0.00188868  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.72 min) Epoch 69/300 -- Iteration 526200 - Batch 2464/7702 - Train loss: 0.00188954  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.74 min) Epoch 69/300 -- Iteration 526277 - Batch 2541/7702 - Train loss: 0.00188952  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.76 min) Epoch 69/300 -- Iteration 526354 - Batch 2618/7702 - Train loss: 0.00188926  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.78 min) Epoch 69/300 -- Iteration 526431 - Batch 2695/7702 - Train loss: 0.00188959  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.80 min) Epoch 69/300 -- Iteration 526508 - Batch 2772/7702 - Train loss: 0.00189015  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.82 min) Epoch 69/300 -- Iteration 526585 - Batch 2849/7702 - Train loss: 0.00188908  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.84 min) Epoch 69/300 -- Iteration 526662 - Batch 2926/7702 - Train loss: 0.00189001  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.86 min) Epoch 69/300 -- Iteration 526739 - Batch 3003/7702 - Train loss: 0.00189024  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.88 min) Epoch 69/300 -- Iteration 526816 - Batch 3080/7702 - Train loss: 0.00189058  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.90 min) Epoch 69/300 -- Iteration 526893 - Batch 3157/7702 - Train loss: 0.00189080  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.93 min) Epoch 69/300 -- Iteration 526970 - Batch 3234/7702 - Train loss: 0.00189053  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.95 min) Epoch 69/300 -- Iteration 527047 - Batch 3311/7702 - Train loss: 0.00188976  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.97 min) Epoch 69/300 -- Iteration 527124 - Batch 3388/7702 - Train loss: 0.00188991  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(141.99 min) Epoch 69/300 -- Iteration 527201 - Batch 3465/7702 - Train loss: 0.00188929  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.01 min) Epoch 69/300 -- Iteration 527278 - Batch 3542/7702 - Train loss: 0.00188879  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.03 min) Epoch 69/300 -- Iteration 527355 - Batch 3619/7702 - Train loss: 0.00188814  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.05 min) Epoch 69/300 -- Iteration 527432 - Batch 3696/7702 - Train loss: 0.00188766  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.07 min) Epoch 69/300 -- Iteration 527509 - Batch 3773/7702 - Train loss: 0.00188762  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.09 min) Epoch 69/300 -- Iteration 527586 - Batch 3850/7702 - Train loss: 0.00188797  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.11 min) Epoch 69/300 -- Iteration 527663 - Batch 3927/7702 - Train loss: 0.00188723  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.13 min) Epoch 69/300 -- Iteration 527740 - Batch 4004/7702 - Train loss: 0.00188712  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.15 min) Epoch 69/300 -- Iteration 527817 - Batch 4081/7702 - Train loss: 0.00188734  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.17 min) Epoch 69/300 -- Iteration 527894 - Batch 4158/7702 - Train loss: 0.00188699  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.20 min) Epoch 69/300 -- Iteration 527971 - Batch 4235/7702 - Train loss: 0.00188760  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.22 min) Epoch 69/300 -- Iteration 528048 - Batch 4312/7702 - Train loss: 0.00188722  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.24 min) Epoch 69/300 -- Iteration 528125 - Batch 4389/7702 - Train loss: 0.00188711  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.26 min) Epoch 69/300 -- Iteration 528202 - Batch 4466/7702 - Train loss: 0.00188665  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.28 min) Epoch 69/300 -- Iteration 528279 - Batch 4543/7702 - Train loss: 0.00188637  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.30 min) Epoch 69/300 -- Iteration 528356 - Batch 4620/7702 - Train loss: 0.00188608  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.32 min) Epoch 69/300 -- Iteration 528433 - Batch 4697/7702 - Train loss: 0.00188553  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.34 min) Epoch 69/300 -- Iteration 528510 - Batch 4774/7702 - Train loss: 0.00188487  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.36 min) Epoch 69/300 -- Iteration 528587 - Batch 4851/7702 - Train loss: 0.00188442  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.38 min) Epoch 69/300 -- Iteration 528664 - Batch 4928/7702 - Train loss: 0.00188469  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.40 min) Epoch 69/300 -- Iteration 528741 - Batch 5005/7702 - Train loss: 0.00188440  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.42 min) Epoch 69/300 -- Iteration 528818 - Batch 5082/7702 - Train loss: 0.00188403  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.44 min) Epoch 69/300 -- Iteration 528895 - Batch 5159/7702 - Train loss: 0.00188369  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.47 min) Epoch 69/300 -- Iteration 528972 - Batch 5236/7702 - Train loss: 0.00188384  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.49 min) Epoch 69/300 -- Iteration 529049 - Batch 5313/7702 - Train loss: 0.00188348  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.51 min) Epoch 69/300 -- Iteration 529126 - Batch 5390/7702 - Train loss: 0.00188410  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.53 min) Epoch 69/300 -- Iteration 529203 - Batch 5467/7702 - Train loss: 0.00188412  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.55 min) Epoch 69/300 -- Iteration 529280 - Batch 5544/7702 - Train loss: 0.00188389  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.57 min) Epoch 69/300 -- Iteration 529357 - Batch 5621/7702 - Train loss: 0.00188411  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.59 min) Epoch 69/300 -- Iteration 529434 - Batch 5698/7702 - Train loss: 0.00188472  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.61 min) Epoch 69/300 -- Iteration 529511 - Batch 5775/7702 - Train loss: 0.00188491  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.63 min) Epoch 69/300 -- Iteration 529588 - Batch 5852/7702 - Train loss: 0.00188501  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.65 min) Epoch 69/300 -- Iteration 529665 - Batch 5929/7702 - Train loss: 0.00188524  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.67 min) Epoch 69/300 -- Iteration 529742 - Batch 6006/7702 - Train loss: 0.00188554  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.69 min) Epoch 69/300 -- Iteration 529819 - Batch 6083/7702 - Train loss: 0.00188570  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.71 min) Epoch 69/300 -- Iteration 529896 - Batch 6160/7702 - Train loss: 0.00188578  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.74 min) Epoch 69/300 -- Iteration 529973 - Batch 6237/7702 - Train loss: 0.00188667  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.76 min) Epoch 69/300 -- Iteration 530050 - Batch 6314/7702 - Train loss: 0.00188704  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.78 min) Epoch 69/300 -- Iteration 530127 - Batch 6391/7702 - Train loss: 0.00188710  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.80 min) Epoch 69/300 -- Iteration 530204 - Batch 6468/7702 - Train loss: 0.00188684  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.82 min) Epoch 69/300 -- Iteration 530281 - Batch 6545/7702 - Train loss: 0.00188681  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.84 min) Epoch 69/300 -- Iteration 530358 - Batch 6622/7702 - Train loss: 0.00188673  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.86 min) Epoch 69/300 -- Iteration 530435 - Batch 6699/7702 - Train loss: 0.00188687  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.88 min) Epoch 69/300 -- Iteration 530512 - Batch 6776/7702 - Train loss: 0.00188653  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.90 min) Epoch 69/300 -- Iteration 530589 - Batch 6853/7702 - Train loss: 0.00188652  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.92 min) Epoch 69/300 -- Iteration 530666 - Batch 6930/7702 - Train loss: 0.00188642  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.94 min) Epoch 69/300 -- Iteration 530743 - Batch 7007/7702 - Train loss: 0.00188658  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.96 min) Epoch 69/300 -- Iteration 530820 - Batch 7084/7702 - Train loss: 0.00188675  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(142.98 min) Epoch 69/300 -- Iteration 530897 - Batch 7161/7702 - Train loss: 0.00188661  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(143.00 min) Epoch 69/300 -- Iteration 530974 - Batch 7238/7702 - Train loss: 0.00188678  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(143.02 min) Epoch 69/300 -- Iteration 531051 - Batch 7315/7702 - Train loss: 0.00188696  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(143.05 min) Epoch 69/300 -- Iteration 531128 - Batch 7392/7702 - Train loss: 0.00188696  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(143.07 min) Epoch 69/300 -- Iteration 531205 - Batch 7469/7702 - Train loss: 0.00188744  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(143.09 min) Epoch 69/300 -- Iteration 531282 - Batch 7546/7702 - Train loss: 0.00188729  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(143.11 min) Epoch 69/300 -- Iteration 531359 - Batch 7623/7702 - Train loss: 0.00188758  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(143.13 min) Epoch 69/300 -- Iteration 531436 - Batch 7700/7702 - Train loss: 0.00188772  - Train acc: -0.0000 - Val loss: 0.00037382\n",
      "(143.13 min) Epoch 69/300 -- Iteration 531438 - Batch 7701/7702 - Train loss: 0.00188765  - Train acc: -0.0000 - Val loss: 0.00047677 - Val acc: -0.0000\n",
      "(143.15 min) Epoch 70/300 -- Iteration 531515 - Batch 77/7702 - Train loss: 0.00189524  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.17 min) Epoch 70/300 -- Iteration 531592 - Batch 154/7702 - Train loss: 0.00187665  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.19 min) Epoch 70/300 -- Iteration 531669 - Batch 231/7702 - Train loss: 0.00188180  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.21 min) Epoch 70/300 -- Iteration 531746 - Batch 308/7702 - Train loss: 0.00188316  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.23 min) Epoch 70/300 -- Iteration 531823 - Batch 385/7702 - Train loss: 0.00188244  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.26 min) Epoch 70/300 -- Iteration 531900 - Batch 462/7702 - Train loss: 0.00187861  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.28 min) Epoch 70/300 -- Iteration 531977 - Batch 539/7702 - Train loss: 0.00187589  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.30 min) Epoch 70/300 -- Iteration 532054 - Batch 616/7702 - Train loss: 0.00187812  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.32 min) Epoch 70/300 -- Iteration 532131 - Batch 693/7702 - Train loss: 0.00187824  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.34 min) Epoch 70/300 -- Iteration 532208 - Batch 770/7702 - Train loss: 0.00188001  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.36 min) Epoch 70/300 -- Iteration 532285 - Batch 847/7702 - Train loss: 0.00188061  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.38 min) Epoch 70/300 -- Iteration 532362 - Batch 924/7702 - Train loss: 0.00187955  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.40 min) Epoch 70/300 -- Iteration 532439 - Batch 1001/7702 - Train loss: 0.00187887  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.42 min) Epoch 70/300 -- Iteration 532516 - Batch 1078/7702 - Train loss: 0.00187793  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.44 min) Epoch 70/300 -- Iteration 532593 - Batch 1155/7702 - Train loss: 0.00187912  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.46 min) Epoch 70/300 -- Iteration 532670 - Batch 1232/7702 - Train loss: 0.00187757  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.48 min) Epoch 70/300 -- Iteration 532747 - Batch 1309/7702 - Train loss: 0.00187916  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.50 min) Epoch 70/300 -- Iteration 532824 - Batch 1386/7702 - Train loss: 0.00187981  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.53 min) Epoch 70/300 -- Iteration 532901 - Batch 1463/7702 - Train loss: 0.00188046  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.55 min) Epoch 70/300 -- Iteration 532978 - Batch 1540/7702 - Train loss: 0.00187814  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.57 min) Epoch 70/300 -- Iteration 533055 - Batch 1617/7702 - Train loss: 0.00187786  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.59 min) Epoch 70/300 -- Iteration 533132 - Batch 1694/7702 - Train loss: 0.00187861  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.61 min) Epoch 70/300 -- Iteration 533209 - Batch 1771/7702 - Train loss: 0.00187696  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.63 min) Epoch 70/300 -- Iteration 533286 - Batch 1848/7702 - Train loss: 0.00187720  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.65 min) Epoch 70/300 -- Iteration 533363 - Batch 1925/7702 - Train loss: 0.00187701  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.67 min) Epoch 70/300 -- Iteration 533440 - Batch 2002/7702 - Train loss: 0.00187774  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.69 min) Epoch 70/300 -- Iteration 533517 - Batch 2079/7702 - Train loss: 0.00187816  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.71 min) Epoch 70/300 -- Iteration 533594 - Batch 2156/7702 - Train loss: 0.00187758  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.73 min) Epoch 70/300 -- Iteration 533671 - Batch 2233/7702 - Train loss: 0.00187847  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.75 min) Epoch 70/300 -- Iteration 533748 - Batch 2310/7702 - Train loss: 0.00187815  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.77 min) Epoch 70/300 -- Iteration 533825 - Batch 2387/7702 - Train loss: 0.00187884  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.79 min) Epoch 70/300 -- Iteration 533902 - Batch 2464/7702 - Train loss: 0.00187906  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.82 min) Epoch 70/300 -- Iteration 533979 - Batch 2541/7702 - Train loss: 0.00187952  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.84 min) Epoch 70/300 -- Iteration 534056 - Batch 2618/7702 - Train loss: 0.00187976  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.86 min) Epoch 70/300 -- Iteration 534133 - Batch 2695/7702 - Train loss: 0.00188007  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.88 min) Epoch 70/300 -- Iteration 534210 - Batch 2772/7702 - Train loss: 0.00188018  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.90 min) Epoch 70/300 -- Iteration 534287 - Batch 2849/7702 - Train loss: 0.00188134  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.92 min) Epoch 70/300 -- Iteration 534364 - Batch 2926/7702 - Train loss: 0.00188104  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.94 min) Epoch 70/300 -- Iteration 534441 - Batch 3003/7702 - Train loss: 0.00188148  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.96 min) Epoch 70/300 -- Iteration 534518 - Batch 3080/7702 - Train loss: 0.00188235  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(143.98 min) Epoch 70/300 -- Iteration 534595 - Batch 3157/7702 - Train loss: 0.00188278  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.00 min) Epoch 70/300 -- Iteration 534672 - Batch 3234/7702 - Train loss: 0.00188328  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.02 min) Epoch 70/300 -- Iteration 534749 - Batch 3311/7702 - Train loss: 0.00188368  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.04 min) Epoch 70/300 -- Iteration 534826 - Batch 3388/7702 - Train loss: 0.00188390  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.06 min) Epoch 70/300 -- Iteration 534903 - Batch 3465/7702 - Train loss: 0.00188459  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.09 min) Epoch 70/300 -- Iteration 534980 - Batch 3542/7702 - Train loss: 0.00188430  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.11 min) Epoch 70/300 -- Iteration 535057 - Batch 3619/7702 - Train loss: 0.00188456  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.13 min) Epoch 70/300 -- Iteration 535134 - Batch 3696/7702 - Train loss: 0.00188487  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.15 min) Epoch 70/300 -- Iteration 535211 - Batch 3773/7702 - Train loss: 0.00188562  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.17 min) Epoch 70/300 -- Iteration 535288 - Batch 3850/7702 - Train loss: 0.00188562  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.19 min) Epoch 70/300 -- Iteration 535365 - Batch 3927/7702 - Train loss: 0.00188531  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.21 min) Epoch 70/300 -- Iteration 535442 - Batch 4004/7702 - Train loss: 0.00188512  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.23 min) Epoch 70/300 -- Iteration 535519 - Batch 4081/7702 - Train loss: 0.00188473  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.25 min) Epoch 70/300 -- Iteration 535596 - Batch 4158/7702 - Train loss: 0.00188471  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.27 min) Epoch 70/300 -- Iteration 535673 - Batch 4235/7702 - Train loss: 0.00188434  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.29 min) Epoch 70/300 -- Iteration 535750 - Batch 4312/7702 - Train loss: 0.00188416  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.31 min) Epoch 70/300 -- Iteration 535827 - Batch 4389/7702 - Train loss: 0.00188432  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.33 min) Epoch 70/300 -- Iteration 535904 - Batch 4466/7702 - Train loss: 0.00188437  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.35 min) Epoch 70/300 -- Iteration 535981 - Batch 4543/7702 - Train loss: 0.00188492  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.37 min) Epoch 70/300 -- Iteration 536058 - Batch 4620/7702 - Train loss: 0.00188492  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.40 min) Epoch 70/300 -- Iteration 536135 - Batch 4697/7702 - Train loss: 0.00188531  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.42 min) Epoch 70/300 -- Iteration 536212 - Batch 4774/7702 - Train loss: 0.00188511  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.44 min) Epoch 70/300 -- Iteration 536289 - Batch 4851/7702 - Train loss: 0.00188547  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.46 min) Epoch 70/300 -- Iteration 536366 - Batch 4928/7702 - Train loss: 0.00188517  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.48 min) Epoch 70/300 -- Iteration 536443 - Batch 5005/7702 - Train loss: 0.00188509  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.50 min) Epoch 70/300 -- Iteration 536520 - Batch 5082/7702 - Train loss: 0.00188510  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.52 min) Epoch 70/300 -- Iteration 536597 - Batch 5159/7702 - Train loss: 0.00188557  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.54 min) Epoch 70/300 -- Iteration 536674 - Batch 5236/7702 - Train loss: 0.00188537  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.56 min) Epoch 70/300 -- Iteration 536751 - Batch 5313/7702 - Train loss: 0.00188512  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.58 min) Epoch 70/300 -- Iteration 536828 - Batch 5390/7702 - Train loss: 0.00188542  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.60 min) Epoch 70/300 -- Iteration 536905 - Batch 5467/7702 - Train loss: 0.00188504  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.62 min) Epoch 70/300 -- Iteration 536982 - Batch 5544/7702 - Train loss: 0.00188509  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.64 min) Epoch 70/300 -- Iteration 537059 - Batch 5621/7702 - Train loss: 0.00188487  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.66 min) Epoch 70/300 -- Iteration 537136 - Batch 5698/7702 - Train loss: 0.00188487  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.69 min) Epoch 70/300 -- Iteration 537213 - Batch 5775/7702 - Train loss: 0.00188505  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.71 min) Epoch 70/300 -- Iteration 537290 - Batch 5852/7702 - Train loss: 0.00188455  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.73 min) Epoch 70/300 -- Iteration 537367 - Batch 5929/7702 - Train loss: 0.00188450  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.75 min) Epoch 70/300 -- Iteration 537444 - Batch 6006/7702 - Train loss: 0.00188469  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.77 min) Epoch 70/300 -- Iteration 537521 - Batch 6083/7702 - Train loss: 0.00188459  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.79 min) Epoch 70/300 -- Iteration 537598 - Batch 6160/7702 - Train loss: 0.00188469  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.81 min) Epoch 70/300 -- Iteration 537675 - Batch 6237/7702 - Train loss: 0.00188475  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.83 min) Epoch 70/300 -- Iteration 537752 - Batch 6314/7702 - Train loss: 0.00188504  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.85 min) Epoch 70/300 -- Iteration 537829 - Batch 6391/7702 - Train loss: 0.00188525  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.87 min) Epoch 70/300 -- Iteration 537906 - Batch 6468/7702 - Train loss: 0.00188515  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.89 min) Epoch 70/300 -- Iteration 537983 - Batch 6545/7702 - Train loss: 0.00188525  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.91 min) Epoch 70/300 -- Iteration 538060 - Batch 6622/7702 - Train loss: 0.00188516  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.93 min) Epoch 70/300 -- Iteration 538137 - Batch 6699/7702 - Train loss: 0.00188508  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.96 min) Epoch 70/300 -- Iteration 538214 - Batch 6776/7702 - Train loss: 0.00188519  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(144.98 min) Epoch 70/300 -- Iteration 538291 - Batch 6853/7702 - Train loss: 0.00188507  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.00 min) Epoch 70/300 -- Iteration 538368 - Batch 6930/7702 - Train loss: 0.00188496  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.02 min) Epoch 70/300 -- Iteration 538445 - Batch 7007/7702 - Train loss: 0.00188466  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.04 min) Epoch 70/300 -- Iteration 538522 - Batch 7084/7702 - Train loss: 0.00188477  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.06 min) Epoch 70/300 -- Iteration 538599 - Batch 7161/7702 - Train loss: 0.00188470  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.08 min) Epoch 70/300 -- Iteration 538676 - Batch 7238/7702 - Train loss: 0.00188494  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.10 min) Epoch 70/300 -- Iteration 538753 - Batch 7315/7702 - Train loss: 0.00188501  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.12 min) Epoch 70/300 -- Iteration 538830 - Batch 7392/7702 - Train loss: 0.00188482  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.14 min) Epoch 70/300 -- Iteration 538907 - Batch 7469/7702 - Train loss: 0.00188460  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.16 min) Epoch 70/300 -- Iteration 538984 - Batch 7546/7702 - Train loss: 0.00188483  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.18 min) Epoch 70/300 -- Iteration 539061 - Batch 7623/7702 - Train loss: 0.00188475  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.20 min) Epoch 70/300 -- Iteration 539138 - Batch 7700/7702 - Train loss: 0.00188447  - Train acc: -0.0000 - Val loss: 0.00047677\n",
      "(145.20 min) Epoch 70/300 -- Iteration 539140 - Batch 7701/7702 - Train loss: 0.00188454  - Train acc: -0.0000 - Val loss: 0.00033519 - Val acc: -0.0000\n",
      "(145.23 min) Epoch 71/300 -- Iteration 539217 - Batch 77/7702 - Train loss: 0.00188589  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.25 min) Epoch 71/300 -- Iteration 539294 - Batch 154/7702 - Train loss: 0.00187745  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.27 min) Epoch 71/300 -- Iteration 539371 - Batch 231/7702 - Train loss: 0.00189827  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.29 min) Epoch 71/300 -- Iteration 539448 - Batch 308/7702 - Train loss: 0.00189615  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.31 min) Epoch 71/300 -- Iteration 539525 - Batch 385/7702 - Train loss: 0.00189542  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.33 min) Epoch 71/300 -- Iteration 539602 - Batch 462/7702 - Train loss: 0.00189067  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.35 min) Epoch 71/300 -- Iteration 539679 - Batch 539/7702 - Train loss: 0.00189170  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.37 min) Epoch 71/300 -- Iteration 539756 - Batch 616/7702 - Train loss: 0.00189023  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.39 min) Epoch 71/300 -- Iteration 539833 - Batch 693/7702 - Train loss: 0.00189002  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.41 min) Epoch 71/300 -- Iteration 539910 - Batch 770/7702 - Train loss: 0.00188603  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.43 min) Epoch 71/300 -- Iteration 539987 - Batch 847/7702 - Train loss: 0.00188603  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.46 min) Epoch 71/300 -- Iteration 540064 - Batch 924/7702 - Train loss: 0.00188675  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.48 min) Epoch 71/300 -- Iteration 540141 - Batch 1001/7702 - Train loss: 0.00188785  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.50 min) Epoch 71/300 -- Iteration 540218 - Batch 1078/7702 - Train loss: 0.00188774  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.52 min) Epoch 71/300 -- Iteration 540295 - Batch 1155/7702 - Train loss: 0.00188751  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.54 min) Epoch 71/300 -- Iteration 540372 - Batch 1232/7702 - Train loss: 0.00188739  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.56 min) Epoch 71/300 -- Iteration 540449 - Batch 1309/7702 - Train loss: 0.00188718  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.58 min) Epoch 71/300 -- Iteration 540526 - Batch 1386/7702 - Train loss: 0.00188597  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.60 min) Epoch 71/300 -- Iteration 540603 - Batch 1463/7702 - Train loss: 0.00188650  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.62 min) Epoch 71/300 -- Iteration 540680 - Batch 1540/7702 - Train loss: 0.00188565  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.64 min) Epoch 71/300 -- Iteration 540757 - Batch 1617/7702 - Train loss: 0.00188553  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.66 min) Epoch 71/300 -- Iteration 540834 - Batch 1694/7702 - Train loss: 0.00188563  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.68 min) Epoch 71/300 -- Iteration 540911 - Batch 1771/7702 - Train loss: 0.00188574  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.70 min) Epoch 71/300 -- Iteration 540988 - Batch 1848/7702 - Train loss: 0.00188624  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.73 min) Epoch 71/300 -- Iteration 541065 - Batch 1925/7702 - Train loss: 0.00188752  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.75 min) Epoch 71/300 -- Iteration 541142 - Batch 2002/7702 - Train loss: 0.00188737  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.77 min) Epoch 71/300 -- Iteration 541219 - Batch 2079/7702 - Train loss: 0.00188828  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.79 min) Epoch 71/300 -- Iteration 541296 - Batch 2156/7702 - Train loss: 0.00188923  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.81 min) Epoch 71/300 -- Iteration 541373 - Batch 2233/7702 - Train loss: 0.00188898  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.83 min) Epoch 71/300 -- Iteration 541450 - Batch 2310/7702 - Train loss: 0.00188870  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.85 min) Epoch 71/300 -- Iteration 541527 - Batch 2387/7702 - Train loss: 0.00188873  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.87 min) Epoch 71/300 -- Iteration 541604 - Batch 2464/7702 - Train loss: 0.00188717  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.89 min) Epoch 71/300 -- Iteration 541681 - Batch 2541/7702 - Train loss: 0.00188715  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.91 min) Epoch 71/300 -- Iteration 541758 - Batch 2618/7702 - Train loss: 0.00188751  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.93 min) Epoch 71/300 -- Iteration 541835 - Batch 2695/7702 - Train loss: 0.00188691  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.95 min) Epoch 71/300 -- Iteration 541912 - Batch 2772/7702 - Train loss: 0.00188811  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(145.97 min) Epoch 71/300 -- Iteration 541989 - Batch 2849/7702 - Train loss: 0.00188875  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.00 min) Epoch 71/300 -- Iteration 542066 - Batch 2926/7702 - Train loss: 0.00188867  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.02 min) Epoch 71/300 -- Iteration 542143 - Batch 3003/7702 - Train loss: 0.00188822  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.04 min) Epoch 71/300 -- Iteration 542220 - Batch 3080/7702 - Train loss: 0.00188820  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.06 min) Epoch 71/300 -- Iteration 542297 - Batch 3157/7702 - Train loss: 0.00188851  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.08 min) Epoch 71/300 -- Iteration 542374 - Batch 3234/7702 - Train loss: 0.00188878  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.10 min) Epoch 71/300 -- Iteration 542451 - Batch 3311/7702 - Train loss: 0.00188922  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.12 min) Epoch 71/300 -- Iteration 542528 - Batch 3388/7702 - Train loss: 0.00189004  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.14 min) Epoch 71/300 -- Iteration 542605 - Batch 3465/7702 - Train loss: 0.00189022  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.16 min) Epoch 71/300 -- Iteration 542682 - Batch 3542/7702 - Train loss: 0.00188998  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.18 min) Epoch 71/300 -- Iteration 542759 - Batch 3619/7702 - Train loss: 0.00188960  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.20 min) Epoch 71/300 -- Iteration 542836 - Batch 3696/7702 - Train loss: 0.00188941  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.22 min) Epoch 71/300 -- Iteration 542913 - Batch 3773/7702 - Train loss: 0.00188982  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.25 min) Epoch 71/300 -- Iteration 542990 - Batch 3850/7702 - Train loss: 0.00189039  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.27 min) Epoch 71/300 -- Iteration 543067 - Batch 3927/7702 - Train loss: 0.00189018  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.29 min) Epoch 71/300 -- Iteration 543144 - Batch 4004/7702 - Train loss: 0.00189001  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.31 min) Epoch 71/300 -- Iteration 543221 - Batch 4081/7702 - Train loss: 0.00189032  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.33 min) Epoch 71/300 -- Iteration 543298 - Batch 4158/7702 - Train loss: 0.00189015  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.35 min) Epoch 71/300 -- Iteration 543375 - Batch 4235/7702 - Train loss: 0.00189006  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.37 min) Epoch 71/300 -- Iteration 543452 - Batch 4312/7702 - Train loss: 0.00188929  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.39 min) Epoch 71/300 -- Iteration 543529 - Batch 4389/7702 - Train loss: 0.00188956  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.41 min) Epoch 71/300 -- Iteration 543606 - Batch 4466/7702 - Train loss: 0.00189004  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.43 min) Epoch 71/300 -- Iteration 543683 - Batch 4543/7702 - Train loss: 0.00189061  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.45 min) Epoch 71/300 -- Iteration 543760 - Batch 4620/7702 - Train loss: 0.00189051  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.47 min) Epoch 71/300 -- Iteration 543837 - Batch 4697/7702 - Train loss: 0.00189104  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.50 min) Epoch 71/300 -- Iteration 543914 - Batch 4774/7702 - Train loss: 0.00189151  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.52 min) Epoch 71/300 -- Iteration 543991 - Batch 4851/7702 - Train loss: 0.00189128  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.54 min) Epoch 71/300 -- Iteration 544068 - Batch 4928/7702 - Train loss: 0.00189048  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.56 min) Epoch 71/300 -- Iteration 544145 - Batch 5005/7702 - Train loss: 0.00189061  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.58 min) Epoch 71/300 -- Iteration 544222 - Batch 5082/7702 - Train loss: 0.00189038  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.60 min) Epoch 71/300 -- Iteration 544299 - Batch 5159/7702 - Train loss: 0.00188943  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.62 min) Epoch 71/300 -- Iteration 544376 - Batch 5236/7702 - Train loss: 0.00188953  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.64 min) Epoch 71/300 -- Iteration 544453 - Batch 5313/7702 - Train loss: 0.00188914  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.66 min) Epoch 71/300 -- Iteration 544530 - Batch 5390/7702 - Train loss: 0.00188886  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.68 min) Epoch 71/300 -- Iteration 544607 - Batch 5467/7702 - Train loss: 0.00188907  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.70 min) Epoch 71/300 -- Iteration 544684 - Batch 5544/7702 - Train loss: 0.00188939  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.72 min) Epoch 71/300 -- Iteration 544761 - Batch 5621/7702 - Train loss: 0.00188896  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.75 min) Epoch 71/300 -- Iteration 544838 - Batch 5698/7702 - Train loss: 0.00188897  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.77 min) Epoch 71/300 -- Iteration 544915 - Batch 5775/7702 - Train loss: 0.00188816  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.79 min) Epoch 71/300 -- Iteration 544992 - Batch 5852/7702 - Train loss: 0.00188780  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.81 min) Epoch 71/300 -- Iteration 545069 - Batch 5929/7702 - Train loss: 0.00188807  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.83 min) Epoch 71/300 -- Iteration 545146 - Batch 6006/7702 - Train loss: 0.00188814  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.85 min) Epoch 71/300 -- Iteration 545223 - Batch 6083/7702 - Train loss: 0.00188812  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.87 min) Epoch 71/300 -- Iteration 545300 - Batch 6160/7702 - Train loss: 0.00188833  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.89 min) Epoch 71/300 -- Iteration 545377 - Batch 6237/7702 - Train loss: 0.00188821  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.91 min) Epoch 71/300 -- Iteration 545454 - Batch 6314/7702 - Train loss: 0.00188838  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.93 min) Epoch 71/300 -- Iteration 545531 - Batch 6391/7702 - Train loss: 0.00188823  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.95 min) Epoch 71/300 -- Iteration 545608 - Batch 6468/7702 - Train loss: 0.00188807  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.97 min) Epoch 71/300 -- Iteration 545685 - Batch 6545/7702 - Train loss: 0.00188808  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(146.99 min) Epoch 71/300 -- Iteration 545762 - Batch 6622/7702 - Train loss: 0.00188840  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.01 min) Epoch 71/300 -- Iteration 545839 - Batch 6699/7702 - Train loss: 0.00188879  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.03 min) Epoch 71/300 -- Iteration 545916 - Batch 6776/7702 - Train loss: 0.00188891  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.06 min) Epoch 71/300 -- Iteration 545993 - Batch 6853/7702 - Train loss: 0.00188906  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.08 min) Epoch 71/300 -- Iteration 546070 - Batch 6930/7702 - Train loss: 0.00188935  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.10 min) Epoch 71/300 -- Iteration 546147 - Batch 7007/7702 - Train loss: 0.00188908  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.12 min) Epoch 71/300 -- Iteration 546224 - Batch 7084/7702 - Train loss: 0.00188895  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.14 min) Epoch 71/300 -- Iteration 546301 - Batch 7161/7702 - Train loss: 0.00188893  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.16 min) Epoch 71/300 -- Iteration 546378 - Batch 7238/7702 - Train loss: 0.00188894  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.18 min) Epoch 71/300 -- Iteration 546455 - Batch 7315/7702 - Train loss: 0.00188897  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.20 min) Epoch 71/300 -- Iteration 546532 - Batch 7392/7702 - Train loss: 0.00188859  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.22 min) Epoch 71/300 -- Iteration 546609 - Batch 7469/7702 - Train loss: 0.00188816  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.24 min) Epoch 71/300 -- Iteration 546686 - Batch 7546/7702 - Train loss: 0.00188787  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.26 min) Epoch 71/300 -- Iteration 546763 - Batch 7623/7702 - Train loss: 0.00188755  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.28 min) Epoch 71/300 -- Iteration 546840 - Batch 7700/7702 - Train loss: 0.00188793  - Train acc: -0.0000 - Val loss: 0.00033519\n",
      "(147.28 min) Epoch 71/300 -- Iteration 546842 - Batch 7701/7702 - Train loss: 0.00188794  - Train acc: -0.0000 - Val loss: 0.00042036 - Val acc: -0.0000\n",
      "(147.31 min) Epoch 72/300 -- Iteration 546919 - Batch 77/7702 - Train loss: 0.00190945  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.33 min) Epoch 72/300 -- Iteration 546996 - Batch 154/7702 - Train loss: 0.00190208  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.35 min) Epoch 72/300 -- Iteration 547073 - Batch 231/7702 - Train loss: 0.00188393  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.37 min) Epoch 72/300 -- Iteration 547150 - Batch 308/7702 - Train loss: 0.00188991  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.39 min) Epoch 72/300 -- Iteration 547227 - Batch 385/7702 - Train loss: 0.00189151  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.41 min) Epoch 72/300 -- Iteration 547304 - Batch 462/7702 - Train loss: 0.00189004  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.43 min) Epoch 72/300 -- Iteration 547381 - Batch 539/7702 - Train loss: 0.00188386  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.45 min) Epoch 72/300 -- Iteration 547458 - Batch 616/7702 - Train loss: 0.00188483  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.47 min) Epoch 72/300 -- Iteration 547535 - Batch 693/7702 - Train loss: 0.00188769  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.49 min) Epoch 72/300 -- Iteration 547612 - Batch 770/7702 - Train loss: 0.00188729  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.52 min) Epoch 72/300 -- Iteration 547689 - Batch 847/7702 - Train loss: 0.00188647  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.54 min) Epoch 72/300 -- Iteration 547766 - Batch 924/7702 - Train loss: 0.00188668  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.56 min) Epoch 72/300 -- Iteration 547843 - Batch 1001/7702 - Train loss: 0.00188707  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.58 min) Epoch 72/300 -- Iteration 547920 - Batch 1078/7702 - Train loss: 0.00188843  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.60 min) Epoch 72/300 -- Iteration 547997 - Batch 1155/7702 - Train loss: 0.00188635  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.62 min) Epoch 72/300 -- Iteration 548074 - Batch 1232/7702 - Train loss: 0.00188567  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.64 min) Epoch 72/300 -- Iteration 548151 - Batch 1309/7702 - Train loss: 0.00188701  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.66 min) Epoch 72/300 -- Iteration 548228 - Batch 1386/7702 - Train loss: 0.00188464  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.68 min) Epoch 72/300 -- Iteration 548305 - Batch 1463/7702 - Train loss: 0.00188601  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.70 min) Epoch 72/300 -- Iteration 548382 - Batch 1540/7702 - Train loss: 0.00188619  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.72 min) Epoch 72/300 -- Iteration 548459 - Batch 1617/7702 - Train loss: 0.00188514  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.74 min) Epoch 72/300 -- Iteration 548536 - Batch 1694/7702 - Train loss: 0.00188544  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.76 min) Epoch 72/300 -- Iteration 548613 - Batch 1771/7702 - Train loss: 0.00188428  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.78 min) Epoch 72/300 -- Iteration 548690 - Batch 1848/7702 - Train loss: 0.00188497  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.81 min) Epoch 72/300 -- Iteration 548767 - Batch 1925/7702 - Train loss: 0.00188556  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.83 min) Epoch 72/300 -- Iteration 548844 - Batch 2002/7702 - Train loss: 0.00188555  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.85 min) Epoch 72/300 -- Iteration 548921 - Batch 2079/7702 - Train loss: 0.00188484  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.87 min) Epoch 72/300 -- Iteration 548998 - Batch 2156/7702 - Train loss: 0.00188396  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.89 min) Epoch 72/300 -- Iteration 549075 - Batch 2233/7702 - Train loss: 0.00188374  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.91 min) Epoch 72/300 -- Iteration 549152 - Batch 2310/7702 - Train loss: 0.00188390  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.93 min) Epoch 72/300 -- Iteration 549229 - Batch 2387/7702 - Train loss: 0.00188422  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.95 min) Epoch 72/300 -- Iteration 549306 - Batch 2464/7702 - Train loss: 0.00188457  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.97 min) Epoch 72/300 -- Iteration 549383 - Batch 2541/7702 - Train loss: 0.00188352  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(147.99 min) Epoch 72/300 -- Iteration 549460 - Batch 2618/7702 - Train loss: 0.00188435  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.01 min) Epoch 72/300 -- Iteration 549537 - Batch 2695/7702 - Train loss: 0.00188432  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.03 min) Epoch 72/300 -- Iteration 549614 - Batch 2772/7702 - Train loss: 0.00188434  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.05 min) Epoch 72/300 -- Iteration 549691 - Batch 2849/7702 - Train loss: 0.00188484  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.07 min) Epoch 72/300 -- Iteration 549768 - Batch 2926/7702 - Train loss: 0.00188534  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.10 min) Epoch 72/300 -- Iteration 549845 - Batch 3003/7702 - Train loss: 0.00188507  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.12 min) Epoch 72/300 -- Iteration 549922 - Batch 3080/7702 - Train loss: 0.00188676  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.14 min) Epoch 72/300 -- Iteration 549999 - Batch 3157/7702 - Train loss: 0.00188712  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.16 min) Epoch 72/300 -- Iteration 550076 - Batch 3234/7702 - Train loss: 0.00188686  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.18 min) Epoch 72/300 -- Iteration 550153 - Batch 3311/7702 - Train loss: 0.00188774  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.20 min) Epoch 72/300 -- Iteration 550230 - Batch 3388/7702 - Train loss: 0.00188724  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.22 min) Epoch 72/300 -- Iteration 550307 - Batch 3465/7702 - Train loss: 0.00188710  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.24 min) Epoch 72/300 -- Iteration 550384 - Batch 3542/7702 - Train loss: 0.00188710  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.26 min) Epoch 72/300 -- Iteration 550461 - Batch 3619/7702 - Train loss: 0.00188608  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.28 min) Epoch 72/300 -- Iteration 550538 - Batch 3696/7702 - Train loss: 0.00188632  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.30 min) Epoch 72/300 -- Iteration 550615 - Batch 3773/7702 - Train loss: 0.00188514  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.32 min) Epoch 72/300 -- Iteration 550692 - Batch 3850/7702 - Train loss: 0.00188403  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.34 min) Epoch 72/300 -- Iteration 550769 - Batch 3927/7702 - Train loss: 0.00188330  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.37 min) Epoch 72/300 -- Iteration 550846 - Batch 4004/7702 - Train loss: 0.00188262  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.39 min) Epoch 72/300 -- Iteration 550923 - Batch 4081/7702 - Train loss: 0.00188295  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.41 min) Epoch 72/300 -- Iteration 551000 - Batch 4158/7702 - Train loss: 0.00188257  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.43 min) Epoch 72/300 -- Iteration 551077 - Batch 4235/7702 - Train loss: 0.00188242  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.45 min) Epoch 72/300 -- Iteration 551154 - Batch 4312/7702 - Train loss: 0.00188303  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.47 min) Epoch 72/300 -- Iteration 551231 - Batch 4389/7702 - Train loss: 0.00188269  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.49 min) Epoch 72/300 -- Iteration 551308 - Batch 4466/7702 - Train loss: 0.00188232  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.51 min) Epoch 72/300 -- Iteration 551385 - Batch 4543/7702 - Train loss: 0.00188242  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.53 min) Epoch 72/300 -- Iteration 551462 - Batch 4620/7702 - Train loss: 0.00188188  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.55 min) Epoch 72/300 -- Iteration 551539 - Batch 4697/7702 - Train loss: 0.00188210  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.57 min) Epoch 72/300 -- Iteration 551616 - Batch 4774/7702 - Train loss: 0.00188229  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.59 min) Epoch 72/300 -- Iteration 551693 - Batch 4851/7702 - Train loss: 0.00188296  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.61 min) Epoch 72/300 -- Iteration 551770 - Batch 4928/7702 - Train loss: 0.00188265  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.63 min) Epoch 72/300 -- Iteration 551847 - Batch 5005/7702 - Train loss: 0.00188235  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.65 min) Epoch 72/300 -- Iteration 551924 - Batch 5082/7702 - Train loss: 0.00188236  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.68 min) Epoch 72/300 -- Iteration 552001 - Batch 5159/7702 - Train loss: 0.00188212  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.70 min) Epoch 72/300 -- Iteration 552078 - Batch 5236/7702 - Train loss: 0.00188178  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.72 min) Epoch 72/300 -- Iteration 552155 - Batch 5313/7702 - Train loss: 0.00188208  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.74 min) Epoch 72/300 -- Iteration 552232 - Batch 5390/7702 - Train loss: 0.00188166  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.76 min) Epoch 72/300 -- Iteration 552309 - Batch 5467/7702 - Train loss: 0.00188190  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.78 min) Epoch 72/300 -- Iteration 552386 - Batch 5544/7702 - Train loss: 0.00188150  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.80 min) Epoch 72/300 -- Iteration 552463 - Batch 5621/7702 - Train loss: 0.00188178  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.82 min) Epoch 72/300 -- Iteration 552540 - Batch 5698/7702 - Train loss: 0.00188207  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.84 min) Epoch 72/300 -- Iteration 552617 - Batch 5775/7702 - Train loss: 0.00188136  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.86 min) Epoch 72/300 -- Iteration 552694 - Batch 5852/7702 - Train loss: 0.00188124  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.88 min) Epoch 72/300 -- Iteration 552771 - Batch 5929/7702 - Train loss: 0.00188144  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.90 min) Epoch 72/300 -- Iteration 552848 - Batch 6006/7702 - Train loss: 0.00188169  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.92 min) Epoch 72/300 -- Iteration 552925 - Batch 6083/7702 - Train loss: 0.00188141  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.95 min) Epoch 72/300 -- Iteration 553002 - Batch 6160/7702 - Train loss: 0.00188159  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.97 min) Epoch 72/300 -- Iteration 553079 - Batch 6237/7702 - Train loss: 0.00188176  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(148.99 min) Epoch 72/300 -- Iteration 553156 - Batch 6314/7702 - Train loss: 0.00188194  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.01 min) Epoch 72/300 -- Iteration 553233 - Batch 6391/7702 - Train loss: 0.00188182  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.03 min) Epoch 72/300 -- Iteration 553310 - Batch 6468/7702 - Train loss: 0.00188132  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.05 min) Epoch 72/300 -- Iteration 553387 - Batch 6545/7702 - Train loss: 0.00188133  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.07 min) Epoch 72/300 -- Iteration 553464 - Batch 6622/7702 - Train loss: 0.00188198  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.09 min) Epoch 72/300 -- Iteration 553541 - Batch 6699/7702 - Train loss: 0.00188188  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.11 min) Epoch 72/300 -- Iteration 553618 - Batch 6776/7702 - Train loss: 0.00188174  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.13 min) Epoch 72/300 -- Iteration 553695 - Batch 6853/7702 - Train loss: 0.00188177  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.15 min) Epoch 72/300 -- Iteration 553772 - Batch 6930/7702 - Train loss: 0.00188199  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.17 min) Epoch 72/300 -- Iteration 553849 - Batch 7007/7702 - Train loss: 0.00188168  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.19 min) Epoch 72/300 -- Iteration 553926 - Batch 7084/7702 - Train loss: 0.00188181  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.21 min) Epoch 72/300 -- Iteration 554003 - Batch 7161/7702 - Train loss: 0.00188187  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.24 min) Epoch 72/300 -- Iteration 554080 - Batch 7238/7702 - Train loss: 0.00188175  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.26 min) Epoch 72/300 -- Iteration 554157 - Batch 7315/7702 - Train loss: 0.00188147  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.28 min) Epoch 72/300 -- Iteration 554234 - Batch 7392/7702 - Train loss: 0.00188147  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.30 min) Epoch 72/300 -- Iteration 554311 - Batch 7469/7702 - Train loss: 0.00188193  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.32 min) Epoch 72/300 -- Iteration 554388 - Batch 7546/7702 - Train loss: 0.00188200  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.34 min) Epoch 72/300 -- Iteration 554465 - Batch 7623/7702 - Train loss: 0.00188183  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.36 min) Epoch 72/300 -- Iteration 554542 - Batch 7700/7702 - Train loss: 0.00188199  - Train acc: -0.0000 - Val loss: 0.00042036\n",
      "(149.36 min) Epoch 72/300 -- Iteration 554544 - Batch 7701/7702 - Train loss: 0.00188202  - Train acc: -0.0000 - Val loss: 0.00041600 - Val acc: -0.0000\n",
      "(149.38 min) Epoch 73/300 -- Iteration 554621 - Batch 77/7702 - Train loss: 0.00187094  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.40 min) Epoch 73/300 -- Iteration 554698 - Batch 154/7702 - Train loss: 0.00188650  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.43 min) Epoch 73/300 -- Iteration 554775 - Batch 231/7702 - Train loss: 0.00188954  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.45 min) Epoch 73/300 -- Iteration 554852 - Batch 308/7702 - Train loss: 0.00189148  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.47 min) Epoch 73/300 -- Iteration 554929 - Batch 385/7702 - Train loss: 0.00188813  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.49 min) Epoch 73/300 -- Iteration 555006 - Batch 462/7702 - Train loss: 0.00188644  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.51 min) Epoch 73/300 -- Iteration 555083 - Batch 539/7702 - Train loss: 0.00188465  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.53 min) Epoch 73/300 -- Iteration 555160 - Batch 616/7702 - Train loss: 0.00188436  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.55 min) Epoch 73/300 -- Iteration 555237 - Batch 693/7702 - Train loss: 0.00188872  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.57 min) Epoch 73/300 -- Iteration 555314 - Batch 770/7702 - Train loss: 0.00188870  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.59 min) Epoch 73/300 -- Iteration 555391 - Batch 847/7702 - Train loss: 0.00188887  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.61 min) Epoch 73/300 -- Iteration 555468 - Batch 924/7702 - Train loss: 0.00188709  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.63 min) Epoch 73/300 -- Iteration 555545 - Batch 1001/7702 - Train loss: 0.00188721  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.65 min) Epoch 73/300 -- Iteration 555622 - Batch 1078/7702 - Train loss: 0.00188545  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.67 min) Epoch 73/300 -- Iteration 555699 - Batch 1155/7702 - Train loss: 0.00188593  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.70 min) Epoch 73/300 -- Iteration 555776 - Batch 1232/7702 - Train loss: 0.00189017  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.72 min) Epoch 73/300 -- Iteration 555853 - Batch 1309/7702 - Train loss: 0.00188808  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.74 min) Epoch 73/300 -- Iteration 555930 - Batch 1386/7702 - Train loss: 0.00188895  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.76 min) Epoch 73/300 -- Iteration 556007 - Batch 1463/7702 - Train loss: 0.00188712  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.78 min) Epoch 73/300 -- Iteration 556084 - Batch 1540/7702 - Train loss: 0.00188874  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.80 min) Epoch 73/300 -- Iteration 556161 - Batch 1617/7702 - Train loss: 0.00189024  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.82 min) Epoch 73/300 -- Iteration 556238 - Batch 1694/7702 - Train loss: 0.00188971  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.84 min) Epoch 73/300 -- Iteration 556315 - Batch 1771/7702 - Train loss: 0.00188930  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.86 min) Epoch 73/300 -- Iteration 556392 - Batch 1848/7702 - Train loss: 0.00188737  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.88 min) Epoch 73/300 -- Iteration 556469 - Batch 1925/7702 - Train loss: 0.00188687  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.90 min) Epoch 73/300 -- Iteration 556546 - Batch 2002/7702 - Train loss: 0.00188473  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.92 min) Epoch 73/300 -- Iteration 556623 - Batch 2079/7702 - Train loss: 0.00188499  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.94 min) Epoch 73/300 -- Iteration 556700 - Batch 2156/7702 - Train loss: 0.00188422  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.96 min) Epoch 73/300 -- Iteration 556777 - Batch 2233/7702 - Train loss: 0.00188421  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(149.99 min) Epoch 73/300 -- Iteration 556854 - Batch 2310/7702 - Train loss: 0.00188482  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.01 min) Epoch 73/300 -- Iteration 556931 - Batch 2387/7702 - Train loss: 0.00188500  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.03 min) Epoch 73/300 -- Iteration 557008 - Batch 2464/7702 - Train loss: 0.00188506  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.05 min) Epoch 73/300 -- Iteration 557085 - Batch 2541/7702 - Train loss: 0.00188534  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.07 min) Epoch 73/300 -- Iteration 557162 - Batch 2618/7702 - Train loss: 0.00188451  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.09 min) Epoch 73/300 -- Iteration 557239 - Batch 2695/7702 - Train loss: 0.00188484  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.11 min) Epoch 73/300 -- Iteration 557316 - Batch 2772/7702 - Train loss: 0.00188587  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.13 min) Epoch 73/300 -- Iteration 557393 - Batch 2849/7702 - Train loss: 0.00188556  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.15 min) Epoch 73/300 -- Iteration 557470 - Batch 2926/7702 - Train loss: 0.00188572  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.17 min) Epoch 73/300 -- Iteration 557547 - Batch 3003/7702 - Train loss: 0.00188580  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.19 min) Epoch 73/300 -- Iteration 557624 - Batch 3080/7702 - Train loss: 0.00188605  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.21 min) Epoch 73/300 -- Iteration 557701 - Batch 3157/7702 - Train loss: 0.00188532  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.23 min) Epoch 73/300 -- Iteration 557778 - Batch 3234/7702 - Train loss: 0.00188610  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.26 min) Epoch 73/300 -- Iteration 557855 - Batch 3311/7702 - Train loss: 0.00188559  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.28 min) Epoch 73/300 -- Iteration 557932 - Batch 3388/7702 - Train loss: 0.00188518  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.30 min) Epoch 73/300 -- Iteration 558009 - Batch 3465/7702 - Train loss: 0.00188596  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.32 min) Epoch 73/300 -- Iteration 558086 - Batch 3542/7702 - Train loss: 0.00188594  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.34 min) Epoch 73/300 -- Iteration 558163 - Batch 3619/7702 - Train loss: 0.00188597  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.36 min) Epoch 73/300 -- Iteration 558240 - Batch 3696/7702 - Train loss: 0.00188604  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.38 min) Epoch 73/300 -- Iteration 558317 - Batch 3773/7702 - Train loss: 0.00188604  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.40 min) Epoch 73/300 -- Iteration 558394 - Batch 3850/7702 - Train loss: 0.00188621  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.42 min) Epoch 73/300 -- Iteration 558471 - Batch 3927/7702 - Train loss: 0.00188574  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.44 min) Epoch 73/300 -- Iteration 558548 - Batch 4004/7702 - Train loss: 0.00188610  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.46 min) Epoch 73/300 -- Iteration 558625 - Batch 4081/7702 - Train loss: 0.00188586  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.48 min) Epoch 73/300 -- Iteration 558702 - Batch 4158/7702 - Train loss: 0.00188585  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.50 min) Epoch 73/300 -- Iteration 558779 - Batch 4235/7702 - Train loss: 0.00188567  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.53 min) Epoch 73/300 -- Iteration 558856 - Batch 4312/7702 - Train loss: 0.00188581  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.55 min) Epoch 73/300 -- Iteration 558933 - Batch 4389/7702 - Train loss: 0.00188583  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.57 min) Epoch 73/300 -- Iteration 559010 - Batch 4466/7702 - Train loss: 0.00188539  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.59 min) Epoch 73/300 -- Iteration 559087 - Batch 4543/7702 - Train loss: 0.00188499  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.61 min) Epoch 73/300 -- Iteration 559164 - Batch 4620/7702 - Train loss: 0.00188561  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.63 min) Epoch 73/300 -- Iteration 559241 - Batch 4697/7702 - Train loss: 0.00188590  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.65 min) Epoch 73/300 -- Iteration 559318 - Batch 4774/7702 - Train loss: 0.00188613  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.67 min) Epoch 73/300 -- Iteration 559395 - Batch 4851/7702 - Train loss: 0.00188598  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.69 min) Epoch 73/300 -- Iteration 559472 - Batch 4928/7702 - Train loss: 0.00188607  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.71 min) Epoch 73/300 -- Iteration 559549 - Batch 5005/7702 - Train loss: 0.00188614  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.73 min) Epoch 73/300 -- Iteration 559626 - Batch 5082/7702 - Train loss: 0.00188558  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.75 min) Epoch 73/300 -- Iteration 559703 - Batch 5159/7702 - Train loss: 0.00188601  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.77 min) Epoch 73/300 -- Iteration 559780 - Batch 5236/7702 - Train loss: 0.00188594  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.80 min) Epoch 73/300 -- Iteration 559857 - Batch 5313/7702 - Train loss: 0.00188592  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.82 min) Epoch 73/300 -- Iteration 559934 - Batch 5390/7702 - Train loss: 0.00188606  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.84 min) Epoch 73/300 -- Iteration 560011 - Batch 5467/7702 - Train loss: 0.00188586  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.86 min) Epoch 73/300 -- Iteration 560088 - Batch 5544/7702 - Train loss: 0.00188588  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.88 min) Epoch 73/300 -- Iteration 560165 - Batch 5621/7702 - Train loss: 0.00188630  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.90 min) Epoch 73/300 -- Iteration 560242 - Batch 5698/7702 - Train loss: 0.00188635  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.92 min) Epoch 73/300 -- Iteration 560319 - Batch 5775/7702 - Train loss: 0.00188672  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.94 min) Epoch 73/300 -- Iteration 560396 - Batch 5852/7702 - Train loss: 0.00188652  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.96 min) Epoch 73/300 -- Iteration 560473 - Batch 5929/7702 - Train loss: 0.00188644  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(150.98 min) Epoch 73/300 -- Iteration 560550 - Batch 6006/7702 - Train loss: 0.00188617  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.00 min) Epoch 73/300 -- Iteration 560627 - Batch 6083/7702 - Train loss: 0.00188674  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.02 min) Epoch 73/300 -- Iteration 560704 - Batch 6160/7702 - Train loss: 0.00188659  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.04 min) Epoch 73/300 -- Iteration 560781 - Batch 6237/7702 - Train loss: 0.00188644  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.06 min) Epoch 73/300 -- Iteration 560858 - Batch 6314/7702 - Train loss: 0.00188652  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.09 min) Epoch 73/300 -- Iteration 560935 - Batch 6391/7702 - Train loss: 0.00188626  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.11 min) Epoch 73/300 -- Iteration 561012 - Batch 6468/7702 - Train loss: 0.00188593  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.13 min) Epoch 73/300 -- Iteration 561089 - Batch 6545/7702 - Train loss: 0.00188569  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.15 min) Epoch 73/300 -- Iteration 561166 - Batch 6622/7702 - Train loss: 0.00188540  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.17 min) Epoch 73/300 -- Iteration 561243 - Batch 6699/7702 - Train loss: 0.00188557  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.19 min) Epoch 73/300 -- Iteration 561320 - Batch 6776/7702 - Train loss: 0.00188555  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.21 min) Epoch 73/300 -- Iteration 561397 - Batch 6853/7702 - Train loss: 0.00188526  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.23 min) Epoch 73/300 -- Iteration 561474 - Batch 6930/7702 - Train loss: 0.00188490  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.25 min) Epoch 73/300 -- Iteration 561551 - Batch 7007/7702 - Train loss: 0.00188486  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.27 min) Epoch 73/300 -- Iteration 561628 - Batch 7084/7702 - Train loss: 0.00188473  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.29 min) Epoch 73/300 -- Iteration 561705 - Batch 7161/7702 - Train loss: 0.00188507  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.31 min) Epoch 73/300 -- Iteration 561782 - Batch 7238/7702 - Train loss: 0.00188475  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.34 min) Epoch 73/300 -- Iteration 561859 - Batch 7315/7702 - Train loss: 0.00188479  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.36 min) Epoch 73/300 -- Iteration 561936 - Batch 7392/7702 - Train loss: 0.00188497  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.38 min) Epoch 73/300 -- Iteration 562013 - Batch 7469/7702 - Train loss: 0.00188542  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.40 min) Epoch 73/300 -- Iteration 562090 - Batch 7546/7702 - Train loss: 0.00188501  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.42 min) Epoch 73/300 -- Iteration 562167 - Batch 7623/7702 - Train loss: 0.00188489  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.44 min) Epoch 73/300 -- Iteration 562244 - Batch 7700/7702 - Train loss: 0.00188475  - Train acc: -0.0000 - Val loss: 0.00041600\n",
      "(151.44 min) Epoch 73/300 -- Iteration 562246 - Batch 7701/7702 - Train loss: 0.00188475  - Train acc: -0.0000 - Val loss: 0.00040938 - Val acc: -0.0000\n",
      "(151.46 min) Epoch 74/300 -- Iteration 562323 - Batch 77/7702 - Train loss: 0.00186837  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.48 min) Epoch 74/300 -- Iteration 562400 - Batch 154/7702 - Train loss: 0.00186069  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.50 min) Epoch 74/300 -- Iteration 562477 - Batch 231/7702 - Train loss: 0.00187475  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.52 min) Epoch 74/300 -- Iteration 562554 - Batch 308/7702 - Train loss: 0.00188621  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.55 min) Epoch 74/300 -- Iteration 562631 - Batch 385/7702 - Train loss: 0.00188494  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.57 min) Epoch 74/300 -- Iteration 562708 - Batch 462/7702 - Train loss: 0.00188320  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.59 min) Epoch 74/300 -- Iteration 562785 - Batch 539/7702 - Train loss: 0.00187979  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.61 min) Epoch 74/300 -- Iteration 562862 - Batch 616/7702 - Train loss: 0.00188846  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.63 min) Epoch 74/300 -- Iteration 562939 - Batch 693/7702 - Train loss: 0.00188937  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.65 min) Epoch 74/300 -- Iteration 563016 - Batch 770/7702 - Train loss: 0.00188851  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.67 min) Epoch 74/300 -- Iteration 563093 - Batch 847/7702 - Train loss: 0.00188973  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.69 min) Epoch 74/300 -- Iteration 563170 - Batch 924/7702 - Train loss: 0.00188961  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.71 min) Epoch 74/300 -- Iteration 563247 - Batch 1001/7702 - Train loss: 0.00189055  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.73 min) Epoch 74/300 -- Iteration 563324 - Batch 1078/7702 - Train loss: 0.00189186  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.75 min) Epoch 74/300 -- Iteration 563401 - Batch 1155/7702 - Train loss: 0.00189111  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.77 min) Epoch 74/300 -- Iteration 563478 - Batch 1232/7702 - Train loss: 0.00189006  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.79 min) Epoch 74/300 -- Iteration 563555 - Batch 1309/7702 - Train loss: 0.00189142  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.82 min) Epoch 74/300 -- Iteration 563632 - Batch 1386/7702 - Train loss: 0.00189129  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.84 min) Epoch 74/300 -- Iteration 563709 - Batch 1463/7702 - Train loss: 0.00189221  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.86 min) Epoch 74/300 -- Iteration 563786 - Batch 1540/7702 - Train loss: 0.00189235  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.88 min) Epoch 74/300 -- Iteration 563863 - Batch 1617/7702 - Train loss: 0.00189181  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.90 min) Epoch 74/300 -- Iteration 563940 - Batch 1694/7702 - Train loss: 0.00189145  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.92 min) Epoch 74/300 -- Iteration 564017 - Batch 1771/7702 - Train loss: 0.00189035  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.94 min) Epoch 74/300 -- Iteration 564094 - Batch 1848/7702 - Train loss: 0.00189018  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.96 min) Epoch 74/300 -- Iteration 564171 - Batch 1925/7702 - Train loss: 0.00189054  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(151.98 min) Epoch 74/300 -- Iteration 564248 - Batch 2002/7702 - Train loss: 0.00188967  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.00 min) Epoch 74/300 -- Iteration 564325 - Batch 2079/7702 - Train loss: 0.00188978  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.02 min) Epoch 74/300 -- Iteration 564402 - Batch 2156/7702 - Train loss: 0.00188994  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.04 min) Epoch 74/300 -- Iteration 564479 - Batch 2233/7702 - Train loss: 0.00189103  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.06 min) Epoch 74/300 -- Iteration 564556 - Batch 2310/7702 - Train loss: 0.00189167  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.09 min) Epoch 74/300 -- Iteration 564633 - Batch 2387/7702 - Train loss: 0.00189007  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.11 min) Epoch 74/300 -- Iteration 564710 - Batch 2464/7702 - Train loss: 0.00188973  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.13 min) Epoch 74/300 -- Iteration 564787 - Batch 2541/7702 - Train loss: 0.00188966  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.15 min) Epoch 74/300 -- Iteration 564864 - Batch 2618/7702 - Train loss: 0.00188935  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.17 min) Epoch 74/300 -- Iteration 564941 - Batch 2695/7702 - Train loss: 0.00188934  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.19 min) Epoch 74/300 -- Iteration 565018 - Batch 2772/7702 - Train loss: 0.00188984  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.21 min) Epoch 74/300 -- Iteration 565095 - Batch 2849/7702 - Train loss: 0.00188959  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.23 min) Epoch 74/300 -- Iteration 565172 - Batch 2926/7702 - Train loss: 0.00188939  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.25 min) Epoch 74/300 -- Iteration 565249 - Batch 3003/7702 - Train loss: 0.00188833  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.27 min) Epoch 74/300 -- Iteration 565326 - Batch 3080/7702 - Train loss: 0.00188720  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.29 min) Epoch 74/300 -- Iteration 565403 - Batch 3157/7702 - Train loss: 0.00188644  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.31 min) Epoch 74/300 -- Iteration 565480 - Batch 3234/7702 - Train loss: 0.00188575  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.33 min) Epoch 74/300 -- Iteration 565557 - Batch 3311/7702 - Train loss: 0.00188555  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.35 min) Epoch 74/300 -- Iteration 565634 - Batch 3388/7702 - Train loss: 0.00188427  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.38 min) Epoch 74/300 -- Iteration 565711 - Batch 3465/7702 - Train loss: 0.00188413  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.40 min) Epoch 74/300 -- Iteration 565788 - Batch 3542/7702 - Train loss: 0.00188405  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.42 min) Epoch 74/300 -- Iteration 565865 - Batch 3619/7702 - Train loss: 0.00188458  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.44 min) Epoch 74/300 -- Iteration 565942 - Batch 3696/7702 - Train loss: 0.00188423  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.46 min) Epoch 74/300 -- Iteration 566019 - Batch 3773/7702 - Train loss: 0.00188404  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.48 min) Epoch 74/300 -- Iteration 566096 - Batch 3850/7702 - Train loss: 0.00188404  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.50 min) Epoch 74/300 -- Iteration 566173 - Batch 3927/7702 - Train loss: 0.00188386  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.52 min) Epoch 74/300 -- Iteration 566250 - Batch 4004/7702 - Train loss: 0.00188459  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.54 min) Epoch 74/300 -- Iteration 566327 - Batch 4081/7702 - Train loss: 0.00188494  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.56 min) Epoch 74/300 -- Iteration 566404 - Batch 4158/7702 - Train loss: 0.00188554  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.58 min) Epoch 74/300 -- Iteration 566481 - Batch 4235/7702 - Train loss: 0.00188547  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.60 min) Epoch 74/300 -- Iteration 566558 - Batch 4312/7702 - Train loss: 0.00188505  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.62 min) Epoch 74/300 -- Iteration 566635 - Batch 4389/7702 - Train loss: 0.00188495  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.64 min) Epoch 74/300 -- Iteration 566712 - Batch 4466/7702 - Train loss: 0.00188507  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.67 min) Epoch 74/300 -- Iteration 566789 - Batch 4543/7702 - Train loss: 0.00188566  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.69 min) Epoch 74/300 -- Iteration 566866 - Batch 4620/7702 - Train loss: 0.00188577  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.71 min) Epoch 74/300 -- Iteration 566943 - Batch 4697/7702 - Train loss: 0.00188566  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.73 min) Epoch 74/300 -- Iteration 567020 - Batch 4774/7702 - Train loss: 0.00188554  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.75 min) Epoch 74/300 -- Iteration 567097 - Batch 4851/7702 - Train loss: 0.00188540  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.77 min) Epoch 74/300 -- Iteration 567174 - Batch 4928/7702 - Train loss: 0.00188585  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.79 min) Epoch 74/300 -- Iteration 567251 - Batch 5005/7702 - Train loss: 0.00188615  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.81 min) Epoch 74/300 -- Iteration 567328 - Batch 5082/7702 - Train loss: 0.00188632  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.83 min) Epoch 74/300 -- Iteration 567405 - Batch 5159/7702 - Train loss: 0.00188601  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.85 min) Epoch 74/300 -- Iteration 567482 - Batch 5236/7702 - Train loss: 0.00188590  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.87 min) Epoch 74/300 -- Iteration 567559 - Batch 5313/7702 - Train loss: 0.00188538  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.89 min) Epoch 74/300 -- Iteration 567636 - Batch 5390/7702 - Train loss: 0.00188513  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.91 min) Epoch 74/300 -- Iteration 567713 - Batch 5467/7702 - Train loss: 0.00188469  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.94 min) Epoch 74/300 -- Iteration 567790 - Batch 5544/7702 - Train loss: 0.00188466  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.96 min) Epoch 74/300 -- Iteration 567867 - Batch 5621/7702 - Train loss: 0.00188504  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(152.98 min) Epoch 74/300 -- Iteration 567944 - Batch 5698/7702 - Train loss: 0.00188461  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.00 min) Epoch 74/300 -- Iteration 568021 - Batch 5775/7702 - Train loss: 0.00188468  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.02 min) Epoch 74/300 -- Iteration 568098 - Batch 5852/7702 - Train loss: 0.00188523  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.04 min) Epoch 74/300 -- Iteration 568175 - Batch 5929/7702 - Train loss: 0.00188481  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.06 min) Epoch 74/300 -- Iteration 568252 - Batch 6006/7702 - Train loss: 0.00188460  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.08 min) Epoch 74/300 -- Iteration 568329 - Batch 6083/7702 - Train loss: 0.00188455  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.10 min) Epoch 74/300 -- Iteration 568406 - Batch 6160/7702 - Train loss: 0.00188450  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.12 min) Epoch 74/300 -- Iteration 568483 - Batch 6237/7702 - Train loss: 0.00188422  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.14 min) Epoch 74/300 -- Iteration 568560 - Batch 6314/7702 - Train loss: 0.00188400  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.16 min) Epoch 74/300 -- Iteration 568637 - Batch 6391/7702 - Train loss: 0.00188392  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.18 min) Epoch 74/300 -- Iteration 568714 - Batch 6468/7702 - Train loss: 0.00188366  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.20 min) Epoch 74/300 -- Iteration 568791 - Batch 6545/7702 - Train loss: 0.00188346  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.22 min) Epoch 74/300 -- Iteration 568868 - Batch 6622/7702 - Train loss: 0.00188338  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.24 min) Epoch 74/300 -- Iteration 568945 - Batch 6699/7702 - Train loss: 0.00188387  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.27 min) Epoch 74/300 -- Iteration 569022 - Batch 6776/7702 - Train loss: 0.00188379  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.29 min) Epoch 74/300 -- Iteration 569099 - Batch 6853/7702 - Train loss: 0.00188384  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.31 min) Epoch 74/300 -- Iteration 569176 - Batch 6930/7702 - Train loss: 0.00188374  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.33 min) Epoch 74/300 -- Iteration 569253 - Batch 7007/7702 - Train loss: 0.00188400  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.35 min) Epoch 74/300 -- Iteration 569330 - Batch 7084/7702 - Train loss: 0.00188404  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.37 min) Epoch 74/300 -- Iteration 569407 - Batch 7161/7702 - Train loss: 0.00188426  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.39 min) Epoch 74/300 -- Iteration 569484 - Batch 7238/7702 - Train loss: 0.00188405  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.41 min) Epoch 74/300 -- Iteration 569561 - Batch 7315/7702 - Train loss: 0.00188364  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.43 min) Epoch 74/300 -- Iteration 569638 - Batch 7392/7702 - Train loss: 0.00188366  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.45 min) Epoch 74/300 -- Iteration 569715 - Batch 7469/7702 - Train loss: 0.00188348  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.47 min) Epoch 74/300 -- Iteration 569792 - Batch 7546/7702 - Train loss: 0.00188353  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.49 min) Epoch 74/300 -- Iteration 569869 - Batch 7623/7702 - Train loss: 0.00188381  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.51 min) Epoch 74/300 -- Iteration 569946 - Batch 7700/7702 - Train loss: 0.00188407  - Train acc: -0.0000 - Val loss: 0.00040938\n",
      "(153.51 min) Epoch 74/300 -- Iteration 569948 - Batch 7701/7702 - Train loss: 0.00188404  - Train acc: -0.0000 - Val loss: 0.00047533 - Val acc: -0.0000\n",
      "(153.54 min) Epoch 75/300 -- Iteration 570025 - Batch 77/7702 - Train loss: 0.00189262  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.56 min) Epoch 75/300 -- Iteration 570102 - Batch 154/7702 - Train loss: 0.00187441  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.58 min) Epoch 75/300 -- Iteration 570179 - Batch 231/7702 - Train loss: 0.00187107  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.60 min) Epoch 75/300 -- Iteration 570256 - Batch 308/7702 - Train loss: 0.00186680  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.62 min) Epoch 75/300 -- Iteration 570333 - Batch 385/7702 - Train loss: 0.00187303  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.64 min) Epoch 75/300 -- Iteration 570410 - Batch 462/7702 - Train loss: 0.00187585  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.66 min) Epoch 75/300 -- Iteration 570487 - Batch 539/7702 - Train loss: 0.00187025  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.68 min) Epoch 75/300 -- Iteration 570564 - Batch 616/7702 - Train loss: 0.00187038  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.70 min) Epoch 75/300 -- Iteration 570641 - Batch 693/7702 - Train loss: 0.00187176  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.72 min) Epoch 75/300 -- Iteration 570718 - Batch 770/7702 - Train loss: 0.00187080  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.74 min) Epoch 75/300 -- Iteration 570795 - Batch 847/7702 - Train loss: 0.00187131  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.77 min) Epoch 75/300 -- Iteration 570872 - Batch 924/7702 - Train loss: 0.00187136  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.79 min) Epoch 75/300 -- Iteration 570949 - Batch 1001/7702 - Train loss: 0.00187108  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.81 min) Epoch 75/300 -- Iteration 571026 - Batch 1078/7702 - Train loss: 0.00187632  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.83 min) Epoch 75/300 -- Iteration 571103 - Batch 1155/7702 - Train loss: 0.00187737  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.85 min) Epoch 75/300 -- Iteration 571180 - Batch 1232/7702 - Train loss: 0.00187880  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.87 min) Epoch 75/300 -- Iteration 571257 - Batch 1309/7702 - Train loss: 0.00187781  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.89 min) Epoch 75/300 -- Iteration 571334 - Batch 1386/7702 - Train loss: 0.00187892  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.91 min) Epoch 75/300 -- Iteration 571411 - Batch 1463/7702 - Train loss: 0.00187900  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.93 min) Epoch 75/300 -- Iteration 571488 - Batch 1540/7702 - Train loss: 0.00188005  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.95 min) Epoch 75/300 -- Iteration 571565 - Batch 1617/7702 - Train loss: 0.00188015  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.97 min) Epoch 75/300 -- Iteration 571642 - Batch 1694/7702 - Train loss: 0.00187918  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(153.99 min) Epoch 75/300 -- Iteration 571719 - Batch 1771/7702 - Train loss: 0.00187983  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.01 min) Epoch 75/300 -- Iteration 571796 - Batch 1848/7702 - Train loss: 0.00187952  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.04 min) Epoch 75/300 -- Iteration 571873 - Batch 1925/7702 - Train loss: 0.00188038  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.06 min) Epoch 75/300 -- Iteration 571950 - Batch 2002/7702 - Train loss: 0.00188056  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.08 min) Epoch 75/300 -- Iteration 572027 - Batch 2079/7702 - Train loss: 0.00188102  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.10 min) Epoch 75/300 -- Iteration 572104 - Batch 2156/7702 - Train loss: 0.00188161  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.12 min) Epoch 75/300 -- Iteration 572181 - Batch 2233/7702 - Train loss: 0.00188195  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.14 min) Epoch 75/300 -- Iteration 572258 - Batch 2310/7702 - Train loss: 0.00188274  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.16 min) Epoch 75/300 -- Iteration 572335 - Batch 2387/7702 - Train loss: 0.00188257  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.18 min) Epoch 75/300 -- Iteration 572412 - Batch 2464/7702 - Train loss: 0.00188245  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.20 min) Epoch 75/300 -- Iteration 572489 - Batch 2541/7702 - Train loss: 0.00188293  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.22 min) Epoch 75/300 -- Iteration 572566 - Batch 2618/7702 - Train loss: 0.00188280  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.24 min) Epoch 75/300 -- Iteration 572643 - Batch 2695/7702 - Train loss: 0.00188252  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.26 min) Epoch 75/300 -- Iteration 572720 - Batch 2772/7702 - Train loss: 0.00188319  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.28 min) Epoch 75/300 -- Iteration 572797 - Batch 2849/7702 - Train loss: 0.00188285  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.30 min) Epoch 75/300 -- Iteration 572874 - Batch 2926/7702 - Train loss: 0.00188359  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.33 min) Epoch 75/300 -- Iteration 572951 - Batch 3003/7702 - Train loss: 0.00188337  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.35 min) Epoch 75/300 -- Iteration 573028 - Batch 3080/7702 - Train loss: 0.00188319  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.37 min) Epoch 75/300 -- Iteration 573105 - Batch 3157/7702 - Train loss: 0.00188288  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.39 min) Epoch 75/300 -- Iteration 573182 - Batch 3234/7702 - Train loss: 0.00188286  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.41 min) Epoch 75/300 -- Iteration 573259 - Batch 3311/7702 - Train loss: 0.00188339  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.43 min) Epoch 75/300 -- Iteration 573336 - Batch 3388/7702 - Train loss: 0.00188303  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.45 min) Epoch 75/300 -- Iteration 573413 - Batch 3465/7702 - Train loss: 0.00188442  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.47 min) Epoch 75/300 -- Iteration 573490 - Batch 3542/7702 - Train loss: 0.00188432  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.49 min) Epoch 75/300 -- Iteration 573567 - Batch 3619/7702 - Train loss: 0.00188380  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.51 min) Epoch 75/300 -- Iteration 573644 - Batch 3696/7702 - Train loss: 0.00188503  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.53 min) Epoch 75/300 -- Iteration 573721 - Batch 3773/7702 - Train loss: 0.00188474  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.55 min) Epoch 75/300 -- Iteration 573798 - Batch 3850/7702 - Train loss: 0.00188423  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.57 min) Epoch 75/300 -- Iteration 573875 - Batch 3927/7702 - Train loss: 0.00188414  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.59 min) Epoch 75/300 -- Iteration 573952 - Batch 4004/7702 - Train loss: 0.00188474  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.61 min) Epoch 75/300 -- Iteration 574029 - Batch 4081/7702 - Train loss: 0.00188442  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.64 min) Epoch 75/300 -- Iteration 574106 - Batch 4158/7702 - Train loss: 0.00188512  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.66 min) Epoch 75/300 -- Iteration 574183 - Batch 4235/7702 - Train loss: 0.00188505  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.68 min) Epoch 75/300 -- Iteration 574260 - Batch 4312/7702 - Train loss: 0.00188518  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.70 min) Epoch 75/300 -- Iteration 574337 - Batch 4389/7702 - Train loss: 0.00188518  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.72 min) Epoch 75/300 -- Iteration 574414 - Batch 4466/7702 - Train loss: 0.00188512  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.74 min) Epoch 75/300 -- Iteration 574491 - Batch 4543/7702 - Train loss: 0.00188495  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.76 min) Epoch 75/300 -- Iteration 574568 - Batch 4620/7702 - Train loss: 0.00188502  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.78 min) Epoch 75/300 -- Iteration 574645 - Batch 4697/7702 - Train loss: 0.00188506  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.80 min) Epoch 75/300 -- Iteration 574722 - Batch 4774/7702 - Train loss: 0.00188479  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.82 min) Epoch 75/300 -- Iteration 574799 - Batch 4851/7702 - Train loss: 0.00188451  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.84 min) Epoch 75/300 -- Iteration 574876 - Batch 4928/7702 - Train loss: 0.00188433  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.87 min) Epoch 75/300 -- Iteration 574953 - Batch 5005/7702 - Train loss: 0.00188463  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.89 min) Epoch 75/300 -- Iteration 575030 - Batch 5082/7702 - Train loss: 0.00188496  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.91 min) Epoch 75/300 -- Iteration 575107 - Batch 5159/7702 - Train loss: 0.00188454  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.93 min) Epoch 75/300 -- Iteration 575184 - Batch 5236/7702 - Train loss: 0.00188468  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.95 min) Epoch 75/300 -- Iteration 575261 - Batch 5313/7702 - Train loss: 0.00188467  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.97 min) Epoch 75/300 -- Iteration 575338 - Batch 5390/7702 - Train loss: 0.00188563  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(154.99 min) Epoch 75/300 -- Iteration 575415 - Batch 5467/7702 - Train loss: 0.00188555  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.01 min) Epoch 75/300 -- Iteration 575492 - Batch 5544/7702 - Train loss: 0.00188533  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.03 min) Epoch 75/300 -- Iteration 575569 - Batch 5621/7702 - Train loss: 0.00188552  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.05 min) Epoch 75/300 -- Iteration 575646 - Batch 5698/7702 - Train loss: 0.00188575  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.07 min) Epoch 75/300 -- Iteration 575723 - Batch 5775/7702 - Train loss: 0.00188574  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.09 min) Epoch 75/300 -- Iteration 575800 - Batch 5852/7702 - Train loss: 0.00188521  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.11 min) Epoch 75/300 -- Iteration 575877 - Batch 5929/7702 - Train loss: 0.00188511  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.13 min) Epoch 75/300 -- Iteration 575954 - Batch 6006/7702 - Train loss: 0.00188488  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.16 min) Epoch 75/300 -- Iteration 576031 - Batch 6083/7702 - Train loss: 0.00188558  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.18 min) Epoch 75/300 -- Iteration 576108 - Batch 6160/7702 - Train loss: 0.00188570  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.20 min) Epoch 75/300 -- Iteration 576185 - Batch 6237/7702 - Train loss: 0.00188529  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.22 min) Epoch 75/300 -- Iteration 576262 - Batch 6314/7702 - Train loss: 0.00188517  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.24 min) Epoch 75/300 -- Iteration 576339 - Batch 6391/7702 - Train loss: 0.00188556  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.26 min) Epoch 75/300 -- Iteration 576416 - Batch 6468/7702 - Train loss: 0.00188507  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.28 min) Epoch 75/300 -- Iteration 576493 - Batch 6545/7702 - Train loss: 0.00188539  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.30 min) Epoch 75/300 -- Iteration 576570 - Batch 6622/7702 - Train loss: 0.00188564  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.32 min) Epoch 75/300 -- Iteration 576647 - Batch 6699/7702 - Train loss: 0.00188590  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.34 min) Epoch 75/300 -- Iteration 576724 - Batch 6776/7702 - Train loss: 0.00188586  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.36 min) Epoch 75/300 -- Iteration 576801 - Batch 6853/7702 - Train loss: 0.00188547  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.38 min) Epoch 75/300 -- Iteration 576878 - Batch 6930/7702 - Train loss: 0.00188531  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.40 min) Epoch 75/300 -- Iteration 576955 - Batch 7007/7702 - Train loss: 0.00188508  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.43 min) Epoch 75/300 -- Iteration 577032 - Batch 7084/7702 - Train loss: 0.00188493  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.45 min) Epoch 75/300 -- Iteration 577109 - Batch 7161/7702 - Train loss: 0.00188473  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.47 min) Epoch 75/300 -- Iteration 577186 - Batch 7238/7702 - Train loss: 0.00188481  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.49 min) Epoch 75/300 -- Iteration 577263 - Batch 7315/7702 - Train loss: 0.00188486  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.51 min) Epoch 75/300 -- Iteration 577340 - Batch 7392/7702 - Train loss: 0.00188483  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.53 min) Epoch 75/300 -- Iteration 577417 - Batch 7469/7702 - Train loss: 0.00188494  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.55 min) Epoch 75/300 -- Iteration 577494 - Batch 7546/7702 - Train loss: 0.00188501  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.57 min) Epoch 75/300 -- Iteration 577571 - Batch 7623/7702 - Train loss: 0.00188507  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.59 min) Epoch 75/300 -- Iteration 577648 - Batch 7700/7702 - Train loss: 0.00188497  - Train acc: -0.0000 - Val loss: 0.00047533\n",
      "(155.59 min) Epoch 75/300 -- Iteration 577650 - Batch 7701/7702 - Train loss: 0.00188496  - Train acc: -0.0000 - Val loss: 0.00042338 - Val acc: -0.0000\n",
      "(155.61 min) Epoch 76/300 -- Iteration 577727 - Batch 77/7702 - Train loss: 0.00186130  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.64 min) Epoch 76/300 -- Iteration 577804 - Batch 154/7702 - Train loss: 0.00189080  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.66 min) Epoch 76/300 -- Iteration 577881 - Batch 231/7702 - Train loss: 0.00188642  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.68 min) Epoch 76/300 -- Iteration 577958 - Batch 308/7702 - Train loss: 0.00187687  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.70 min) Epoch 76/300 -- Iteration 578035 - Batch 385/7702 - Train loss: 0.00187731  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.72 min) Epoch 76/300 -- Iteration 578112 - Batch 462/7702 - Train loss: 0.00187545  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.74 min) Epoch 76/300 -- Iteration 578189 - Batch 539/7702 - Train loss: 0.00187350  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.76 min) Epoch 76/300 -- Iteration 578266 - Batch 616/7702 - Train loss: 0.00187058  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.78 min) Epoch 76/300 -- Iteration 578343 - Batch 693/7702 - Train loss: 0.00187109  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.80 min) Epoch 76/300 -- Iteration 578420 - Batch 770/7702 - Train loss: 0.00187114  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.82 min) Epoch 76/300 -- Iteration 578497 - Batch 847/7702 - Train loss: 0.00187181  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.84 min) Epoch 76/300 -- Iteration 578574 - Batch 924/7702 - Train loss: 0.00187119  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.86 min) Epoch 76/300 -- Iteration 578651 - Batch 1001/7702 - Train loss: 0.00187004  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.88 min) Epoch 76/300 -- Iteration 578728 - Batch 1078/7702 - Train loss: 0.00187172  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.91 min) Epoch 76/300 -- Iteration 578805 - Batch 1155/7702 - Train loss: 0.00187258  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.93 min) Epoch 76/300 -- Iteration 578882 - Batch 1232/7702 - Train loss: 0.00187152  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.95 min) Epoch 76/300 -- Iteration 578959 - Batch 1309/7702 - Train loss: 0.00187135  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.97 min) Epoch 76/300 -- Iteration 579036 - Batch 1386/7702 - Train loss: 0.00186938  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(155.99 min) Epoch 76/300 -- Iteration 579113 - Batch 1463/7702 - Train loss: 0.00187054  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.01 min) Epoch 76/300 -- Iteration 579190 - Batch 1540/7702 - Train loss: 0.00186943  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.03 min) Epoch 76/300 -- Iteration 579267 - Batch 1617/7702 - Train loss: 0.00186866  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.05 min) Epoch 76/300 -- Iteration 579344 - Batch 1694/7702 - Train loss: 0.00186945  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.07 min) Epoch 76/300 -- Iteration 579421 - Batch 1771/7702 - Train loss: 0.00187021  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.09 min) Epoch 76/300 -- Iteration 579498 - Batch 1848/7702 - Train loss: 0.00187108  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.11 min) Epoch 76/300 -- Iteration 579575 - Batch 1925/7702 - Train loss: 0.00187101  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.13 min) Epoch 76/300 -- Iteration 579652 - Batch 2002/7702 - Train loss: 0.00187139  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.15 min) Epoch 76/300 -- Iteration 579729 - Batch 2079/7702 - Train loss: 0.00187160  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.17 min) Epoch 76/300 -- Iteration 579806 - Batch 2156/7702 - Train loss: 0.00187122  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.20 min) Epoch 76/300 -- Iteration 579883 - Batch 2233/7702 - Train loss: 0.00187188  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.22 min) Epoch 76/300 -- Iteration 579960 - Batch 2310/7702 - Train loss: 0.00187237  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.24 min) Epoch 76/300 -- Iteration 580037 - Batch 2387/7702 - Train loss: 0.00187160  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.26 min) Epoch 76/300 -- Iteration 580114 - Batch 2464/7702 - Train loss: 0.00187113  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.28 min) Epoch 76/300 -- Iteration 580191 - Batch 2541/7702 - Train loss: 0.00187071  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.30 min) Epoch 76/300 -- Iteration 580268 - Batch 2618/7702 - Train loss: 0.00187180  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.32 min) Epoch 76/300 -- Iteration 580345 - Batch 2695/7702 - Train loss: 0.00187223  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.34 min) Epoch 76/300 -- Iteration 580422 - Batch 2772/7702 - Train loss: 0.00187167  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.36 min) Epoch 76/300 -- Iteration 580499 - Batch 2849/7702 - Train loss: 0.00187281  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.38 min) Epoch 76/300 -- Iteration 580576 - Batch 2926/7702 - Train loss: 0.00187308  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.40 min) Epoch 76/300 -- Iteration 580653 - Batch 3003/7702 - Train loss: 0.00187352  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.42 min) Epoch 76/300 -- Iteration 580730 - Batch 3080/7702 - Train loss: 0.00187386  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.44 min) Epoch 76/300 -- Iteration 580807 - Batch 3157/7702 - Train loss: 0.00187417  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.46 min) Epoch 76/300 -- Iteration 580884 - Batch 3234/7702 - Train loss: 0.00187383  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.49 min) Epoch 76/300 -- Iteration 580961 - Batch 3311/7702 - Train loss: 0.00187371  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.51 min) Epoch 76/300 -- Iteration 581038 - Batch 3388/7702 - Train loss: 0.00187464  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.53 min) Epoch 76/300 -- Iteration 581115 - Batch 3465/7702 - Train loss: 0.00187561  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.55 min) Epoch 76/300 -- Iteration 581192 - Batch 3542/7702 - Train loss: 0.00187595  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.57 min) Epoch 76/300 -- Iteration 581269 - Batch 3619/7702 - Train loss: 0.00187601  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.59 min) Epoch 76/300 -- Iteration 581346 - Batch 3696/7702 - Train loss: 0.00187647  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.61 min) Epoch 76/300 -- Iteration 581423 - Batch 3773/7702 - Train loss: 0.00187640  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.63 min) Epoch 76/300 -- Iteration 581500 - Batch 3850/7702 - Train loss: 0.00187632  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.65 min) Epoch 76/300 -- Iteration 581577 - Batch 3927/7702 - Train loss: 0.00187622  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.67 min) Epoch 76/300 -- Iteration 581654 - Batch 4004/7702 - Train loss: 0.00187633  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.69 min) Epoch 76/300 -- Iteration 581731 - Batch 4081/7702 - Train loss: 0.00187677  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.71 min) Epoch 76/300 -- Iteration 581808 - Batch 4158/7702 - Train loss: 0.00187665  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.73 min) Epoch 76/300 -- Iteration 581885 - Batch 4235/7702 - Train loss: 0.00187789  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.76 min) Epoch 76/300 -- Iteration 581962 - Batch 4312/7702 - Train loss: 0.00187835  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.78 min) Epoch 76/300 -- Iteration 582039 - Batch 4389/7702 - Train loss: 0.00187792  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.80 min) Epoch 76/300 -- Iteration 582116 - Batch 4466/7702 - Train loss: 0.00187808  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.82 min) Epoch 76/300 -- Iteration 582193 - Batch 4543/7702 - Train loss: 0.00187816  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.84 min) Epoch 76/300 -- Iteration 582270 - Batch 4620/7702 - Train loss: 0.00187844  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.86 min) Epoch 76/300 -- Iteration 582347 - Batch 4697/7702 - Train loss: 0.00187791  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.88 min) Epoch 76/300 -- Iteration 582424 - Batch 4774/7702 - Train loss: 0.00187764  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.90 min) Epoch 76/300 -- Iteration 582501 - Batch 4851/7702 - Train loss: 0.00187811  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.92 min) Epoch 76/300 -- Iteration 582578 - Batch 4928/7702 - Train loss: 0.00187765  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.94 min) Epoch 76/300 -- Iteration 582655 - Batch 5005/7702 - Train loss: 0.00187720  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.96 min) Epoch 76/300 -- Iteration 582732 - Batch 5082/7702 - Train loss: 0.00187741  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(156.98 min) Epoch 76/300 -- Iteration 582809 - Batch 5159/7702 - Train loss: 0.00187735  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.00 min) Epoch 76/300 -- Iteration 582886 - Batch 5236/7702 - Train loss: 0.00187728  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.03 min) Epoch 76/300 -- Iteration 582963 - Batch 5313/7702 - Train loss: 0.00187713  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.05 min) Epoch 76/300 -- Iteration 583040 - Batch 5390/7702 - Train loss: 0.00187716  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.07 min) Epoch 76/300 -- Iteration 583117 - Batch 5467/7702 - Train loss: 0.00187762  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.09 min) Epoch 76/300 -- Iteration 583194 - Batch 5544/7702 - Train loss: 0.00187823  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.11 min) Epoch 76/300 -- Iteration 583271 - Batch 5621/7702 - Train loss: 0.00187788  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.13 min) Epoch 76/300 -- Iteration 583348 - Batch 5698/7702 - Train loss: 0.00187775  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.15 min) Epoch 76/300 -- Iteration 583425 - Batch 5775/7702 - Train loss: 0.00187755  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.17 min) Epoch 76/300 -- Iteration 583502 - Batch 5852/7702 - Train loss: 0.00187756  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.19 min) Epoch 76/300 -- Iteration 583579 - Batch 5929/7702 - Train loss: 0.00187772  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.21 min) Epoch 76/300 -- Iteration 583656 - Batch 6006/7702 - Train loss: 0.00187757  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.23 min) Epoch 76/300 -- Iteration 583733 - Batch 6083/7702 - Train loss: 0.00187779  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.25 min) Epoch 76/300 -- Iteration 583810 - Batch 6160/7702 - Train loss: 0.00187888  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.27 min) Epoch 76/300 -- Iteration 583887 - Batch 6237/7702 - Train loss: 0.00187960  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.29 min) Epoch 76/300 -- Iteration 583964 - Batch 6314/7702 - Train loss: 0.00187953  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.32 min) Epoch 76/300 -- Iteration 584041 - Batch 6391/7702 - Train loss: 0.00187994  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.34 min) Epoch 76/300 -- Iteration 584118 - Batch 6468/7702 - Train loss: 0.00188022  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.36 min) Epoch 76/300 -- Iteration 584195 - Batch 6545/7702 - Train loss: 0.00188020  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.38 min) Epoch 76/300 -- Iteration 584272 - Batch 6622/7702 - Train loss: 0.00188022  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.40 min) Epoch 76/300 -- Iteration 584349 - Batch 6699/7702 - Train loss: 0.00188007  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.42 min) Epoch 76/300 -- Iteration 584426 - Batch 6776/7702 - Train loss: 0.00188010  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.44 min) Epoch 76/300 -- Iteration 584503 - Batch 6853/7702 - Train loss: 0.00188004  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.46 min) Epoch 76/300 -- Iteration 584580 - Batch 6930/7702 - Train loss: 0.00187993  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.48 min) Epoch 76/300 -- Iteration 584657 - Batch 7007/7702 - Train loss: 0.00188030  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.50 min) Epoch 76/300 -- Iteration 584734 - Batch 7084/7702 - Train loss: 0.00188029  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.52 min) Epoch 76/300 -- Iteration 584811 - Batch 7161/7702 - Train loss: 0.00188021  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.54 min) Epoch 76/300 -- Iteration 584888 - Batch 7238/7702 - Train loss: 0.00188008  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.57 min) Epoch 76/300 -- Iteration 584965 - Batch 7315/7702 - Train loss: 0.00188027  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.59 min) Epoch 76/300 -- Iteration 585042 - Batch 7392/7702 - Train loss: 0.00188030  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.61 min) Epoch 76/300 -- Iteration 585119 - Batch 7469/7702 - Train loss: 0.00188049  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.63 min) Epoch 76/300 -- Iteration 585196 - Batch 7546/7702 - Train loss: 0.00188041  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.65 min) Epoch 76/300 -- Iteration 585273 - Batch 7623/7702 - Train loss: 0.00188037  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.67 min) Epoch 76/300 -- Iteration 585350 - Batch 7700/7702 - Train loss: 0.00188045  - Train acc: -0.0000 - Val loss: 0.00042338\n",
      "(157.67 min) Epoch 76/300 -- Iteration 585352 - Batch 7701/7702 - Train loss: 0.00188051  - Train acc: -0.0000 - Val loss: 0.00037853 - Val acc: -0.0000\n",
      "(157.69 min) Epoch 77/300 -- Iteration 585429 - Batch 77/7702 - Train loss: 0.00186316  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.71 min) Epoch 77/300 -- Iteration 585506 - Batch 154/7702 - Train loss: 0.00186471  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.73 min) Epoch 77/300 -- Iteration 585583 - Batch 231/7702 - Train loss: 0.00188059  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.75 min) Epoch 77/300 -- Iteration 585660 - Batch 308/7702 - Train loss: 0.00187605  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.78 min) Epoch 77/300 -- Iteration 585737 - Batch 385/7702 - Train loss: 0.00187683  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.80 min) Epoch 77/300 -- Iteration 585814 - Batch 462/7702 - Train loss: 0.00187268  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.82 min) Epoch 77/300 -- Iteration 585891 - Batch 539/7702 - Train loss: 0.00187345  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.84 min) Epoch 77/300 -- Iteration 585968 - Batch 616/7702 - Train loss: 0.00187877  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.86 min) Epoch 77/300 -- Iteration 586045 - Batch 693/7702 - Train loss: 0.00187939  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.88 min) Epoch 77/300 -- Iteration 586122 - Batch 770/7702 - Train loss: 0.00187921  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.90 min) Epoch 77/300 -- Iteration 586199 - Batch 847/7702 - Train loss: 0.00187774  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.92 min) Epoch 77/300 -- Iteration 586276 - Batch 924/7702 - Train loss: 0.00187868  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.94 min) Epoch 77/300 -- Iteration 586353 - Batch 1001/7702 - Train loss: 0.00187974  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.96 min) Epoch 77/300 -- Iteration 586430 - Batch 1078/7702 - Train loss: 0.00188139  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(157.98 min) Epoch 77/300 -- Iteration 586507 - Batch 1155/7702 - Train loss: 0.00188120  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.00 min) Epoch 77/300 -- Iteration 586584 - Batch 1232/7702 - Train loss: 0.00188202  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.02 min) Epoch 77/300 -- Iteration 586661 - Batch 1309/7702 - Train loss: 0.00188136  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.05 min) Epoch 77/300 -- Iteration 586738 - Batch 1386/7702 - Train loss: 0.00188026  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.07 min) Epoch 77/300 -- Iteration 586815 - Batch 1463/7702 - Train loss: 0.00188140  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.09 min) Epoch 77/300 -- Iteration 586892 - Batch 1540/7702 - Train loss: 0.00188232  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.11 min) Epoch 77/300 -- Iteration 586969 - Batch 1617/7702 - Train loss: 0.00188130  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.13 min) Epoch 77/300 -- Iteration 587046 - Batch 1694/7702 - Train loss: 0.00188142  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.15 min) Epoch 77/300 -- Iteration 587123 - Batch 1771/7702 - Train loss: 0.00188062  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.17 min) Epoch 77/300 -- Iteration 587200 - Batch 1848/7702 - Train loss: 0.00188170  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.19 min) Epoch 77/300 -- Iteration 587277 - Batch 1925/7702 - Train loss: 0.00188096  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.21 min) Epoch 77/300 -- Iteration 587354 - Batch 2002/7702 - Train loss: 0.00188112  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.23 min) Epoch 77/300 -- Iteration 587431 - Batch 2079/7702 - Train loss: 0.00188200  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.25 min) Epoch 77/300 -- Iteration 587508 - Batch 2156/7702 - Train loss: 0.00188164  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.27 min) Epoch 77/300 -- Iteration 587585 - Batch 2233/7702 - Train loss: 0.00188196  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.30 min) Epoch 77/300 -- Iteration 587662 - Batch 2310/7702 - Train loss: 0.00188165  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.32 min) Epoch 77/300 -- Iteration 587739 - Batch 2387/7702 - Train loss: 0.00188213  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.34 min) Epoch 77/300 -- Iteration 587816 - Batch 2464/7702 - Train loss: 0.00188223  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.36 min) Epoch 77/300 -- Iteration 587893 - Batch 2541/7702 - Train loss: 0.00188208  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.38 min) Epoch 77/300 -- Iteration 587970 - Batch 2618/7702 - Train loss: 0.00188109  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.40 min) Epoch 77/300 -- Iteration 588047 - Batch 2695/7702 - Train loss: 0.00188184  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.42 min) Epoch 77/300 -- Iteration 588124 - Batch 2772/7702 - Train loss: 0.00188220  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.44 min) Epoch 77/300 -- Iteration 588201 - Batch 2849/7702 - Train loss: 0.00188205  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.46 min) Epoch 77/300 -- Iteration 588278 - Batch 2926/7702 - Train loss: 0.00188232  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.48 min) Epoch 77/300 -- Iteration 588355 - Batch 3003/7702 - Train loss: 0.00188221  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.50 min) Epoch 77/300 -- Iteration 588432 - Batch 3080/7702 - Train loss: 0.00188226  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.52 min) Epoch 77/300 -- Iteration 588509 - Batch 3157/7702 - Train loss: 0.00188176  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.54 min) Epoch 77/300 -- Iteration 588586 - Batch 3234/7702 - Train loss: 0.00188169  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.57 min) Epoch 77/300 -- Iteration 588663 - Batch 3311/7702 - Train loss: 0.00188079  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.59 min) Epoch 77/300 -- Iteration 588740 - Batch 3388/7702 - Train loss: 0.00187990  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.61 min) Epoch 77/300 -- Iteration 588817 - Batch 3465/7702 - Train loss: 0.00187926  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.63 min) Epoch 77/300 -- Iteration 588894 - Batch 3542/7702 - Train loss: 0.00187875  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.65 min) Epoch 77/300 -- Iteration 588971 - Batch 3619/7702 - Train loss: 0.00187852  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.67 min) Epoch 77/300 -- Iteration 589048 - Batch 3696/7702 - Train loss: 0.00187788  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.69 min) Epoch 77/300 -- Iteration 589125 - Batch 3773/7702 - Train loss: 0.00187785  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.71 min) Epoch 77/300 -- Iteration 589202 - Batch 3850/7702 - Train loss: 0.00187719  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.73 min) Epoch 77/300 -- Iteration 589279 - Batch 3927/7702 - Train loss: 0.00187792  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.75 min) Epoch 77/300 -- Iteration 589356 - Batch 4004/7702 - Train loss: 0.00187825  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.77 min) Epoch 77/300 -- Iteration 589433 - Batch 4081/7702 - Train loss: 0.00187824  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.79 min) Epoch 77/300 -- Iteration 589510 - Batch 4158/7702 - Train loss: 0.00187846  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.81 min) Epoch 77/300 -- Iteration 589587 - Batch 4235/7702 - Train loss: 0.00187917  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.84 min) Epoch 77/300 -- Iteration 589664 - Batch 4312/7702 - Train loss: 0.00187882  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.86 min) Epoch 77/300 -- Iteration 589741 - Batch 4389/7702 - Train loss: 0.00187886  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.88 min) Epoch 77/300 -- Iteration 589818 - Batch 4466/7702 - Train loss: 0.00187900  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.90 min) Epoch 77/300 -- Iteration 589895 - Batch 4543/7702 - Train loss: 0.00187822  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.92 min) Epoch 77/300 -- Iteration 589972 - Batch 4620/7702 - Train loss: 0.00187780  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.94 min) Epoch 77/300 -- Iteration 590049 - Batch 4697/7702 - Train loss: 0.00187747  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.96 min) Epoch 77/300 -- Iteration 590126 - Batch 4774/7702 - Train loss: 0.00187786  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(158.98 min) Epoch 77/300 -- Iteration 590203 - Batch 4851/7702 - Train loss: 0.00187697  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.00 min) Epoch 77/300 -- Iteration 590280 - Batch 4928/7702 - Train loss: 0.00187686  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.02 min) Epoch 77/300 -- Iteration 590357 - Batch 5005/7702 - Train loss: 0.00187721  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.04 min) Epoch 77/300 -- Iteration 590434 - Batch 5082/7702 - Train loss: 0.00187705  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.06 min) Epoch 77/300 -- Iteration 590511 - Batch 5159/7702 - Train loss: 0.00187690  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.08 min) Epoch 77/300 -- Iteration 590588 - Batch 5236/7702 - Train loss: 0.00187677  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.10 min) Epoch 77/300 -- Iteration 590665 - Batch 5313/7702 - Train loss: 0.00187709  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.13 min) Epoch 77/300 -- Iteration 590742 - Batch 5390/7702 - Train loss: 0.00187696  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.15 min) Epoch 77/300 -- Iteration 590819 - Batch 5467/7702 - Train loss: 0.00187701  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.17 min) Epoch 77/300 -- Iteration 590896 - Batch 5544/7702 - Train loss: 0.00187679  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.19 min) Epoch 77/300 -- Iteration 590973 - Batch 5621/7702 - Train loss: 0.00187717  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.21 min) Epoch 77/300 -- Iteration 591050 - Batch 5698/7702 - Train loss: 0.00187713  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.23 min) Epoch 77/300 -- Iteration 591127 - Batch 5775/7702 - Train loss: 0.00187704  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.25 min) Epoch 77/300 -- Iteration 591204 - Batch 5852/7702 - Train loss: 0.00187695  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.27 min) Epoch 77/300 -- Iteration 591281 - Batch 5929/7702 - Train loss: 0.00187689  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.29 min) Epoch 77/300 -- Iteration 591358 - Batch 6006/7702 - Train loss: 0.00187754  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.31 min) Epoch 77/300 -- Iteration 591435 - Batch 6083/7702 - Train loss: 0.00187715  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.33 min) Epoch 77/300 -- Iteration 591512 - Batch 6160/7702 - Train loss: 0.00187692  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.35 min) Epoch 77/300 -- Iteration 591589 - Batch 6237/7702 - Train loss: 0.00187687  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.37 min) Epoch 77/300 -- Iteration 591666 - Batch 6314/7702 - Train loss: 0.00187712  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.39 min) Epoch 77/300 -- Iteration 591743 - Batch 6391/7702 - Train loss: 0.00187733  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.42 min) Epoch 77/300 -- Iteration 591820 - Batch 6468/7702 - Train loss: 0.00187766  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.44 min) Epoch 77/300 -- Iteration 591897 - Batch 6545/7702 - Train loss: 0.00187786  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.46 min) Epoch 77/300 -- Iteration 591974 - Batch 6622/7702 - Train loss: 0.00187835  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.48 min) Epoch 77/300 -- Iteration 592051 - Batch 6699/7702 - Train loss: 0.00187796  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.50 min) Epoch 77/300 -- Iteration 592128 - Batch 6776/7702 - Train loss: 0.00187778  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.52 min) Epoch 77/300 -- Iteration 592205 - Batch 6853/7702 - Train loss: 0.00187757  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.54 min) Epoch 77/300 -- Iteration 592282 - Batch 6930/7702 - Train loss: 0.00187722  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.56 min) Epoch 77/300 -- Iteration 592359 - Batch 7007/7702 - Train loss: 0.00187751  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.58 min) Epoch 77/300 -- Iteration 592436 - Batch 7084/7702 - Train loss: 0.00187737  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.60 min) Epoch 77/300 -- Iteration 592513 - Batch 7161/7702 - Train loss: 0.00187739  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.62 min) Epoch 77/300 -- Iteration 592590 - Batch 7238/7702 - Train loss: 0.00187745  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.64 min) Epoch 77/300 -- Iteration 592667 - Batch 7315/7702 - Train loss: 0.00187721  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.66 min) Epoch 77/300 -- Iteration 592744 - Batch 7392/7702 - Train loss: 0.00187677  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.68 min) Epoch 77/300 -- Iteration 592821 - Batch 7469/7702 - Train loss: 0.00187672  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.71 min) Epoch 77/300 -- Iteration 592898 - Batch 7546/7702 - Train loss: 0.00187671  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.73 min) Epoch 77/300 -- Iteration 592975 - Batch 7623/7702 - Train loss: 0.00187701  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.75 min) Epoch 77/300 -- Iteration 593052 - Batch 7700/7702 - Train loss: 0.00187708  - Train acc: -0.0000 - Val loss: 0.00037853\n",
      "(159.75 min) Epoch 77/300 -- Iteration 593054 - Batch 7701/7702 - Train loss: 0.00187712  - Train acc: -0.0000 - Val loss: 0.00035321 - Val acc: -0.0000\n",
      "(159.77 min) Epoch 78/300 -- Iteration 593131 - Batch 77/7702 - Train loss: 0.00186989  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.79 min) Epoch 78/300 -- Iteration 593208 - Batch 154/7702 - Train loss: 0.00186113  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.81 min) Epoch 78/300 -- Iteration 593285 - Batch 231/7702 - Train loss: 0.00186678  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.83 min) Epoch 78/300 -- Iteration 593362 - Batch 308/7702 - Train loss: 0.00187680  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.85 min) Epoch 78/300 -- Iteration 593439 - Batch 385/7702 - Train loss: 0.00187966  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.88 min) Epoch 78/300 -- Iteration 593516 - Batch 462/7702 - Train loss: 0.00187907  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.90 min) Epoch 78/300 -- Iteration 593593 - Batch 539/7702 - Train loss: 0.00187987  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.92 min) Epoch 78/300 -- Iteration 593670 - Batch 616/7702 - Train loss: 0.00187630  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.94 min) Epoch 78/300 -- Iteration 593747 - Batch 693/7702 - Train loss: 0.00187312  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.96 min) Epoch 78/300 -- Iteration 593824 - Batch 770/7702 - Train loss: 0.00186928  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(159.98 min) Epoch 78/300 -- Iteration 593901 - Batch 847/7702 - Train loss: 0.00186768  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.00 min) Epoch 78/300 -- Iteration 593978 - Batch 924/7702 - Train loss: 0.00186836  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.02 min) Epoch 78/300 -- Iteration 594055 - Batch 1001/7702 - Train loss: 0.00186864  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.04 min) Epoch 78/300 -- Iteration 594132 - Batch 1078/7702 - Train loss: 0.00186824  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.06 min) Epoch 78/300 -- Iteration 594209 - Batch 1155/7702 - Train loss: 0.00186942  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.08 min) Epoch 78/300 -- Iteration 594286 - Batch 1232/7702 - Train loss: 0.00186813  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.10 min) Epoch 78/300 -- Iteration 594363 - Batch 1309/7702 - Train loss: 0.00186971  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.12 min) Epoch 78/300 -- Iteration 594440 - Batch 1386/7702 - Train loss: 0.00187238  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.14 min) Epoch 78/300 -- Iteration 594517 - Batch 1463/7702 - Train loss: 0.00187290  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.16 min) Epoch 78/300 -- Iteration 594594 - Batch 1540/7702 - Train loss: 0.00187316  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.19 min) Epoch 78/300 -- Iteration 594671 - Batch 1617/7702 - Train loss: 0.00187491  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.21 min) Epoch 78/300 -- Iteration 594748 - Batch 1694/7702 - Train loss: 0.00187453  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.23 min) Epoch 78/300 -- Iteration 594825 - Batch 1771/7702 - Train loss: 0.00187416  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.25 min) Epoch 78/300 -- Iteration 594902 - Batch 1848/7702 - Train loss: 0.00187427  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.27 min) Epoch 78/300 -- Iteration 594979 - Batch 1925/7702 - Train loss: 0.00187329  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.29 min) Epoch 78/300 -- Iteration 595056 - Batch 2002/7702 - Train loss: 0.00187369  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.31 min) Epoch 78/300 -- Iteration 595133 - Batch 2079/7702 - Train loss: 0.00187297  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.33 min) Epoch 78/300 -- Iteration 595210 - Batch 2156/7702 - Train loss: 0.00187317  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.35 min) Epoch 78/300 -- Iteration 595287 - Batch 2233/7702 - Train loss: 0.00187311  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.37 min) Epoch 78/300 -- Iteration 595364 - Batch 2310/7702 - Train loss: 0.00187332  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.39 min) Epoch 78/300 -- Iteration 595441 - Batch 2387/7702 - Train loss: 0.00187402  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.41 min) Epoch 78/300 -- Iteration 595518 - Batch 2464/7702 - Train loss: 0.00187502  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.43 min) Epoch 78/300 -- Iteration 595595 - Batch 2541/7702 - Train loss: 0.00187536  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.46 min) Epoch 78/300 -- Iteration 595672 - Batch 2618/7702 - Train loss: 0.00187573  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.48 min) Epoch 78/300 -- Iteration 595749 - Batch 2695/7702 - Train loss: 0.00187602  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.50 min) Epoch 78/300 -- Iteration 595826 - Batch 2772/7702 - Train loss: 0.00187610  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.52 min) Epoch 78/300 -- Iteration 595903 - Batch 2849/7702 - Train loss: 0.00187521  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.54 min) Epoch 78/300 -- Iteration 595980 - Batch 2926/7702 - Train loss: 0.00187715  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.56 min) Epoch 78/300 -- Iteration 596057 - Batch 3003/7702 - Train loss: 0.00187701  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.58 min) Epoch 78/300 -- Iteration 596134 - Batch 3080/7702 - Train loss: 0.00187791  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.60 min) Epoch 78/300 -- Iteration 596211 - Batch 3157/7702 - Train loss: 0.00187818  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.62 min) Epoch 78/300 -- Iteration 596288 - Batch 3234/7702 - Train loss: 0.00187838  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.64 min) Epoch 78/300 -- Iteration 596365 - Batch 3311/7702 - Train loss: 0.00187816  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.66 min) Epoch 78/300 -- Iteration 596442 - Batch 3388/7702 - Train loss: 0.00187855  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.68 min) Epoch 78/300 -- Iteration 596519 - Batch 3465/7702 - Train loss: 0.00187923  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.70 min) Epoch 78/300 -- Iteration 596596 - Batch 3542/7702 - Train loss: 0.00187977  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.73 min) Epoch 78/300 -- Iteration 596673 - Batch 3619/7702 - Train loss: 0.00187912  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.75 min) Epoch 78/300 -- Iteration 596750 - Batch 3696/7702 - Train loss: 0.00187997  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.77 min) Epoch 78/300 -- Iteration 596827 - Batch 3773/7702 - Train loss: 0.00187977  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.79 min) Epoch 78/300 -- Iteration 596904 - Batch 3850/7702 - Train loss: 0.00187996  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.81 min) Epoch 78/300 -- Iteration 596981 - Batch 3927/7702 - Train loss: 0.00188056  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.83 min) Epoch 78/300 -- Iteration 597058 - Batch 4004/7702 - Train loss: 0.00188041  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.85 min) Epoch 78/300 -- Iteration 597135 - Batch 4081/7702 - Train loss: 0.00188064  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.87 min) Epoch 78/300 -- Iteration 597212 - Batch 4158/7702 - Train loss: 0.00188027  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.89 min) Epoch 78/300 -- Iteration 597289 - Batch 4235/7702 - Train loss: 0.00188064  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.91 min) Epoch 78/300 -- Iteration 597366 - Batch 4312/7702 - Train loss: 0.00188063  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.93 min) Epoch 78/300 -- Iteration 597443 - Batch 4389/7702 - Train loss: 0.00188074  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.95 min) Epoch 78/300 -- Iteration 597520 - Batch 4466/7702 - Train loss: 0.00188069  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(160.97 min) Epoch 78/300 -- Iteration 597597 - Batch 4543/7702 - Train loss: 0.00188062  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.00 min) Epoch 78/300 -- Iteration 597674 - Batch 4620/7702 - Train loss: 0.00188104  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.02 min) Epoch 78/300 -- Iteration 597751 - Batch 4697/7702 - Train loss: 0.00188153  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.04 min) Epoch 78/300 -- Iteration 597828 - Batch 4774/7702 - Train loss: 0.00188104  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.06 min) Epoch 78/300 -- Iteration 597905 - Batch 4851/7702 - Train loss: 0.00188089  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.08 min) Epoch 78/300 -- Iteration 597982 - Batch 4928/7702 - Train loss: 0.00188081  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.10 min) Epoch 78/300 -- Iteration 598059 - Batch 5005/7702 - Train loss: 0.00188095  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.12 min) Epoch 78/300 -- Iteration 598136 - Batch 5082/7702 - Train loss: 0.00188111  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.14 min) Epoch 78/300 -- Iteration 598213 - Batch 5159/7702 - Train loss: 0.00188151  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.16 min) Epoch 78/300 -- Iteration 598290 - Batch 5236/7702 - Train loss: 0.00188156  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.18 min) Epoch 78/300 -- Iteration 598367 - Batch 5313/7702 - Train loss: 0.00188082  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.20 min) Epoch 78/300 -- Iteration 598444 - Batch 5390/7702 - Train loss: 0.00188074  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.22 min) Epoch 78/300 -- Iteration 598521 - Batch 5467/7702 - Train loss: 0.00188119  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.24 min) Epoch 78/300 -- Iteration 598598 - Batch 5544/7702 - Train loss: 0.00188114  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.26 min) Epoch 78/300 -- Iteration 598675 - Batch 5621/7702 - Train loss: 0.00188083  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.29 min) Epoch 78/300 -- Iteration 598752 - Batch 5698/7702 - Train loss: 0.00188115  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.31 min) Epoch 78/300 -- Iteration 598829 - Batch 5775/7702 - Train loss: 0.00188179  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.33 min) Epoch 78/300 -- Iteration 598906 - Batch 5852/7702 - Train loss: 0.00188176  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.35 min) Epoch 78/300 -- Iteration 598983 - Batch 5929/7702 - Train loss: 0.00188145  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.37 min) Epoch 78/300 -- Iteration 599060 - Batch 6006/7702 - Train loss: 0.00188159  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.39 min) Epoch 78/300 -- Iteration 599137 - Batch 6083/7702 - Train loss: 0.00188148  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.41 min) Epoch 78/300 -- Iteration 599214 - Batch 6160/7702 - Train loss: 0.00188158  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.43 min) Epoch 78/300 -- Iteration 599291 - Batch 6237/7702 - Train loss: 0.00188143  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.45 min) Epoch 78/300 -- Iteration 599368 - Batch 6314/7702 - Train loss: 0.00188072  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.47 min) Epoch 78/300 -- Iteration 599445 - Batch 6391/7702 - Train loss: 0.00188085  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.49 min) Epoch 78/300 -- Iteration 599522 - Batch 6468/7702 - Train loss: 0.00188067  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.51 min) Epoch 78/300 -- Iteration 599599 - Batch 6545/7702 - Train loss: 0.00188040  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.53 min) Epoch 78/300 -- Iteration 599676 - Batch 6622/7702 - Train loss: 0.00187991  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.56 min) Epoch 78/300 -- Iteration 599753 - Batch 6699/7702 - Train loss: 0.00188043  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.58 min) Epoch 78/300 -- Iteration 599830 - Batch 6776/7702 - Train loss: 0.00188041  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.60 min) Epoch 78/300 -- Iteration 599907 - Batch 6853/7702 - Train loss: 0.00188028  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.62 min) Epoch 78/300 -- Iteration 599984 - Batch 6930/7702 - Train loss: 0.00188016  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.64 min) Epoch 78/300 -- Iteration 600061 - Batch 7007/7702 - Train loss: 0.00188011  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.66 min) Epoch 78/300 -- Iteration 600138 - Batch 7084/7702 - Train loss: 0.00188015  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.68 min) Epoch 78/300 -- Iteration 600215 - Batch 7161/7702 - Train loss: 0.00188026  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.70 min) Epoch 78/300 -- Iteration 600292 - Batch 7238/7702 - Train loss: 0.00188019  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.72 min) Epoch 78/300 -- Iteration 600369 - Batch 7315/7702 - Train loss: 0.00187985  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.74 min) Epoch 78/300 -- Iteration 600446 - Batch 7392/7702 - Train loss: 0.00187999  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.76 min) Epoch 78/300 -- Iteration 600523 - Batch 7469/7702 - Train loss: 0.00187985  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.78 min) Epoch 78/300 -- Iteration 600600 - Batch 7546/7702 - Train loss: 0.00187985  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.80 min) Epoch 78/300 -- Iteration 600677 - Batch 7623/7702 - Train loss: 0.00187992  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.83 min) Epoch 78/300 -- Iteration 600754 - Batch 7700/7702 - Train loss: 0.00187986  - Train acc: -0.0000 - Val loss: 0.00035321\n",
      "(161.83 min) Epoch 78/300 -- Iteration 600756 - Batch 7701/7702 - Train loss: 0.00187985  - Train acc: -0.0000 - Val loss: 0.00041236 - Val acc: -0.0000\n",
      "(161.85 min) Epoch 79/300 -- Iteration 600833 - Batch 77/7702 - Train loss: 0.00187115  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(161.87 min) Epoch 79/300 -- Iteration 600910 - Batch 154/7702 - Train loss: 0.00186069  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(161.89 min) Epoch 79/300 -- Iteration 600987 - Batch 231/7702 - Train loss: 0.00186473  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(161.91 min) Epoch 79/300 -- Iteration 601064 - Batch 308/7702 - Train loss: 0.00186224  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(161.93 min) Epoch 79/300 -- Iteration 601141 - Batch 385/7702 - Train loss: 0.00186440  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(161.95 min) Epoch 79/300 -- Iteration 601218 - Batch 462/7702 - Train loss: 0.00186425  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(161.97 min) Epoch 79/300 -- Iteration 601295 - Batch 539/7702 - Train loss: 0.00186014  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(161.99 min) Epoch 79/300 -- Iteration 601372 - Batch 616/7702 - Train loss: 0.00186805  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.01 min) Epoch 79/300 -- Iteration 601449 - Batch 693/7702 - Train loss: 0.00187121  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.04 min) Epoch 79/300 -- Iteration 601526 - Batch 770/7702 - Train loss: 0.00187412  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.06 min) Epoch 79/300 -- Iteration 601603 - Batch 847/7702 - Train loss: 0.00187622  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.08 min) Epoch 79/300 -- Iteration 601680 - Batch 924/7702 - Train loss: 0.00187839  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.10 min) Epoch 79/300 -- Iteration 601757 - Batch 1001/7702 - Train loss: 0.00187910  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.12 min) Epoch 79/300 -- Iteration 601834 - Batch 1078/7702 - Train loss: 0.00187866  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.14 min) Epoch 79/300 -- Iteration 601911 - Batch 1155/7702 - Train loss: 0.00188051  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.16 min) Epoch 79/300 -- Iteration 601988 - Batch 1232/7702 - Train loss: 0.00188261  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.18 min) Epoch 79/300 -- Iteration 602065 - Batch 1309/7702 - Train loss: 0.00188233  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.20 min) Epoch 79/300 -- Iteration 602142 - Batch 1386/7702 - Train loss: 0.00188403  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.22 min) Epoch 79/300 -- Iteration 602219 - Batch 1463/7702 - Train loss: 0.00188406  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.24 min) Epoch 79/300 -- Iteration 602296 - Batch 1540/7702 - Train loss: 0.00188400  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.26 min) Epoch 79/300 -- Iteration 602373 - Batch 1617/7702 - Train loss: 0.00188400  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.28 min) Epoch 79/300 -- Iteration 602450 - Batch 1694/7702 - Train loss: 0.00188367  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.31 min) Epoch 79/300 -- Iteration 602527 - Batch 1771/7702 - Train loss: 0.00188402  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.33 min) Epoch 79/300 -- Iteration 602604 - Batch 1848/7702 - Train loss: 0.00188539  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.35 min) Epoch 79/300 -- Iteration 602681 - Batch 1925/7702 - Train loss: 0.00188483  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.37 min) Epoch 79/300 -- Iteration 602758 - Batch 2002/7702 - Train loss: 0.00188585  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.39 min) Epoch 79/300 -- Iteration 602835 - Batch 2079/7702 - Train loss: 0.00188501  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.41 min) Epoch 79/300 -- Iteration 602912 - Batch 2156/7702 - Train loss: 0.00188501  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.43 min) Epoch 79/300 -- Iteration 602989 - Batch 2233/7702 - Train loss: 0.00188604  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.45 min) Epoch 79/300 -- Iteration 603066 - Batch 2310/7702 - Train loss: 0.00188527  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.47 min) Epoch 79/300 -- Iteration 603143 - Batch 2387/7702 - Train loss: 0.00188465  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.49 min) Epoch 79/300 -- Iteration 603220 - Batch 2464/7702 - Train loss: 0.00188396  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.51 min) Epoch 79/300 -- Iteration 603297 - Batch 2541/7702 - Train loss: 0.00188341  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.53 min) Epoch 79/300 -- Iteration 603374 - Batch 2618/7702 - Train loss: 0.00188417  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.56 min) Epoch 79/300 -- Iteration 603451 - Batch 2695/7702 - Train loss: 0.00188369  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.58 min) Epoch 79/300 -- Iteration 603528 - Batch 2772/7702 - Train loss: 0.00188359  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.60 min) Epoch 79/300 -- Iteration 603605 - Batch 2849/7702 - Train loss: 0.00188304  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.62 min) Epoch 79/300 -- Iteration 603682 - Batch 2926/7702 - Train loss: 0.00188205  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.64 min) Epoch 79/300 -- Iteration 603759 - Batch 3003/7702 - Train loss: 0.00188385  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.66 min) Epoch 79/300 -- Iteration 603836 - Batch 3080/7702 - Train loss: 0.00188372  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.68 min) Epoch 79/300 -- Iteration 603913 - Batch 3157/7702 - Train loss: 0.00188407  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.70 min) Epoch 79/300 -- Iteration 603990 - Batch 3234/7702 - Train loss: 0.00188412  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.72 min) Epoch 79/300 -- Iteration 604067 - Batch 3311/7702 - Train loss: 0.00188433  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.74 min) Epoch 79/300 -- Iteration 604144 - Batch 3388/7702 - Train loss: 0.00188470  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.76 min) Epoch 79/300 -- Iteration 604221 - Batch 3465/7702 - Train loss: 0.00188472  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.78 min) Epoch 79/300 -- Iteration 604298 - Batch 3542/7702 - Train loss: 0.00188422  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.80 min) Epoch 79/300 -- Iteration 604375 - Batch 3619/7702 - Train loss: 0.00188425  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.82 min) Epoch 79/300 -- Iteration 604452 - Batch 3696/7702 - Train loss: 0.00188493  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.85 min) Epoch 79/300 -- Iteration 604529 - Batch 3773/7702 - Train loss: 0.00188520  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.87 min) Epoch 79/300 -- Iteration 604606 - Batch 3850/7702 - Train loss: 0.00188552  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.89 min) Epoch 79/300 -- Iteration 604683 - Batch 3927/7702 - Train loss: 0.00188593  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.91 min) Epoch 79/300 -- Iteration 604760 - Batch 4004/7702 - Train loss: 0.00188568  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.93 min) Epoch 79/300 -- Iteration 604837 - Batch 4081/7702 - Train loss: 0.00188594  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.95 min) Epoch 79/300 -- Iteration 604914 - Batch 4158/7702 - Train loss: 0.00188605  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.97 min) Epoch 79/300 -- Iteration 604991 - Batch 4235/7702 - Train loss: 0.00188566  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(162.99 min) Epoch 79/300 -- Iteration 605068 - Batch 4312/7702 - Train loss: 0.00188487  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.01 min) Epoch 79/300 -- Iteration 605145 - Batch 4389/7702 - Train loss: 0.00188450  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.03 min) Epoch 79/300 -- Iteration 605222 - Batch 4466/7702 - Train loss: 0.00188522  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.05 min) Epoch 79/300 -- Iteration 605299 - Batch 4543/7702 - Train loss: 0.00188512  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.07 min) Epoch 79/300 -- Iteration 605376 - Batch 4620/7702 - Train loss: 0.00188531  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.09 min) Epoch 79/300 -- Iteration 605453 - Batch 4697/7702 - Train loss: 0.00188504  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.12 min) Epoch 79/300 -- Iteration 605530 - Batch 4774/7702 - Train loss: 0.00188496  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.14 min) Epoch 79/300 -- Iteration 605607 - Batch 4851/7702 - Train loss: 0.00188420  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.16 min) Epoch 79/300 -- Iteration 605684 - Batch 4928/7702 - Train loss: 0.00188438  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.18 min) Epoch 79/300 -- Iteration 605761 - Batch 5005/7702 - Train loss: 0.00188427  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.20 min) Epoch 79/300 -- Iteration 605838 - Batch 5082/7702 - Train loss: 0.00188458  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.22 min) Epoch 79/300 -- Iteration 605915 - Batch 5159/7702 - Train loss: 0.00188452  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.24 min) Epoch 79/300 -- Iteration 605992 - Batch 5236/7702 - Train loss: 0.00188460  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.26 min) Epoch 79/300 -- Iteration 606069 - Batch 5313/7702 - Train loss: 0.00188414  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.28 min) Epoch 79/300 -- Iteration 606146 - Batch 5390/7702 - Train loss: 0.00188405  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.30 min) Epoch 79/300 -- Iteration 606223 - Batch 5467/7702 - Train loss: 0.00188386  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.32 min) Epoch 79/300 -- Iteration 606300 - Batch 5544/7702 - Train loss: 0.00188399  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.34 min) Epoch 79/300 -- Iteration 606377 - Batch 5621/7702 - Train loss: 0.00188405  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.36 min) Epoch 79/300 -- Iteration 606454 - Batch 5698/7702 - Train loss: 0.00188431  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.38 min) Epoch 79/300 -- Iteration 606531 - Batch 5775/7702 - Train loss: 0.00188353  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.41 min) Epoch 79/300 -- Iteration 606608 - Batch 5852/7702 - Train loss: 0.00188339  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.43 min) Epoch 79/300 -- Iteration 606685 - Batch 5929/7702 - Train loss: 0.00188333  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.45 min) Epoch 79/300 -- Iteration 606762 - Batch 6006/7702 - Train loss: 0.00188296  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.47 min) Epoch 79/300 -- Iteration 606839 - Batch 6083/7702 - Train loss: 0.00188291  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.49 min) Epoch 79/300 -- Iteration 606916 - Batch 6160/7702 - Train loss: 0.00188366  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.51 min) Epoch 79/300 -- Iteration 606993 - Batch 6237/7702 - Train loss: 0.00188426  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.53 min) Epoch 79/300 -- Iteration 607070 - Batch 6314/7702 - Train loss: 0.00188414  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.55 min) Epoch 79/300 -- Iteration 607147 - Batch 6391/7702 - Train loss: 0.00188413  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.57 min) Epoch 79/300 -- Iteration 607224 - Batch 6468/7702 - Train loss: 0.00188423  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.59 min) Epoch 79/300 -- Iteration 607301 - Batch 6545/7702 - Train loss: 0.00188413  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.61 min) Epoch 79/300 -- Iteration 607378 - Batch 6622/7702 - Train loss: 0.00188444  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.63 min) Epoch 79/300 -- Iteration 607455 - Batch 6699/7702 - Train loss: 0.00188426  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.65 min) Epoch 79/300 -- Iteration 607532 - Batch 6776/7702 - Train loss: 0.00188416  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.67 min) Epoch 79/300 -- Iteration 607609 - Batch 6853/7702 - Train loss: 0.00188428  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.70 min) Epoch 79/300 -- Iteration 607686 - Batch 6930/7702 - Train loss: 0.00188428  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.72 min) Epoch 79/300 -- Iteration 607763 - Batch 7007/7702 - Train loss: 0.00188418  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.74 min) Epoch 79/300 -- Iteration 607840 - Batch 7084/7702 - Train loss: 0.00188404  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.76 min) Epoch 79/300 -- Iteration 607917 - Batch 7161/7702 - Train loss: 0.00188387  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.78 min) Epoch 79/300 -- Iteration 607994 - Batch 7238/7702 - Train loss: 0.00188377  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.80 min) Epoch 79/300 -- Iteration 608071 - Batch 7315/7702 - Train loss: 0.00188331  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.82 min) Epoch 79/300 -- Iteration 608148 - Batch 7392/7702 - Train loss: 0.00188302  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.84 min) Epoch 79/300 -- Iteration 608225 - Batch 7469/7702 - Train loss: 0.00188299  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.86 min) Epoch 79/300 -- Iteration 608302 - Batch 7546/7702 - Train loss: 0.00188298  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.88 min) Epoch 79/300 -- Iteration 608379 - Batch 7623/7702 - Train loss: 0.00188316  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.90 min) Epoch 79/300 -- Iteration 608456 - Batch 7700/7702 - Train loss: 0.00188314  - Train acc: -0.0000 - Val loss: 0.00041236\n",
      "(163.90 min) Epoch 79/300 -- Iteration 608458 - Batch 7701/7702 - Train loss: 0.00188316  - Train acc: -0.0000 - Val loss: 0.00051600 - Val acc: -0.0000\n",
      "(163.93 min) Epoch 80/300 -- Iteration 608535 - Batch 77/7702 - Train loss: 0.00186120  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(163.95 min) Epoch 80/300 -- Iteration 608612 - Batch 154/7702 - Train loss: 0.00188442  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(163.97 min) Epoch 80/300 -- Iteration 608689 - Batch 231/7702 - Train loss: 0.00188275  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(163.99 min) Epoch 80/300 -- Iteration 608766 - Batch 308/7702 - Train loss: 0.00187925  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.01 min) Epoch 80/300 -- Iteration 608843 - Batch 385/7702 - Train loss: 0.00188151  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.03 min) Epoch 80/300 -- Iteration 608920 - Batch 462/7702 - Train loss: 0.00188545  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.05 min) Epoch 80/300 -- Iteration 608997 - Batch 539/7702 - Train loss: 0.00188947  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.07 min) Epoch 80/300 -- Iteration 609074 - Batch 616/7702 - Train loss: 0.00188461  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.09 min) Epoch 80/300 -- Iteration 609151 - Batch 693/7702 - Train loss: 0.00188728  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.11 min) Epoch 80/300 -- Iteration 609228 - Batch 770/7702 - Train loss: 0.00188692  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.13 min) Epoch 80/300 -- Iteration 609305 - Batch 847/7702 - Train loss: 0.00188585  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.15 min) Epoch 80/300 -- Iteration 609382 - Batch 924/7702 - Train loss: 0.00188325  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.17 min) Epoch 80/300 -- Iteration 609459 - Batch 1001/7702 - Train loss: 0.00188318  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.20 min) Epoch 80/300 -- Iteration 609536 - Batch 1078/7702 - Train loss: 0.00188403  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.22 min) Epoch 80/300 -- Iteration 609613 - Batch 1155/7702 - Train loss: 0.00188411  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.24 min) Epoch 80/300 -- Iteration 609690 - Batch 1232/7702 - Train loss: 0.00188423  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.26 min) Epoch 80/300 -- Iteration 609767 - Batch 1309/7702 - Train loss: 0.00188387  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.28 min) Epoch 80/300 -- Iteration 609844 - Batch 1386/7702 - Train loss: 0.00188246  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.30 min) Epoch 80/300 -- Iteration 609921 - Batch 1463/7702 - Train loss: 0.00188219  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.32 min) Epoch 80/300 -- Iteration 609998 - Batch 1540/7702 - Train loss: 0.00188159  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.34 min) Epoch 80/300 -- Iteration 610075 - Batch 1617/7702 - Train loss: 0.00188133  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.36 min) Epoch 80/300 -- Iteration 610152 - Batch 1694/7702 - Train loss: 0.00188073  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.38 min) Epoch 80/300 -- Iteration 610229 - Batch 1771/7702 - Train loss: 0.00188065  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.40 min) Epoch 80/300 -- Iteration 610306 - Batch 1848/7702 - Train loss: 0.00187955  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.42 min) Epoch 80/300 -- Iteration 610383 - Batch 1925/7702 - Train loss: 0.00187996  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.44 min) Epoch 80/300 -- Iteration 610460 - Batch 2002/7702 - Train loss: 0.00187916  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.46 min) Epoch 80/300 -- Iteration 610537 - Batch 2079/7702 - Train loss: 0.00187988  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.49 min) Epoch 80/300 -- Iteration 610614 - Batch 2156/7702 - Train loss: 0.00187978  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.51 min) Epoch 80/300 -- Iteration 610691 - Batch 2233/7702 - Train loss: 0.00187886  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.53 min) Epoch 80/300 -- Iteration 610768 - Batch 2310/7702 - Train loss: 0.00187817  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.55 min) Epoch 80/300 -- Iteration 610845 - Batch 2387/7702 - Train loss: 0.00187778  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.57 min) Epoch 80/300 -- Iteration 610922 - Batch 2464/7702 - Train loss: 0.00187829  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.59 min) Epoch 80/300 -- Iteration 610999 - Batch 2541/7702 - Train loss: 0.00187798  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.61 min) Epoch 80/300 -- Iteration 611076 - Batch 2618/7702 - Train loss: 0.00187715  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.63 min) Epoch 80/300 -- Iteration 611153 - Batch 2695/7702 - Train loss: 0.00187662  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.65 min) Epoch 80/300 -- Iteration 611230 - Batch 2772/7702 - Train loss: 0.00187583  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.67 min) Epoch 80/300 -- Iteration 611307 - Batch 2849/7702 - Train loss: 0.00187547  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.69 min) Epoch 80/300 -- Iteration 611384 - Batch 2926/7702 - Train loss: 0.00187564  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.71 min) Epoch 80/300 -- Iteration 611461 - Batch 3003/7702 - Train loss: 0.00187714  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.73 min) Epoch 80/300 -- Iteration 611538 - Batch 3080/7702 - Train loss: 0.00187775  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.76 min) Epoch 80/300 -- Iteration 611615 - Batch 3157/7702 - Train loss: 0.00187726  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.78 min) Epoch 80/300 -- Iteration 611692 - Batch 3234/7702 - Train loss: 0.00187763  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.80 min) Epoch 80/300 -- Iteration 611769 - Batch 3311/7702 - Train loss: 0.00187688  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.82 min) Epoch 80/300 -- Iteration 611846 - Batch 3388/7702 - Train loss: 0.00187656  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.84 min) Epoch 80/300 -- Iteration 611923 - Batch 3465/7702 - Train loss: 0.00187703  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.86 min) Epoch 80/300 -- Iteration 612000 - Batch 3542/7702 - Train loss: 0.00187659  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.88 min) Epoch 80/300 -- Iteration 612077 - Batch 3619/7702 - Train loss: 0.00187634  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.90 min) Epoch 80/300 -- Iteration 612154 - Batch 3696/7702 - Train loss: 0.00187666  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.92 min) Epoch 80/300 -- Iteration 612231 - Batch 3773/7702 - Train loss: 0.00187696  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.94 min) Epoch 80/300 -- Iteration 612308 - Batch 3850/7702 - Train loss: 0.00187678  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.96 min) Epoch 80/300 -- Iteration 612385 - Batch 3927/7702 - Train loss: 0.00187588  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(164.98 min) Epoch 80/300 -- Iteration 612462 - Batch 4004/7702 - Train loss: 0.00187638  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.00 min) Epoch 80/300 -- Iteration 612539 - Batch 4081/7702 - Train loss: 0.00187748  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.03 min) Epoch 80/300 -- Iteration 612616 - Batch 4158/7702 - Train loss: 0.00187724  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.05 min) Epoch 80/300 -- Iteration 612693 - Batch 4235/7702 - Train loss: 0.00187699  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.07 min) Epoch 80/300 -- Iteration 612770 - Batch 4312/7702 - Train loss: 0.00187743  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.09 min) Epoch 80/300 -- Iteration 612847 - Batch 4389/7702 - Train loss: 0.00187770  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.11 min) Epoch 80/300 -- Iteration 612924 - Batch 4466/7702 - Train loss: 0.00187700  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.13 min) Epoch 80/300 -- Iteration 613001 - Batch 4543/7702 - Train loss: 0.00187668  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.15 min) Epoch 80/300 -- Iteration 613078 - Batch 4620/7702 - Train loss: 0.00187675  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.17 min) Epoch 80/300 -- Iteration 613155 - Batch 4697/7702 - Train loss: 0.00187676  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.19 min) Epoch 80/300 -- Iteration 613232 - Batch 4774/7702 - Train loss: 0.00187677  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.21 min) Epoch 80/300 -- Iteration 613309 - Batch 4851/7702 - Train loss: 0.00187701  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.23 min) Epoch 80/300 -- Iteration 613386 - Batch 4928/7702 - Train loss: 0.00187740  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.25 min) Epoch 80/300 -- Iteration 613463 - Batch 5005/7702 - Train loss: 0.00187790  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.27 min) Epoch 80/300 -- Iteration 613540 - Batch 5082/7702 - Train loss: 0.00187753  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.29 min) Epoch 80/300 -- Iteration 613617 - Batch 5159/7702 - Train loss: 0.00187759  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.32 min) Epoch 80/300 -- Iteration 613694 - Batch 5236/7702 - Train loss: 0.00187766  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.34 min) Epoch 80/300 -- Iteration 613771 - Batch 5313/7702 - Train loss: 0.00187766  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.36 min) Epoch 80/300 -- Iteration 613848 - Batch 5390/7702 - Train loss: 0.00187717  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.38 min) Epoch 80/300 -- Iteration 613925 - Batch 5467/7702 - Train loss: 0.00187704  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.40 min) Epoch 80/300 -- Iteration 614002 - Batch 5544/7702 - Train loss: 0.00187735  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.42 min) Epoch 80/300 -- Iteration 614079 - Batch 5621/7702 - Train loss: 0.00187701  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.44 min) Epoch 80/300 -- Iteration 614156 - Batch 5698/7702 - Train loss: 0.00187696  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.46 min) Epoch 80/300 -- Iteration 614233 - Batch 5775/7702 - Train loss: 0.00187696  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.48 min) Epoch 80/300 -- Iteration 614310 - Batch 5852/7702 - Train loss: 0.00187692  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.50 min) Epoch 80/300 -- Iteration 614387 - Batch 5929/7702 - Train loss: 0.00187706  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.52 min) Epoch 80/300 -- Iteration 614464 - Batch 6006/7702 - Train loss: 0.00187757  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.54 min) Epoch 80/300 -- Iteration 614541 - Batch 6083/7702 - Train loss: 0.00187707  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.57 min) Epoch 80/300 -- Iteration 614618 - Batch 6160/7702 - Train loss: 0.00187718  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.59 min) Epoch 80/300 -- Iteration 614695 - Batch 6237/7702 - Train loss: 0.00187694  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.61 min) Epoch 80/300 -- Iteration 614772 - Batch 6314/7702 - Train loss: 0.00187705  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.63 min) Epoch 80/300 -- Iteration 614849 - Batch 6391/7702 - Train loss: 0.00187703  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.65 min) Epoch 80/300 -- Iteration 614926 - Batch 6468/7702 - Train loss: 0.00187703  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.67 min) Epoch 80/300 -- Iteration 615003 - Batch 6545/7702 - Train loss: 0.00187736  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.69 min) Epoch 80/300 -- Iteration 615080 - Batch 6622/7702 - Train loss: 0.00187749  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.71 min) Epoch 80/300 -- Iteration 615157 - Batch 6699/7702 - Train loss: 0.00187724  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.73 min) Epoch 80/300 -- Iteration 615234 - Batch 6776/7702 - Train loss: 0.00187742  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.75 min) Epoch 80/300 -- Iteration 615311 - Batch 6853/7702 - Train loss: 0.00187771  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.77 min) Epoch 80/300 -- Iteration 615388 - Batch 6930/7702 - Train loss: 0.00187783  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.79 min) Epoch 80/300 -- Iteration 615465 - Batch 7007/7702 - Train loss: 0.00187798  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.81 min) Epoch 80/300 -- Iteration 615542 - Batch 7084/7702 - Train loss: 0.00187822  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.83 min) Epoch 80/300 -- Iteration 615619 - Batch 7161/7702 - Train loss: 0.00187819  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.86 min) Epoch 80/300 -- Iteration 615696 - Batch 7238/7702 - Train loss: 0.00187843  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.88 min) Epoch 80/300 -- Iteration 615773 - Batch 7315/7702 - Train loss: 0.00187880  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.90 min) Epoch 80/300 -- Iteration 615850 - Batch 7392/7702 - Train loss: 0.00187912  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.92 min) Epoch 80/300 -- Iteration 615927 - Batch 7469/7702 - Train loss: 0.00187906  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.94 min) Epoch 80/300 -- Iteration 616004 - Batch 7546/7702 - Train loss: 0.00187891  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.96 min) Epoch 80/300 -- Iteration 616081 - Batch 7623/7702 - Train loss: 0.00187909  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.98 min) Epoch 80/300 -- Iteration 616158 - Batch 7700/7702 - Train loss: 0.00187918  - Train acc: -0.0000 - Val loss: 0.00051600\n",
      "(165.98 min) Epoch 80/300 -- Iteration 616160 - Batch 7701/7702 - Train loss: 0.00187914  - Train acc: -0.0000 - Val loss: 0.00048990 - Val acc: -0.0000\n",
      "(166.00 min) Epoch 81/300 -- Iteration 616237 - Batch 77/7702 - Train loss: 0.00189059  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.02 min) Epoch 81/300 -- Iteration 616314 - Batch 154/7702 - Train loss: 0.00185862  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.05 min) Epoch 81/300 -- Iteration 616391 - Batch 231/7702 - Train loss: 0.00185927  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.07 min) Epoch 81/300 -- Iteration 616468 - Batch 308/7702 - Train loss: 0.00186387  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.09 min) Epoch 81/300 -- Iteration 616545 - Batch 385/7702 - Train loss: 0.00186914  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.11 min) Epoch 81/300 -- Iteration 616622 - Batch 462/7702 - Train loss: 0.00186626  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.13 min) Epoch 81/300 -- Iteration 616699 - Batch 539/7702 - Train loss: 0.00186951  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.15 min) Epoch 81/300 -- Iteration 616776 - Batch 616/7702 - Train loss: 0.00186735  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.17 min) Epoch 81/300 -- Iteration 616853 - Batch 693/7702 - Train loss: 0.00186908  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.19 min) Epoch 81/300 -- Iteration 616930 - Batch 770/7702 - Train loss: 0.00186846  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.21 min) Epoch 81/300 -- Iteration 617007 - Batch 847/7702 - Train loss: 0.00187067  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.23 min) Epoch 81/300 -- Iteration 617084 - Batch 924/7702 - Train loss: 0.00187307  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.25 min) Epoch 81/300 -- Iteration 617161 - Batch 1001/7702 - Train loss: 0.00187447  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.27 min) Epoch 81/300 -- Iteration 617238 - Batch 1078/7702 - Train loss: 0.00187359  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.29 min) Epoch 81/300 -- Iteration 617315 - Batch 1155/7702 - Train loss: 0.00187235  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.31 min) Epoch 81/300 -- Iteration 617392 - Batch 1232/7702 - Train loss: 0.00187453  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.33 min) Epoch 81/300 -- Iteration 617469 - Batch 1309/7702 - Train loss: 0.00187249  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.36 min) Epoch 81/300 -- Iteration 617546 - Batch 1386/7702 - Train loss: 0.00187186  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.38 min) Epoch 81/300 -- Iteration 617623 - Batch 1463/7702 - Train loss: 0.00187199  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.40 min) Epoch 81/300 -- Iteration 617700 - Batch 1540/7702 - Train loss: 0.00187167  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.42 min) Epoch 81/300 -- Iteration 617777 - Batch 1617/7702 - Train loss: 0.00186984  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.44 min) Epoch 81/300 -- Iteration 617854 - Batch 1694/7702 - Train loss: 0.00186814  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.46 min) Epoch 81/300 -- Iteration 617931 - Batch 1771/7702 - Train loss: 0.00186729  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.48 min) Epoch 81/300 -- Iteration 618008 - Batch 1848/7702 - Train loss: 0.00186837  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.50 min) Epoch 81/300 -- Iteration 618085 - Batch 1925/7702 - Train loss: 0.00186821  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.52 min) Epoch 81/300 -- Iteration 618162 - Batch 2002/7702 - Train loss: 0.00186775  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.54 min) Epoch 81/300 -- Iteration 618239 - Batch 2079/7702 - Train loss: 0.00186776  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.56 min) Epoch 81/300 -- Iteration 618316 - Batch 2156/7702 - Train loss: 0.00186763  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.58 min) Epoch 81/300 -- Iteration 618393 - Batch 2233/7702 - Train loss: 0.00186728  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.60 min) Epoch 81/300 -- Iteration 618470 - Batch 2310/7702 - Train loss: 0.00186821  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.63 min) Epoch 81/300 -- Iteration 618547 - Batch 2387/7702 - Train loss: 0.00186929  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.65 min) Epoch 81/300 -- Iteration 618624 - Batch 2464/7702 - Train loss: 0.00186966  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.67 min) Epoch 81/300 -- Iteration 618701 - Batch 2541/7702 - Train loss: 0.00187091  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.69 min) Epoch 81/300 -- Iteration 618778 - Batch 2618/7702 - Train loss: 0.00187121  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.71 min) Epoch 81/300 -- Iteration 618855 - Batch 2695/7702 - Train loss: 0.00187089  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.73 min) Epoch 81/300 -- Iteration 618932 - Batch 2772/7702 - Train loss: 0.00187056  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.75 min) Epoch 81/300 -- Iteration 619009 - Batch 2849/7702 - Train loss: 0.00187082  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.77 min) Epoch 81/300 -- Iteration 619086 - Batch 2926/7702 - Train loss: 0.00187052  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.79 min) Epoch 81/300 -- Iteration 619163 - Batch 3003/7702 - Train loss: 0.00187144  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.81 min) Epoch 81/300 -- Iteration 619240 - Batch 3080/7702 - Train loss: 0.00187184  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.83 min) Epoch 81/300 -- Iteration 619317 - Batch 3157/7702 - Train loss: 0.00187098  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.85 min) Epoch 81/300 -- Iteration 619394 - Batch 3234/7702 - Train loss: 0.00187190  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.87 min) Epoch 81/300 -- Iteration 619471 - Batch 3311/7702 - Train loss: 0.00187179  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.89 min) Epoch 81/300 -- Iteration 619548 - Batch 3388/7702 - Train loss: 0.00187291  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.92 min) Epoch 81/300 -- Iteration 619625 - Batch 3465/7702 - Train loss: 0.00187329  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.94 min) Epoch 81/300 -- Iteration 619702 - Batch 3542/7702 - Train loss: 0.00187365  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.96 min) Epoch 81/300 -- Iteration 619779 - Batch 3619/7702 - Train loss: 0.00187443  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(166.98 min) Epoch 81/300 -- Iteration 619856 - Batch 3696/7702 - Train loss: 0.00187415  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.00 min) Epoch 81/300 -- Iteration 619933 - Batch 3773/7702 - Train loss: 0.00187372  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.02 min) Epoch 81/300 -- Iteration 620010 - Batch 3850/7702 - Train loss: 0.00187355  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.04 min) Epoch 81/300 -- Iteration 620087 - Batch 3927/7702 - Train loss: 0.00187377  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.06 min) Epoch 81/300 -- Iteration 620164 - Batch 4004/7702 - Train loss: 0.00187392  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.08 min) Epoch 81/300 -- Iteration 620241 - Batch 4081/7702 - Train loss: 0.00187411  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.10 min) Epoch 81/300 -- Iteration 620318 - Batch 4158/7702 - Train loss: 0.00187417  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.12 min) Epoch 81/300 -- Iteration 620395 - Batch 4235/7702 - Train loss: 0.00187458  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.14 min) Epoch 81/300 -- Iteration 620472 - Batch 4312/7702 - Train loss: 0.00187531  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.16 min) Epoch 81/300 -- Iteration 620549 - Batch 4389/7702 - Train loss: 0.00187495  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.18 min) Epoch 81/300 -- Iteration 620626 - Batch 4466/7702 - Train loss: 0.00187502  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.21 min) Epoch 81/300 -- Iteration 620703 - Batch 4543/7702 - Train loss: 0.00187446  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.23 min) Epoch 81/300 -- Iteration 620780 - Batch 4620/7702 - Train loss: 0.00187367  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.25 min) Epoch 81/300 -- Iteration 620857 - Batch 4697/7702 - Train loss: 0.00187407  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.27 min) Epoch 81/300 -- Iteration 620934 - Batch 4774/7702 - Train loss: 0.00187625  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.29 min) Epoch 81/300 -- Iteration 621011 - Batch 4851/7702 - Train loss: 0.00187662  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.31 min) Epoch 81/300 -- Iteration 621088 - Batch 4928/7702 - Train loss: 0.00187680  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.33 min) Epoch 81/300 -- Iteration 621165 - Batch 5005/7702 - Train loss: 0.00187654  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.35 min) Epoch 81/300 -- Iteration 621242 - Batch 5082/7702 - Train loss: 0.00187674  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.37 min) Epoch 81/300 -- Iteration 621319 - Batch 5159/7702 - Train loss: 0.00187692  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.39 min) Epoch 81/300 -- Iteration 621396 - Batch 5236/7702 - Train loss: 0.00187742  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.41 min) Epoch 81/300 -- Iteration 621473 - Batch 5313/7702 - Train loss: 0.00187733  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.43 min) Epoch 81/300 -- Iteration 621550 - Batch 5390/7702 - Train loss: 0.00187771  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.45 min) Epoch 81/300 -- Iteration 621627 - Batch 5467/7702 - Train loss: 0.00187747  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.48 min) Epoch 81/300 -- Iteration 621704 - Batch 5544/7702 - Train loss: 0.00187744  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.50 min) Epoch 81/300 -- Iteration 621781 - Batch 5621/7702 - Train loss: 0.00187705  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.52 min) Epoch 81/300 -- Iteration 621858 - Batch 5698/7702 - Train loss: 0.00187726  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.54 min) Epoch 81/300 -- Iteration 621935 - Batch 5775/7702 - Train loss: 0.00187677  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.56 min) Epoch 81/300 -- Iteration 622012 - Batch 5852/7702 - Train loss: 0.00187679  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.58 min) Epoch 81/300 -- Iteration 622089 - Batch 5929/7702 - Train loss: 0.00187682  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.60 min) Epoch 81/300 -- Iteration 622166 - Batch 6006/7702 - Train loss: 0.00187683  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.62 min) Epoch 81/300 -- Iteration 622243 - Batch 6083/7702 - Train loss: 0.00187692  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.64 min) Epoch 81/300 -- Iteration 622320 - Batch 6160/7702 - Train loss: 0.00187683  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.66 min) Epoch 81/300 -- Iteration 622397 - Batch 6237/7702 - Train loss: 0.00187657  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.68 min) Epoch 81/300 -- Iteration 622474 - Batch 6314/7702 - Train loss: 0.00187678  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.70 min) Epoch 81/300 -- Iteration 622551 - Batch 6391/7702 - Train loss: 0.00187685  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.72 min) Epoch 81/300 -- Iteration 622628 - Batch 6468/7702 - Train loss: 0.00187651  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.74 min) Epoch 81/300 -- Iteration 622705 - Batch 6545/7702 - Train loss: 0.00187643  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.77 min) Epoch 81/300 -- Iteration 622782 - Batch 6622/7702 - Train loss: 0.00187628  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.79 min) Epoch 81/300 -- Iteration 622859 - Batch 6699/7702 - Train loss: 0.00187654  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.81 min) Epoch 81/300 -- Iteration 622936 - Batch 6776/7702 - Train loss: 0.00187696  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.83 min) Epoch 81/300 -- Iteration 623013 - Batch 6853/7702 - Train loss: 0.00187660  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.85 min) Epoch 81/300 -- Iteration 623090 - Batch 6930/7702 - Train loss: 0.00187661  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.87 min) Epoch 81/300 -- Iteration 623167 - Batch 7007/7702 - Train loss: 0.00187634  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.89 min) Epoch 81/300 -- Iteration 623244 - Batch 7084/7702 - Train loss: 0.00187634  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.91 min) Epoch 81/300 -- Iteration 623321 - Batch 7161/7702 - Train loss: 0.00187631  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.93 min) Epoch 81/300 -- Iteration 623398 - Batch 7238/7702 - Train loss: 0.00187641  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.95 min) Epoch 81/300 -- Iteration 623475 - Batch 7315/7702 - Train loss: 0.00187655  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.97 min) Epoch 81/300 -- Iteration 623552 - Batch 7392/7702 - Train loss: 0.00187592  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(167.99 min) Epoch 81/300 -- Iteration 623629 - Batch 7469/7702 - Train loss: 0.00187607  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(168.01 min) Epoch 81/300 -- Iteration 623706 - Batch 7546/7702 - Train loss: 0.00187599  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(168.03 min) Epoch 81/300 -- Iteration 623783 - Batch 7623/7702 - Train loss: 0.00187583  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(168.06 min) Epoch 81/300 -- Iteration 623860 - Batch 7700/7702 - Train loss: 0.00187588  - Train acc: -0.0000 - Val loss: 0.00048990\n",
      "(168.06 min) Epoch 81/300 -- Iteration 623862 - Batch 7701/7702 - Train loss: 0.00187585  - Train acc: -0.0000 - Val loss: 0.00036612 - Val acc: -0.0000\n",
      "(168.08 min) Epoch 82/300 -- Iteration 623939 - Batch 77/7702 - Train loss: 0.00192172  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.10 min) Epoch 82/300 -- Iteration 624016 - Batch 154/7702 - Train loss: 0.00189639  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.12 min) Epoch 82/300 -- Iteration 624093 - Batch 231/7702 - Train loss: 0.00188286  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.14 min) Epoch 82/300 -- Iteration 624170 - Batch 308/7702 - Train loss: 0.00188793  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.16 min) Epoch 82/300 -- Iteration 624247 - Batch 385/7702 - Train loss: 0.00188633  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.18 min) Epoch 82/300 -- Iteration 624324 - Batch 462/7702 - Train loss: 0.00188802  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.20 min) Epoch 82/300 -- Iteration 624401 - Batch 539/7702 - Train loss: 0.00188674  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.22 min) Epoch 82/300 -- Iteration 624478 - Batch 616/7702 - Train loss: 0.00188453  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.24 min) Epoch 82/300 -- Iteration 624555 - Batch 693/7702 - Train loss: 0.00187898  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.27 min) Epoch 82/300 -- Iteration 624632 - Batch 770/7702 - Train loss: 0.00188100  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.29 min) Epoch 82/300 -- Iteration 624709 - Batch 847/7702 - Train loss: 0.00188028  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.31 min) Epoch 82/300 -- Iteration 624786 - Batch 924/7702 - Train loss: 0.00187950  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.33 min) Epoch 82/300 -- Iteration 624863 - Batch 1001/7702 - Train loss: 0.00187926  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.35 min) Epoch 82/300 -- Iteration 624940 - Batch 1078/7702 - Train loss: 0.00187619  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.37 min) Epoch 82/300 -- Iteration 625017 - Batch 1155/7702 - Train loss: 0.00187497  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.39 min) Epoch 82/300 -- Iteration 625094 - Batch 1232/7702 - Train loss: 0.00187457  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.41 min) Epoch 82/300 -- Iteration 625171 - Batch 1309/7702 - Train loss: 0.00187398  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.43 min) Epoch 82/300 -- Iteration 625248 - Batch 1386/7702 - Train loss: 0.00187403  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.45 min) Epoch 82/300 -- Iteration 625325 - Batch 1463/7702 - Train loss: 0.00187341  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.47 min) Epoch 82/300 -- Iteration 625402 - Batch 1540/7702 - Train loss: 0.00187449  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.49 min) Epoch 82/300 -- Iteration 625479 - Batch 1617/7702 - Train loss: 0.00187439  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.51 min) Epoch 82/300 -- Iteration 625556 - Batch 1694/7702 - Train loss: 0.00187420  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.54 min) Epoch 82/300 -- Iteration 625633 - Batch 1771/7702 - Train loss: 0.00187336  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.56 min) Epoch 82/300 -- Iteration 625710 - Batch 1848/7702 - Train loss: 0.00187336  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.58 min) Epoch 82/300 -- Iteration 625787 - Batch 1925/7702 - Train loss: 0.00187260  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.60 min) Epoch 82/300 -- Iteration 625864 - Batch 2002/7702 - Train loss: 0.00187354  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.62 min) Epoch 82/300 -- Iteration 625941 - Batch 2079/7702 - Train loss: 0.00187298  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.64 min) Epoch 82/300 -- Iteration 626018 - Batch 2156/7702 - Train loss: 0.00187308  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.66 min) Epoch 82/300 -- Iteration 626095 - Batch 2233/7702 - Train loss: 0.00187347  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.68 min) Epoch 82/300 -- Iteration 626172 - Batch 2310/7702 - Train loss: 0.00187353  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.70 min) Epoch 82/300 -- Iteration 626249 - Batch 2387/7702 - Train loss: 0.00187294  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.72 min) Epoch 82/300 -- Iteration 626326 - Batch 2464/7702 - Train loss: 0.00187316  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.74 min) Epoch 82/300 -- Iteration 626403 - Batch 2541/7702 - Train loss: 0.00187399  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.76 min) Epoch 82/300 -- Iteration 626480 - Batch 2618/7702 - Train loss: 0.00187476  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.78 min) Epoch 82/300 -- Iteration 626557 - Batch 2695/7702 - Train loss: 0.00187490  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.80 min) Epoch 82/300 -- Iteration 626634 - Batch 2772/7702 - Train loss: 0.00187502  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.83 min) Epoch 82/300 -- Iteration 626711 - Batch 2849/7702 - Train loss: 0.00187536  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.85 min) Epoch 82/300 -- Iteration 626788 - Batch 2926/7702 - Train loss: 0.00187527  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.87 min) Epoch 82/300 -- Iteration 626865 - Batch 3003/7702 - Train loss: 0.00187506  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.89 min) Epoch 82/300 -- Iteration 626942 - Batch 3080/7702 - Train loss: 0.00187684  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.91 min) Epoch 82/300 -- Iteration 627019 - Batch 3157/7702 - Train loss: 0.00187679  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.93 min) Epoch 82/300 -- Iteration 627096 - Batch 3234/7702 - Train loss: 0.00187667  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.95 min) Epoch 82/300 -- Iteration 627173 - Batch 3311/7702 - Train loss: 0.00187595  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.97 min) Epoch 82/300 -- Iteration 627250 - Batch 3388/7702 - Train loss: 0.00187639  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(168.99 min) Epoch 82/300 -- Iteration 627327 - Batch 3465/7702 - Train loss: 0.00187640  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.01 min) Epoch 82/300 -- Iteration 627404 - Batch 3542/7702 - Train loss: 0.00187547  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.03 min) Epoch 82/300 -- Iteration 627481 - Batch 3619/7702 - Train loss: 0.00187581  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.05 min) Epoch 82/300 -- Iteration 627558 - Batch 3696/7702 - Train loss: 0.00187674  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.07 min) Epoch 82/300 -- Iteration 627635 - Batch 3773/7702 - Train loss: 0.00187694  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.10 min) Epoch 82/300 -- Iteration 627712 - Batch 3850/7702 - Train loss: 0.00187673  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.12 min) Epoch 82/300 -- Iteration 627789 - Batch 3927/7702 - Train loss: 0.00187658  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.14 min) Epoch 82/300 -- Iteration 627866 - Batch 4004/7702 - Train loss: 0.00187592  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.16 min) Epoch 82/300 -- Iteration 627943 - Batch 4081/7702 - Train loss: 0.00187583  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.18 min) Epoch 82/300 -- Iteration 628020 - Batch 4158/7702 - Train loss: 0.00187610  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.20 min) Epoch 82/300 -- Iteration 628097 - Batch 4235/7702 - Train loss: 0.00187655  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.22 min) Epoch 82/300 -- Iteration 628174 - Batch 4312/7702 - Train loss: 0.00187590  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.24 min) Epoch 82/300 -- Iteration 628251 - Batch 4389/7702 - Train loss: 0.00187581  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.26 min) Epoch 82/300 -- Iteration 628328 - Batch 4466/7702 - Train loss: 0.00187518  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.28 min) Epoch 82/300 -- Iteration 628405 - Batch 4543/7702 - Train loss: 0.00187547  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.30 min) Epoch 82/300 -- Iteration 628482 - Batch 4620/7702 - Train loss: 0.00187624  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.32 min) Epoch 82/300 -- Iteration 628559 - Batch 4697/7702 - Train loss: 0.00187643  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.34 min) Epoch 82/300 -- Iteration 628636 - Batch 4774/7702 - Train loss: 0.00187647  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.36 min) Epoch 82/300 -- Iteration 628713 - Batch 4851/7702 - Train loss: 0.00187669  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.39 min) Epoch 82/300 -- Iteration 628790 - Batch 4928/7702 - Train loss: 0.00187721  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.41 min) Epoch 82/300 -- Iteration 628867 - Batch 5005/7702 - Train loss: 0.00187729  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.43 min) Epoch 82/300 -- Iteration 628944 - Batch 5082/7702 - Train loss: 0.00187735  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.45 min) Epoch 82/300 -- Iteration 629021 - Batch 5159/7702 - Train loss: 0.00187731  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.47 min) Epoch 82/300 -- Iteration 629098 - Batch 5236/7702 - Train loss: 0.00187709  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.49 min) Epoch 82/300 -- Iteration 629175 - Batch 5313/7702 - Train loss: 0.00187705  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.51 min) Epoch 82/300 -- Iteration 629252 - Batch 5390/7702 - Train loss: 0.00187727  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.53 min) Epoch 82/300 -- Iteration 629329 - Batch 5467/7702 - Train loss: 0.00187764  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.55 min) Epoch 82/300 -- Iteration 629406 - Batch 5544/7702 - Train loss: 0.00187905  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.57 min) Epoch 82/300 -- Iteration 629483 - Batch 5621/7702 - Train loss: 0.00187915  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.59 min) Epoch 82/300 -- Iteration 629560 - Batch 5698/7702 - Train loss: 0.00187905  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.61 min) Epoch 82/300 -- Iteration 629637 - Batch 5775/7702 - Train loss: 0.00187911  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.63 min) Epoch 82/300 -- Iteration 629714 - Batch 5852/7702 - Train loss: 0.00187890  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.65 min) Epoch 82/300 -- Iteration 629791 - Batch 5929/7702 - Train loss: 0.00187912  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.67 min) Epoch 82/300 -- Iteration 629868 - Batch 6006/7702 - Train loss: 0.00187938  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.70 min) Epoch 82/300 -- Iteration 629945 - Batch 6083/7702 - Train loss: 0.00187978  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.72 min) Epoch 82/300 -- Iteration 630022 - Batch 6160/7702 - Train loss: 0.00188000  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.74 min) Epoch 82/300 -- Iteration 630099 - Batch 6237/7702 - Train loss: 0.00188008  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.76 min) Epoch 82/300 -- Iteration 630176 - Batch 6314/7702 - Train loss: 0.00187988  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.78 min) Epoch 82/300 -- Iteration 630253 - Batch 6391/7702 - Train loss: 0.00188026  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.80 min) Epoch 82/300 -- Iteration 630330 - Batch 6468/7702 - Train loss: 0.00188021  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.82 min) Epoch 82/300 -- Iteration 630407 - Batch 6545/7702 - Train loss: 0.00188023  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.84 min) Epoch 82/300 -- Iteration 630484 - Batch 6622/7702 - Train loss: 0.00188027  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.86 min) Epoch 82/300 -- Iteration 630561 - Batch 6699/7702 - Train loss: 0.00188025  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.88 min) Epoch 82/300 -- Iteration 630638 - Batch 6776/7702 - Train loss: 0.00188035  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.90 min) Epoch 82/300 -- Iteration 630715 - Batch 6853/7702 - Train loss: 0.00188037  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.92 min) Epoch 82/300 -- Iteration 630792 - Batch 6930/7702 - Train loss: 0.00188066  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.94 min) Epoch 82/300 -- Iteration 630869 - Batch 7007/7702 - Train loss: 0.00188040  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.96 min) Epoch 82/300 -- Iteration 630946 - Batch 7084/7702 - Train loss: 0.00188014  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(169.99 min) Epoch 82/300 -- Iteration 631023 - Batch 7161/7702 - Train loss: 0.00188006  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(170.01 min) Epoch 82/300 -- Iteration 631100 - Batch 7238/7702 - Train loss: 0.00188053  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(170.03 min) Epoch 82/300 -- Iteration 631177 - Batch 7315/7702 - Train loss: 0.00188056  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(170.05 min) Epoch 82/300 -- Iteration 631254 - Batch 7392/7702 - Train loss: 0.00188029  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(170.07 min) Epoch 82/300 -- Iteration 631331 - Batch 7469/7702 - Train loss: 0.00188035  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(170.09 min) Epoch 82/300 -- Iteration 631408 - Batch 7546/7702 - Train loss: 0.00188033  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(170.11 min) Epoch 82/300 -- Iteration 631485 - Batch 7623/7702 - Train loss: 0.00188005  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(170.13 min) Epoch 82/300 -- Iteration 631562 - Batch 7700/7702 - Train loss: 0.00188028  - Train acc: -0.0000 - Val loss: 0.00036612\n",
      "(170.13 min) Epoch 82/300 -- Iteration 631564 - Batch 7701/7702 - Train loss: 0.00188028  - Train acc: -0.0000 - Val loss: 0.00041104 - Val acc: -0.0000\n",
      "(170.15 min) Epoch 83/300 -- Iteration 631641 - Batch 77/7702 - Train loss: 0.00186240  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.18 min) Epoch 83/300 -- Iteration 631718 - Batch 154/7702 - Train loss: 0.00185770  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.20 min) Epoch 83/300 -- Iteration 631795 - Batch 231/7702 - Train loss: 0.00185876  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.22 min) Epoch 83/300 -- Iteration 631872 - Batch 308/7702 - Train loss: 0.00186171  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.24 min) Epoch 83/300 -- Iteration 631949 - Batch 385/7702 - Train loss: 0.00188494  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.26 min) Epoch 83/300 -- Iteration 632026 - Batch 462/7702 - Train loss: 0.00188471  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.28 min) Epoch 83/300 -- Iteration 632103 - Batch 539/7702 - Train loss: 0.00188394  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.30 min) Epoch 83/300 -- Iteration 632180 - Batch 616/7702 - Train loss: 0.00188111  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.32 min) Epoch 83/300 -- Iteration 632257 - Batch 693/7702 - Train loss: 0.00187958  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.34 min) Epoch 83/300 -- Iteration 632334 - Batch 770/7702 - Train loss: 0.00188055  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.36 min) Epoch 83/300 -- Iteration 632411 - Batch 847/7702 - Train loss: 0.00187695  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.38 min) Epoch 83/300 -- Iteration 632488 - Batch 924/7702 - Train loss: 0.00187592  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.40 min) Epoch 83/300 -- Iteration 632565 - Batch 1001/7702 - Train loss: 0.00187573  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.43 min) Epoch 83/300 -- Iteration 632642 - Batch 1078/7702 - Train loss: 0.00187562  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.45 min) Epoch 83/300 -- Iteration 632719 - Batch 1155/7702 - Train loss: 0.00187479  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.47 min) Epoch 83/300 -- Iteration 632796 - Batch 1232/7702 - Train loss: 0.00187416  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.49 min) Epoch 83/300 -- Iteration 632873 - Batch 1309/7702 - Train loss: 0.00187428  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.51 min) Epoch 83/300 -- Iteration 632950 - Batch 1386/7702 - Train loss: 0.00187211  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.53 min) Epoch 83/300 -- Iteration 633027 - Batch 1463/7702 - Train loss: 0.00187261  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.55 min) Epoch 83/300 -- Iteration 633104 - Batch 1540/7702 - Train loss: 0.00187272  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.57 min) Epoch 83/300 -- Iteration 633181 - Batch 1617/7702 - Train loss: 0.00187296  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.59 min) Epoch 83/300 -- Iteration 633258 - Batch 1694/7702 - Train loss: 0.00187243  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.61 min) Epoch 83/300 -- Iteration 633335 - Batch 1771/7702 - Train loss: 0.00187260  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.63 min) Epoch 83/300 -- Iteration 633412 - Batch 1848/7702 - Train loss: 0.00187252  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.65 min) Epoch 83/300 -- Iteration 633489 - Batch 1925/7702 - Train loss: 0.00187290  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.67 min) Epoch 83/300 -- Iteration 633566 - Batch 2002/7702 - Train loss: 0.00187365  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.70 min) Epoch 83/300 -- Iteration 633643 - Batch 2079/7702 - Train loss: 0.00187405  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.72 min) Epoch 83/300 -- Iteration 633720 - Batch 2156/7702 - Train loss: 0.00187414  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.74 min) Epoch 83/300 -- Iteration 633797 - Batch 2233/7702 - Train loss: 0.00187451  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.76 min) Epoch 83/300 -- Iteration 633874 - Batch 2310/7702 - Train loss: 0.00187522  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.78 min) Epoch 83/300 -- Iteration 633951 - Batch 2387/7702 - Train loss: 0.00187420  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.80 min) Epoch 83/300 -- Iteration 634028 - Batch 2464/7702 - Train loss: 0.00187415  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.82 min) Epoch 83/300 -- Iteration 634105 - Batch 2541/7702 - Train loss: 0.00187408  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.84 min) Epoch 83/300 -- Iteration 634182 - Batch 2618/7702 - Train loss: 0.00187515  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.86 min) Epoch 83/300 -- Iteration 634259 - Batch 2695/7702 - Train loss: 0.00187641  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.88 min) Epoch 83/300 -- Iteration 634336 - Batch 2772/7702 - Train loss: 0.00187714  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.90 min) Epoch 83/300 -- Iteration 634413 - Batch 2849/7702 - Train loss: 0.00187641  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.92 min) Epoch 83/300 -- Iteration 634490 - Batch 2926/7702 - Train loss: 0.00187617  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.94 min) Epoch 83/300 -- Iteration 634567 - Batch 3003/7702 - Train loss: 0.00187601  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.97 min) Epoch 83/300 -- Iteration 634644 - Batch 3080/7702 - Train loss: 0.00187715  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(170.99 min) Epoch 83/300 -- Iteration 634721 - Batch 3157/7702 - Train loss: 0.00187727  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.01 min) Epoch 83/300 -- Iteration 634798 - Batch 3234/7702 - Train loss: 0.00187695  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.03 min) Epoch 83/300 -- Iteration 634875 - Batch 3311/7702 - Train loss: 0.00187694  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.05 min) Epoch 83/300 -- Iteration 634952 - Batch 3388/7702 - Train loss: 0.00187666  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.07 min) Epoch 83/300 -- Iteration 635029 - Batch 3465/7702 - Train loss: 0.00187631  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.09 min) Epoch 83/300 -- Iteration 635106 - Batch 3542/7702 - Train loss: 0.00187633  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.11 min) Epoch 83/300 -- Iteration 635183 - Batch 3619/7702 - Train loss: 0.00187672  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.13 min) Epoch 83/300 -- Iteration 635260 - Batch 3696/7702 - Train loss: 0.00187701  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.15 min) Epoch 83/300 -- Iteration 635337 - Batch 3773/7702 - Train loss: 0.00187638  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.17 min) Epoch 83/300 -- Iteration 635414 - Batch 3850/7702 - Train loss: 0.00187675  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.19 min) Epoch 83/300 -- Iteration 635491 - Batch 3927/7702 - Train loss: 0.00187703  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.21 min) Epoch 83/300 -- Iteration 635568 - Batch 4004/7702 - Train loss: 0.00187655  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.24 min) Epoch 83/300 -- Iteration 635645 - Batch 4081/7702 - Train loss: 0.00187671  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.26 min) Epoch 83/300 -- Iteration 635722 - Batch 4158/7702 - Train loss: 0.00187644  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.28 min) Epoch 83/300 -- Iteration 635799 - Batch 4235/7702 - Train loss: 0.00187666  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.30 min) Epoch 83/300 -- Iteration 635876 - Batch 4312/7702 - Train loss: 0.00187649  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.32 min) Epoch 83/300 -- Iteration 635953 - Batch 4389/7702 - Train loss: 0.00187617  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.34 min) Epoch 83/300 -- Iteration 636030 - Batch 4466/7702 - Train loss: 0.00187612  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.36 min) Epoch 83/300 -- Iteration 636107 - Batch 4543/7702 - Train loss: 0.00187611  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.38 min) Epoch 83/300 -- Iteration 636184 - Batch 4620/7702 - Train loss: 0.00187596  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.40 min) Epoch 83/300 -- Iteration 636261 - Batch 4697/7702 - Train loss: 0.00187546  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.42 min) Epoch 83/300 -- Iteration 636338 - Batch 4774/7702 - Train loss: 0.00187546  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.44 min) Epoch 83/300 -- Iteration 636415 - Batch 4851/7702 - Train loss: 0.00187503  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.46 min) Epoch 83/300 -- Iteration 636492 - Batch 4928/7702 - Train loss: 0.00187530  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.48 min) Epoch 83/300 -- Iteration 636569 - Batch 5005/7702 - Train loss: 0.00187547  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.51 min) Epoch 83/300 -- Iteration 636646 - Batch 5082/7702 - Train loss: 0.00187589  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.53 min) Epoch 83/300 -- Iteration 636723 - Batch 5159/7702 - Train loss: 0.00187550  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.55 min) Epoch 83/300 -- Iteration 636800 - Batch 5236/7702 - Train loss: 0.00187558  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.57 min) Epoch 83/300 -- Iteration 636877 - Batch 5313/7702 - Train loss: 0.00187567  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.59 min) Epoch 83/300 -- Iteration 636954 - Batch 5390/7702 - Train loss: 0.00187518  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.61 min) Epoch 83/300 -- Iteration 637031 - Batch 5467/7702 - Train loss: 0.00187507  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.63 min) Epoch 83/300 -- Iteration 637108 - Batch 5544/7702 - Train loss: 0.00187522  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.65 min) Epoch 83/300 -- Iteration 637185 - Batch 5621/7702 - Train loss: 0.00187516  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.67 min) Epoch 83/300 -- Iteration 637262 - Batch 5698/7702 - Train loss: 0.00187558  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.69 min) Epoch 83/300 -- Iteration 637339 - Batch 5775/7702 - Train loss: 0.00187559  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.71 min) Epoch 83/300 -- Iteration 637416 - Batch 5852/7702 - Train loss: 0.00187552  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.73 min) Epoch 83/300 -- Iteration 637493 - Batch 5929/7702 - Train loss: 0.00187601  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.76 min) Epoch 83/300 -- Iteration 637570 - Batch 6006/7702 - Train loss: 0.00187603  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.78 min) Epoch 83/300 -- Iteration 637647 - Batch 6083/7702 - Train loss: 0.00187580  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.80 min) Epoch 83/300 -- Iteration 637724 - Batch 6160/7702 - Train loss: 0.00187590  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.82 min) Epoch 83/300 -- Iteration 637801 - Batch 6237/7702 - Train loss: 0.00187597  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.84 min) Epoch 83/300 -- Iteration 637878 - Batch 6314/7702 - Train loss: 0.00187584  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.86 min) Epoch 83/300 -- Iteration 637955 - Batch 6391/7702 - Train loss: 0.00187590  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.88 min) Epoch 83/300 -- Iteration 638032 - Batch 6468/7702 - Train loss: 0.00187566  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.90 min) Epoch 83/300 -- Iteration 638109 - Batch 6545/7702 - Train loss: 0.00187614  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.92 min) Epoch 83/300 -- Iteration 638186 - Batch 6622/7702 - Train loss: 0.00187656  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.94 min) Epoch 83/300 -- Iteration 638263 - Batch 6699/7702 - Train loss: 0.00187638  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.96 min) Epoch 83/300 -- Iteration 638340 - Batch 6776/7702 - Train loss: 0.00187650  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(171.98 min) Epoch 83/300 -- Iteration 638417 - Batch 6853/7702 - Train loss: 0.00187683  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.00 min) Epoch 83/300 -- Iteration 638494 - Batch 6930/7702 - Train loss: 0.00187682  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.02 min) Epoch 83/300 -- Iteration 638571 - Batch 7007/7702 - Train loss: 0.00187685  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.05 min) Epoch 83/300 -- Iteration 638648 - Batch 7084/7702 - Train loss: 0.00187678  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.07 min) Epoch 83/300 -- Iteration 638725 - Batch 7161/7702 - Train loss: 0.00187648  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.09 min) Epoch 83/300 -- Iteration 638802 - Batch 7238/7702 - Train loss: 0.00187638  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.11 min) Epoch 83/300 -- Iteration 638879 - Batch 7315/7702 - Train loss: 0.00187610  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.13 min) Epoch 83/300 -- Iteration 638956 - Batch 7392/7702 - Train loss: 0.00187584  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.15 min) Epoch 83/300 -- Iteration 639033 - Batch 7469/7702 - Train loss: 0.00187543  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.17 min) Epoch 83/300 -- Iteration 639110 - Batch 7546/7702 - Train loss: 0.00187491  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.19 min) Epoch 83/300 -- Iteration 639187 - Batch 7623/7702 - Train loss: 0.00187491  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.21 min) Epoch 83/300 -- Iteration 639264 - Batch 7700/7702 - Train loss: 0.00187495  - Train acc: -0.0000 - Val loss: 0.00041104\n",
      "(172.21 min) Epoch 83/300 -- Iteration 639266 - Batch 7701/7702 - Train loss: 0.00187501  - Train acc: -0.0000 - Val loss: 0.00045058 - Val acc: -0.0000\n",
      "(172.23 min) Epoch 84/300 -- Iteration 639343 - Batch 77/7702 - Train loss: 0.00189345  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.26 min) Epoch 84/300 -- Iteration 639420 - Batch 154/7702 - Train loss: 0.00187686  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.28 min) Epoch 84/300 -- Iteration 639497 - Batch 231/7702 - Train loss: 0.00187335  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.30 min) Epoch 84/300 -- Iteration 639574 - Batch 308/7702 - Train loss: 0.00188124  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.32 min) Epoch 84/300 -- Iteration 639651 - Batch 385/7702 - Train loss: 0.00187513  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.34 min) Epoch 84/300 -- Iteration 639728 - Batch 462/7702 - Train loss: 0.00187985  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.36 min) Epoch 84/300 -- Iteration 639805 - Batch 539/7702 - Train loss: 0.00187531  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.38 min) Epoch 84/300 -- Iteration 639882 - Batch 616/7702 - Train loss: 0.00187914  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.40 min) Epoch 84/300 -- Iteration 639959 - Batch 693/7702 - Train loss: 0.00187950  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.42 min) Epoch 84/300 -- Iteration 640036 - Batch 770/7702 - Train loss: 0.00187753  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.44 min) Epoch 84/300 -- Iteration 640113 - Batch 847/7702 - Train loss: 0.00187830  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.46 min) Epoch 84/300 -- Iteration 640190 - Batch 924/7702 - Train loss: 0.00187491  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.48 min) Epoch 84/300 -- Iteration 640267 - Batch 1001/7702 - Train loss: 0.00187437  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.50 min) Epoch 84/300 -- Iteration 640344 - Batch 1078/7702 - Train loss: 0.00187302  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.52 min) Epoch 84/300 -- Iteration 640421 - Batch 1155/7702 - Train loss: 0.00187429  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.55 min) Epoch 84/300 -- Iteration 640498 - Batch 1232/7702 - Train loss: 0.00187395  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.57 min) Epoch 84/300 -- Iteration 640575 - Batch 1309/7702 - Train loss: 0.00187337  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.59 min) Epoch 84/300 -- Iteration 640652 - Batch 1386/7702 - Train loss: 0.00187274  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.61 min) Epoch 84/300 -- Iteration 640729 - Batch 1463/7702 - Train loss: 0.00187458  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.63 min) Epoch 84/300 -- Iteration 640806 - Batch 1540/7702 - Train loss: 0.00187375  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.65 min) Epoch 84/300 -- Iteration 640883 - Batch 1617/7702 - Train loss: 0.00187430  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.67 min) Epoch 84/300 -- Iteration 640960 - Batch 1694/7702 - Train loss: 0.00187333  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.69 min) Epoch 84/300 -- Iteration 641037 - Batch 1771/7702 - Train loss: 0.00187327  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.71 min) Epoch 84/300 -- Iteration 641114 - Batch 1848/7702 - Train loss: 0.00187236  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.73 min) Epoch 84/300 -- Iteration 641191 - Batch 1925/7702 - Train loss: 0.00187290  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.75 min) Epoch 84/300 -- Iteration 641268 - Batch 2002/7702 - Train loss: 0.00187243  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.77 min) Epoch 84/300 -- Iteration 641345 - Batch 2079/7702 - Train loss: 0.00187171  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.79 min) Epoch 84/300 -- Iteration 641422 - Batch 2156/7702 - Train loss: 0.00187095  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.81 min) Epoch 84/300 -- Iteration 641499 - Batch 2233/7702 - Train loss: 0.00186910  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.84 min) Epoch 84/300 -- Iteration 641576 - Batch 2310/7702 - Train loss: 0.00186964  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.86 min) Epoch 84/300 -- Iteration 641653 - Batch 2387/7702 - Train loss: 0.00186995  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.88 min) Epoch 84/300 -- Iteration 641730 - Batch 2464/7702 - Train loss: 0.00186974  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.90 min) Epoch 84/300 -- Iteration 641807 - Batch 2541/7702 - Train loss: 0.00187010  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.92 min) Epoch 84/300 -- Iteration 641884 - Batch 2618/7702 - Train loss: 0.00187083  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.94 min) Epoch 84/300 -- Iteration 641961 - Batch 2695/7702 - Train loss: 0.00187132  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.96 min) Epoch 84/300 -- Iteration 642038 - Batch 2772/7702 - Train loss: 0.00187096  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(172.98 min) Epoch 84/300 -- Iteration 642115 - Batch 2849/7702 - Train loss: 0.00187100  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.00 min) Epoch 84/300 -- Iteration 642192 - Batch 2926/7702 - Train loss: 0.00187086  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.02 min) Epoch 84/300 -- Iteration 642269 - Batch 3003/7702 - Train loss: 0.00187105  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.04 min) Epoch 84/300 -- Iteration 642346 - Batch 3080/7702 - Train loss: 0.00187235  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.06 min) Epoch 84/300 -- Iteration 642423 - Batch 3157/7702 - Train loss: 0.00187185  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.08 min) Epoch 84/300 -- Iteration 642500 - Batch 3234/7702 - Train loss: 0.00187181  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.11 min) Epoch 84/300 -- Iteration 642577 - Batch 3311/7702 - Train loss: 0.00187133  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.13 min) Epoch 84/300 -- Iteration 642654 - Batch 3388/7702 - Train loss: 0.00187119  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.15 min) Epoch 84/300 -- Iteration 642731 - Batch 3465/7702 - Train loss: 0.00187107  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.17 min) Epoch 84/300 -- Iteration 642808 - Batch 3542/7702 - Train loss: 0.00187135  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.19 min) Epoch 84/300 -- Iteration 642885 - Batch 3619/7702 - Train loss: 0.00187161  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.21 min) Epoch 84/300 -- Iteration 642962 - Batch 3696/7702 - Train loss: 0.00187158  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.23 min) Epoch 84/300 -- Iteration 643039 - Batch 3773/7702 - Train loss: 0.00187230  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.25 min) Epoch 84/300 -- Iteration 643116 - Batch 3850/7702 - Train loss: 0.00187283  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.27 min) Epoch 84/300 -- Iteration 643193 - Batch 3927/7702 - Train loss: 0.00187352  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.29 min) Epoch 84/300 -- Iteration 643270 - Batch 4004/7702 - Train loss: 0.00187401  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.31 min) Epoch 84/300 -- Iteration 643347 - Batch 4081/7702 - Train loss: 0.00187328  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.33 min) Epoch 84/300 -- Iteration 643424 - Batch 4158/7702 - Train loss: 0.00187294  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.35 min) Epoch 84/300 -- Iteration 643501 - Batch 4235/7702 - Train loss: 0.00187239  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.37 min) Epoch 84/300 -- Iteration 643578 - Batch 4312/7702 - Train loss: 0.00187232  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.40 min) Epoch 84/300 -- Iteration 643655 - Batch 4389/7702 - Train loss: 0.00187237  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.42 min) Epoch 84/300 -- Iteration 643732 - Batch 4466/7702 - Train loss: 0.00187203  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.44 min) Epoch 84/300 -- Iteration 643809 - Batch 4543/7702 - Train loss: 0.00187214  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.46 min) Epoch 84/300 -- Iteration 643886 - Batch 4620/7702 - Train loss: 0.00187241  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.48 min) Epoch 84/300 -- Iteration 643963 - Batch 4697/7702 - Train loss: 0.00187223  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.50 min) Epoch 84/300 -- Iteration 644040 - Batch 4774/7702 - Train loss: 0.00187236  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.52 min) Epoch 84/300 -- Iteration 644117 - Batch 4851/7702 - Train loss: 0.00187215  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.54 min) Epoch 84/300 -- Iteration 644194 - Batch 4928/7702 - Train loss: 0.00187200  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.56 min) Epoch 84/300 -- Iteration 644271 - Batch 5005/7702 - Train loss: 0.00187173  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.58 min) Epoch 84/300 -- Iteration 644348 - Batch 5082/7702 - Train loss: 0.00187162  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.60 min) Epoch 84/300 -- Iteration 644425 - Batch 5159/7702 - Train loss: 0.00187106  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.62 min) Epoch 84/300 -- Iteration 644502 - Batch 5236/7702 - Train loss: 0.00187024  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.64 min) Epoch 84/300 -- Iteration 644579 - Batch 5313/7702 - Train loss: 0.00187001  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.66 min) Epoch 84/300 -- Iteration 644656 - Batch 5390/7702 - Train loss: 0.00186991  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.68 min) Epoch 84/300 -- Iteration 644733 - Batch 5467/7702 - Train loss: 0.00187000  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.71 min) Epoch 84/300 -- Iteration 644810 - Batch 5544/7702 - Train loss: 0.00187041  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.73 min) Epoch 84/300 -- Iteration 644887 - Batch 5621/7702 - Train loss: 0.00187061  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.75 min) Epoch 84/300 -- Iteration 644964 - Batch 5698/7702 - Train loss: 0.00187066  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.77 min) Epoch 84/300 -- Iteration 645041 - Batch 5775/7702 - Train loss: 0.00187059  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.79 min) Epoch 84/300 -- Iteration 645118 - Batch 5852/7702 - Train loss: 0.00187070  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.81 min) Epoch 84/300 -- Iteration 645195 - Batch 5929/7702 - Train loss: 0.00187117  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.83 min) Epoch 84/300 -- Iteration 645272 - Batch 6006/7702 - Train loss: 0.00187091  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.85 min) Epoch 84/300 -- Iteration 645349 - Batch 6083/7702 - Train loss: 0.00187059  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.87 min) Epoch 84/300 -- Iteration 645426 - Batch 6160/7702 - Train loss: 0.00187087  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.89 min) Epoch 84/300 -- Iteration 645503 - Batch 6237/7702 - Train loss: 0.00187125  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.91 min) Epoch 84/300 -- Iteration 645580 - Batch 6314/7702 - Train loss: 0.00187138  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.93 min) Epoch 84/300 -- Iteration 645657 - Batch 6391/7702 - Train loss: 0.00187093  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.95 min) Epoch 84/300 -- Iteration 645734 - Batch 6468/7702 - Train loss: 0.00187088  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(173.97 min) Epoch 84/300 -- Iteration 645811 - Batch 6545/7702 - Train loss: 0.00187099  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.00 min) Epoch 84/300 -- Iteration 645888 - Batch 6622/7702 - Train loss: 0.00187078  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.02 min) Epoch 84/300 -- Iteration 645965 - Batch 6699/7702 - Train loss: 0.00187118  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.04 min) Epoch 84/300 -- Iteration 646042 - Batch 6776/7702 - Train loss: 0.00187117  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.06 min) Epoch 84/300 -- Iteration 646119 - Batch 6853/7702 - Train loss: 0.00187112  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.08 min) Epoch 84/300 -- Iteration 646196 - Batch 6930/7702 - Train loss: 0.00187103  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.10 min) Epoch 84/300 -- Iteration 646273 - Batch 7007/7702 - Train loss: 0.00187049  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.12 min) Epoch 84/300 -- Iteration 646350 - Batch 7084/7702 - Train loss: 0.00187063  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.14 min) Epoch 84/300 -- Iteration 646427 - Batch 7161/7702 - Train loss: 0.00187046  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.16 min) Epoch 84/300 -- Iteration 646504 - Batch 7238/7702 - Train loss: 0.00187051  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.18 min) Epoch 84/300 -- Iteration 646581 - Batch 7315/7702 - Train loss: 0.00187015  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.20 min) Epoch 84/300 -- Iteration 646658 - Batch 7392/7702 - Train loss: 0.00186994  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.22 min) Epoch 84/300 -- Iteration 646735 - Batch 7469/7702 - Train loss: 0.00186998  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.24 min) Epoch 84/300 -- Iteration 646812 - Batch 7546/7702 - Train loss: 0.00186999  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.26 min) Epoch 84/300 -- Iteration 646889 - Batch 7623/7702 - Train loss: 0.00187033  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.29 min) Epoch 84/300 -- Iteration 646966 - Batch 7700/7702 - Train loss: 0.00187036  - Train acc: -0.0000 - Val loss: 0.00045058\n",
      "(174.29 min) Epoch 84/300 -- Iteration 646968 - Batch 7701/7702 - Train loss: 0.00187035  - Train acc: -0.0000 - Val loss: 0.00033968 - Val acc: -0.0000\n",
      "(174.31 min) Epoch 85/300 -- Iteration 647045 - Batch 77/7702 - Train loss: 0.00186412  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.33 min) Epoch 85/300 -- Iteration 647122 - Batch 154/7702 - Train loss: 0.00187980  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.35 min) Epoch 85/300 -- Iteration 647199 - Batch 231/7702 - Train loss: 0.00186032  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.37 min) Epoch 85/300 -- Iteration 647276 - Batch 308/7702 - Train loss: 0.00186641  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.39 min) Epoch 85/300 -- Iteration 647353 - Batch 385/7702 - Train loss: 0.00186432  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.41 min) Epoch 85/300 -- Iteration 647430 - Batch 462/7702 - Train loss: 0.00187462  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.43 min) Epoch 85/300 -- Iteration 647507 - Batch 539/7702 - Train loss: 0.00187581  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.45 min) Epoch 85/300 -- Iteration 647584 - Batch 616/7702 - Train loss: 0.00187749  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.47 min) Epoch 85/300 -- Iteration 647661 - Batch 693/7702 - Train loss: 0.00187906  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.50 min) Epoch 85/300 -- Iteration 647738 - Batch 770/7702 - Train loss: 0.00187762  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.52 min) Epoch 85/300 -- Iteration 647815 - Batch 847/7702 - Train loss: 0.00187358  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.54 min) Epoch 85/300 -- Iteration 647892 - Batch 924/7702 - Train loss: 0.00187350  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.56 min) Epoch 85/300 -- Iteration 647969 - Batch 1001/7702 - Train loss: 0.00187258  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.58 min) Epoch 85/300 -- Iteration 648046 - Batch 1078/7702 - Train loss: 0.00187257  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.60 min) Epoch 85/300 -- Iteration 648123 - Batch 1155/7702 - Train loss: 0.00187261  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.62 min) Epoch 85/300 -- Iteration 648200 - Batch 1232/7702 - Train loss: 0.00187201  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.64 min) Epoch 85/300 -- Iteration 648277 - Batch 1309/7702 - Train loss: 0.00186966  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.66 min) Epoch 85/300 -- Iteration 648354 - Batch 1386/7702 - Train loss: 0.00186839  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.68 min) Epoch 85/300 -- Iteration 648431 - Batch 1463/7702 - Train loss: 0.00187086  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.70 min) Epoch 85/300 -- Iteration 648508 - Batch 1540/7702 - Train loss: 0.00187244  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.72 min) Epoch 85/300 -- Iteration 648585 - Batch 1617/7702 - Train loss: 0.00187418  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.74 min) Epoch 85/300 -- Iteration 648662 - Batch 1694/7702 - Train loss: 0.00187336  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.76 min) Epoch 85/300 -- Iteration 648739 - Batch 1771/7702 - Train loss: 0.00187196  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.79 min) Epoch 85/300 -- Iteration 648816 - Batch 1848/7702 - Train loss: 0.00187207  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.81 min) Epoch 85/300 -- Iteration 648893 - Batch 1925/7702 - Train loss: 0.00187189  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.83 min) Epoch 85/300 -- Iteration 648970 - Batch 2002/7702 - Train loss: 0.00187031  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.85 min) Epoch 85/300 -- Iteration 649047 - Batch 2079/7702 - Train loss: 0.00187102  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.87 min) Epoch 85/300 -- Iteration 649124 - Batch 2156/7702 - Train loss: 0.00187115  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.89 min) Epoch 85/300 -- Iteration 649201 - Batch 2233/7702 - Train loss: 0.00187115  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.91 min) Epoch 85/300 -- Iteration 649278 - Batch 2310/7702 - Train loss: 0.00187152  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.93 min) Epoch 85/300 -- Iteration 649355 - Batch 2387/7702 - Train loss: 0.00187208  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.95 min) Epoch 85/300 -- Iteration 649432 - Batch 2464/7702 - Train loss: 0.00187205  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.97 min) Epoch 85/300 -- Iteration 649509 - Batch 2541/7702 - Train loss: 0.00187276  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(174.99 min) Epoch 85/300 -- Iteration 649586 - Batch 2618/7702 - Train loss: 0.00187262  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.01 min) Epoch 85/300 -- Iteration 649663 - Batch 2695/7702 - Train loss: 0.00187159  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.03 min) Epoch 85/300 -- Iteration 649740 - Batch 2772/7702 - Train loss: 0.00187139  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.05 min) Epoch 85/300 -- Iteration 649817 - Batch 2849/7702 - Train loss: 0.00187211  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.08 min) Epoch 85/300 -- Iteration 649894 - Batch 2926/7702 - Train loss: 0.00187223  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.10 min) Epoch 85/300 -- Iteration 649971 - Batch 3003/7702 - Train loss: 0.00187313  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.12 min) Epoch 85/300 -- Iteration 650048 - Batch 3080/7702 - Train loss: 0.00187394  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.14 min) Epoch 85/300 -- Iteration 650125 - Batch 3157/7702 - Train loss: 0.00187436  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.16 min) Epoch 85/300 -- Iteration 650202 - Batch 3234/7702 - Train loss: 0.00187431  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.18 min) Epoch 85/300 -- Iteration 650279 - Batch 3311/7702 - Train loss: 0.00187430  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.20 min) Epoch 85/300 -- Iteration 650356 - Batch 3388/7702 - Train loss: 0.00187454  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.22 min) Epoch 85/300 -- Iteration 650433 - Batch 3465/7702 - Train loss: 0.00187458  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.24 min) Epoch 85/300 -- Iteration 650510 - Batch 3542/7702 - Train loss: 0.00187483  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.26 min) Epoch 85/300 -- Iteration 650587 - Batch 3619/7702 - Train loss: 0.00187478  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.28 min) Epoch 85/300 -- Iteration 650664 - Batch 3696/7702 - Train loss: 0.00187517  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.30 min) Epoch 85/300 -- Iteration 650741 - Batch 3773/7702 - Train loss: 0.00187455  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.32 min) Epoch 85/300 -- Iteration 650818 - Batch 3850/7702 - Train loss: 0.00187449  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.35 min) Epoch 85/300 -- Iteration 650895 - Batch 3927/7702 - Train loss: 0.00187473  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.37 min) Epoch 85/300 -- Iteration 650972 - Batch 4004/7702 - Train loss: 0.00187425  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.39 min) Epoch 85/300 -- Iteration 651049 - Batch 4081/7702 - Train loss: 0.00187446  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.41 min) Epoch 85/300 -- Iteration 651126 - Batch 4158/7702 - Train loss: 0.00187484  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.43 min) Epoch 85/300 -- Iteration 651203 - Batch 4235/7702 - Train loss: 0.00187518  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.45 min) Epoch 85/300 -- Iteration 651280 - Batch 4312/7702 - Train loss: 0.00187510  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.47 min) Epoch 85/300 -- Iteration 651357 - Batch 4389/7702 - Train loss: 0.00187423  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.49 min) Epoch 85/300 -- Iteration 651434 - Batch 4466/7702 - Train loss: 0.00187381  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.51 min) Epoch 85/300 -- Iteration 651511 - Batch 4543/7702 - Train loss: 0.00187382  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.53 min) Epoch 85/300 -- Iteration 651588 - Batch 4620/7702 - Train loss: 0.00187350  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.55 min) Epoch 85/300 -- Iteration 651665 - Batch 4697/7702 - Train loss: 0.00187319  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.57 min) Epoch 85/300 -- Iteration 651742 - Batch 4774/7702 - Train loss: 0.00187276  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.59 min) Epoch 85/300 -- Iteration 651819 - Batch 4851/7702 - Train loss: 0.00187276  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.62 min) Epoch 85/300 -- Iteration 651896 - Batch 4928/7702 - Train loss: 0.00187246  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.64 min) Epoch 85/300 -- Iteration 651973 - Batch 5005/7702 - Train loss: 0.00187204  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.66 min) Epoch 85/300 -- Iteration 652050 - Batch 5082/7702 - Train loss: 0.00187225  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.68 min) Epoch 85/300 -- Iteration 652127 - Batch 5159/7702 - Train loss: 0.00187195  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.70 min) Epoch 85/300 -- Iteration 652204 - Batch 5236/7702 - Train loss: 0.00187214  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.72 min) Epoch 85/300 -- Iteration 652281 - Batch 5313/7702 - Train loss: 0.00187252  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.74 min) Epoch 85/300 -- Iteration 652358 - Batch 5390/7702 - Train loss: 0.00187268  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.76 min) Epoch 85/300 -- Iteration 652435 - Batch 5467/7702 - Train loss: 0.00187315  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.78 min) Epoch 85/300 -- Iteration 652512 - Batch 5544/7702 - Train loss: 0.00187311  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.80 min) Epoch 85/300 -- Iteration 652589 - Batch 5621/7702 - Train loss: 0.00187291  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.82 min) Epoch 85/300 -- Iteration 652666 - Batch 5698/7702 - Train loss: 0.00187301  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.84 min) Epoch 85/300 -- Iteration 652743 - Batch 5775/7702 - Train loss: 0.00187319  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.87 min) Epoch 85/300 -- Iteration 652820 - Batch 5852/7702 - Train loss: 0.00187328  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.89 min) Epoch 85/300 -- Iteration 652897 - Batch 5929/7702 - Train loss: 0.00187351  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.91 min) Epoch 85/300 -- Iteration 652974 - Batch 6006/7702 - Train loss: 0.00187335  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.93 min) Epoch 85/300 -- Iteration 653051 - Batch 6083/7702 - Train loss: 0.00187349  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.95 min) Epoch 85/300 -- Iteration 653128 - Batch 6160/7702 - Train loss: 0.00187339  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.97 min) Epoch 85/300 -- Iteration 653205 - Batch 6237/7702 - Train loss: 0.00187344  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(175.99 min) Epoch 85/300 -- Iteration 653282 - Batch 6314/7702 - Train loss: 0.00187354  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.01 min) Epoch 85/300 -- Iteration 653359 - Batch 6391/7702 - Train loss: 0.00187361  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.03 min) Epoch 85/300 -- Iteration 653436 - Batch 6468/7702 - Train loss: 0.00187401  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.05 min) Epoch 85/300 -- Iteration 653513 - Batch 6545/7702 - Train loss: 0.00187418  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.07 min) Epoch 85/300 -- Iteration 653590 - Batch 6622/7702 - Train loss: 0.00187412  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.09 min) Epoch 85/300 -- Iteration 653667 - Batch 6699/7702 - Train loss: 0.00187417  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.11 min) Epoch 85/300 -- Iteration 653744 - Batch 6776/7702 - Train loss: 0.00187416  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.13 min) Epoch 85/300 -- Iteration 653821 - Batch 6853/7702 - Train loss: 0.00187426  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.16 min) Epoch 85/300 -- Iteration 653898 - Batch 6930/7702 - Train loss: 0.00187449  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.18 min) Epoch 85/300 -- Iteration 653975 - Batch 7007/7702 - Train loss: 0.00187466  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.20 min) Epoch 85/300 -- Iteration 654052 - Batch 7084/7702 - Train loss: 0.00187480  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.22 min) Epoch 85/300 -- Iteration 654129 - Batch 7161/7702 - Train loss: 0.00187525  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.24 min) Epoch 85/300 -- Iteration 654206 - Batch 7238/7702 - Train loss: 0.00187503  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.26 min) Epoch 85/300 -- Iteration 654283 - Batch 7315/7702 - Train loss: 0.00187532  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.28 min) Epoch 85/300 -- Iteration 654360 - Batch 7392/7702 - Train loss: 0.00187540  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.30 min) Epoch 85/300 -- Iteration 654437 - Batch 7469/7702 - Train loss: 0.00187535  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.32 min) Epoch 85/300 -- Iteration 654514 - Batch 7546/7702 - Train loss: 0.00187551  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.34 min) Epoch 85/300 -- Iteration 654591 - Batch 7623/7702 - Train loss: 0.00187511  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.36 min) Epoch 85/300 -- Iteration 654668 - Batch 7700/7702 - Train loss: 0.00187487  - Train acc: -0.0000 - Val loss: 0.00033968\n",
      "(176.36 min) Epoch 85/300 -- Iteration 654670 - Batch 7701/7702 - Train loss: 0.00187481  - Train acc: -0.0000 - Val loss: 0.00041676 - Val acc: -0.0000\n",
      "(176.39 min) Epoch 86/300 -- Iteration 654747 - Batch 77/7702 - Train loss: 0.00187133  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.41 min) Epoch 86/300 -- Iteration 654824 - Batch 154/7702 - Train loss: 0.00186172  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.43 min) Epoch 86/300 -- Iteration 654901 - Batch 231/7702 - Train loss: 0.00187029  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.45 min) Epoch 86/300 -- Iteration 654978 - Batch 308/7702 - Train loss: 0.00186881  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.47 min) Epoch 86/300 -- Iteration 655055 - Batch 385/7702 - Train loss: 0.00187037  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.49 min) Epoch 86/300 -- Iteration 655132 - Batch 462/7702 - Train loss: 0.00186709  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.51 min) Epoch 86/300 -- Iteration 655209 - Batch 539/7702 - Train loss: 0.00187032  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.53 min) Epoch 86/300 -- Iteration 655286 - Batch 616/7702 - Train loss: 0.00186982  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.55 min) Epoch 86/300 -- Iteration 655363 - Batch 693/7702 - Train loss: 0.00186629  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.57 min) Epoch 86/300 -- Iteration 655440 - Batch 770/7702 - Train loss: 0.00186705  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.59 min) Epoch 86/300 -- Iteration 655517 - Batch 847/7702 - Train loss: 0.00186750  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.61 min) Epoch 86/300 -- Iteration 655594 - Batch 924/7702 - Train loss: 0.00186564  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.63 min) Epoch 86/300 -- Iteration 655671 - Batch 1001/7702 - Train loss: 0.00186585  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.66 min) Epoch 86/300 -- Iteration 655748 - Batch 1078/7702 - Train loss: 0.00186692  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.68 min) Epoch 86/300 -- Iteration 655825 - Batch 1155/7702 - Train loss: 0.00186544  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.70 min) Epoch 86/300 -- Iteration 655902 - Batch 1232/7702 - Train loss: 0.00186273  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.72 min) Epoch 86/300 -- Iteration 655979 - Batch 1309/7702 - Train loss: 0.00186445  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.74 min) Epoch 86/300 -- Iteration 656056 - Batch 1386/7702 - Train loss: 0.00186102  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.76 min) Epoch 86/300 -- Iteration 656133 - Batch 1463/7702 - Train loss: 0.00186223  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.78 min) Epoch 86/300 -- Iteration 656210 - Batch 1540/7702 - Train loss: 0.00186179  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.80 min) Epoch 86/300 -- Iteration 656287 - Batch 1617/7702 - Train loss: 0.00186410  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.82 min) Epoch 86/300 -- Iteration 656364 - Batch 1694/7702 - Train loss: 0.00186511  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.84 min) Epoch 86/300 -- Iteration 656441 - Batch 1771/7702 - Train loss: 0.00186574  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.86 min) Epoch 86/300 -- Iteration 656518 - Batch 1848/7702 - Train loss: 0.00186732  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.88 min) Epoch 86/300 -- Iteration 656595 - Batch 1925/7702 - Train loss: 0.00186890  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.91 min) Epoch 86/300 -- Iteration 656672 - Batch 2002/7702 - Train loss: 0.00187047  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.93 min) Epoch 86/300 -- Iteration 656749 - Batch 2079/7702 - Train loss: 0.00187174  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.95 min) Epoch 86/300 -- Iteration 656826 - Batch 2156/7702 - Train loss: 0.00187124  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.97 min) Epoch 86/300 -- Iteration 656903 - Batch 2233/7702 - Train loss: 0.00187117  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(176.99 min) Epoch 86/300 -- Iteration 656980 - Batch 2310/7702 - Train loss: 0.00187142  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.01 min) Epoch 86/300 -- Iteration 657057 - Batch 2387/7702 - Train loss: 0.00187128  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.03 min) Epoch 86/300 -- Iteration 657134 - Batch 2464/7702 - Train loss: 0.00187145  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.05 min) Epoch 86/300 -- Iteration 657211 - Batch 2541/7702 - Train loss: 0.00187142  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.07 min) Epoch 86/300 -- Iteration 657288 - Batch 2618/7702 - Train loss: 0.00187153  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.09 min) Epoch 86/300 -- Iteration 657365 - Batch 2695/7702 - Train loss: 0.00187120  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.11 min) Epoch 86/300 -- Iteration 657442 - Batch 2772/7702 - Train loss: 0.00187141  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.13 min) Epoch 86/300 -- Iteration 657519 - Batch 2849/7702 - Train loss: 0.00187171  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.15 min) Epoch 86/300 -- Iteration 657596 - Batch 2926/7702 - Train loss: 0.00187021  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.18 min) Epoch 86/300 -- Iteration 657673 - Batch 3003/7702 - Train loss: 0.00186980  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.20 min) Epoch 86/300 -- Iteration 657750 - Batch 3080/7702 - Train loss: 0.00186980  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.22 min) Epoch 86/300 -- Iteration 657827 - Batch 3157/7702 - Train loss: 0.00186984  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.24 min) Epoch 86/300 -- Iteration 657904 - Batch 3234/7702 - Train loss: 0.00186980  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.26 min) Epoch 86/300 -- Iteration 657981 - Batch 3311/7702 - Train loss: 0.00186993  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.28 min) Epoch 86/300 -- Iteration 658058 - Batch 3388/7702 - Train loss: 0.00186994  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.30 min) Epoch 86/300 -- Iteration 658135 - Batch 3465/7702 - Train loss: 0.00186992  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.32 min) Epoch 86/300 -- Iteration 658212 - Batch 3542/7702 - Train loss: 0.00186973  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.34 min) Epoch 86/300 -- Iteration 658289 - Batch 3619/7702 - Train loss: 0.00186976  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.36 min) Epoch 86/300 -- Iteration 658366 - Batch 3696/7702 - Train loss: 0.00186974  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.38 min) Epoch 86/300 -- Iteration 658443 - Batch 3773/7702 - Train loss: 0.00186970  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.40 min) Epoch 86/300 -- Iteration 658520 - Batch 3850/7702 - Train loss: 0.00186958  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.42 min) Epoch 86/300 -- Iteration 658597 - Batch 3927/7702 - Train loss: 0.00186973  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.44 min) Epoch 86/300 -- Iteration 658674 - Batch 4004/7702 - Train loss: 0.00187026  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.47 min) Epoch 86/300 -- Iteration 658751 - Batch 4081/7702 - Train loss: 0.00187040  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.49 min) Epoch 86/300 -- Iteration 658828 - Batch 4158/7702 - Train loss: 0.00187083  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.51 min) Epoch 86/300 -- Iteration 658905 - Batch 4235/7702 - Train loss: 0.00187167  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.53 min) Epoch 86/300 -- Iteration 658982 - Batch 4312/7702 - Train loss: 0.00187175  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.55 min) Epoch 86/300 -- Iteration 659059 - Batch 4389/7702 - Train loss: 0.00187182  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.57 min) Epoch 86/300 -- Iteration 659136 - Batch 4466/7702 - Train loss: 0.00187179  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.59 min) Epoch 86/300 -- Iteration 659213 - Batch 4543/7702 - Train loss: 0.00187210  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.61 min) Epoch 86/300 -- Iteration 659290 - Batch 4620/7702 - Train loss: 0.00187215  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.63 min) Epoch 86/300 -- Iteration 659367 - Batch 4697/7702 - Train loss: 0.00187244  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.65 min) Epoch 86/300 -- Iteration 659444 - Batch 4774/7702 - Train loss: 0.00187250  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.67 min) Epoch 86/300 -- Iteration 659521 - Batch 4851/7702 - Train loss: 0.00187320  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.69 min) Epoch 86/300 -- Iteration 659598 - Batch 4928/7702 - Train loss: 0.00187354  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.71 min) Epoch 86/300 -- Iteration 659675 - Batch 5005/7702 - Train loss: 0.00187417  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.73 min) Epoch 86/300 -- Iteration 659752 - Batch 5082/7702 - Train loss: 0.00187344  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.76 min) Epoch 86/300 -- Iteration 659829 - Batch 5159/7702 - Train loss: 0.00187323  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.78 min) Epoch 86/300 -- Iteration 659906 - Batch 5236/7702 - Train loss: 0.00187301  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.80 min) Epoch 86/300 -- Iteration 659983 - Batch 5313/7702 - Train loss: 0.00187365  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.82 min) Epoch 86/300 -- Iteration 660060 - Batch 5390/7702 - Train loss: 0.00187356  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.84 min) Epoch 86/300 -- Iteration 660137 - Batch 5467/7702 - Train loss: 0.00187353  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.86 min) Epoch 86/300 -- Iteration 660214 - Batch 5544/7702 - Train loss: 0.00187334  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.88 min) Epoch 86/300 -- Iteration 660291 - Batch 5621/7702 - Train loss: 0.00187331  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.90 min) Epoch 86/300 -- Iteration 660368 - Batch 5698/7702 - Train loss: 0.00187320  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.92 min) Epoch 86/300 -- Iteration 660445 - Batch 5775/7702 - Train loss: 0.00187316  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.94 min) Epoch 86/300 -- Iteration 660522 - Batch 5852/7702 - Train loss: 0.00187310  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.96 min) Epoch 86/300 -- Iteration 660599 - Batch 5929/7702 - Train loss: 0.00187270  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(177.98 min) Epoch 86/300 -- Iteration 660676 - Batch 6006/7702 - Train loss: 0.00187295  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.01 min) Epoch 86/300 -- Iteration 660753 - Batch 6083/7702 - Train loss: 0.00187278  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.03 min) Epoch 86/300 -- Iteration 660830 - Batch 6160/7702 - Train loss: 0.00187299  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.05 min) Epoch 86/300 -- Iteration 660907 - Batch 6237/7702 - Train loss: 0.00187311  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.07 min) Epoch 86/300 -- Iteration 660984 - Batch 6314/7702 - Train loss: 0.00187326  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.09 min) Epoch 86/300 -- Iteration 661061 - Batch 6391/7702 - Train loss: 0.00187336  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.11 min) Epoch 86/300 -- Iteration 661138 - Batch 6468/7702 - Train loss: 0.00187324  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.13 min) Epoch 86/300 -- Iteration 661215 - Batch 6545/7702 - Train loss: 0.00187296  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.15 min) Epoch 86/300 -- Iteration 661292 - Batch 6622/7702 - Train loss: 0.00187288  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.17 min) Epoch 86/300 -- Iteration 661369 - Batch 6699/7702 - Train loss: 0.00187302  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.19 min) Epoch 86/300 -- Iteration 661446 - Batch 6776/7702 - Train loss: 0.00187333  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.21 min) Epoch 86/300 -- Iteration 661523 - Batch 6853/7702 - Train loss: 0.00187345  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.23 min) Epoch 86/300 -- Iteration 661600 - Batch 6930/7702 - Train loss: 0.00187355  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.25 min) Epoch 86/300 -- Iteration 661677 - Batch 7007/7702 - Train loss: 0.00187383  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.27 min) Epoch 86/300 -- Iteration 661754 - Batch 7084/7702 - Train loss: 0.00187381  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.30 min) Epoch 86/300 -- Iteration 661831 - Batch 7161/7702 - Train loss: 0.00187397  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.32 min) Epoch 86/300 -- Iteration 661908 - Batch 7238/7702 - Train loss: 0.00187385  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.34 min) Epoch 86/300 -- Iteration 661985 - Batch 7315/7702 - Train loss: 0.00187412  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.36 min) Epoch 86/300 -- Iteration 662062 - Batch 7392/7702 - Train loss: 0.00187433  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.38 min) Epoch 86/300 -- Iteration 662139 - Batch 7469/7702 - Train loss: 0.00187466  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.40 min) Epoch 86/300 -- Iteration 662216 - Batch 7546/7702 - Train loss: 0.00187470  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.42 min) Epoch 86/300 -- Iteration 662293 - Batch 7623/7702 - Train loss: 0.00187418  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.44 min) Epoch 86/300 -- Iteration 662370 - Batch 7700/7702 - Train loss: 0.00187428  - Train acc: -0.0000 - Val loss: 0.00041676\n",
      "(178.44 min) Epoch 86/300 -- Iteration 662372 - Batch 7701/7702 - Train loss: 0.00187430  - Train acc: -0.0000 - Val loss: 0.00034218 - Val acc: -0.0000\n",
      "(178.46 min) Epoch 87/300 -- Iteration 662449 - Batch 77/7702 - Train loss: 0.00190930  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.48 min) Epoch 87/300 -- Iteration 662526 - Batch 154/7702 - Train loss: 0.00189817  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.51 min) Epoch 87/300 -- Iteration 662603 - Batch 231/7702 - Train loss: 0.00189643  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.53 min) Epoch 87/300 -- Iteration 662680 - Batch 308/7702 - Train loss: 0.00188643  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.55 min) Epoch 87/300 -- Iteration 662757 - Batch 385/7702 - Train loss: 0.00188930  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.57 min) Epoch 87/300 -- Iteration 662834 - Batch 462/7702 - Train loss: 0.00189024  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.59 min) Epoch 87/300 -- Iteration 662911 - Batch 539/7702 - Train loss: 0.00188854  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.61 min) Epoch 87/300 -- Iteration 662988 - Batch 616/7702 - Train loss: 0.00188297  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.63 min) Epoch 87/300 -- Iteration 663065 - Batch 693/7702 - Train loss: 0.00187893  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.65 min) Epoch 87/300 -- Iteration 663142 - Batch 770/7702 - Train loss: 0.00188046  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.67 min) Epoch 87/300 -- Iteration 663219 - Batch 847/7702 - Train loss: 0.00187829  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.69 min) Epoch 87/300 -- Iteration 663296 - Batch 924/7702 - Train loss: 0.00187596  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.71 min) Epoch 87/300 -- Iteration 663373 - Batch 1001/7702 - Train loss: 0.00187659  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.73 min) Epoch 87/300 -- Iteration 663450 - Batch 1078/7702 - Train loss: 0.00187419  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.75 min) Epoch 87/300 -- Iteration 663527 - Batch 1155/7702 - Train loss: 0.00187557  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.78 min) Epoch 87/300 -- Iteration 663604 - Batch 1232/7702 - Train loss: 0.00187436  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.80 min) Epoch 87/300 -- Iteration 663681 - Batch 1309/7702 - Train loss: 0.00187429  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.82 min) Epoch 87/300 -- Iteration 663758 - Batch 1386/7702 - Train loss: 0.00187353  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.84 min) Epoch 87/300 -- Iteration 663835 - Batch 1463/7702 - Train loss: 0.00187319  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.86 min) Epoch 87/300 -- Iteration 663912 - Batch 1540/7702 - Train loss: 0.00187399  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.88 min) Epoch 87/300 -- Iteration 663989 - Batch 1617/7702 - Train loss: 0.00187288  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.90 min) Epoch 87/300 -- Iteration 664066 - Batch 1694/7702 - Train loss: 0.00187474  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.92 min) Epoch 87/300 -- Iteration 664143 - Batch 1771/7702 - Train loss: 0.00187467  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.94 min) Epoch 87/300 -- Iteration 664220 - Batch 1848/7702 - Train loss: 0.00187397  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.96 min) Epoch 87/300 -- Iteration 664297 - Batch 1925/7702 - Train loss: 0.00187382  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(178.98 min) Epoch 87/300 -- Iteration 664374 - Batch 2002/7702 - Train loss: 0.00187329  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.00 min) Epoch 87/300 -- Iteration 664451 - Batch 2079/7702 - Train loss: 0.00187444  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.03 min) Epoch 87/300 -- Iteration 664528 - Batch 2156/7702 - Train loss: 0.00187451  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.05 min) Epoch 87/300 -- Iteration 664605 - Batch 2233/7702 - Train loss: 0.00187580  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.07 min) Epoch 87/300 -- Iteration 664682 - Batch 2310/7702 - Train loss: 0.00187528  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.09 min) Epoch 87/300 -- Iteration 664759 - Batch 2387/7702 - Train loss: 0.00187362  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.11 min) Epoch 87/300 -- Iteration 664836 - Batch 2464/7702 - Train loss: 0.00187259  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.13 min) Epoch 87/300 -- Iteration 664913 - Batch 2541/7702 - Train loss: 0.00187364  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.15 min) Epoch 87/300 -- Iteration 664990 - Batch 2618/7702 - Train loss: 0.00187313  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.17 min) Epoch 87/300 -- Iteration 665067 - Batch 2695/7702 - Train loss: 0.00187281  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.19 min) Epoch 87/300 -- Iteration 665144 - Batch 2772/7702 - Train loss: 0.00187242  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.21 min) Epoch 87/300 -- Iteration 665221 - Batch 2849/7702 - Train loss: 0.00187242  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.23 min) Epoch 87/300 -- Iteration 665298 - Batch 2926/7702 - Train loss: 0.00187182  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.25 min) Epoch 87/300 -- Iteration 665375 - Batch 3003/7702 - Train loss: 0.00187172  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.27 min) Epoch 87/300 -- Iteration 665452 - Batch 3080/7702 - Train loss: 0.00187216  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.29 min) Epoch 87/300 -- Iteration 665529 - Batch 3157/7702 - Train loss: 0.00187247  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.32 min) Epoch 87/300 -- Iteration 665606 - Batch 3234/7702 - Train loss: 0.00187284  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.34 min) Epoch 87/300 -- Iteration 665683 - Batch 3311/7702 - Train loss: 0.00187342  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.36 min) Epoch 87/300 -- Iteration 665760 - Batch 3388/7702 - Train loss: 0.00187381  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.38 min) Epoch 87/300 -- Iteration 665837 - Batch 3465/7702 - Train loss: 0.00187397  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.40 min) Epoch 87/300 -- Iteration 665914 - Batch 3542/7702 - Train loss: 0.00187435  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.42 min) Epoch 87/300 -- Iteration 665991 - Batch 3619/7702 - Train loss: 0.00187463  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.44 min) Epoch 87/300 -- Iteration 666068 - Batch 3696/7702 - Train loss: 0.00187412  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.46 min) Epoch 87/300 -- Iteration 666145 - Batch 3773/7702 - Train loss: 0.00187393  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.48 min) Epoch 87/300 -- Iteration 666222 - Batch 3850/7702 - Train loss: 0.00187355  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.50 min) Epoch 87/300 -- Iteration 666299 - Batch 3927/7702 - Train loss: 0.00187378  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.52 min) Epoch 87/300 -- Iteration 666376 - Batch 4004/7702 - Train loss: 0.00187399  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.54 min) Epoch 87/300 -- Iteration 666453 - Batch 4081/7702 - Train loss: 0.00187419  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.56 min) Epoch 87/300 -- Iteration 666530 - Batch 4158/7702 - Train loss: 0.00187409  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.59 min) Epoch 87/300 -- Iteration 666607 - Batch 4235/7702 - Train loss: 0.00187430  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.61 min) Epoch 87/300 -- Iteration 666684 - Batch 4312/7702 - Train loss: 0.00187429  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.63 min) Epoch 87/300 -- Iteration 666761 - Batch 4389/7702 - Train loss: 0.00187317  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.65 min) Epoch 87/300 -- Iteration 666838 - Batch 4466/7702 - Train loss: 0.00187399  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.67 min) Epoch 87/300 -- Iteration 666915 - Batch 4543/7702 - Train loss: 0.00187378  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.69 min) Epoch 87/300 -- Iteration 666992 - Batch 4620/7702 - Train loss: 0.00187343  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.71 min) Epoch 87/300 -- Iteration 667069 - Batch 4697/7702 - Train loss: 0.00187293  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.73 min) Epoch 87/300 -- Iteration 667146 - Batch 4774/7702 - Train loss: 0.00187308  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.75 min) Epoch 87/300 -- Iteration 667223 - Batch 4851/7702 - Train loss: 0.00187230  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.77 min) Epoch 87/300 -- Iteration 667300 - Batch 4928/7702 - Train loss: 0.00187215  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.79 min) Epoch 87/300 -- Iteration 667377 - Batch 5005/7702 - Train loss: 0.00187144  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.81 min) Epoch 87/300 -- Iteration 667454 - Batch 5082/7702 - Train loss: 0.00187156  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.83 min) Epoch 87/300 -- Iteration 667531 - Batch 5159/7702 - Train loss: 0.00187106  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.86 min) Epoch 87/300 -- Iteration 667608 - Batch 5236/7702 - Train loss: 0.00187120  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.88 min) Epoch 87/300 -- Iteration 667685 - Batch 5313/7702 - Train loss: 0.00187083  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.90 min) Epoch 87/300 -- Iteration 667762 - Batch 5390/7702 - Train loss: 0.00187022  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.92 min) Epoch 87/300 -- Iteration 667839 - Batch 5467/7702 - Train loss: 0.00186989  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.94 min) Epoch 87/300 -- Iteration 667916 - Batch 5544/7702 - Train loss: 0.00186968  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.96 min) Epoch 87/300 -- Iteration 667993 - Batch 5621/7702 - Train loss: 0.00186990  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(179.98 min) Epoch 87/300 -- Iteration 668070 - Batch 5698/7702 - Train loss: 0.00187027  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.00 min) Epoch 87/300 -- Iteration 668147 - Batch 5775/7702 - Train loss: 0.00187012  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.02 min) Epoch 87/300 -- Iteration 668224 - Batch 5852/7702 - Train loss: 0.00186950  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.04 min) Epoch 87/300 -- Iteration 668301 - Batch 5929/7702 - Train loss: 0.00186941  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.06 min) Epoch 87/300 -- Iteration 668378 - Batch 6006/7702 - Train loss: 0.00186993  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.08 min) Epoch 87/300 -- Iteration 668455 - Batch 6083/7702 - Train loss: 0.00187016  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.11 min) Epoch 87/300 -- Iteration 668532 - Batch 6160/7702 - Train loss: 0.00186942  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.13 min) Epoch 87/300 -- Iteration 668609 - Batch 6237/7702 - Train loss: 0.00186938  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.15 min) Epoch 87/300 -- Iteration 668686 - Batch 6314/7702 - Train loss: 0.00186920  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.17 min) Epoch 87/300 -- Iteration 668763 - Batch 6391/7702 - Train loss: 0.00186935  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.19 min) Epoch 87/300 -- Iteration 668840 - Batch 6468/7702 - Train loss: 0.00186931  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.21 min) Epoch 87/300 -- Iteration 668917 - Batch 6545/7702 - Train loss: 0.00186921  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.23 min) Epoch 87/300 -- Iteration 668994 - Batch 6622/7702 - Train loss: 0.00186922  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.25 min) Epoch 87/300 -- Iteration 669071 - Batch 6699/7702 - Train loss: 0.00186952  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.27 min) Epoch 87/300 -- Iteration 669148 - Batch 6776/7702 - Train loss: 0.00186929  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.29 min) Epoch 87/300 -- Iteration 669225 - Batch 6853/7702 - Train loss: 0.00186940  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.31 min) Epoch 87/300 -- Iteration 669302 - Batch 6930/7702 - Train loss: 0.00186955  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.33 min) Epoch 87/300 -- Iteration 669379 - Batch 7007/7702 - Train loss: 0.00187001  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.35 min) Epoch 87/300 -- Iteration 669456 - Batch 7084/7702 - Train loss: 0.00186985  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.38 min) Epoch 87/300 -- Iteration 669533 - Batch 7161/7702 - Train loss: 0.00187001  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.40 min) Epoch 87/300 -- Iteration 669610 - Batch 7238/7702 - Train loss: 0.00187002  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.42 min) Epoch 87/300 -- Iteration 669687 - Batch 7315/7702 - Train loss: 0.00186987  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.44 min) Epoch 87/300 -- Iteration 669764 - Batch 7392/7702 - Train loss: 0.00186978  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.46 min) Epoch 87/300 -- Iteration 669841 - Batch 7469/7702 - Train loss: 0.00186977  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.48 min) Epoch 87/300 -- Iteration 669918 - Batch 7546/7702 - Train loss: 0.00186980  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.50 min) Epoch 87/300 -- Iteration 669995 - Batch 7623/7702 - Train loss: 0.00186999  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.52 min) Epoch 87/300 -- Iteration 670072 - Batch 7700/7702 - Train loss: 0.00187041  - Train acc: -0.0000 - Val loss: 0.00034218\n",
      "(180.52 min) Epoch 87/300 -- Iteration 670074 - Batch 7701/7702 - Train loss: 0.00187040  - Train acc: -0.0000 - Val loss: 0.00037850 - Val acc: -0.0000\n",
      "(180.54 min) Epoch 88/300 -- Iteration 670151 - Batch 77/7702 - Train loss: 0.00184276  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.56 min) Epoch 88/300 -- Iteration 670228 - Batch 154/7702 - Train loss: 0.00184308  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.58 min) Epoch 88/300 -- Iteration 670305 - Batch 231/7702 - Train loss: 0.00185575  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.61 min) Epoch 88/300 -- Iteration 670382 - Batch 308/7702 - Train loss: 0.00185891  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.63 min) Epoch 88/300 -- Iteration 670459 - Batch 385/7702 - Train loss: 0.00186054  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.65 min) Epoch 88/300 -- Iteration 670536 - Batch 462/7702 - Train loss: 0.00186282  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.67 min) Epoch 88/300 -- Iteration 670613 - Batch 539/7702 - Train loss: 0.00186685  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.69 min) Epoch 88/300 -- Iteration 670690 - Batch 616/7702 - Train loss: 0.00186912  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.71 min) Epoch 88/300 -- Iteration 670767 - Batch 693/7702 - Train loss: 0.00187044  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.73 min) Epoch 88/300 -- Iteration 670844 - Batch 770/7702 - Train loss: 0.00186965  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.75 min) Epoch 88/300 -- Iteration 670921 - Batch 847/7702 - Train loss: 0.00186867  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.77 min) Epoch 88/300 -- Iteration 670998 - Batch 924/7702 - Train loss: 0.00186887  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.79 min) Epoch 88/300 -- Iteration 671075 - Batch 1001/7702 - Train loss: 0.00186704  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.81 min) Epoch 88/300 -- Iteration 671152 - Batch 1078/7702 - Train loss: 0.00186888  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.83 min) Epoch 88/300 -- Iteration 671229 - Batch 1155/7702 - Train loss: 0.00187004  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.86 min) Epoch 88/300 -- Iteration 671306 - Batch 1232/7702 - Train loss: 0.00187150  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.88 min) Epoch 88/300 -- Iteration 671383 - Batch 1309/7702 - Train loss: 0.00187131  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.90 min) Epoch 88/300 -- Iteration 671460 - Batch 1386/7702 - Train loss: 0.00187086  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.92 min) Epoch 88/300 -- Iteration 671537 - Batch 1463/7702 - Train loss: 0.00187073  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.94 min) Epoch 88/300 -- Iteration 671614 - Batch 1540/7702 - Train loss: 0.00187045  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.96 min) Epoch 88/300 -- Iteration 671691 - Batch 1617/7702 - Train loss: 0.00187050  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(180.98 min) Epoch 88/300 -- Iteration 671768 - Batch 1694/7702 - Train loss: 0.00187113  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.00 min) Epoch 88/300 -- Iteration 671845 - Batch 1771/7702 - Train loss: 0.00187099  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.02 min) Epoch 88/300 -- Iteration 671922 - Batch 1848/7702 - Train loss: 0.00187092  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.04 min) Epoch 88/300 -- Iteration 671999 - Batch 1925/7702 - Train loss: 0.00187116  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.06 min) Epoch 88/300 -- Iteration 672076 - Batch 2002/7702 - Train loss: 0.00187058  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.08 min) Epoch 88/300 -- Iteration 672153 - Batch 2079/7702 - Train loss: 0.00187030  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.11 min) Epoch 88/300 -- Iteration 672230 - Batch 2156/7702 - Train loss: 0.00187026  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.13 min) Epoch 88/300 -- Iteration 672307 - Batch 2233/7702 - Train loss: 0.00187058  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.15 min) Epoch 88/300 -- Iteration 672384 - Batch 2310/7702 - Train loss: 0.00187047  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.17 min) Epoch 88/300 -- Iteration 672461 - Batch 2387/7702 - Train loss: 0.00186949  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.19 min) Epoch 88/300 -- Iteration 672538 - Batch 2464/7702 - Train loss: 0.00186998  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.21 min) Epoch 88/300 -- Iteration 672615 - Batch 2541/7702 - Train loss: 0.00187058  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.23 min) Epoch 88/300 -- Iteration 672692 - Batch 2618/7702 - Train loss: 0.00187179  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.25 min) Epoch 88/300 -- Iteration 672769 - Batch 2695/7702 - Train loss: 0.00187195  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.27 min) Epoch 88/300 -- Iteration 672846 - Batch 2772/7702 - Train loss: 0.00187071  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.29 min) Epoch 88/300 -- Iteration 672923 - Batch 2849/7702 - Train loss: 0.00187138  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.31 min) Epoch 88/300 -- Iteration 673000 - Batch 2926/7702 - Train loss: 0.00187142  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.33 min) Epoch 88/300 -- Iteration 673077 - Batch 3003/7702 - Train loss: 0.00187123  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.35 min) Epoch 88/300 -- Iteration 673154 - Batch 3080/7702 - Train loss: 0.00187167  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.38 min) Epoch 88/300 -- Iteration 673231 - Batch 3157/7702 - Train loss: 0.00187162  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.40 min) Epoch 88/300 -- Iteration 673308 - Batch 3234/7702 - Train loss: 0.00187171  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.42 min) Epoch 88/300 -- Iteration 673385 - Batch 3311/7702 - Train loss: 0.00187249  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.44 min) Epoch 88/300 -- Iteration 673462 - Batch 3388/7702 - Train loss: 0.00187261  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.46 min) Epoch 88/300 -- Iteration 673539 - Batch 3465/7702 - Train loss: 0.00187183  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.48 min) Epoch 88/300 -- Iteration 673616 - Batch 3542/7702 - Train loss: 0.00187204  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.50 min) Epoch 88/300 -- Iteration 673693 - Batch 3619/7702 - Train loss: 0.00187225  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.52 min) Epoch 88/300 -- Iteration 673770 - Batch 3696/7702 - Train loss: 0.00187256  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.54 min) Epoch 88/300 -- Iteration 673847 - Batch 3773/7702 - Train loss: 0.00187316  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.56 min) Epoch 88/300 -- Iteration 673924 - Batch 3850/7702 - Train loss: 0.00187331  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.58 min) Epoch 88/300 -- Iteration 674001 - Batch 3927/7702 - Train loss: 0.00187253  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.60 min) Epoch 88/300 -- Iteration 674078 - Batch 4004/7702 - Train loss: 0.00187266  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.62 min) Epoch 88/300 -- Iteration 674155 - Batch 4081/7702 - Train loss: 0.00187264  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.64 min) Epoch 88/300 -- Iteration 674232 - Batch 4158/7702 - Train loss: 0.00187271  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.67 min) Epoch 88/300 -- Iteration 674309 - Batch 4235/7702 - Train loss: 0.00187229  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.69 min) Epoch 88/300 -- Iteration 674386 - Batch 4312/7702 - Train loss: 0.00187251  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.71 min) Epoch 88/300 -- Iteration 674463 - Batch 4389/7702 - Train loss: 0.00187199  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.73 min) Epoch 88/300 -- Iteration 674540 - Batch 4466/7702 - Train loss: 0.00187251  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.75 min) Epoch 88/300 -- Iteration 674617 - Batch 4543/7702 - Train loss: 0.00187212  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.77 min) Epoch 88/300 -- Iteration 674694 - Batch 4620/7702 - Train loss: 0.00187171  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.79 min) Epoch 88/300 -- Iteration 674771 - Batch 4697/7702 - Train loss: 0.00187200  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.81 min) Epoch 88/300 -- Iteration 674848 - Batch 4774/7702 - Train loss: 0.00187260  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.83 min) Epoch 88/300 -- Iteration 674925 - Batch 4851/7702 - Train loss: 0.00187237  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.85 min) Epoch 88/300 -- Iteration 675002 - Batch 4928/7702 - Train loss: 0.00187244  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.87 min) Epoch 88/300 -- Iteration 675079 - Batch 5005/7702 - Train loss: 0.00187262  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.89 min) Epoch 88/300 -- Iteration 675156 - Batch 5082/7702 - Train loss: 0.00187294  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.91 min) Epoch 88/300 -- Iteration 675233 - Batch 5159/7702 - Train loss: 0.00187286  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.94 min) Epoch 88/300 -- Iteration 675310 - Batch 5236/7702 - Train loss: 0.00187298  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.96 min) Epoch 88/300 -- Iteration 675387 - Batch 5313/7702 - Train loss: 0.00187273  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(181.98 min) Epoch 88/300 -- Iteration 675464 - Batch 5390/7702 - Train loss: 0.00187247  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.00 min) Epoch 88/300 -- Iteration 675541 - Batch 5467/7702 - Train loss: 0.00187250  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.02 min) Epoch 88/300 -- Iteration 675618 - Batch 5544/7702 - Train loss: 0.00187258  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.04 min) Epoch 88/300 -- Iteration 675695 - Batch 5621/7702 - Train loss: 0.00187259  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.06 min) Epoch 88/300 -- Iteration 675772 - Batch 5698/7702 - Train loss: 0.00187288  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.08 min) Epoch 88/300 -- Iteration 675849 - Batch 5775/7702 - Train loss: 0.00187261  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.10 min) Epoch 88/300 -- Iteration 675926 - Batch 5852/7702 - Train loss: 0.00187223  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.12 min) Epoch 88/300 -- Iteration 676003 - Batch 5929/7702 - Train loss: 0.00187189  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.14 min) Epoch 88/300 -- Iteration 676080 - Batch 6006/7702 - Train loss: 0.00187192  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.16 min) Epoch 88/300 -- Iteration 676157 - Batch 6083/7702 - Train loss: 0.00187191  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.18 min) Epoch 88/300 -- Iteration 676234 - Batch 6160/7702 - Train loss: 0.00187187  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.21 min) Epoch 88/300 -- Iteration 676311 - Batch 6237/7702 - Train loss: 0.00187177  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.23 min) Epoch 88/300 -- Iteration 676388 - Batch 6314/7702 - Train loss: 0.00187169  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.25 min) Epoch 88/300 -- Iteration 676465 - Batch 6391/7702 - Train loss: 0.00187181  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.27 min) Epoch 88/300 -- Iteration 676542 - Batch 6468/7702 - Train loss: 0.00187177  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.29 min) Epoch 88/300 -- Iteration 676619 - Batch 6545/7702 - Train loss: 0.00187113  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.31 min) Epoch 88/300 -- Iteration 676696 - Batch 6622/7702 - Train loss: 0.00187137  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.33 min) Epoch 88/300 -- Iteration 676773 - Batch 6699/7702 - Train loss: 0.00187153  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.35 min) Epoch 88/300 -- Iteration 676850 - Batch 6776/7702 - Train loss: 0.00187196  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.37 min) Epoch 88/300 -- Iteration 676927 - Batch 6853/7702 - Train loss: 0.00187191  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.39 min) Epoch 88/300 -- Iteration 677004 - Batch 6930/7702 - Train loss: 0.00187181  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.41 min) Epoch 88/300 -- Iteration 677081 - Batch 7007/7702 - Train loss: 0.00187167  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.43 min) Epoch 88/300 -- Iteration 677158 - Batch 7084/7702 - Train loss: 0.00187172  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.45 min) Epoch 88/300 -- Iteration 677235 - Batch 7161/7702 - Train loss: 0.00187180  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.48 min) Epoch 88/300 -- Iteration 677312 - Batch 7238/7702 - Train loss: 0.00187187  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.50 min) Epoch 88/300 -- Iteration 677389 - Batch 7315/7702 - Train loss: 0.00187179  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.52 min) Epoch 88/300 -- Iteration 677466 - Batch 7392/7702 - Train loss: 0.00187174  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.54 min) Epoch 88/300 -- Iteration 677543 - Batch 7469/7702 - Train loss: 0.00187144  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.56 min) Epoch 88/300 -- Iteration 677620 - Batch 7546/7702 - Train loss: 0.00187186  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.58 min) Epoch 88/300 -- Iteration 677697 - Batch 7623/7702 - Train loss: 0.00187208  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.60 min) Epoch 88/300 -- Iteration 677774 - Batch 7700/7702 - Train loss: 0.00187192  - Train acc: -0.0000 - Val loss: 0.00037850\n",
      "(182.60 min) Epoch 88/300 -- Iteration 677776 - Batch 7701/7702 - Train loss: 0.00187187  - Train acc: -0.0000 - Val loss: 0.00042356 - Val acc: -0.0000\n",
      "(182.62 min) Epoch 89/300 -- Iteration 677853 - Batch 77/7702 - Train loss: 0.00187357  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.64 min) Epoch 89/300 -- Iteration 677930 - Batch 154/7702 - Train loss: 0.00187563  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.66 min) Epoch 89/300 -- Iteration 678007 - Batch 231/7702 - Train loss: 0.00188421  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.68 min) Epoch 89/300 -- Iteration 678084 - Batch 308/7702 - Train loss: 0.00188704  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.71 min) Epoch 89/300 -- Iteration 678161 - Batch 385/7702 - Train loss: 0.00188235  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.73 min) Epoch 89/300 -- Iteration 678238 - Batch 462/7702 - Train loss: 0.00187925  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.75 min) Epoch 89/300 -- Iteration 678315 - Batch 539/7702 - Train loss: 0.00187596  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.77 min) Epoch 89/300 -- Iteration 678392 - Batch 616/7702 - Train loss: 0.00187651  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.79 min) Epoch 89/300 -- Iteration 678469 - Batch 693/7702 - Train loss: 0.00187237  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.81 min) Epoch 89/300 -- Iteration 678546 - Batch 770/7702 - Train loss: 0.00187327  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.83 min) Epoch 89/300 -- Iteration 678623 - Batch 847/7702 - Train loss: 0.00187190  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.85 min) Epoch 89/300 -- Iteration 678700 - Batch 924/7702 - Train loss: 0.00187416  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.87 min) Epoch 89/300 -- Iteration 678777 - Batch 1001/7702 - Train loss: 0.00187269  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.89 min) Epoch 89/300 -- Iteration 678854 - Batch 1078/7702 - Train loss: 0.00187069  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.91 min) Epoch 89/300 -- Iteration 678931 - Batch 1155/7702 - Train loss: 0.00186865  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.93 min) Epoch 89/300 -- Iteration 679008 - Batch 1232/7702 - Train loss: 0.00186991  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.95 min) Epoch 89/300 -- Iteration 679085 - Batch 1309/7702 - Train loss: 0.00186853  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(182.97 min) Epoch 89/300 -- Iteration 679162 - Batch 1386/7702 - Train loss: 0.00186976  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.00 min) Epoch 89/300 -- Iteration 679239 - Batch 1463/7702 - Train loss: 0.00187029  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.02 min) Epoch 89/300 -- Iteration 679316 - Batch 1540/7702 - Train loss: 0.00186916  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.04 min) Epoch 89/300 -- Iteration 679393 - Batch 1617/7702 - Train loss: 0.00186934  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.06 min) Epoch 89/300 -- Iteration 679470 - Batch 1694/7702 - Train loss: 0.00186966  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.08 min) Epoch 89/300 -- Iteration 679547 - Batch 1771/7702 - Train loss: 0.00186882  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.10 min) Epoch 89/300 -- Iteration 679624 - Batch 1848/7702 - Train loss: 0.00186814  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.12 min) Epoch 89/300 -- Iteration 679701 - Batch 1925/7702 - Train loss: 0.00186859  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.14 min) Epoch 89/300 -- Iteration 679778 - Batch 2002/7702 - Train loss: 0.00186844  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.16 min) Epoch 89/300 -- Iteration 679855 - Batch 2079/7702 - Train loss: 0.00186801  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.18 min) Epoch 89/300 -- Iteration 679932 - Batch 2156/7702 - Train loss: 0.00186861  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.20 min) Epoch 89/300 -- Iteration 680009 - Batch 2233/7702 - Train loss: 0.00186982  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.22 min) Epoch 89/300 -- Iteration 680086 - Batch 2310/7702 - Train loss: 0.00187174  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.24 min) Epoch 89/300 -- Iteration 680163 - Batch 2387/7702 - Train loss: 0.00187114  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.26 min) Epoch 89/300 -- Iteration 680240 - Batch 2464/7702 - Train loss: 0.00187101  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.29 min) Epoch 89/300 -- Iteration 680317 - Batch 2541/7702 - Train loss: 0.00187043  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.31 min) Epoch 89/300 -- Iteration 680394 - Batch 2618/7702 - Train loss: 0.00187056  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.33 min) Epoch 89/300 -- Iteration 680471 - Batch 2695/7702 - Train loss: 0.00187117  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.35 min) Epoch 89/300 -- Iteration 680548 - Batch 2772/7702 - Train loss: 0.00187116  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.37 min) Epoch 89/300 -- Iteration 680625 - Batch 2849/7702 - Train loss: 0.00187024  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.39 min) Epoch 89/300 -- Iteration 680702 - Batch 2926/7702 - Train loss: 0.00187033  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.41 min) Epoch 89/300 -- Iteration 680779 - Batch 3003/7702 - Train loss: 0.00187005  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.43 min) Epoch 89/300 -- Iteration 680856 - Batch 3080/7702 - Train loss: 0.00187045  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.45 min) Epoch 89/300 -- Iteration 680933 - Batch 3157/7702 - Train loss: 0.00186972  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.47 min) Epoch 89/300 -- Iteration 681010 - Batch 3234/7702 - Train loss: 0.00186870  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.49 min) Epoch 89/300 -- Iteration 681087 - Batch 3311/7702 - Train loss: 0.00186810  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.51 min) Epoch 89/300 -- Iteration 681164 - Batch 3388/7702 - Train loss: 0.00186794  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.54 min) Epoch 89/300 -- Iteration 681241 - Batch 3465/7702 - Train loss: 0.00186791  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.56 min) Epoch 89/300 -- Iteration 681318 - Batch 3542/7702 - Train loss: 0.00186753  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.58 min) Epoch 89/300 -- Iteration 681395 - Batch 3619/7702 - Train loss: 0.00186676  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.60 min) Epoch 89/300 -- Iteration 681472 - Batch 3696/7702 - Train loss: 0.00186711  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.62 min) Epoch 89/300 -- Iteration 681549 - Batch 3773/7702 - Train loss: 0.00186791  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.64 min) Epoch 89/300 -- Iteration 681626 - Batch 3850/7702 - Train loss: 0.00186868  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.66 min) Epoch 89/300 -- Iteration 681703 - Batch 3927/7702 - Train loss: 0.00186878  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.68 min) Epoch 89/300 -- Iteration 681780 - Batch 4004/7702 - Train loss: 0.00186934  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.70 min) Epoch 89/300 -- Iteration 681857 - Batch 4081/7702 - Train loss: 0.00186927  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.72 min) Epoch 89/300 -- Iteration 681934 - Batch 4158/7702 - Train loss: 0.00186989  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.74 min) Epoch 89/300 -- Iteration 682011 - Batch 4235/7702 - Train loss: 0.00186991  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.76 min) Epoch 89/300 -- Iteration 682088 - Batch 4312/7702 - Train loss: 0.00187028  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.78 min) Epoch 89/300 -- Iteration 682165 - Batch 4389/7702 - Train loss: 0.00186963  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.80 min) Epoch 89/300 -- Iteration 682242 - Batch 4466/7702 - Train loss: 0.00186917  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.83 min) Epoch 89/300 -- Iteration 682319 - Batch 4543/7702 - Train loss: 0.00186905  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.85 min) Epoch 89/300 -- Iteration 682396 - Batch 4620/7702 - Train loss: 0.00186935  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.87 min) Epoch 89/300 -- Iteration 682473 - Batch 4697/7702 - Train loss: 0.00186900  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.89 min) Epoch 89/300 -- Iteration 682550 - Batch 4774/7702 - Train loss: 0.00186934  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.91 min) Epoch 89/300 -- Iteration 682627 - Batch 4851/7702 - Train loss: 0.00186944  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.93 min) Epoch 89/300 -- Iteration 682704 - Batch 4928/7702 - Train loss: 0.00186934  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.95 min) Epoch 89/300 -- Iteration 682781 - Batch 5005/7702 - Train loss: 0.00186932  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.97 min) Epoch 89/300 -- Iteration 682858 - Batch 5082/7702 - Train loss: 0.00186943  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(183.99 min) Epoch 89/300 -- Iteration 682935 - Batch 5159/7702 - Train loss: 0.00186933  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.01 min) Epoch 89/300 -- Iteration 683012 - Batch 5236/7702 - Train loss: 0.00186901  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.03 min) Epoch 89/300 -- Iteration 683089 - Batch 5313/7702 - Train loss: 0.00186890  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.05 min) Epoch 89/300 -- Iteration 683166 - Batch 5390/7702 - Train loss: 0.00186872  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.07 min) Epoch 89/300 -- Iteration 683243 - Batch 5467/7702 - Train loss: 0.00186879  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.10 min) Epoch 89/300 -- Iteration 683320 - Batch 5544/7702 - Train loss: 0.00186868  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.12 min) Epoch 89/300 -- Iteration 683397 - Batch 5621/7702 - Train loss: 0.00186884  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.14 min) Epoch 89/300 -- Iteration 683474 - Batch 5698/7702 - Train loss: 0.00186884  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.16 min) Epoch 89/300 -- Iteration 683551 - Batch 5775/7702 - Train loss: 0.00186914  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.18 min) Epoch 89/300 -- Iteration 683628 - Batch 5852/7702 - Train loss: 0.00186967  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.20 min) Epoch 89/300 -- Iteration 683705 - Batch 5929/7702 - Train loss: 0.00186921  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.22 min) Epoch 89/300 -- Iteration 683782 - Batch 6006/7702 - Train loss: 0.00186994  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.24 min) Epoch 89/300 -- Iteration 683859 - Batch 6083/7702 - Train loss: 0.00186988  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.26 min) Epoch 89/300 -- Iteration 683936 - Batch 6160/7702 - Train loss: 0.00187036  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.28 min) Epoch 89/300 -- Iteration 684013 - Batch 6237/7702 - Train loss: 0.00187032  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.30 min) Epoch 89/300 -- Iteration 684090 - Batch 6314/7702 - Train loss: 0.00187043  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.32 min) Epoch 89/300 -- Iteration 684167 - Batch 6391/7702 - Train loss: 0.00187061  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.34 min) Epoch 89/300 -- Iteration 684244 - Batch 6468/7702 - Train loss: 0.00187077  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.36 min) Epoch 89/300 -- Iteration 684321 - Batch 6545/7702 - Train loss: 0.00187060  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.39 min) Epoch 89/300 -- Iteration 684398 - Batch 6622/7702 - Train loss: 0.00187034  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.41 min) Epoch 89/300 -- Iteration 684475 - Batch 6699/7702 - Train loss: 0.00187111  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.43 min) Epoch 89/300 -- Iteration 684552 - Batch 6776/7702 - Train loss: 0.00187130  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.45 min) Epoch 89/300 -- Iteration 684629 - Batch 6853/7702 - Train loss: 0.00187136  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.47 min) Epoch 89/300 -- Iteration 684706 - Batch 6930/7702 - Train loss: 0.00187145  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.49 min) Epoch 89/300 -- Iteration 684783 - Batch 7007/7702 - Train loss: 0.00187113  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.51 min) Epoch 89/300 -- Iteration 684860 - Batch 7084/7702 - Train loss: 0.00187057  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.53 min) Epoch 89/300 -- Iteration 684937 - Batch 7161/7702 - Train loss: 0.00187060  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.55 min) Epoch 89/300 -- Iteration 685014 - Batch 7238/7702 - Train loss: 0.00187078  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.57 min) Epoch 89/300 -- Iteration 685091 - Batch 7315/7702 - Train loss: 0.00187092  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.59 min) Epoch 89/300 -- Iteration 685168 - Batch 7392/7702 - Train loss: 0.00187126  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.61 min) Epoch 89/300 -- Iteration 685245 - Batch 7469/7702 - Train loss: 0.00187143  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.63 min) Epoch 89/300 -- Iteration 685322 - Batch 7546/7702 - Train loss: 0.00187119  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.66 min) Epoch 89/300 -- Iteration 685399 - Batch 7623/7702 - Train loss: 0.00187134  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.68 min) Epoch 89/300 -- Iteration 685476 - Batch 7700/7702 - Train loss: 0.00187140  - Train acc: -0.0000 - Val loss: 0.00042356\n",
      "(184.68 min) Epoch 89/300 -- Iteration 685478 - Batch 7701/7702 - Train loss: 0.00187134  - Train acc: -0.0000 - Val loss: 0.00032405 - Val acc: -0.0000\n",
      "(184.70 min) Epoch 90/300 -- Iteration 685555 - Batch 77/7702 - Train loss: 0.00185042  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.72 min) Epoch 90/300 -- Iteration 685632 - Batch 154/7702 - Train loss: 0.00184554  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.74 min) Epoch 90/300 -- Iteration 685709 - Batch 231/7702 - Train loss: 0.00185948  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.76 min) Epoch 90/300 -- Iteration 685786 - Batch 308/7702 - Train loss: 0.00186664  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.78 min) Epoch 90/300 -- Iteration 685863 - Batch 385/7702 - Train loss: 0.00186190  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.80 min) Epoch 90/300 -- Iteration 685940 - Batch 462/7702 - Train loss: 0.00186362  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.82 min) Epoch 90/300 -- Iteration 686017 - Batch 539/7702 - Train loss: 0.00186350  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.84 min) Epoch 90/300 -- Iteration 686094 - Batch 616/7702 - Train loss: 0.00186418  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.87 min) Epoch 90/300 -- Iteration 686171 - Batch 693/7702 - Train loss: 0.00186677  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.89 min) Epoch 90/300 -- Iteration 686248 - Batch 770/7702 - Train loss: 0.00186748  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.91 min) Epoch 90/300 -- Iteration 686325 - Batch 847/7702 - Train loss: 0.00186997  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.93 min) Epoch 90/300 -- Iteration 686402 - Batch 924/7702 - Train loss: 0.00187166  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.95 min) Epoch 90/300 -- Iteration 686479 - Batch 1001/7702 - Train loss: 0.00187435  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.97 min) Epoch 90/300 -- Iteration 686556 - Batch 1078/7702 - Train loss: 0.00187284  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(184.99 min) Epoch 90/300 -- Iteration 686633 - Batch 1155/7702 - Train loss: 0.00187254  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.01 min) Epoch 90/300 -- Iteration 686710 - Batch 1232/7702 - Train loss: 0.00187133  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.03 min) Epoch 90/300 -- Iteration 686787 - Batch 1309/7702 - Train loss: 0.00187265  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.05 min) Epoch 90/300 -- Iteration 686864 - Batch 1386/7702 - Train loss: 0.00187394  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.07 min) Epoch 90/300 -- Iteration 686941 - Batch 1463/7702 - Train loss: 0.00187218  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.09 min) Epoch 90/300 -- Iteration 687018 - Batch 1540/7702 - Train loss: 0.00187227  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.12 min) Epoch 90/300 -- Iteration 687095 - Batch 1617/7702 - Train loss: 0.00187104  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.14 min) Epoch 90/300 -- Iteration 687172 - Batch 1694/7702 - Train loss: 0.00187213  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.16 min) Epoch 90/300 -- Iteration 687249 - Batch 1771/7702 - Train loss: 0.00187259  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.18 min) Epoch 90/300 -- Iteration 687326 - Batch 1848/7702 - Train loss: 0.00186985  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.20 min) Epoch 90/300 -- Iteration 687403 - Batch 1925/7702 - Train loss: 0.00187095  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.22 min) Epoch 90/300 -- Iteration 687480 - Batch 2002/7702 - Train loss: 0.00187070  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.24 min) Epoch 90/300 -- Iteration 687557 - Batch 2079/7702 - Train loss: 0.00186927  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.26 min) Epoch 90/300 -- Iteration 687634 - Batch 2156/7702 - Train loss: 0.00186996  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.28 min) Epoch 90/300 -- Iteration 687711 - Batch 2233/7702 - Train loss: 0.00187040  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.30 min) Epoch 90/300 -- Iteration 687788 - Batch 2310/7702 - Train loss: 0.00186981  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.32 min) Epoch 90/300 -- Iteration 687865 - Batch 2387/7702 - Train loss: 0.00187057  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.34 min) Epoch 90/300 -- Iteration 687942 - Batch 2464/7702 - Train loss: 0.00186957  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.36 min) Epoch 90/300 -- Iteration 688019 - Batch 2541/7702 - Train loss: 0.00186935  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.39 min) Epoch 90/300 -- Iteration 688096 - Batch 2618/7702 - Train loss: 0.00186998  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.41 min) Epoch 90/300 -- Iteration 688173 - Batch 2695/7702 - Train loss: 0.00187022  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.43 min) Epoch 90/300 -- Iteration 688250 - Batch 2772/7702 - Train loss: 0.00186991  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.45 min) Epoch 90/300 -- Iteration 688327 - Batch 2849/7702 - Train loss: 0.00187003  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.47 min) Epoch 90/300 -- Iteration 688404 - Batch 2926/7702 - Train loss: 0.00186955  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.49 min) Epoch 90/300 -- Iteration 688481 - Batch 3003/7702 - Train loss: 0.00186918  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.51 min) Epoch 90/300 -- Iteration 688558 - Batch 3080/7702 - Train loss: 0.00186941  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.53 min) Epoch 90/300 -- Iteration 688635 - Batch 3157/7702 - Train loss: 0.00186979  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.55 min) Epoch 90/300 -- Iteration 688712 - Batch 3234/7702 - Train loss: 0.00187046  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.57 min) Epoch 90/300 -- Iteration 688789 - Batch 3311/7702 - Train loss: 0.00187045  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.59 min) Epoch 90/300 -- Iteration 688866 - Batch 3388/7702 - Train loss: 0.00186946  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.61 min) Epoch 90/300 -- Iteration 688943 - Batch 3465/7702 - Train loss: 0.00186955  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.63 min) Epoch 90/300 -- Iteration 689020 - Batch 3542/7702 - Train loss: 0.00186988  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.66 min) Epoch 90/300 -- Iteration 689097 - Batch 3619/7702 - Train loss: 0.00186929  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.68 min) Epoch 90/300 -- Iteration 689174 - Batch 3696/7702 - Train loss: 0.00186864  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.70 min) Epoch 90/300 -- Iteration 689251 - Batch 3773/7702 - Train loss: 0.00186785  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.72 min) Epoch 90/300 -- Iteration 689328 - Batch 3850/7702 - Train loss: 0.00186773  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.74 min) Epoch 90/300 -- Iteration 689405 - Batch 3927/7702 - Train loss: 0.00186726  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.76 min) Epoch 90/300 -- Iteration 689482 - Batch 4004/7702 - Train loss: 0.00186662  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.78 min) Epoch 90/300 -- Iteration 689559 - Batch 4081/7702 - Train loss: 0.00186653  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.80 min) Epoch 90/300 -- Iteration 689636 - Batch 4158/7702 - Train loss: 0.00186617  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.82 min) Epoch 90/300 -- Iteration 689713 - Batch 4235/7702 - Train loss: 0.00186572  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.84 min) Epoch 90/300 -- Iteration 689790 - Batch 4312/7702 - Train loss: 0.00186590  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.86 min) Epoch 90/300 -- Iteration 689867 - Batch 4389/7702 - Train loss: 0.00186603  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.88 min) Epoch 90/300 -- Iteration 689944 - Batch 4466/7702 - Train loss: 0.00186606  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.90 min) Epoch 90/300 -- Iteration 690021 - Batch 4543/7702 - Train loss: 0.00186610  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.93 min) Epoch 90/300 -- Iteration 690098 - Batch 4620/7702 - Train loss: 0.00186655  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.95 min) Epoch 90/300 -- Iteration 690175 - Batch 4697/7702 - Train loss: 0.00186613  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.97 min) Epoch 90/300 -- Iteration 690252 - Batch 4774/7702 - Train loss: 0.00186667  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(185.99 min) Epoch 90/300 -- Iteration 690329 - Batch 4851/7702 - Train loss: 0.00186632  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.01 min) Epoch 90/300 -- Iteration 690406 - Batch 4928/7702 - Train loss: 0.00186627  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.03 min) Epoch 90/300 -- Iteration 690483 - Batch 5005/7702 - Train loss: 0.00186581  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.05 min) Epoch 90/300 -- Iteration 690560 - Batch 5082/7702 - Train loss: 0.00186613  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.07 min) Epoch 90/300 -- Iteration 690637 - Batch 5159/7702 - Train loss: 0.00186603  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.09 min) Epoch 90/300 -- Iteration 690714 - Batch 5236/7702 - Train loss: 0.00186660  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.11 min) Epoch 90/300 -- Iteration 690791 - Batch 5313/7702 - Train loss: 0.00186707  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.13 min) Epoch 90/300 -- Iteration 690868 - Batch 5390/7702 - Train loss: 0.00186736  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.15 min) Epoch 90/300 -- Iteration 690945 - Batch 5467/7702 - Train loss: 0.00186746  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.17 min) Epoch 90/300 -- Iteration 691022 - Batch 5544/7702 - Train loss: 0.00186726  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.19 min) Epoch 90/300 -- Iteration 691099 - Batch 5621/7702 - Train loss: 0.00186730  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.22 min) Epoch 90/300 -- Iteration 691176 - Batch 5698/7702 - Train loss: 0.00186757  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.24 min) Epoch 90/300 -- Iteration 691253 - Batch 5775/7702 - Train loss: 0.00186729  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.26 min) Epoch 90/300 -- Iteration 691330 - Batch 5852/7702 - Train loss: 0.00186744  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.28 min) Epoch 90/300 -- Iteration 691407 - Batch 5929/7702 - Train loss: 0.00186754  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.30 min) Epoch 90/300 -- Iteration 691484 - Batch 6006/7702 - Train loss: 0.00186737  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.32 min) Epoch 90/300 -- Iteration 691561 - Batch 6083/7702 - Train loss: 0.00186725  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.34 min) Epoch 90/300 -- Iteration 691638 - Batch 6160/7702 - Train loss: 0.00186770  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.36 min) Epoch 90/300 -- Iteration 691715 - Batch 6237/7702 - Train loss: 0.00186750  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.38 min) Epoch 90/300 -- Iteration 691792 - Batch 6314/7702 - Train loss: 0.00186809  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.40 min) Epoch 90/300 -- Iteration 691869 - Batch 6391/7702 - Train loss: 0.00186829  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.42 min) Epoch 90/300 -- Iteration 691946 - Batch 6468/7702 - Train loss: 0.00186834  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.44 min) Epoch 90/300 -- Iteration 692023 - Batch 6545/7702 - Train loss: 0.00186809  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.46 min) Epoch 90/300 -- Iteration 692100 - Batch 6622/7702 - Train loss: 0.00186804  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.49 min) Epoch 90/300 -- Iteration 692177 - Batch 6699/7702 - Train loss: 0.00186812  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.51 min) Epoch 90/300 -- Iteration 692254 - Batch 6776/7702 - Train loss: 0.00186795  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.53 min) Epoch 90/300 -- Iteration 692331 - Batch 6853/7702 - Train loss: 0.00186816  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.55 min) Epoch 90/300 -- Iteration 692408 - Batch 6930/7702 - Train loss: 0.00186805  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.57 min) Epoch 90/300 -- Iteration 692485 - Batch 7007/7702 - Train loss: 0.00186790  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.59 min) Epoch 90/300 -- Iteration 692562 - Batch 7084/7702 - Train loss: 0.00186822  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.61 min) Epoch 90/300 -- Iteration 692639 - Batch 7161/7702 - Train loss: 0.00186796  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.63 min) Epoch 90/300 -- Iteration 692716 - Batch 7238/7702 - Train loss: 0.00186812  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.65 min) Epoch 90/300 -- Iteration 692793 - Batch 7315/7702 - Train loss: 0.00186807  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.67 min) Epoch 90/300 -- Iteration 692870 - Batch 7392/7702 - Train loss: 0.00186805  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.69 min) Epoch 90/300 -- Iteration 692947 - Batch 7469/7702 - Train loss: 0.00186823  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.71 min) Epoch 90/300 -- Iteration 693024 - Batch 7546/7702 - Train loss: 0.00186794  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.73 min) Epoch 90/300 -- Iteration 693101 - Batch 7623/7702 - Train loss: 0.00186809  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.75 min) Epoch 90/300 -- Iteration 693178 - Batch 7700/7702 - Train loss: 0.00186839  - Train acc: -0.0000 - Val loss: 0.00032405\n",
      "(186.76 min) Epoch 90/300 -- Iteration 693180 - Batch 7701/7702 - Train loss: 0.00186841  - Train acc: -0.0000 - Val loss: 0.00033868 - Val acc: -0.0000\n",
      "(186.78 min) Epoch 91/300 -- Iteration 693257 - Batch 77/7702 - Train loss: 0.00188087  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.80 min) Epoch 91/300 -- Iteration 693334 - Batch 154/7702 - Train loss: 0.00187810  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.82 min) Epoch 91/300 -- Iteration 693411 - Batch 231/7702 - Train loss: 0.00186362  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.84 min) Epoch 91/300 -- Iteration 693488 - Batch 308/7702 - Train loss: 0.00186607  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.86 min) Epoch 91/300 -- Iteration 693565 - Batch 385/7702 - Train loss: 0.00186281  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.88 min) Epoch 91/300 -- Iteration 693642 - Batch 462/7702 - Train loss: 0.00186386  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.90 min) Epoch 91/300 -- Iteration 693719 - Batch 539/7702 - Train loss: 0.00186617  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.92 min) Epoch 91/300 -- Iteration 693796 - Batch 616/7702 - Train loss: 0.00186578  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.94 min) Epoch 91/300 -- Iteration 693873 - Batch 693/7702 - Train loss: 0.00187001  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.97 min) Epoch 91/300 -- Iteration 693950 - Batch 770/7702 - Train loss: 0.00187326  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(186.99 min) Epoch 91/300 -- Iteration 694027 - Batch 847/7702 - Train loss: 0.00187622  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.01 min) Epoch 91/300 -- Iteration 694104 - Batch 924/7702 - Train loss: 0.00187500  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.03 min) Epoch 91/300 -- Iteration 694181 - Batch 1001/7702 - Train loss: 0.00187302  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.05 min) Epoch 91/300 -- Iteration 694258 - Batch 1078/7702 - Train loss: 0.00187410  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.07 min) Epoch 91/300 -- Iteration 694335 - Batch 1155/7702 - Train loss: 0.00187284  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.09 min) Epoch 91/300 -- Iteration 694412 - Batch 1232/7702 - Train loss: 0.00187516  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.11 min) Epoch 91/300 -- Iteration 694489 - Batch 1309/7702 - Train loss: 0.00187230  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.13 min) Epoch 91/300 -- Iteration 694566 - Batch 1386/7702 - Train loss: 0.00187283  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.15 min) Epoch 91/300 -- Iteration 694643 - Batch 1463/7702 - Train loss: 0.00187486  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.17 min) Epoch 91/300 -- Iteration 694720 - Batch 1540/7702 - Train loss: 0.00187417  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.19 min) Epoch 91/300 -- Iteration 694797 - Batch 1617/7702 - Train loss: 0.00187341  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.21 min) Epoch 91/300 -- Iteration 694874 - Batch 1694/7702 - Train loss: 0.00187254  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.23 min) Epoch 91/300 -- Iteration 694951 - Batch 1771/7702 - Train loss: 0.00187173  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.26 min) Epoch 91/300 -- Iteration 695028 - Batch 1848/7702 - Train loss: 0.00187159  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.28 min) Epoch 91/300 -- Iteration 695105 - Batch 1925/7702 - Train loss: 0.00187109  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.30 min) Epoch 91/300 -- Iteration 695182 - Batch 2002/7702 - Train loss: 0.00187217  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.32 min) Epoch 91/300 -- Iteration 695259 - Batch 2079/7702 - Train loss: 0.00187260  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.34 min) Epoch 91/300 -- Iteration 695336 - Batch 2156/7702 - Train loss: 0.00187389  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.36 min) Epoch 91/300 -- Iteration 695413 - Batch 2233/7702 - Train loss: 0.00187484  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.38 min) Epoch 91/300 -- Iteration 695490 - Batch 2310/7702 - Train loss: 0.00187463  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.40 min) Epoch 91/300 -- Iteration 695567 - Batch 2387/7702 - Train loss: 0.00187407  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.42 min) Epoch 91/300 -- Iteration 695644 - Batch 2464/7702 - Train loss: 0.00187437  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.44 min) Epoch 91/300 -- Iteration 695721 - Batch 2541/7702 - Train loss: 0.00187386  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.46 min) Epoch 91/300 -- Iteration 695798 - Batch 2618/7702 - Train loss: 0.00187460  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.48 min) Epoch 91/300 -- Iteration 695875 - Batch 2695/7702 - Train loss: 0.00187489  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.50 min) Epoch 91/300 -- Iteration 695952 - Batch 2772/7702 - Train loss: 0.00187413  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.53 min) Epoch 91/300 -- Iteration 696029 - Batch 2849/7702 - Train loss: 0.00187340  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.55 min) Epoch 91/300 -- Iteration 696106 - Batch 2926/7702 - Train loss: 0.00187336  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.57 min) Epoch 91/300 -- Iteration 696183 - Batch 3003/7702 - Train loss: 0.00187350  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.59 min) Epoch 91/300 -- Iteration 696260 - Batch 3080/7702 - Train loss: 0.00187320  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.61 min) Epoch 91/300 -- Iteration 696337 - Batch 3157/7702 - Train loss: 0.00187283  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.63 min) Epoch 91/300 -- Iteration 696414 - Batch 3234/7702 - Train loss: 0.00187245  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.65 min) Epoch 91/300 -- Iteration 696491 - Batch 3311/7702 - Train loss: 0.00187193  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.67 min) Epoch 91/300 -- Iteration 696568 - Batch 3388/7702 - Train loss: 0.00187171  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.69 min) Epoch 91/300 -- Iteration 696645 - Batch 3465/7702 - Train loss: 0.00187156  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.71 min) Epoch 91/300 -- Iteration 696722 - Batch 3542/7702 - Train loss: 0.00187209  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.73 min) Epoch 91/300 -- Iteration 696799 - Batch 3619/7702 - Train loss: 0.00187134  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.75 min) Epoch 91/300 -- Iteration 696876 - Batch 3696/7702 - Train loss: 0.00187079  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.78 min) Epoch 91/300 -- Iteration 696953 - Batch 3773/7702 - Train loss: 0.00187080  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.80 min) Epoch 91/300 -- Iteration 697030 - Batch 3850/7702 - Train loss: 0.00187065  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.82 min) Epoch 91/300 -- Iteration 697107 - Batch 3927/7702 - Train loss: 0.00187016  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.84 min) Epoch 91/300 -- Iteration 697184 - Batch 4004/7702 - Train loss: 0.00186978  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.86 min) Epoch 91/300 -- Iteration 697261 - Batch 4081/7702 - Train loss: 0.00186949  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.88 min) Epoch 91/300 -- Iteration 697338 - Batch 4158/7702 - Train loss: 0.00186846  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.90 min) Epoch 91/300 -- Iteration 697415 - Batch 4235/7702 - Train loss: 0.00186840  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.92 min) Epoch 91/300 -- Iteration 697492 - Batch 4312/7702 - Train loss: 0.00186821  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.94 min) Epoch 91/300 -- Iteration 697569 - Batch 4389/7702 - Train loss: 0.00186839  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.96 min) Epoch 91/300 -- Iteration 697646 - Batch 4466/7702 - Train loss: 0.00186873  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(187.98 min) Epoch 91/300 -- Iteration 697723 - Batch 4543/7702 - Train loss: 0.00186816  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.00 min) Epoch 91/300 -- Iteration 697800 - Batch 4620/7702 - Train loss: 0.00186816  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.02 min) Epoch 91/300 -- Iteration 697877 - Batch 4697/7702 - Train loss: 0.00186797  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.05 min) Epoch 91/300 -- Iteration 697954 - Batch 4774/7702 - Train loss: 0.00186792  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.07 min) Epoch 91/300 -- Iteration 698031 - Batch 4851/7702 - Train loss: 0.00186818  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.09 min) Epoch 91/300 -- Iteration 698108 - Batch 4928/7702 - Train loss: 0.00186857  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.11 min) Epoch 91/300 -- Iteration 698185 - Batch 5005/7702 - Train loss: 0.00186876  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.13 min) Epoch 91/300 -- Iteration 698262 - Batch 5082/7702 - Train loss: 0.00186805  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.15 min) Epoch 91/300 -- Iteration 698339 - Batch 5159/7702 - Train loss: 0.00186786  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.17 min) Epoch 91/300 -- Iteration 698416 - Batch 5236/7702 - Train loss: 0.00186845  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.19 min) Epoch 91/300 -- Iteration 698493 - Batch 5313/7702 - Train loss: 0.00186866  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.21 min) Epoch 91/300 -- Iteration 698570 - Batch 5390/7702 - Train loss: 0.00186912  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.23 min) Epoch 91/300 -- Iteration 698647 - Batch 5467/7702 - Train loss: 0.00186942  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.25 min) Epoch 91/300 -- Iteration 698724 - Batch 5544/7702 - Train loss: 0.00186919  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.27 min) Epoch 91/300 -- Iteration 698801 - Batch 5621/7702 - Train loss: 0.00186909  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.29 min) Epoch 91/300 -- Iteration 698878 - Batch 5698/7702 - Train loss: 0.00186929  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.32 min) Epoch 91/300 -- Iteration 698955 - Batch 5775/7702 - Train loss: 0.00186941  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.34 min) Epoch 91/300 -- Iteration 699032 - Batch 5852/7702 - Train loss: 0.00186889  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.36 min) Epoch 91/300 -- Iteration 699109 - Batch 5929/7702 - Train loss: 0.00186927  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.38 min) Epoch 91/300 -- Iteration 699186 - Batch 6006/7702 - Train loss: 0.00186969  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.40 min) Epoch 91/300 -- Iteration 699263 - Batch 6083/7702 - Train loss: 0.00186983  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.42 min) Epoch 91/300 -- Iteration 699340 - Batch 6160/7702 - Train loss: 0.00187011  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.44 min) Epoch 91/300 -- Iteration 699417 - Batch 6237/7702 - Train loss: 0.00186978  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.46 min) Epoch 91/300 -- Iteration 699494 - Batch 6314/7702 - Train loss: 0.00186948  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.48 min) Epoch 91/300 -- Iteration 699571 - Batch 6391/7702 - Train loss: 0.00186974  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.50 min) Epoch 91/300 -- Iteration 699648 - Batch 6468/7702 - Train loss: 0.00186953  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.52 min) Epoch 91/300 -- Iteration 699725 - Batch 6545/7702 - Train loss: 0.00186971  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.54 min) Epoch 91/300 -- Iteration 699802 - Batch 6622/7702 - Train loss: 0.00186934  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.56 min) Epoch 91/300 -- Iteration 699879 - Batch 6699/7702 - Train loss: 0.00186954  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.59 min) Epoch 91/300 -- Iteration 699956 - Batch 6776/7702 - Train loss: 0.00186881  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.61 min) Epoch 91/300 -- Iteration 700033 - Batch 6853/7702 - Train loss: 0.00186897  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.63 min) Epoch 91/300 -- Iteration 700110 - Batch 6930/7702 - Train loss: 0.00186901  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.65 min) Epoch 91/300 -- Iteration 700187 - Batch 7007/7702 - Train loss: 0.00186908  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.67 min) Epoch 91/300 -- Iteration 700264 - Batch 7084/7702 - Train loss: 0.00186905  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.69 min) Epoch 91/300 -- Iteration 700341 - Batch 7161/7702 - Train loss: 0.00186886  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.71 min) Epoch 91/300 -- Iteration 700418 - Batch 7238/7702 - Train loss: 0.00186866  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.73 min) Epoch 91/300 -- Iteration 700495 - Batch 7315/7702 - Train loss: 0.00186902  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.75 min) Epoch 91/300 -- Iteration 700572 - Batch 7392/7702 - Train loss: 0.00186915  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.77 min) Epoch 91/300 -- Iteration 700649 - Batch 7469/7702 - Train loss: 0.00186925  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.79 min) Epoch 91/300 -- Iteration 700726 - Batch 7546/7702 - Train loss: 0.00186932  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.81 min) Epoch 91/300 -- Iteration 700803 - Batch 7623/7702 - Train loss: 0.00186932  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.83 min) Epoch 91/300 -- Iteration 700880 - Batch 7700/7702 - Train loss: 0.00186913  - Train acc: -0.0000 - Val loss: 0.00033868\n",
      "(188.83 min) Epoch 91/300 -- Iteration 700882 - Batch 7701/7702 - Train loss: 0.00186912  - Train acc: -0.0000 - Val loss: 0.00037478 - Val acc: -0.0000\n",
      "(188.86 min) Epoch 92/300 -- Iteration 700959 - Batch 77/7702 - Train loss: 0.00187310  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(188.88 min) Epoch 92/300 -- Iteration 701036 - Batch 154/7702 - Train loss: 0.00187393  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(188.90 min) Epoch 92/300 -- Iteration 701113 - Batch 231/7702 - Train loss: 0.00186850  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(188.92 min) Epoch 92/300 -- Iteration 701190 - Batch 308/7702 - Train loss: 0.00186210  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(188.94 min) Epoch 92/300 -- Iteration 701267 - Batch 385/7702 - Train loss: 0.00186465  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(188.96 min) Epoch 92/300 -- Iteration 701344 - Batch 462/7702 - Train loss: 0.00187381  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(188.98 min) Epoch 92/300 -- Iteration 701421 - Batch 539/7702 - Train loss: 0.00187068  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.00 min) Epoch 92/300 -- Iteration 701498 - Batch 616/7702 - Train loss: 0.00187253  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.02 min) Epoch 92/300 -- Iteration 701575 - Batch 693/7702 - Train loss: 0.00187700  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.04 min) Epoch 92/300 -- Iteration 701652 - Batch 770/7702 - Train loss: 0.00187159  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.07 min) Epoch 92/300 -- Iteration 701729 - Batch 847/7702 - Train loss: 0.00187422  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.09 min) Epoch 92/300 -- Iteration 701806 - Batch 924/7702 - Train loss: 0.00187391  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.11 min) Epoch 92/300 -- Iteration 701883 - Batch 1001/7702 - Train loss: 0.00187380  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.13 min) Epoch 92/300 -- Iteration 701960 - Batch 1078/7702 - Train loss: 0.00187551  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.15 min) Epoch 92/300 -- Iteration 702037 - Batch 1155/7702 - Train loss: 0.00187545  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.17 min) Epoch 92/300 -- Iteration 702114 - Batch 1232/7702 - Train loss: 0.00187386  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.19 min) Epoch 92/300 -- Iteration 702191 - Batch 1309/7702 - Train loss: 0.00187410  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.21 min) Epoch 92/300 -- Iteration 702268 - Batch 1386/7702 - Train loss: 0.00187371  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.23 min) Epoch 92/300 -- Iteration 702345 - Batch 1463/7702 - Train loss: 0.00187379  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.25 min) Epoch 92/300 -- Iteration 702422 - Batch 1540/7702 - Train loss: 0.00187274  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.27 min) Epoch 92/300 -- Iteration 702499 - Batch 1617/7702 - Train loss: 0.00187247  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.29 min) Epoch 92/300 -- Iteration 702576 - Batch 1694/7702 - Train loss: 0.00187186  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.31 min) Epoch 92/300 -- Iteration 702653 - Batch 1771/7702 - Train loss: 0.00187169  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.33 min) Epoch 92/300 -- Iteration 702730 - Batch 1848/7702 - Train loss: 0.00187126  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.36 min) Epoch 92/300 -- Iteration 702807 - Batch 1925/7702 - Train loss: 0.00187044  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.38 min) Epoch 92/300 -- Iteration 702884 - Batch 2002/7702 - Train loss: 0.00187138  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.40 min) Epoch 92/300 -- Iteration 702961 - Batch 2079/7702 - Train loss: 0.00187196  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.42 min) Epoch 92/300 -- Iteration 703038 - Batch 2156/7702 - Train loss: 0.00187389  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.44 min) Epoch 92/300 -- Iteration 703115 - Batch 2233/7702 - Train loss: 0.00187385  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.46 min) Epoch 92/300 -- Iteration 703192 - Batch 2310/7702 - Train loss: 0.00187371  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.48 min) Epoch 92/300 -- Iteration 703269 - Batch 2387/7702 - Train loss: 0.00187330  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.50 min) Epoch 92/300 -- Iteration 703346 - Batch 2464/7702 - Train loss: 0.00187275  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.52 min) Epoch 92/300 -- Iteration 703423 - Batch 2541/7702 - Train loss: 0.00187334  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.54 min) Epoch 92/300 -- Iteration 703500 - Batch 2618/7702 - Train loss: 0.00187282  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.56 min) Epoch 92/300 -- Iteration 703577 - Batch 2695/7702 - Train loss: 0.00187285  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.58 min) Epoch 92/300 -- Iteration 703654 - Batch 2772/7702 - Train loss: 0.00187198  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.60 min) Epoch 92/300 -- Iteration 703731 - Batch 2849/7702 - Train loss: 0.00187182  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.62 min) Epoch 92/300 -- Iteration 703808 - Batch 2926/7702 - Train loss: 0.00187260  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.65 min) Epoch 92/300 -- Iteration 703885 - Batch 3003/7702 - Train loss: 0.00187175  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.67 min) Epoch 92/300 -- Iteration 703962 - Batch 3080/7702 - Train loss: 0.00187181  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.69 min) Epoch 92/300 -- Iteration 704039 - Batch 3157/7702 - Train loss: 0.00187147  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.71 min) Epoch 92/300 -- Iteration 704116 - Batch 3234/7702 - Train loss: 0.00187141  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.73 min) Epoch 92/300 -- Iteration 704193 - Batch 3311/7702 - Train loss: 0.00187071  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.75 min) Epoch 92/300 -- Iteration 704270 - Batch 3388/7702 - Train loss: 0.00187017  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.77 min) Epoch 92/300 -- Iteration 704347 - Batch 3465/7702 - Train loss: 0.00187020  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.79 min) Epoch 92/300 -- Iteration 704424 - Batch 3542/7702 - Train loss: 0.00186904  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.81 min) Epoch 92/300 -- Iteration 704501 - Batch 3619/7702 - Train loss: 0.00186872  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.83 min) Epoch 92/300 -- Iteration 704578 - Batch 3696/7702 - Train loss: 0.00186875  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.85 min) Epoch 92/300 -- Iteration 704655 - Batch 3773/7702 - Train loss: 0.00186860  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.87 min) Epoch 92/300 -- Iteration 704732 - Batch 3850/7702 - Train loss: 0.00186866  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.89 min) Epoch 92/300 -- Iteration 704809 - Batch 3927/7702 - Train loss: 0.00186844  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.91 min) Epoch 92/300 -- Iteration 704886 - Batch 4004/7702 - Train loss: 0.00186782  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.94 min) Epoch 92/300 -- Iteration 704963 - Batch 4081/7702 - Train loss: 0.00186823  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.96 min) Epoch 92/300 -- Iteration 705040 - Batch 4158/7702 - Train loss: 0.00186842  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(189.98 min) Epoch 92/300 -- Iteration 705117 - Batch 4235/7702 - Train loss: 0.00186875  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.00 min) Epoch 92/300 -- Iteration 705194 - Batch 4312/7702 - Train loss: 0.00186865  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.02 min) Epoch 92/300 -- Iteration 705271 - Batch 4389/7702 - Train loss: 0.00186782  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.04 min) Epoch 92/300 -- Iteration 705348 - Batch 4466/7702 - Train loss: 0.00186756  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.06 min) Epoch 92/300 -- Iteration 705425 - Batch 4543/7702 - Train loss: 0.00186790  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.08 min) Epoch 92/300 -- Iteration 705502 - Batch 4620/7702 - Train loss: 0.00186804  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.10 min) Epoch 92/300 -- Iteration 705579 - Batch 4697/7702 - Train loss: 0.00186829  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.12 min) Epoch 92/300 -- Iteration 705656 - Batch 4774/7702 - Train loss: 0.00186864  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.14 min) Epoch 92/300 -- Iteration 705733 - Batch 4851/7702 - Train loss: 0.00186816  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.16 min) Epoch 92/300 -- Iteration 705810 - Batch 4928/7702 - Train loss: 0.00186767  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.19 min) Epoch 92/300 -- Iteration 705887 - Batch 5005/7702 - Train loss: 0.00186763  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.21 min) Epoch 92/300 -- Iteration 705964 - Batch 5082/7702 - Train loss: 0.00186778  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.23 min) Epoch 92/300 -- Iteration 706041 - Batch 5159/7702 - Train loss: 0.00186770  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.25 min) Epoch 92/300 -- Iteration 706118 - Batch 5236/7702 - Train loss: 0.00186771  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.27 min) Epoch 92/300 -- Iteration 706195 - Batch 5313/7702 - Train loss: 0.00186742  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.29 min) Epoch 92/300 -- Iteration 706272 - Batch 5390/7702 - Train loss: 0.00186732  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.31 min) Epoch 92/300 -- Iteration 706349 - Batch 5467/7702 - Train loss: 0.00186731  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.33 min) Epoch 92/300 -- Iteration 706426 - Batch 5544/7702 - Train loss: 0.00186717  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.35 min) Epoch 92/300 -- Iteration 706503 - Batch 5621/7702 - Train loss: 0.00186712  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.37 min) Epoch 92/300 -- Iteration 706580 - Batch 5698/7702 - Train loss: 0.00186712  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.39 min) Epoch 92/300 -- Iteration 706657 - Batch 5775/7702 - Train loss: 0.00186738  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.41 min) Epoch 92/300 -- Iteration 706734 - Batch 5852/7702 - Train loss: 0.00186727  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.43 min) Epoch 92/300 -- Iteration 706811 - Batch 5929/7702 - Train loss: 0.00186750  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.45 min) Epoch 92/300 -- Iteration 706888 - Batch 6006/7702 - Train loss: 0.00186771  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.47 min) Epoch 92/300 -- Iteration 706965 - Batch 6083/7702 - Train loss: 0.00186695  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.50 min) Epoch 92/300 -- Iteration 707042 - Batch 6160/7702 - Train loss: 0.00186654  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.52 min) Epoch 92/300 -- Iteration 707119 - Batch 6237/7702 - Train loss: 0.00186626  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.54 min) Epoch 92/300 -- Iteration 707196 - Batch 6314/7702 - Train loss: 0.00186636  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.56 min) Epoch 92/300 -- Iteration 707273 - Batch 6391/7702 - Train loss: 0.00186645  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.58 min) Epoch 92/300 -- Iteration 707350 - Batch 6468/7702 - Train loss: 0.00186639  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.60 min) Epoch 92/300 -- Iteration 707427 - Batch 6545/7702 - Train loss: 0.00186627  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.62 min) Epoch 92/300 -- Iteration 707504 - Batch 6622/7702 - Train loss: 0.00186629  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.64 min) Epoch 92/300 -- Iteration 707581 - Batch 6699/7702 - Train loss: 0.00186638  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.66 min) Epoch 92/300 -- Iteration 707658 - Batch 6776/7702 - Train loss: 0.00186637  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.68 min) Epoch 92/300 -- Iteration 707735 - Batch 6853/7702 - Train loss: 0.00186650  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.70 min) Epoch 92/300 -- Iteration 707812 - Batch 6930/7702 - Train loss: 0.00186616  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.72 min) Epoch 92/300 -- Iteration 707889 - Batch 7007/7702 - Train loss: 0.00186621  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.74 min) Epoch 92/300 -- Iteration 707966 - Batch 7084/7702 - Train loss: 0.00186606  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.77 min) Epoch 92/300 -- Iteration 708043 - Batch 7161/7702 - Train loss: 0.00186611  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.79 min) Epoch 92/300 -- Iteration 708120 - Batch 7238/7702 - Train loss: 0.00186610  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.81 min) Epoch 92/300 -- Iteration 708197 - Batch 7315/7702 - Train loss: 0.00186599  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.83 min) Epoch 92/300 -- Iteration 708274 - Batch 7392/7702 - Train loss: 0.00186678  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.85 min) Epoch 92/300 -- Iteration 708351 - Batch 7469/7702 - Train loss: 0.00186659  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.87 min) Epoch 92/300 -- Iteration 708428 - Batch 7546/7702 - Train loss: 0.00186641  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.89 min) Epoch 92/300 -- Iteration 708505 - Batch 7623/7702 - Train loss: 0.00186650  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.91 min) Epoch 92/300 -- Iteration 708582 - Batch 7700/7702 - Train loss: 0.00186625  - Train acc: -0.0000 - Val loss: 0.00037478\n",
      "(190.91 min) Epoch 92/300 -- Iteration 708584 - Batch 7701/7702 - Train loss: 0.00186622  - Train acc: -0.0000 - Val loss: 0.00039633 - Val acc: -0.0000\n",
      "(190.93 min) Epoch 93/300 -- Iteration 708661 - Batch 77/7702 - Train loss: 0.00186901  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(190.95 min) Epoch 93/300 -- Iteration 708738 - Batch 154/7702 - Train loss: 0.00187152  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(190.98 min) Epoch 93/300 -- Iteration 708815 - Batch 231/7702 - Train loss: 0.00188098  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.00 min) Epoch 93/300 -- Iteration 708892 - Batch 308/7702 - Train loss: 0.00189046  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.02 min) Epoch 93/300 -- Iteration 708969 - Batch 385/7702 - Train loss: 0.00188783  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.04 min) Epoch 93/300 -- Iteration 709046 - Batch 462/7702 - Train loss: 0.00188843  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.06 min) Epoch 93/300 -- Iteration 709123 - Batch 539/7702 - Train loss: 0.00189104  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.08 min) Epoch 93/300 -- Iteration 709200 - Batch 616/7702 - Train loss: 0.00189299  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.10 min) Epoch 93/300 -- Iteration 709277 - Batch 693/7702 - Train loss: 0.00189038  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.12 min) Epoch 93/300 -- Iteration 709354 - Batch 770/7702 - Train loss: 0.00189170  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.14 min) Epoch 93/300 -- Iteration 709431 - Batch 847/7702 - Train loss: 0.00188732  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.16 min) Epoch 93/300 -- Iteration 709508 - Batch 924/7702 - Train loss: 0.00188748  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.18 min) Epoch 93/300 -- Iteration 709585 - Batch 1001/7702 - Train loss: 0.00188665  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.20 min) Epoch 93/300 -- Iteration 709662 - Batch 1078/7702 - Train loss: 0.00188399  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.22 min) Epoch 93/300 -- Iteration 709739 - Batch 1155/7702 - Train loss: 0.00188267  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.25 min) Epoch 93/300 -- Iteration 709816 - Batch 1232/7702 - Train loss: 0.00188162  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.27 min) Epoch 93/300 -- Iteration 709893 - Batch 1309/7702 - Train loss: 0.00187966  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.29 min) Epoch 93/300 -- Iteration 709970 - Batch 1386/7702 - Train loss: 0.00187948  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.31 min) Epoch 93/300 -- Iteration 710047 - Batch 1463/7702 - Train loss: 0.00188036  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.33 min) Epoch 93/300 -- Iteration 710124 - Batch 1540/7702 - Train loss: 0.00187830  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.35 min) Epoch 93/300 -- Iteration 710201 - Batch 1617/7702 - Train loss: 0.00188014  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.37 min) Epoch 93/300 -- Iteration 710278 - Batch 1694/7702 - Train loss: 0.00187892  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.39 min) Epoch 93/300 -- Iteration 710355 - Batch 1771/7702 - Train loss: 0.00187832  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.41 min) Epoch 93/300 -- Iteration 710432 - Batch 1848/7702 - Train loss: 0.00187778  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.43 min) Epoch 93/300 -- Iteration 710509 - Batch 1925/7702 - Train loss: 0.00187751  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.45 min) Epoch 93/300 -- Iteration 710586 - Batch 2002/7702 - Train loss: 0.00187813  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.47 min) Epoch 93/300 -- Iteration 710663 - Batch 2079/7702 - Train loss: 0.00187855  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.49 min) Epoch 93/300 -- Iteration 710740 - Batch 2156/7702 - Train loss: 0.00187831  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.51 min) Epoch 93/300 -- Iteration 710817 - Batch 2233/7702 - Train loss: 0.00187832  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.53 min) Epoch 93/300 -- Iteration 710894 - Batch 2310/7702 - Train loss: 0.00187812  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.56 min) Epoch 93/300 -- Iteration 710971 - Batch 2387/7702 - Train loss: 0.00187706  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.58 min) Epoch 93/300 -- Iteration 711048 - Batch 2464/7702 - Train loss: 0.00187651  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.60 min) Epoch 93/300 -- Iteration 711125 - Batch 2541/7702 - Train loss: 0.00187580  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.62 min) Epoch 93/300 -- Iteration 711202 - Batch 2618/7702 - Train loss: 0.00187403  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.64 min) Epoch 93/300 -- Iteration 711279 - Batch 2695/7702 - Train loss: 0.00187440  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.66 min) Epoch 93/300 -- Iteration 711356 - Batch 2772/7702 - Train loss: 0.00187394  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.68 min) Epoch 93/300 -- Iteration 711433 - Batch 2849/7702 - Train loss: 0.00187437  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.70 min) Epoch 93/300 -- Iteration 711510 - Batch 2926/7702 - Train loss: 0.00187377  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.72 min) Epoch 93/300 -- Iteration 711587 - Batch 3003/7702 - Train loss: 0.00187366  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.74 min) Epoch 93/300 -- Iteration 711664 - Batch 3080/7702 - Train loss: 0.00187338  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.76 min) Epoch 93/300 -- Iteration 711741 - Batch 3157/7702 - Train loss: 0.00187366  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.78 min) Epoch 93/300 -- Iteration 711818 - Batch 3234/7702 - Train loss: 0.00187434  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.81 min) Epoch 93/300 -- Iteration 711895 - Batch 3311/7702 - Train loss: 0.00187540  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.83 min) Epoch 93/300 -- Iteration 711972 - Batch 3388/7702 - Train loss: 0.00187433  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.85 min) Epoch 93/300 -- Iteration 712049 - Batch 3465/7702 - Train loss: 0.00187482  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.87 min) Epoch 93/300 -- Iteration 712126 - Batch 3542/7702 - Train loss: 0.00187407  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.89 min) Epoch 93/300 -- Iteration 712203 - Batch 3619/7702 - Train loss: 0.00187350  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.91 min) Epoch 93/300 -- Iteration 712280 - Batch 3696/7702 - Train loss: 0.00187296  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.93 min) Epoch 93/300 -- Iteration 712357 - Batch 3773/7702 - Train loss: 0.00187228  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.95 min) Epoch 93/300 -- Iteration 712434 - Batch 3850/7702 - Train loss: 0.00187199  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.97 min) Epoch 93/300 -- Iteration 712511 - Batch 3927/7702 - Train loss: 0.00187203  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(191.99 min) Epoch 93/300 -- Iteration 712588 - Batch 4004/7702 - Train loss: 0.00187136  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.01 min) Epoch 93/300 -- Iteration 712665 - Batch 4081/7702 - Train loss: 0.00187096  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.03 min) Epoch 93/300 -- Iteration 712742 - Batch 4158/7702 - Train loss: 0.00187124  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.05 min) Epoch 93/300 -- Iteration 712819 - Batch 4235/7702 - Train loss: 0.00187144  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.07 min) Epoch 93/300 -- Iteration 712896 - Batch 4312/7702 - Train loss: 0.00187103  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.10 min) Epoch 93/300 -- Iteration 712973 - Batch 4389/7702 - Train loss: 0.00187051  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.12 min) Epoch 93/300 -- Iteration 713050 - Batch 4466/7702 - Train loss: 0.00187012  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.14 min) Epoch 93/300 -- Iteration 713127 - Batch 4543/7702 - Train loss: 0.00187017  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.16 min) Epoch 93/300 -- Iteration 713204 - Batch 4620/7702 - Train loss: 0.00187026  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.18 min) Epoch 93/300 -- Iteration 713281 - Batch 4697/7702 - Train loss: 0.00186984  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.20 min) Epoch 93/300 -- Iteration 713358 - Batch 4774/7702 - Train loss: 0.00186984  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.22 min) Epoch 93/300 -- Iteration 713435 - Batch 4851/7702 - Train loss: 0.00187048  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.24 min) Epoch 93/300 -- Iteration 713512 - Batch 4928/7702 - Train loss: 0.00187037  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.26 min) Epoch 93/300 -- Iteration 713589 - Batch 5005/7702 - Train loss: 0.00187057  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.28 min) Epoch 93/300 -- Iteration 713666 - Batch 5082/7702 - Train loss: 0.00187121  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.30 min) Epoch 93/300 -- Iteration 713743 - Batch 5159/7702 - Train loss: 0.00187050  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.32 min) Epoch 93/300 -- Iteration 713820 - Batch 5236/7702 - Train loss: 0.00187053  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.34 min) Epoch 93/300 -- Iteration 713897 - Batch 5313/7702 - Train loss: 0.00187060  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.37 min) Epoch 93/300 -- Iteration 713974 - Batch 5390/7702 - Train loss: 0.00187048  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.39 min) Epoch 93/300 -- Iteration 714051 - Batch 5467/7702 - Train loss: 0.00187031  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.41 min) Epoch 93/300 -- Iteration 714128 - Batch 5544/7702 - Train loss: 0.00187021  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.43 min) Epoch 93/300 -- Iteration 714205 - Batch 5621/7702 - Train loss: 0.00186992  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.45 min) Epoch 93/300 -- Iteration 714282 - Batch 5698/7702 - Train loss: 0.00187003  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.47 min) Epoch 93/300 -- Iteration 714359 - Batch 5775/7702 - Train loss: 0.00187010  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.49 min) Epoch 93/300 -- Iteration 714436 - Batch 5852/7702 - Train loss: 0.00187065  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.51 min) Epoch 93/300 -- Iteration 714513 - Batch 5929/7702 - Train loss: 0.00187037  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.53 min) Epoch 93/300 -- Iteration 714590 - Batch 6006/7702 - Train loss: 0.00187006  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.55 min) Epoch 93/300 -- Iteration 714667 - Batch 6083/7702 - Train loss: 0.00187018  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.57 min) Epoch 93/300 -- Iteration 714744 - Batch 6160/7702 - Train loss: 0.00186998  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.59 min) Epoch 93/300 -- Iteration 714821 - Batch 6237/7702 - Train loss: 0.00186968  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.61 min) Epoch 93/300 -- Iteration 714898 - Batch 6314/7702 - Train loss: 0.00186955  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.63 min) Epoch 93/300 -- Iteration 714975 - Batch 6391/7702 - Train loss: 0.00186935  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.66 min) Epoch 93/300 -- Iteration 715052 - Batch 6468/7702 - Train loss: 0.00186936  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.68 min) Epoch 93/300 -- Iteration 715129 - Batch 6545/7702 - Train loss: 0.00186904  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.70 min) Epoch 93/300 -- Iteration 715206 - Batch 6622/7702 - Train loss: 0.00186933  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.72 min) Epoch 93/300 -- Iteration 715283 - Batch 6699/7702 - Train loss: 0.00186939  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.74 min) Epoch 93/300 -- Iteration 715360 - Batch 6776/7702 - Train loss: 0.00186919  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.76 min) Epoch 93/300 -- Iteration 715437 - Batch 6853/7702 - Train loss: 0.00186904  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.78 min) Epoch 93/300 -- Iteration 715514 - Batch 6930/7702 - Train loss: 0.00186883  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.80 min) Epoch 93/300 -- Iteration 715591 - Batch 7007/7702 - Train loss: 0.00186889  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.82 min) Epoch 93/300 -- Iteration 715668 - Batch 7084/7702 - Train loss: 0.00186893  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.84 min) Epoch 93/300 -- Iteration 715745 - Batch 7161/7702 - Train loss: 0.00186903  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.86 min) Epoch 93/300 -- Iteration 715822 - Batch 7238/7702 - Train loss: 0.00186908  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.88 min) Epoch 93/300 -- Iteration 715899 - Batch 7315/7702 - Train loss: 0.00186898  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.90 min) Epoch 93/300 -- Iteration 715976 - Batch 7392/7702 - Train loss: 0.00186885  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.93 min) Epoch 93/300 -- Iteration 716053 - Batch 7469/7702 - Train loss: 0.00186893  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.95 min) Epoch 93/300 -- Iteration 716130 - Batch 7546/7702 - Train loss: 0.00186887  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.97 min) Epoch 93/300 -- Iteration 716207 - Batch 7623/7702 - Train loss: 0.00186863  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.99 min) Epoch 93/300 -- Iteration 716284 - Batch 7700/7702 - Train loss: 0.00186882  - Train acc: -0.0000 - Val loss: 0.00039633\n",
      "(192.99 min) Epoch 93/300 -- Iteration 716286 - Batch 7701/7702 - Train loss: 0.00186880  - Train acc: -0.0000 - Val loss: 0.00035608 - Val acc: -0.0000\n",
      "(193.01 min) Epoch 94/300 -- Iteration 716363 - Batch 77/7702 - Train loss: 0.00182088  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.03 min) Epoch 94/300 -- Iteration 716440 - Batch 154/7702 - Train loss: 0.00185809  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.05 min) Epoch 94/300 -- Iteration 716517 - Batch 231/7702 - Train loss: 0.00186604  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.07 min) Epoch 94/300 -- Iteration 716594 - Batch 308/7702 - Train loss: 0.00185723  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.10 min) Epoch 94/300 -- Iteration 716671 - Batch 385/7702 - Train loss: 0.00186023  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.12 min) Epoch 94/300 -- Iteration 716748 - Batch 462/7702 - Train loss: 0.00185820  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.14 min) Epoch 94/300 -- Iteration 716825 - Batch 539/7702 - Train loss: 0.00185408  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.16 min) Epoch 94/300 -- Iteration 716902 - Batch 616/7702 - Train loss: 0.00185103  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.18 min) Epoch 94/300 -- Iteration 716979 - Batch 693/7702 - Train loss: 0.00184790  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.20 min) Epoch 94/300 -- Iteration 717056 - Batch 770/7702 - Train loss: 0.00185028  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.22 min) Epoch 94/300 -- Iteration 717133 - Batch 847/7702 - Train loss: 0.00185309  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.24 min) Epoch 94/300 -- Iteration 717210 - Batch 924/7702 - Train loss: 0.00185529  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.26 min) Epoch 94/300 -- Iteration 717287 - Batch 1001/7702 - Train loss: 0.00185671  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.28 min) Epoch 94/300 -- Iteration 717364 - Batch 1078/7702 - Train loss: 0.00185837  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.30 min) Epoch 94/300 -- Iteration 717441 - Batch 1155/7702 - Train loss: 0.00186028  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.32 min) Epoch 94/300 -- Iteration 717518 - Batch 1232/7702 - Train loss: 0.00186181  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.34 min) Epoch 94/300 -- Iteration 717595 - Batch 1309/7702 - Train loss: 0.00186141  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.36 min) Epoch 94/300 -- Iteration 717672 - Batch 1386/7702 - Train loss: 0.00186034  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.39 min) Epoch 94/300 -- Iteration 717749 - Batch 1463/7702 - Train loss: 0.00185831  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.41 min) Epoch 94/300 -- Iteration 717826 - Batch 1540/7702 - Train loss: 0.00185775  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.43 min) Epoch 94/300 -- Iteration 717903 - Batch 1617/7702 - Train loss: 0.00185688  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.45 min) Epoch 94/300 -- Iteration 717980 - Batch 1694/7702 - Train loss: 0.00185733  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.47 min) Epoch 94/300 -- Iteration 718057 - Batch 1771/7702 - Train loss: 0.00185938  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.49 min) Epoch 94/300 -- Iteration 718134 - Batch 1848/7702 - Train loss: 0.00186019  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.51 min) Epoch 94/300 -- Iteration 718211 - Batch 1925/7702 - Train loss: 0.00186078  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.53 min) Epoch 94/300 -- Iteration 718288 - Batch 2002/7702 - Train loss: 0.00186072  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.55 min) Epoch 94/300 -- Iteration 718365 - Batch 2079/7702 - Train loss: 0.00186210  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.57 min) Epoch 94/300 -- Iteration 718442 - Batch 2156/7702 - Train loss: 0.00186122  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.59 min) Epoch 94/300 -- Iteration 718519 - Batch 2233/7702 - Train loss: 0.00186100  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.61 min) Epoch 94/300 -- Iteration 718596 - Batch 2310/7702 - Train loss: 0.00186131  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.63 min) Epoch 94/300 -- Iteration 718673 - Batch 2387/7702 - Train loss: 0.00186170  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.66 min) Epoch 94/300 -- Iteration 718750 - Batch 2464/7702 - Train loss: 0.00186203  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.68 min) Epoch 94/300 -- Iteration 718827 - Batch 2541/7702 - Train loss: 0.00186289  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.70 min) Epoch 94/300 -- Iteration 718904 - Batch 2618/7702 - Train loss: 0.00186394  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.72 min) Epoch 94/300 -- Iteration 718981 - Batch 2695/7702 - Train loss: 0.00186400  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.74 min) Epoch 94/300 -- Iteration 719058 - Batch 2772/7702 - Train loss: 0.00186394  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.76 min) Epoch 94/300 -- Iteration 719135 - Batch 2849/7702 - Train loss: 0.00186439  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.78 min) Epoch 94/300 -- Iteration 719212 - Batch 2926/7702 - Train loss: 0.00186381  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.80 min) Epoch 94/300 -- Iteration 719289 - Batch 3003/7702 - Train loss: 0.00186481  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.82 min) Epoch 94/300 -- Iteration 719366 - Batch 3080/7702 - Train loss: 0.00186570  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.84 min) Epoch 94/300 -- Iteration 719443 - Batch 3157/7702 - Train loss: 0.00186667  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.86 min) Epoch 94/300 -- Iteration 719520 - Batch 3234/7702 - Train loss: 0.00186610  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.88 min) Epoch 94/300 -- Iteration 719597 - Batch 3311/7702 - Train loss: 0.00186670  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.90 min) Epoch 94/300 -- Iteration 719674 - Batch 3388/7702 - Train loss: 0.00186695  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.92 min) Epoch 94/300 -- Iteration 719751 - Batch 3465/7702 - Train loss: 0.00186762  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.95 min) Epoch 94/300 -- Iteration 719828 - Batch 3542/7702 - Train loss: 0.00186779  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.97 min) Epoch 94/300 -- Iteration 719905 - Batch 3619/7702 - Train loss: 0.00186831  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(193.99 min) Epoch 94/300 -- Iteration 719982 - Batch 3696/7702 - Train loss: 0.00186788  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.01 min) Epoch 94/300 -- Iteration 720059 - Batch 3773/7702 - Train loss: 0.00186817  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.03 min) Epoch 94/300 -- Iteration 720136 - Batch 3850/7702 - Train loss: 0.00186833  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.05 min) Epoch 94/300 -- Iteration 720213 - Batch 3927/7702 - Train loss: 0.00186807  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.07 min) Epoch 94/300 -- Iteration 720290 - Batch 4004/7702 - Train loss: 0.00186797  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.09 min) Epoch 94/300 -- Iteration 720367 - Batch 4081/7702 - Train loss: 0.00186826  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.11 min) Epoch 94/300 -- Iteration 720444 - Batch 4158/7702 - Train loss: 0.00186847  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.13 min) Epoch 94/300 -- Iteration 720521 - Batch 4235/7702 - Train loss: 0.00186826  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.15 min) Epoch 94/300 -- Iteration 720598 - Batch 4312/7702 - Train loss: 0.00186789  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.17 min) Epoch 94/300 -- Iteration 720675 - Batch 4389/7702 - Train loss: 0.00186737  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.19 min) Epoch 94/300 -- Iteration 720752 - Batch 4466/7702 - Train loss: 0.00186693  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.21 min) Epoch 94/300 -- Iteration 720829 - Batch 4543/7702 - Train loss: 0.00186687  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.23 min) Epoch 94/300 -- Iteration 720906 - Batch 4620/7702 - Train loss: 0.00186692  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.26 min) Epoch 94/300 -- Iteration 720983 - Batch 4697/7702 - Train loss: 0.00186727  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.28 min) Epoch 94/300 -- Iteration 721060 - Batch 4774/7702 - Train loss: 0.00186670  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.30 min) Epoch 94/300 -- Iteration 721137 - Batch 4851/7702 - Train loss: 0.00186674  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.32 min) Epoch 94/300 -- Iteration 721214 - Batch 4928/7702 - Train loss: 0.00186677  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.34 min) Epoch 94/300 -- Iteration 721291 - Batch 5005/7702 - Train loss: 0.00186738  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.36 min) Epoch 94/300 -- Iteration 721368 - Batch 5082/7702 - Train loss: 0.00186790  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.38 min) Epoch 94/300 -- Iteration 721445 - Batch 5159/7702 - Train loss: 0.00186839  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.40 min) Epoch 94/300 -- Iteration 721522 - Batch 5236/7702 - Train loss: 0.00186858  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.42 min) Epoch 94/300 -- Iteration 721599 - Batch 5313/7702 - Train loss: 0.00186871  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.44 min) Epoch 94/300 -- Iteration 721676 - Batch 5390/7702 - Train loss: 0.00186852  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.46 min) Epoch 94/300 -- Iteration 721753 - Batch 5467/7702 - Train loss: 0.00186822  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.48 min) Epoch 94/300 -- Iteration 721830 - Batch 5544/7702 - Train loss: 0.00186831  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.50 min) Epoch 94/300 -- Iteration 721907 - Batch 5621/7702 - Train loss: 0.00186789  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.53 min) Epoch 94/300 -- Iteration 721984 - Batch 5698/7702 - Train loss: 0.00186752  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.55 min) Epoch 94/300 -- Iteration 722061 - Batch 5775/7702 - Train loss: 0.00186797  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.57 min) Epoch 94/300 -- Iteration 722138 - Batch 5852/7702 - Train loss: 0.00186794  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.59 min) Epoch 94/300 -- Iteration 722215 - Batch 5929/7702 - Train loss: 0.00186784  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.61 min) Epoch 94/300 -- Iteration 722292 - Batch 6006/7702 - Train loss: 0.00186787  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.63 min) Epoch 94/300 -- Iteration 722369 - Batch 6083/7702 - Train loss: 0.00186762  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.65 min) Epoch 94/300 -- Iteration 722446 - Batch 6160/7702 - Train loss: 0.00186795  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.67 min) Epoch 94/300 -- Iteration 722523 - Batch 6237/7702 - Train loss: 0.00186802  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.69 min) Epoch 94/300 -- Iteration 722600 - Batch 6314/7702 - Train loss: 0.00186844  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.71 min) Epoch 94/300 -- Iteration 722677 - Batch 6391/7702 - Train loss: 0.00186840  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.73 min) Epoch 94/300 -- Iteration 722754 - Batch 6468/7702 - Train loss: 0.00186841  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.75 min) Epoch 94/300 -- Iteration 722831 - Batch 6545/7702 - Train loss: 0.00186814  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.77 min) Epoch 94/300 -- Iteration 722908 - Batch 6622/7702 - Train loss: 0.00186830  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.79 min) Epoch 94/300 -- Iteration 722985 - Batch 6699/7702 - Train loss: 0.00186811  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.82 min) Epoch 94/300 -- Iteration 723062 - Batch 6776/7702 - Train loss: 0.00186842  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.84 min) Epoch 94/300 -- Iteration 723139 - Batch 6853/7702 - Train loss: 0.00186908  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.86 min) Epoch 94/300 -- Iteration 723216 - Batch 6930/7702 - Train loss: 0.00186916  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.88 min) Epoch 94/300 -- Iteration 723293 - Batch 7007/7702 - Train loss: 0.00186942  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.90 min) Epoch 94/300 -- Iteration 723370 - Batch 7084/7702 - Train loss: 0.00186929  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.92 min) Epoch 94/300 -- Iteration 723447 - Batch 7161/7702 - Train loss: 0.00186965  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.94 min) Epoch 94/300 -- Iteration 723524 - Batch 7238/7702 - Train loss: 0.00186971  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.96 min) Epoch 94/300 -- Iteration 723601 - Batch 7315/7702 - Train loss: 0.00186987  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(194.98 min) Epoch 94/300 -- Iteration 723678 - Batch 7392/7702 - Train loss: 0.00186985  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(195.00 min) Epoch 94/300 -- Iteration 723755 - Batch 7469/7702 - Train loss: 0.00186982  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(195.02 min) Epoch 94/300 -- Iteration 723832 - Batch 7546/7702 - Train loss: 0.00187074  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(195.04 min) Epoch 94/300 -- Iteration 723909 - Batch 7623/7702 - Train loss: 0.00187100  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(195.06 min) Epoch 94/300 -- Iteration 723986 - Batch 7700/7702 - Train loss: 0.00187090  - Train acc: -0.0000 - Val loss: 0.00035608\n",
      "(195.06 min) Epoch 94/300 -- Iteration 723988 - Batch 7701/7702 - Train loss: 0.00187088  - Train acc: -0.0000 - Val loss: 0.00032373 - Val acc: -0.0000\n",
      "(195.09 min) Epoch 95/300 -- Iteration 724065 - Batch 77/7702 - Train loss: 0.00186101  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.11 min) Epoch 95/300 -- Iteration 724142 - Batch 154/7702 - Train loss: 0.00185065  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.13 min) Epoch 95/300 -- Iteration 724219 - Batch 231/7702 - Train loss: 0.00185877  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.15 min) Epoch 95/300 -- Iteration 724296 - Batch 308/7702 - Train loss: 0.00187525  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.17 min) Epoch 95/300 -- Iteration 724373 - Batch 385/7702 - Train loss: 0.00187449  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.19 min) Epoch 95/300 -- Iteration 724450 - Batch 462/7702 - Train loss: 0.00187549  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.21 min) Epoch 95/300 -- Iteration 724527 - Batch 539/7702 - Train loss: 0.00187951  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.23 min) Epoch 95/300 -- Iteration 724604 - Batch 616/7702 - Train loss: 0.00187746  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.25 min) Epoch 95/300 -- Iteration 724681 - Batch 693/7702 - Train loss: 0.00187567  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.27 min) Epoch 95/300 -- Iteration 724758 - Batch 770/7702 - Train loss: 0.00187546  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.30 min) Epoch 95/300 -- Iteration 724835 - Batch 847/7702 - Train loss: 0.00187271  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.32 min) Epoch 95/300 -- Iteration 724912 - Batch 924/7702 - Train loss: 0.00187305  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.34 min) Epoch 95/300 -- Iteration 724989 - Batch 1001/7702 - Train loss: 0.00187288  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.36 min) Epoch 95/300 -- Iteration 725066 - Batch 1078/7702 - Train loss: 0.00187360  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.38 min) Epoch 95/300 -- Iteration 725143 - Batch 1155/7702 - Train loss: 0.00187275  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.40 min) Epoch 95/300 -- Iteration 725220 - Batch 1232/7702 - Train loss: 0.00187365  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.42 min) Epoch 95/300 -- Iteration 725297 - Batch 1309/7702 - Train loss: 0.00187457  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.44 min) Epoch 95/300 -- Iteration 725374 - Batch 1386/7702 - Train loss: 0.00187146  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.46 min) Epoch 95/300 -- Iteration 725451 - Batch 1463/7702 - Train loss: 0.00187112  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.48 min) Epoch 95/300 -- Iteration 725528 - Batch 1540/7702 - Train loss: 0.00186955  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.50 min) Epoch 95/300 -- Iteration 725605 - Batch 1617/7702 - Train loss: 0.00186956  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.52 min) Epoch 95/300 -- Iteration 725682 - Batch 1694/7702 - Train loss: 0.00186819  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.54 min) Epoch 95/300 -- Iteration 725759 - Batch 1771/7702 - Train loss: 0.00186891  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.56 min) Epoch 95/300 -- Iteration 725836 - Batch 1848/7702 - Train loss: 0.00186862  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.59 min) Epoch 95/300 -- Iteration 725913 - Batch 1925/7702 - Train loss: 0.00186735  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.61 min) Epoch 95/300 -- Iteration 725990 - Batch 2002/7702 - Train loss: 0.00186662  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.63 min) Epoch 95/300 -- Iteration 726067 - Batch 2079/7702 - Train loss: 0.00186643  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.65 min) Epoch 95/300 -- Iteration 726144 - Batch 2156/7702 - Train loss: 0.00186700  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.67 min) Epoch 95/300 -- Iteration 726221 - Batch 2233/7702 - Train loss: 0.00186581  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.69 min) Epoch 95/300 -- Iteration 726298 - Batch 2310/7702 - Train loss: 0.00186546  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.71 min) Epoch 95/300 -- Iteration 726375 - Batch 2387/7702 - Train loss: 0.00186361  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.73 min) Epoch 95/300 -- Iteration 726452 - Batch 2464/7702 - Train loss: 0.00186317  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.75 min) Epoch 95/300 -- Iteration 726529 - Batch 2541/7702 - Train loss: 0.00186325  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.77 min) Epoch 95/300 -- Iteration 726606 - Batch 2618/7702 - Train loss: 0.00186215  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.79 min) Epoch 95/300 -- Iteration 726683 - Batch 2695/7702 - Train loss: 0.00186154  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.81 min) Epoch 95/300 -- Iteration 726760 - Batch 2772/7702 - Train loss: 0.00186085  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.83 min) Epoch 95/300 -- Iteration 726837 - Batch 2849/7702 - Train loss: 0.00186179  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.85 min) Epoch 95/300 -- Iteration 726914 - Batch 2926/7702 - Train loss: 0.00186186  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.88 min) Epoch 95/300 -- Iteration 726991 - Batch 3003/7702 - Train loss: 0.00186167  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.90 min) Epoch 95/300 -- Iteration 727068 - Batch 3080/7702 - Train loss: 0.00186118  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.92 min) Epoch 95/300 -- Iteration 727145 - Batch 3157/7702 - Train loss: 0.00186124  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.94 min) Epoch 95/300 -- Iteration 727222 - Batch 3234/7702 - Train loss: 0.00186135  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.96 min) Epoch 95/300 -- Iteration 727299 - Batch 3311/7702 - Train loss: 0.00186123  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(195.98 min) Epoch 95/300 -- Iteration 727376 - Batch 3388/7702 - Train loss: 0.00186155  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.00 min) Epoch 95/300 -- Iteration 727453 - Batch 3465/7702 - Train loss: 0.00186174  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.02 min) Epoch 95/300 -- Iteration 727530 - Batch 3542/7702 - Train loss: 0.00186153  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.04 min) Epoch 95/300 -- Iteration 727607 - Batch 3619/7702 - Train loss: 0.00186165  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.06 min) Epoch 95/300 -- Iteration 727684 - Batch 3696/7702 - Train loss: 0.00186244  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.08 min) Epoch 95/300 -- Iteration 727761 - Batch 3773/7702 - Train loss: 0.00186246  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.10 min) Epoch 95/300 -- Iteration 727838 - Batch 3850/7702 - Train loss: 0.00186274  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.12 min) Epoch 95/300 -- Iteration 727915 - Batch 3927/7702 - Train loss: 0.00186258  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.15 min) Epoch 95/300 -- Iteration 727992 - Batch 4004/7702 - Train loss: 0.00186353  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.17 min) Epoch 95/300 -- Iteration 728069 - Batch 4081/7702 - Train loss: 0.00186354  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.19 min) Epoch 95/300 -- Iteration 728146 - Batch 4158/7702 - Train loss: 0.00186350  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.21 min) Epoch 95/300 -- Iteration 728223 - Batch 4235/7702 - Train loss: 0.00186342  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.23 min) Epoch 95/300 -- Iteration 728300 - Batch 4312/7702 - Train loss: 0.00186296  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.25 min) Epoch 95/300 -- Iteration 728377 - Batch 4389/7702 - Train loss: 0.00186287  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.27 min) Epoch 95/300 -- Iteration 728454 - Batch 4466/7702 - Train loss: 0.00186327  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.29 min) Epoch 95/300 -- Iteration 728531 - Batch 4543/7702 - Train loss: 0.00186332  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.31 min) Epoch 95/300 -- Iteration 728608 - Batch 4620/7702 - Train loss: 0.00186383  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.33 min) Epoch 95/300 -- Iteration 728685 - Batch 4697/7702 - Train loss: 0.00186398  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.35 min) Epoch 95/300 -- Iteration 728762 - Batch 4774/7702 - Train loss: 0.00186387  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.37 min) Epoch 95/300 -- Iteration 728839 - Batch 4851/7702 - Train loss: 0.00186418  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.39 min) Epoch 95/300 -- Iteration 728916 - Batch 4928/7702 - Train loss: 0.00186423  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.42 min) Epoch 95/300 -- Iteration 728993 - Batch 5005/7702 - Train loss: 0.00186394  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.44 min) Epoch 95/300 -- Iteration 729070 - Batch 5082/7702 - Train loss: 0.00186391  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.46 min) Epoch 95/300 -- Iteration 729147 - Batch 5159/7702 - Train loss: 0.00186491  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.48 min) Epoch 95/300 -- Iteration 729224 - Batch 5236/7702 - Train loss: 0.00186558  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.50 min) Epoch 95/300 -- Iteration 729301 - Batch 5313/7702 - Train loss: 0.00186587  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.52 min) Epoch 95/300 -- Iteration 729378 - Batch 5390/7702 - Train loss: 0.00186637  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.54 min) Epoch 95/300 -- Iteration 729455 - Batch 5467/7702 - Train loss: 0.00186663  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.56 min) Epoch 95/300 -- Iteration 729532 - Batch 5544/7702 - Train loss: 0.00186684  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.58 min) Epoch 95/300 -- Iteration 729609 - Batch 5621/7702 - Train loss: 0.00186729  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.60 min) Epoch 95/300 -- Iteration 729686 - Batch 5698/7702 - Train loss: 0.00186684  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.62 min) Epoch 95/300 -- Iteration 729763 - Batch 5775/7702 - Train loss: 0.00186680  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.64 min) Epoch 95/300 -- Iteration 729840 - Batch 5852/7702 - Train loss: 0.00186683  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.67 min) Epoch 95/300 -- Iteration 729917 - Batch 5929/7702 - Train loss: 0.00186675  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.69 min) Epoch 95/300 -- Iteration 729994 - Batch 6006/7702 - Train loss: 0.00186652  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.71 min) Epoch 95/300 -- Iteration 730071 - Batch 6083/7702 - Train loss: 0.00186652  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.73 min) Epoch 95/300 -- Iteration 730148 - Batch 6160/7702 - Train loss: 0.00186666  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.75 min) Epoch 95/300 -- Iteration 730225 - Batch 6237/7702 - Train loss: 0.00186699  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.77 min) Epoch 95/300 -- Iteration 730302 - Batch 6314/7702 - Train loss: 0.00186685  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.79 min) Epoch 95/300 -- Iteration 730379 - Batch 6391/7702 - Train loss: 0.00186703  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.81 min) Epoch 95/300 -- Iteration 730456 - Batch 6468/7702 - Train loss: 0.00186649  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.83 min) Epoch 95/300 -- Iteration 730533 - Batch 6545/7702 - Train loss: 0.00186659  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.85 min) Epoch 95/300 -- Iteration 730610 - Batch 6622/7702 - Train loss: 0.00186682  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.87 min) Epoch 95/300 -- Iteration 730687 - Batch 6699/7702 - Train loss: 0.00186696  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.89 min) Epoch 95/300 -- Iteration 730764 - Batch 6776/7702 - Train loss: 0.00186707  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.91 min) Epoch 95/300 -- Iteration 730841 - Batch 6853/7702 - Train loss: 0.00186704  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.94 min) Epoch 95/300 -- Iteration 730918 - Batch 6930/7702 - Train loss: 0.00186674  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.96 min) Epoch 95/300 -- Iteration 730995 - Batch 7007/7702 - Train loss: 0.00186701  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(196.98 min) Epoch 95/300 -- Iteration 731072 - Batch 7084/7702 - Train loss: 0.00186670  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(197.00 min) Epoch 95/300 -- Iteration 731149 - Batch 7161/7702 - Train loss: 0.00186660  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(197.02 min) Epoch 95/300 -- Iteration 731226 - Batch 7238/7702 - Train loss: 0.00186640  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(197.04 min) Epoch 95/300 -- Iteration 731303 - Batch 7315/7702 - Train loss: 0.00186623  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(197.06 min) Epoch 95/300 -- Iteration 731380 - Batch 7392/7702 - Train loss: 0.00186616  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(197.08 min) Epoch 95/300 -- Iteration 731457 - Batch 7469/7702 - Train loss: 0.00186629  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(197.10 min) Epoch 95/300 -- Iteration 731534 - Batch 7546/7702 - Train loss: 0.00186625  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(197.12 min) Epoch 95/300 -- Iteration 731611 - Batch 7623/7702 - Train loss: 0.00186655  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(197.14 min) Epoch 95/300 -- Iteration 731688 - Batch 7700/7702 - Train loss: 0.00186651  - Train acc: -0.0000 - Val loss: 0.00032373\n",
      "(197.14 min) Epoch 95/300 -- Iteration 731690 - Batch 7701/7702 - Train loss: 0.00186650  - Train acc: -0.0000 - Val loss: 0.00042673 - Val acc: -0.0000\n",
      "(197.17 min) Epoch 96/300 -- Iteration 731767 - Batch 77/7702 - Train loss: 0.00187172  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.19 min) Epoch 96/300 -- Iteration 731844 - Batch 154/7702 - Train loss: 0.00188074  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.21 min) Epoch 96/300 -- Iteration 731921 - Batch 231/7702 - Train loss: 0.00188409  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.23 min) Epoch 96/300 -- Iteration 731998 - Batch 308/7702 - Train loss: 0.00187345  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.25 min) Epoch 96/300 -- Iteration 732075 - Batch 385/7702 - Train loss: 0.00187079  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.27 min) Epoch 96/300 -- Iteration 732152 - Batch 462/7702 - Train loss: 0.00186867  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.29 min) Epoch 96/300 -- Iteration 732229 - Batch 539/7702 - Train loss: 0.00186671  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.31 min) Epoch 96/300 -- Iteration 732306 - Batch 616/7702 - Train loss: 0.00186521  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.33 min) Epoch 96/300 -- Iteration 732383 - Batch 693/7702 - Train loss: 0.00186744  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.35 min) Epoch 96/300 -- Iteration 732460 - Batch 770/7702 - Train loss: 0.00186496  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.37 min) Epoch 96/300 -- Iteration 732537 - Batch 847/7702 - Train loss: 0.00186207  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.39 min) Epoch 96/300 -- Iteration 732614 - Batch 924/7702 - Train loss: 0.00186278  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.42 min) Epoch 96/300 -- Iteration 732691 - Batch 1001/7702 - Train loss: 0.00186230  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.44 min) Epoch 96/300 -- Iteration 732768 - Batch 1078/7702 - Train loss: 0.00186210  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.46 min) Epoch 96/300 -- Iteration 732845 - Batch 1155/7702 - Train loss: 0.00186239  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.48 min) Epoch 96/300 -- Iteration 732922 - Batch 1232/7702 - Train loss: 0.00186432  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.50 min) Epoch 96/300 -- Iteration 732999 - Batch 1309/7702 - Train loss: 0.00186317  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.52 min) Epoch 96/300 -- Iteration 733076 - Batch 1386/7702 - Train loss: 0.00186453  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.54 min) Epoch 96/300 -- Iteration 733153 - Batch 1463/7702 - Train loss: 0.00186459  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.56 min) Epoch 96/300 -- Iteration 733230 - Batch 1540/7702 - Train loss: 0.00186350  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.58 min) Epoch 96/300 -- Iteration 733307 - Batch 1617/7702 - Train loss: 0.00186203  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.60 min) Epoch 96/300 -- Iteration 733384 - Batch 1694/7702 - Train loss: 0.00186192  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.62 min) Epoch 96/300 -- Iteration 733461 - Batch 1771/7702 - Train loss: 0.00186354  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.64 min) Epoch 96/300 -- Iteration 733538 - Batch 1848/7702 - Train loss: 0.00186301  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.66 min) Epoch 96/300 -- Iteration 733615 - Batch 1925/7702 - Train loss: 0.00186290  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.68 min) Epoch 96/300 -- Iteration 733692 - Batch 2002/7702 - Train loss: 0.00186357  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.71 min) Epoch 96/300 -- Iteration 733769 - Batch 2079/7702 - Train loss: 0.00186387  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.73 min) Epoch 96/300 -- Iteration 733846 - Batch 2156/7702 - Train loss: 0.00186415  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.75 min) Epoch 96/300 -- Iteration 733923 - Batch 2233/7702 - Train loss: 0.00186354  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.77 min) Epoch 96/300 -- Iteration 734000 - Batch 2310/7702 - Train loss: 0.00186360  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.79 min) Epoch 96/300 -- Iteration 734077 - Batch 2387/7702 - Train loss: 0.00186302  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.81 min) Epoch 96/300 -- Iteration 734154 - Batch 2464/7702 - Train loss: 0.00186367  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.83 min) Epoch 96/300 -- Iteration 734231 - Batch 2541/7702 - Train loss: 0.00186430  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.85 min) Epoch 96/300 -- Iteration 734308 - Batch 2618/7702 - Train loss: 0.00186522  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.87 min) Epoch 96/300 -- Iteration 734385 - Batch 2695/7702 - Train loss: 0.00186441  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.89 min) Epoch 96/300 -- Iteration 734462 - Batch 2772/7702 - Train loss: 0.00186429  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.91 min) Epoch 96/300 -- Iteration 734539 - Batch 2849/7702 - Train loss: 0.00186328  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.93 min) Epoch 96/300 -- Iteration 734616 - Batch 2926/7702 - Train loss: 0.00186320  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.95 min) Epoch 96/300 -- Iteration 734693 - Batch 3003/7702 - Train loss: 0.00186291  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(197.98 min) Epoch 96/300 -- Iteration 734770 - Batch 3080/7702 - Train loss: 0.00186327  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.00 min) Epoch 96/300 -- Iteration 734847 - Batch 3157/7702 - Train loss: 0.00186356  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.02 min) Epoch 96/300 -- Iteration 734924 - Batch 3234/7702 - Train loss: 0.00186305  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.04 min) Epoch 96/300 -- Iteration 735001 - Batch 3311/7702 - Train loss: 0.00186345  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.06 min) Epoch 96/300 -- Iteration 735078 - Batch 3388/7702 - Train loss: 0.00186322  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.08 min) Epoch 96/300 -- Iteration 735155 - Batch 3465/7702 - Train loss: 0.00186300  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.10 min) Epoch 96/300 -- Iteration 735232 - Batch 3542/7702 - Train loss: 0.00186378  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.12 min) Epoch 96/300 -- Iteration 735309 - Batch 3619/7702 - Train loss: 0.00186417  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.14 min) Epoch 96/300 -- Iteration 735386 - Batch 3696/7702 - Train loss: 0.00186459  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.16 min) Epoch 96/300 -- Iteration 735463 - Batch 3773/7702 - Train loss: 0.00186508  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.18 min) Epoch 96/300 -- Iteration 735540 - Batch 3850/7702 - Train loss: 0.00186466  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.20 min) Epoch 96/300 -- Iteration 735617 - Batch 3927/7702 - Train loss: 0.00186476  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.22 min) Epoch 96/300 -- Iteration 735694 - Batch 4004/7702 - Train loss: 0.00186539  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.24 min) Epoch 96/300 -- Iteration 735771 - Batch 4081/7702 - Train loss: 0.00186561  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.27 min) Epoch 96/300 -- Iteration 735848 - Batch 4158/7702 - Train loss: 0.00186550  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.29 min) Epoch 96/300 -- Iteration 735925 - Batch 4235/7702 - Train loss: 0.00186584  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.31 min) Epoch 96/300 -- Iteration 736002 - Batch 4312/7702 - Train loss: 0.00186594  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.33 min) Epoch 96/300 -- Iteration 736079 - Batch 4389/7702 - Train loss: 0.00186598  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.35 min) Epoch 96/300 -- Iteration 736156 - Batch 4466/7702 - Train loss: 0.00186575  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.37 min) Epoch 96/300 -- Iteration 736233 - Batch 4543/7702 - Train loss: 0.00186588  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.39 min) Epoch 96/300 -- Iteration 736310 - Batch 4620/7702 - Train loss: 0.00186580  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.41 min) Epoch 96/300 -- Iteration 736387 - Batch 4697/7702 - Train loss: 0.00186571  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.43 min) Epoch 96/300 -- Iteration 736464 - Batch 4774/7702 - Train loss: 0.00186564  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.45 min) Epoch 96/300 -- Iteration 736541 - Batch 4851/7702 - Train loss: 0.00186576  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.47 min) Epoch 96/300 -- Iteration 736618 - Batch 4928/7702 - Train loss: 0.00186533  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.49 min) Epoch 96/300 -- Iteration 736695 - Batch 5005/7702 - Train loss: 0.00186508  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.51 min) Epoch 96/300 -- Iteration 736772 - Batch 5082/7702 - Train loss: 0.00186558  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.54 min) Epoch 96/300 -- Iteration 736849 - Batch 5159/7702 - Train loss: 0.00186545  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.56 min) Epoch 96/300 -- Iteration 736926 - Batch 5236/7702 - Train loss: 0.00186537  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.58 min) Epoch 96/300 -- Iteration 737003 - Batch 5313/7702 - Train loss: 0.00186537  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.60 min) Epoch 96/300 -- Iteration 737080 - Batch 5390/7702 - Train loss: 0.00186583  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.62 min) Epoch 96/300 -- Iteration 737157 - Batch 5467/7702 - Train loss: 0.00186629  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.64 min) Epoch 96/300 -- Iteration 737234 - Batch 5544/7702 - Train loss: 0.00186684  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.66 min) Epoch 96/300 -- Iteration 737311 - Batch 5621/7702 - Train loss: 0.00186675  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.68 min) Epoch 96/300 -- Iteration 737388 - Batch 5698/7702 - Train loss: 0.00186719  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.70 min) Epoch 96/300 -- Iteration 737465 - Batch 5775/7702 - Train loss: 0.00186666  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.72 min) Epoch 96/300 -- Iteration 737542 - Batch 5852/7702 - Train loss: 0.00186699  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.74 min) Epoch 96/300 -- Iteration 737619 - Batch 5929/7702 - Train loss: 0.00186709  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.76 min) Epoch 96/300 -- Iteration 737696 - Batch 6006/7702 - Train loss: 0.00186727  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.78 min) Epoch 96/300 -- Iteration 737773 - Batch 6083/7702 - Train loss: 0.00186760  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.81 min) Epoch 96/300 -- Iteration 737850 - Batch 6160/7702 - Train loss: 0.00186766  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.83 min) Epoch 96/300 -- Iteration 737927 - Batch 6237/7702 - Train loss: 0.00186783  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.85 min) Epoch 96/300 -- Iteration 738004 - Batch 6314/7702 - Train loss: 0.00186759  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.87 min) Epoch 96/300 -- Iteration 738081 - Batch 6391/7702 - Train loss: 0.00186814  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.89 min) Epoch 96/300 -- Iteration 738158 - Batch 6468/7702 - Train loss: 0.00186856  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.91 min) Epoch 96/300 -- Iteration 738235 - Batch 6545/7702 - Train loss: 0.00186862  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.93 min) Epoch 96/300 -- Iteration 738312 - Batch 6622/7702 - Train loss: 0.00186889  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.95 min) Epoch 96/300 -- Iteration 738389 - Batch 6699/7702 - Train loss: 0.00186933  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.97 min) Epoch 96/300 -- Iteration 738466 - Batch 6776/7702 - Train loss: 0.00186880  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(198.99 min) Epoch 96/300 -- Iteration 738543 - Batch 6853/7702 - Train loss: 0.00186849  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.01 min) Epoch 96/300 -- Iteration 738620 - Batch 6930/7702 - Train loss: 0.00186817  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.03 min) Epoch 96/300 -- Iteration 738697 - Batch 7007/7702 - Train loss: 0.00186795  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.05 min) Epoch 96/300 -- Iteration 738774 - Batch 7084/7702 - Train loss: 0.00186836  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.07 min) Epoch 96/300 -- Iteration 738851 - Batch 7161/7702 - Train loss: 0.00186856  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.10 min) Epoch 96/300 -- Iteration 738928 - Batch 7238/7702 - Train loss: 0.00186851  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.12 min) Epoch 96/300 -- Iteration 739005 - Batch 7315/7702 - Train loss: 0.00186864  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.14 min) Epoch 96/300 -- Iteration 739082 - Batch 7392/7702 - Train loss: 0.00186844  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.16 min) Epoch 96/300 -- Iteration 739159 - Batch 7469/7702 - Train loss: 0.00186816  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.18 min) Epoch 96/300 -- Iteration 739236 - Batch 7546/7702 - Train loss: 0.00186804  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.20 min) Epoch 96/300 -- Iteration 739313 - Batch 7623/7702 - Train loss: 0.00186785  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.22 min) Epoch 96/300 -- Iteration 739390 - Batch 7700/7702 - Train loss: 0.00186758  - Train acc: -0.0000 - Val loss: 0.00042673\n",
      "(199.22 min) Epoch 96/300 -- Iteration 739392 - Batch 7701/7702 - Train loss: 0.00186758  - Train acc: -0.0000 - Val loss: 0.00035588 - Val acc: -0.0000\n",
      "(199.24 min) Epoch 97/300 -- Iteration 739469 - Batch 77/7702 - Train loss: 0.00188617  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.26 min) Epoch 97/300 -- Iteration 739546 - Batch 154/7702 - Train loss: 0.00189002  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.29 min) Epoch 97/300 -- Iteration 739623 - Batch 231/7702 - Train loss: 0.00186670  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.31 min) Epoch 97/300 -- Iteration 739700 - Batch 308/7702 - Train loss: 0.00185594  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.33 min) Epoch 97/300 -- Iteration 739777 - Batch 385/7702 - Train loss: 0.00185755  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.35 min) Epoch 97/300 -- Iteration 739854 - Batch 462/7702 - Train loss: 0.00185897  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.37 min) Epoch 97/300 -- Iteration 739931 - Batch 539/7702 - Train loss: 0.00185883  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.39 min) Epoch 97/300 -- Iteration 740008 - Batch 616/7702 - Train loss: 0.00186262  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.41 min) Epoch 97/300 -- Iteration 740085 - Batch 693/7702 - Train loss: 0.00186003  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.43 min) Epoch 97/300 -- Iteration 740162 - Batch 770/7702 - Train loss: 0.00186094  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.45 min) Epoch 97/300 -- Iteration 740239 - Batch 847/7702 - Train loss: 0.00186189  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.47 min) Epoch 97/300 -- Iteration 740316 - Batch 924/7702 - Train loss: 0.00186188  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.49 min) Epoch 97/300 -- Iteration 740393 - Batch 1001/7702 - Train loss: 0.00186648  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.51 min) Epoch 97/300 -- Iteration 740470 - Batch 1078/7702 - Train loss: 0.00186798  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.53 min) Epoch 97/300 -- Iteration 740547 - Batch 1155/7702 - Train loss: 0.00186948  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.55 min) Epoch 97/300 -- Iteration 740624 - Batch 1232/7702 - Train loss: 0.00187011  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.58 min) Epoch 97/300 -- Iteration 740701 - Batch 1309/7702 - Train loss: 0.00187038  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.60 min) Epoch 97/300 -- Iteration 740778 - Batch 1386/7702 - Train loss: 0.00186933  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.62 min) Epoch 97/300 -- Iteration 740855 - Batch 1463/7702 - Train loss: 0.00186936  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.64 min) Epoch 97/300 -- Iteration 740932 - Batch 1540/7702 - Train loss: 0.00186796  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.66 min) Epoch 97/300 -- Iteration 741009 - Batch 1617/7702 - Train loss: 0.00186820  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.68 min) Epoch 97/300 -- Iteration 741086 - Batch 1694/7702 - Train loss: 0.00186817  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.70 min) Epoch 97/300 -- Iteration 741163 - Batch 1771/7702 - Train loss: 0.00186735  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.72 min) Epoch 97/300 -- Iteration 741240 - Batch 1848/7702 - Train loss: 0.00186663  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.74 min) Epoch 97/300 -- Iteration 741317 - Batch 1925/7702 - Train loss: 0.00186787  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.76 min) Epoch 97/300 -- Iteration 741394 - Batch 2002/7702 - Train loss: 0.00186784  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.78 min) Epoch 97/300 -- Iteration 741471 - Batch 2079/7702 - Train loss: 0.00186882  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.80 min) Epoch 97/300 -- Iteration 741548 - Batch 2156/7702 - Train loss: 0.00186920  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.82 min) Epoch 97/300 -- Iteration 741625 - Batch 2233/7702 - Train loss: 0.00186894  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.85 min) Epoch 97/300 -- Iteration 741702 - Batch 2310/7702 - Train loss: 0.00186913  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.87 min) Epoch 97/300 -- Iteration 741779 - Batch 2387/7702 - Train loss: 0.00186741  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.89 min) Epoch 97/300 -- Iteration 741856 - Batch 2464/7702 - Train loss: 0.00186846  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.91 min) Epoch 97/300 -- Iteration 741933 - Batch 2541/7702 - Train loss: 0.00186807  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.93 min) Epoch 97/300 -- Iteration 742010 - Batch 2618/7702 - Train loss: 0.00186681  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.95 min) Epoch 97/300 -- Iteration 742087 - Batch 2695/7702 - Train loss: 0.00186729  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.97 min) Epoch 97/300 -- Iteration 742164 - Batch 2772/7702 - Train loss: 0.00186676  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(199.99 min) Epoch 97/300 -- Iteration 742241 - Batch 2849/7702 - Train loss: 0.00186706  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.01 min) Epoch 97/300 -- Iteration 742318 - Batch 2926/7702 - Train loss: 0.00186664  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.03 min) Epoch 97/300 -- Iteration 742395 - Batch 3003/7702 - Train loss: 0.00186669  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.05 min) Epoch 97/300 -- Iteration 742472 - Batch 3080/7702 - Train loss: 0.00186714  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.07 min) Epoch 97/300 -- Iteration 742549 - Batch 3157/7702 - Train loss: 0.00186780  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.09 min) Epoch 97/300 -- Iteration 742626 - Batch 3234/7702 - Train loss: 0.00186761  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.11 min) Epoch 97/300 -- Iteration 742703 - Batch 3311/7702 - Train loss: 0.00186784  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.13 min) Epoch 97/300 -- Iteration 742780 - Batch 3388/7702 - Train loss: 0.00186865  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.16 min) Epoch 97/300 -- Iteration 742857 - Batch 3465/7702 - Train loss: 0.00186806  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.18 min) Epoch 97/300 -- Iteration 742934 - Batch 3542/7702 - Train loss: 0.00186742  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.20 min) Epoch 97/300 -- Iteration 743011 - Batch 3619/7702 - Train loss: 0.00186700  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.22 min) Epoch 97/300 -- Iteration 743088 - Batch 3696/7702 - Train loss: 0.00186700  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.24 min) Epoch 97/300 -- Iteration 743165 - Batch 3773/7702 - Train loss: 0.00186681  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.26 min) Epoch 97/300 -- Iteration 743242 - Batch 3850/7702 - Train loss: 0.00186640  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.28 min) Epoch 97/300 -- Iteration 743319 - Batch 3927/7702 - Train loss: 0.00186694  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.30 min) Epoch 97/300 -- Iteration 743396 - Batch 4004/7702 - Train loss: 0.00186686  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.32 min) Epoch 97/300 -- Iteration 743473 - Batch 4081/7702 - Train loss: 0.00186667  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.34 min) Epoch 97/300 -- Iteration 743550 - Batch 4158/7702 - Train loss: 0.00186583  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.36 min) Epoch 97/300 -- Iteration 743627 - Batch 4235/7702 - Train loss: 0.00186562  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.38 min) Epoch 97/300 -- Iteration 743704 - Batch 4312/7702 - Train loss: 0.00186577  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.41 min) Epoch 97/300 -- Iteration 743781 - Batch 4389/7702 - Train loss: 0.00186616  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.43 min) Epoch 97/300 -- Iteration 743858 - Batch 4466/7702 - Train loss: 0.00186653  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.45 min) Epoch 97/300 -- Iteration 743935 - Batch 4543/7702 - Train loss: 0.00186660  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.47 min) Epoch 97/300 -- Iteration 744012 - Batch 4620/7702 - Train loss: 0.00186734  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.49 min) Epoch 97/300 -- Iteration 744089 - Batch 4697/7702 - Train loss: 0.00186746  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.51 min) Epoch 97/300 -- Iteration 744166 - Batch 4774/7702 - Train loss: 0.00186733  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.53 min) Epoch 97/300 -- Iteration 744243 - Batch 4851/7702 - Train loss: 0.00186755  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.55 min) Epoch 97/300 -- Iteration 744320 - Batch 4928/7702 - Train loss: 0.00186766  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.57 min) Epoch 97/300 -- Iteration 744397 - Batch 5005/7702 - Train loss: 0.00186774  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.59 min) Epoch 97/300 -- Iteration 744474 - Batch 5082/7702 - Train loss: 0.00186762  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.61 min) Epoch 97/300 -- Iteration 744551 - Batch 5159/7702 - Train loss: 0.00186797  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.63 min) Epoch 97/300 -- Iteration 744628 - Batch 5236/7702 - Train loss: 0.00186804  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.65 min) Epoch 97/300 -- Iteration 744705 - Batch 5313/7702 - Train loss: 0.00186805  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.68 min) Epoch 97/300 -- Iteration 744782 - Batch 5390/7702 - Train loss: 0.00186785  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.70 min) Epoch 97/300 -- Iteration 744859 - Batch 5467/7702 - Train loss: 0.00186802  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.72 min) Epoch 97/300 -- Iteration 744936 - Batch 5544/7702 - Train loss: 0.00186799  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.74 min) Epoch 97/300 -- Iteration 745013 - Batch 5621/7702 - Train loss: 0.00186784  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.76 min) Epoch 97/300 -- Iteration 745090 - Batch 5698/7702 - Train loss: 0.00186817  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.78 min) Epoch 97/300 -- Iteration 745167 - Batch 5775/7702 - Train loss: 0.00186859  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.80 min) Epoch 97/300 -- Iteration 745244 - Batch 5852/7702 - Train loss: 0.00186914  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.82 min) Epoch 97/300 -- Iteration 745321 - Batch 5929/7702 - Train loss: 0.00186858  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.84 min) Epoch 97/300 -- Iteration 745398 - Batch 6006/7702 - Train loss: 0.00186897  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.86 min) Epoch 97/300 -- Iteration 745475 - Batch 6083/7702 - Train loss: 0.00186910  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.88 min) Epoch 97/300 -- Iteration 745552 - Batch 6160/7702 - Train loss: 0.00186895  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.90 min) Epoch 97/300 -- Iteration 745629 - Batch 6237/7702 - Train loss: 0.00186864  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.92 min) Epoch 97/300 -- Iteration 745706 - Batch 6314/7702 - Train loss: 0.00186858  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.95 min) Epoch 97/300 -- Iteration 745783 - Batch 6391/7702 - Train loss: 0.00186841  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.97 min) Epoch 97/300 -- Iteration 745860 - Batch 6468/7702 - Train loss: 0.00186875  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(200.99 min) Epoch 97/300 -- Iteration 745937 - Batch 6545/7702 - Train loss: 0.00186863  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.01 min) Epoch 97/300 -- Iteration 746014 - Batch 6622/7702 - Train loss: 0.00186858  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.03 min) Epoch 97/300 -- Iteration 746091 - Batch 6699/7702 - Train loss: 0.00186827  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.05 min) Epoch 97/300 -- Iteration 746168 - Batch 6776/7702 - Train loss: 0.00186814  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.07 min) Epoch 97/300 -- Iteration 746245 - Batch 6853/7702 - Train loss: 0.00186761  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.09 min) Epoch 97/300 -- Iteration 746322 - Batch 6930/7702 - Train loss: 0.00186796  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.11 min) Epoch 97/300 -- Iteration 746399 - Batch 7007/7702 - Train loss: 0.00186793  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.13 min) Epoch 97/300 -- Iteration 746476 - Batch 7084/7702 - Train loss: 0.00186768  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.15 min) Epoch 97/300 -- Iteration 746553 - Batch 7161/7702 - Train loss: 0.00186729  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.17 min) Epoch 97/300 -- Iteration 746630 - Batch 7238/7702 - Train loss: 0.00186745  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.19 min) Epoch 97/300 -- Iteration 746707 - Batch 7315/7702 - Train loss: 0.00186770  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.22 min) Epoch 97/300 -- Iteration 746784 - Batch 7392/7702 - Train loss: 0.00186756  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.24 min) Epoch 97/300 -- Iteration 746861 - Batch 7469/7702 - Train loss: 0.00186778  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.26 min) Epoch 97/300 -- Iteration 746938 - Batch 7546/7702 - Train loss: 0.00186729  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.28 min) Epoch 97/300 -- Iteration 747015 - Batch 7623/7702 - Train loss: 0.00186721  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.30 min) Epoch 97/300 -- Iteration 747092 - Batch 7700/7702 - Train loss: 0.00186768  - Train acc: -0.0000 - Val loss: 0.00035588\n",
      "(201.30 min) Epoch 97/300 -- Iteration 747094 - Batch 7701/7702 - Train loss: 0.00186768  - Train acc: -0.0000 - Val loss: 0.00049786 - Val acc: -0.0000\n",
      "(201.33 min) Epoch 98/300 -- Iteration 747171 - Batch 77/7702 - Train loss: 0.00189824  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.35 min) Epoch 98/300 -- Iteration 747248 - Batch 154/7702 - Train loss: 0.00188450  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.37 min) Epoch 98/300 -- Iteration 747325 - Batch 231/7702 - Train loss: 0.00187777  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.39 min) Epoch 98/300 -- Iteration 747402 - Batch 308/7702 - Train loss: 0.00187184  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.41 min) Epoch 98/300 -- Iteration 747479 - Batch 385/7702 - Train loss: 0.00186936  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.43 min) Epoch 98/300 -- Iteration 747556 - Batch 462/7702 - Train loss: 0.00187138  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.45 min) Epoch 98/300 -- Iteration 747633 - Batch 539/7702 - Train loss: 0.00187392  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.48 min) Epoch 98/300 -- Iteration 747710 - Batch 616/7702 - Train loss: 0.00187474  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.50 min) Epoch 98/300 -- Iteration 747787 - Batch 693/7702 - Train loss: 0.00187118  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.52 min) Epoch 98/300 -- Iteration 747864 - Batch 770/7702 - Train loss: 0.00186887  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.54 min) Epoch 98/300 -- Iteration 747941 - Batch 847/7702 - Train loss: 0.00186875  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.56 min) Epoch 98/300 -- Iteration 748018 - Batch 924/7702 - Train loss: 0.00186753  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.58 min) Epoch 98/300 -- Iteration 748095 - Batch 1001/7702 - Train loss: 0.00186945  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.60 min) Epoch 98/300 -- Iteration 748172 - Batch 1078/7702 - Train loss: 0.00186753  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.62 min) Epoch 98/300 -- Iteration 748249 - Batch 1155/7702 - Train loss: 0.00186729  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.64 min) Epoch 98/300 -- Iteration 748326 - Batch 1232/7702 - Train loss: 0.00186762  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.67 min) Epoch 98/300 -- Iteration 748403 - Batch 1309/7702 - Train loss: 0.00187075  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.69 min) Epoch 98/300 -- Iteration 748480 - Batch 1386/7702 - Train loss: 0.00187033  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.71 min) Epoch 98/300 -- Iteration 748557 - Batch 1463/7702 - Train loss: 0.00186896  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.73 min) Epoch 98/300 -- Iteration 748634 - Batch 1540/7702 - Train loss: 0.00186953  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.75 min) Epoch 98/300 -- Iteration 748711 - Batch 1617/7702 - Train loss: 0.00186878  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.77 min) Epoch 98/300 -- Iteration 748788 - Batch 1694/7702 - Train loss: 0.00186725  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.79 min) Epoch 98/300 -- Iteration 748865 - Batch 1771/7702 - Train loss: 0.00186833  - Train acc: -0.0000 - Val loss: 0.00049786\n",
      "(201.82 min) Epoch 98/300 -- Iteration 748942 - Batch 1848/7702 - Train loss: 0.00187004  - Train acc: -0.0000 - Val loss: 0.00049786\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m     17\u001b[0m stage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mae_rnn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 18\u001b[0m curves \u001b[38;5;241m=\u001b[39m \u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_final_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelo_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mae_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mrnn_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mmax_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mrandom_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43monly_classifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mnum_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(modelo_final\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelo_final\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pmarc\\Universidad\\inteli\\ZTF_alert_8a\\src\\final_training.py:159\u001b[0m, in \u001b[0;36mtrain_final_model\u001b[1;34m(model, stage, train_dataset, validation_dataset, ae_criterion, rnn_criterion, max_epochs, max_time, batch_size, learning_rate, random_sampler, only_classifier, augmentation, early_stop, use_gpu, num_cpu, weight_decay)\u001b[0m\n\u001b[0;32m    156\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    157\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 159\u001b[0m cumulative_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m train_loss_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Calculate training accuracy\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import src.final_model as fm\n",
    "import src.final_training as ft\n",
    "importlib.reload(fm)\n",
    "importlib.reload(ft)\n",
    "importlib.reload(src.final_training)\n",
    "importlib.reload(src.final_model)\n",
    "\n",
    "\n",
    "# modelo_final = fm.FinalModel(128, 3, 'LSTM', 128, 1, 0,  3, name='4th_iteration')\n",
    "# modelo_final.load_state_dict(torch.load(f'models/2_2.pth'))\n",
    "\n",
    "modelo_final = fm.FinalModel(latent_dim, n_channels, rnn_type, hidden_dim, num_layers, dropout_prob,  n_classes, name='RNN model with augmentation')\n",
    "\n",
    "early_stop = 30\n",
    "batch_size = 64\n",
    "\n",
    "stage = 'ae_rnn'\n",
    "curves = ft.train_final_model(modelo_final,\n",
    "                                              stage,\n",
    "                                                train_dataset,\n",
    "                                                validation_dataset,\n",
    "                                                ae_loss,\n",
    "                                                rnn_loss,\n",
    "                                                max_epochs,\n",
    "                                                max_time,\n",
    "                                                batch_size,\n",
    "                                                lr,\n",
    "                                                random_sampler,\n",
    "                                                only_classifier,\n",
    "                                                augmentation,\n",
    "                                                early_stop,\n",
    "                                                use_gpu,\n",
    "                                                num_cpu)\n",
    "\n",
    "torch.save(modelo_final.state_dict(), f'models/{modelo_final.name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACJIAAAV7CAYAAAB3yxcmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAC4jAAAuIwF4pT92AAEAAElEQVR4nOzdd3gUxf8H8Hd6r5BAQu8l9BpCCCBdivSiNAERqQqKUgQBqQoqIvKlBinSuyiC0kIn1IQaIAFCSCO9l/39gblf9vbuci13F3i/nuce2MnOztzd3uzu7GdnzARBEEBEREREREREREREREREREREbz1zY1eAiIiIiIiIiIiIiIiIiIiIiEwDA0mIiIiIiIiIiIiIiIiIiIiICAADSYiIiIiIiIiIiIiIiIiIiIjoPwwkISIiIiIiIiIiIiIiIiIiIiIADCQhIiIiIiIiIiIiIiIiIiIiov8wkISIiIiIiIiIiIiIiIiIiIiIADCQhIiIiIiIiIiIiIiIiIiIiIj+w0ASIiIiIiIiIiIiIiIiIiIiIgLAQBIiIiIiIiIiIiIiIiIiIiIi+g8DSYiIiIiIiIiIiIiIiIiIiIgIAANJiIiIiIiIiIiIiIiIiIiIiOg/DCQhIiIiIiIiIiIiIiIiIiIiIgAMJCEiIiIiIiIiIiIiIiIiIiKi/zCQhIiIiIiIiIiIiIiIiIiIiIgAMJCEiIiIiIiIiIiIiIiIiIiIiP7DQBIiIiIiIiIiIiIiIiIiIiIiAsBAEiIiIiIiIiIiIiIiIiIiIiL6DwNJiIiIiIiIiIiIiIiIiIiIiAgAA0mIiIiIiIiIiIiIiIiIiIiI6D8MJCEiIiIiIiIiIiIiIiIiIiIiAAwkISIiIiIiIiIiIiIiIiIiIqL/MJCEiIiIiIiIiIiIiIiIiIiIiAAwkISIiIiIiIiIiIiIiIiIiIiI/sNAEiIiIiIiIiIiIiIiIiIiIiICwEASIiIiIiIiIiIiIiIiIiIiIvoPA0mIiIiIiIiIiIiIiIiIiIiICAADSYiIiIiIiIiIiIiIiIiIiIjoPwwkISIiIiIiIiIiIiIiIiIiIiIADCQhIiIiIiIiIiIiIiIiIiIiov8wkIToLfHNN9/AzMxM9HrbhIeHSz6DwMBAY1eL1CD/vX3zzTfGrpJR8HMgU9KuXTvR/tiuXTtjV6lIPA4QEdHbIDMzE4GBgRg6dCjq1KmD0qVLw9LSUnT8c3V1NXY1iVQqaeeap06dkpxnnjp1ytjVIqJiwjbqzWEqfebs8yMiYltoiiyNXQEiIiIiIiIiItLdgQMHMHbsWMTGxhq7KkRERERERERUgpnMiCSKnhA19UheIiIiIiIiKjmGDx8uue40MzPDjh079FbGyJEjFZahj9ePP/5YZPmabM/a2hoeHh6oVq0a3nnnHUybNg07duxAWlqazp9DYGCg0nJ1faro22+/1fqpWmV1qlGjBnJycrSu0/PnzyXbHDlypNbb08aWLVvQp08fBpEQvUUUjTKg6mVvb4+yZcuiVq1a6NWrF+bOnYsTJ04gPz9f57qoOv7pOvJBx44dtRoxQNXno2sbvXXrVo52SERERERvNJMJJCEiUoRDWRGRMVWuXNmoN4SISH38vVJRkpOTsXfvXoV/27Bhg4FrYxpycnIQFxeHx48f4+TJk1ixYgWGDBkCLy8vfPzxx4iJiSmWcpcvX47o6Ohi2ba2wsLCsHbtWmNXQ2uRkZEYN26cwr+VK1cO9evXR8OGDWWv+vXrG7iGRGQKMjIyEB0djQcPHuDw4cOYP38+OnXqhEqVKmHx4sU6BdSpMn369GLZri62bNmC27dvG7saRFRCyQfPVa5c2dhVIqIS4k2abox9cW8+BpIQERERERHRG2/79u1IT09X+Ld//vkH4eHhhq2QCUtJScHatWvh4+ODQ4cO6X37qampmD9/vt63q6v58+cjNTXV2NXQyurVqyX796hRo/D8+XM8f/4ct27dwo0bN2Svs2fPGqmmRGSKnj9/jpkzZ6Jp06a4f/++3rd/5coV7N69W+/b1UV+fj6++uorY1eDiIiIiMhkWRq7AkRERERERETFTdWoI4IgYNOmTZg3b16xlF2tWjU4OjrqvB0PDw+N81hZWaFu3boK/5aVlYWEhASlo4PExcWhf//+OHz4MLp06aJx2aqsXbsWn376KWrUqKHX7eoiJiYG33//fYkcBfHAgQOi5RYtWmD9+vVqT/9ARG8ONzc3VKxYUeHfUlNTERcXh6SkJIV/v337Njp27Ihz584p3Ya2Zs6ciT59+sDS0nS6o48ePYpTp05xenUiIiIiIgVM58ydiKiYVa5cGYIgGLsaRFrj/kumpCQOucjjANHb6/bt27h69aoozczMTNQmBAYGYu7cuTA31//AnevXrzfaTSpvb2/cuHFD5TqxsbE4duwYli9fLlk3JycHgwcPxqNHj+Du7q63euXm5mLWrFnYtWuX3rapD8uXL8cnn3yCMmXKGLsqaktJScG9e/dEaQMHDmQQCdFbqlevXggMDFS5zuPHj7F79278+OOPePnypehvz58/x9ChQ3HmzBm91qtgCrHx48frdbu6+vLLL3Hp0iVjV4OIjOSbb74pkUHERERvIvbbmh5ObUNERERERERvtPXr14uWzc3NMX36dFHa06dPceLECUNWy2R4eHhg6NChuHLlCiZMmCD5e2JiIhYtWqT3cnfv3o0rV67ofbu6MNVpd1R5/vw58vPzRWm1atUyUm2IqCSoWrUqvvzyS9y+fRutW7eW/P3s2bM4cuSI3ss1xSnELl++bHLT7hARERERmQIGkhAREREREdEbKysrC9u2bROltW/fHtOnT4e1tbUoXdX0N28DS0tL/Pzzz+jQoYPkb1u3btX56aB+/fpJ0uQDegytVatW8Pb2FqWtW7cOYWFhRqqR5hRNUeHg4GCEmhBRSVO6dGkcPHgQpUuXlvxty5YtOm9fvt2Pjo7G8uXLdd6uLvr27SsZsWnWrFnIzc01Uo2IiIiIiEwTA0mIiIiIiIjojXXgwAHEx8eL0oYPHw53d3d07969yHXfNmZmZpg7d64kPTo6Grdu3dJp271794afn58o7dSpUzh69KhO29WFvb295P3m5ORg5syZRqqR5jIzMyVpnNaGiNRVqlQpTJ48WZL+zz//SEY70tTs2bPh7OwsSvv+++8RExOj03Z10bhxYwwePFiU9vDhQ6xdu9ZINSIiIiIiMk2Wxq6AqcnOzsbly5fx/PlzxMTEIC0tDaVKlYKnpyd8fHxQo0YNvZaXl5eHhw8f4vbt24iNjUVycjLy8vJgb28PZ2dnVKhQAVWqVEG1atV0mqtbEAQ8fvwYt27dwsuXL5GcnIzc3FzY2dnB0dER5cuXR+XKlVGzZk1YWprObvH8+XPcvHkTsbGxiI2NhZmZGTw8PODl5QVfX1/JxWhJlpeXh2vXruHJkyeIjY1FUlIS3N3d4eHhgZo1a6J+/frGriIpYCrfW2xsLC5evIjHjx8jNTUVLi4u8PT0RMuWLVGpUiWD1EETjx49ws2bNxEZGYnk5GQ4OzujWrVqaNmyJUqVKmXs6hnU06dPcf36dURERCAlJQUWFhYoU6YMBg0aBHt7e4PVIz8/H9euXcPt27cRExMDMzMzlC5dGlWrVoWfn5/kiW1d5eTk4MKFC4iIiEBUVBSA10/D+fj4oFmzZrCwsNBreSVJXl4ebty4gTt37iA6OhqZmZlwcHBAgwYNFD4hrij/48ePce/ePdlvLC8vD25ubnBzc0Pt2rVRv359nc4rDOHhw4cIDg5GZGQksrKyUKpUKXh7e8Pf3x9ubm7Grp5COTk5uHz5Mu7cuYO4uDhYWVnJjgctWrTQ+36dkZGBc+fO4fnz53j58iUsLCxQtmxZ1K9fHw0bNjTZm4gZGRkIDQ3F3bt3kZCQIGv77O3t4ebmhkqVKqFatWooV66csasqERMTg3v37uHRo0dITExEWloanJyc4O7ujnLlyqFFixZwdHQslrJNpd0MCwvDnTt3ZOfnNjY2KF26NMqXLw9fX1/Y2dkZpB6akB9lxMHBQfaE9IgRI7B//37Z37Kzs7F161ZMmTLFoHU0NX5+fnB2dkZycrIo/c6dO2jYsKFO2162bBn8/f1FaTNmzEDXrl2NdmwaPXo0fvjhB9y7d0+WtmfPHly5cgXNmzc3Sp1MiaH7SZQxlfPmAunp6bh06RLu37+PhIQEWFpaomzZsmjRooVGUwvFx8fj8uXLCAsLQ0pKCpydneHl5YW2bdsqHCVCX/V+8eIFYmNjkZmZCQ8PD3h6eqJx48YoX7683ssEXr/PixcvIjIyErGxsbCzs0PFihXRtGlTVKlSpVjKlJeTk4OrV6/i2bNnsut3V1dXeHh4oG7duvDx8TFIPUxRt27dMGfOHFFafHw8oqOj4eXlpfV2S5cujenTp2P27NmytIIpxFatWqX1dnW1cOFC7N27F9nZ2bK0+fPnY8SIEW/MiE5sozTDNopKipLSVyIIAiIiInDv3j08ffoUycnJyM7OhqurK9zc3FCtWjU0adLEpO5F6dP9+/dx+/ZtREVFITk5GS4uLrK+b3d3d4PWJSkpCRcvXsTDhw+RlJQEW1tblCtXDq1atdLovsGLFy9w5coVhIeHIzU1Fe7u7ihfvjzatm1bbPfpTOH+YEn5zcl7U/qHdSUIAm7fvo2HDx8iNjYWr169kt07q1KlCpo0aWKQz0D+d+jo6AgPDw80adIEtWvXLvbydSKYiCdPnggARK+2bdsarPzjx48LvXr1EhwdHSX1KPyqWrWqMHXqVCEqKkqn8q5evSqMHj1acHFxUVlewcvZ2Vno2LGj8MMPPwjPnj1Tu5z79+8LkydPFsqUKaNWOXZ2dkKbNm2EhQsXCvfv39fpPWorOjpa+OqrrwQfHx+VdbW0tBRat24tbN++XcjLy1Nr2/Xq1RNtw8PDQ8jOztapvtu3b5fU7ZdfflE7/5UrV4TBgwcL7u7uKt+vt7e38NFHHwlhYWFa1XPu3LmSbapDPs/cuXO1Kn/EiBGi7VSqVEnheidPnlRrX1X1UrZtRe3Mpk2btHo/hvre2rZtq7JdPHnypNCpUyfB3NxcaR3q1q0rbN26VcjPz9eqDvqSn58vrF+/XmjSpInSulpYWAhdunQRzpw5I8qrzX5Y1Genrk2bNknKf/LkSZH5KlWqJMozYsQI2d+ys7OF1atXC/Xr11f6WSgqQ5vPQdFv6uTJk7K/JyUlCXPmzFF5nHBwcBBGjhwpPH36tOgPrAgRERHCqFGjBDc3N6Xlubu7C59//rkQExMjy6ft96ApRe2ENi9FivouIiIihAkTJij9bFTtww8ePBCWLFkidO7cWXBwcCiyfi4uLkL//v2Fixcvav1ZafsbU7Uf5+XlCRs3bpQcr+XbiQ4dOggXLlzQuM7aHgeK2v+ioqKEKVOmqDyvc3V1FT799FMhLi5O43rLCwkJEQYMGKDyu/b29hbmz58vpKamyvJpey6gL/v27RN69OghWFlZqfU78vb2FgYMGCBs3bpVSEpKkmyvOH+vBZKTk4WtW7cKw4cPFypWrFjktiwsLIRmzZoJ69atE7KysvTyuZlCu/n48WNh4sSJQtWqVVW+f1tbW6FTp07C0aNH9fLe9SE8PFxynjR8+HDZ37OzswUPDw/R3xs0aKBTmfLnvfLtfXGTL1vZuXFRGjRoINnWypUr1c6vaB/csmWLIAiC0KtXL8nfNm/erPa2FyxYoPVnLJ+vQ4cOgiC8bqPk/9a+fXu16/Ts2TNJ/sLnf/qkqD3X5KXuMdtQ/STFcd6si6Laz3v37glDhw4VbG1tldapSZMmwh9//KGynDNnzghdu3YVLCwslB5TunTpIoSEhOjlfe3atUvo1KmTYGNjo/L79PHxEebOnavw2KuNU6dOCV27dhUsLS2Vllm/fn0hMDBQdM2qr+s5QRCEgwcPCr169RKcnJxUvncvLy/hk08+ESIiIjQuo6hz/eKiqFxt2p5Xr14p/Exu3bql9jYUHf+ePXsmpKWlCV5eXqJ0Kysr4eHDh2pvu0OHDpJtq0PR57NgwQJBEARh8uTJkr/NmzdP7Tpt2bJFkl/bPiZNsI1iG1US2ij53/zYsWPVzrt06VKF7z8yMlKt/Dk5OYKzs7Mo7/Tp05Wur8l1svw5izYvZZ+7/HqG7CvRl2fPngk//fST0KtXL5XXrwUve3t7oWvXrsJff/2lVXn67OvXx286JydHWLFihcr7WhYWFkK3bt2EoKAgUV5V378yRe27ly9fFnr16qWyfWvbtm2R+8yhQ4eE1q1bC2ZmZgq3YW1tLQwaNEirtkmR4rw/WJihf3O6XsMBRZ/jGaJ/uLj74rT5LShy//59YfTo0ZLjkfyrVKlSwvvvvy9cu3ZNq3KKuu95/fp1oV+/fir7QitVqiSsXLlS53vVxeWtDyR59OiR0LlzZ413cgcHB2H+/PkaN1CZmZnC2LFjVd70LerVsmXLIsvJz88XZs+eLVhbW2tdTpkyZbT9WLWSlZUlfP3112o1cPKvevXqCTdv3iyyjOXLl0vy7t+/X6d6y+8/NjY2wqtXr4rMFxsbKwwaNEjpAVjZy8rKSpg8ebKQmZmpUT0ZSKKfk0tDf2/KTmIzMzOFMWPGaFSHzp07i24oGtLjx48Ff39/jeo7adIkITc3VxCENyuQ5MGDBypPQlWVoc3noKqz4PTp00K5cuXU/k7s7OyEgwcPqv/hyfnpp580auNLly4tu5h80wNJ1q9fL9jb26vcpqJ9OC4uTmjcuLFOde3Vq5eQkJCg8Wel70CSZ8+eCa1atdKo7jNnztSozsURSLJnzx7B1dVV7TqXKlVK646dvLw8YebMmWoHYgCvj4lXr14VBMF4gSQRERFCQECATvvpl19+KdlucV+8fvHFFyo74It6lS9fXhIYqSljt5tJSUnChAkTNNrnCl7+/v5660jShaL9/sSJE6J1FN1IunLlitZlvimBJH5+fpJtffvtt2rnVxVIcufOHcmNqYoVK6p9rlwcgSTK3rO6gVFvUiCJoftJiuO8WReq2s9ffvmlyJuchV9TpkyRBPRnZWUJn3zyidrbsLS0FH777Tet309wcLDQvHlzjb/P0qVLC2vWrNG63PT0dGH06NEaXTu3bdtWiI2NFQRBP9dzly5dEnx9fTV+79bW1sKMGTM02pdLeiBJdna2ws9C/kaXKsoCSQRBEP73v/9J/jZw4EC1t10cgSSxsbGSm81OTk5CdHS0Wts2xUAStlHqYxtVvG3U0KFDRWVVqVJF7bydOnVS+L7VDTo+d+6cJO+xY8eUrm/qgSSG6CvRB39/f437ywu//Pz8NHqAWhBMK5Dk+vXrap23FrzMzMyEyZMn69T3rWzfLbg3qO49SDMzM+G7776TbD8xMVHo27ev2u/JwcFBOH78uEafW2GGuD9YmKF/c8UZSGLI/mFTDyRJS0sTPvnkE5UBVMp+B0OHDlXr3m5hyu575uXlCbNnz1YakKvo1bhxY7XPQw3pzR6zpgjBwcFo1aoV/v77b43zpqWlYc6cORg4cKDC+YgVyc7ORvfu3bF27Vqd5xgtyogRI/Dtt9+Khmg0Za9evULnzp2xYMECpKWlaZw/JCQErVu3xuHDh1WuN3ToUFhZWYnSNm3apHF5BZ4/f44TJ06I0nr37l3kkFaPHz+Gn58fdu7cCUEQNCozJycHK1euRKdOnZCQkKBxnUl7pvK9ZWZmolu3bli/fr1G+f7++2+8++67yMvL06l8TT1+/Bht27ZFUFCQRvl+/vlnDBs2TOPP2pTdu3cPrVq1QkhIiLGrgiNHjqBjx46IjIxUO09GRgb69euHv/76S+PyZs+ejSlTpmjUxsfFxaFHjx44cuSIxuWVJN999x3GjBmD9PR0jfOmpKTg+vXrOpV/6NAhtGjRAs+fP9dpO7p4/PgxfH19ceHCBY3yLVq0SDRMtqGtWbMGAwYMQGJiotp54uPj0bFjR9y4cUOjsvLz8zFy5EgsWrQIOTk5aueLiIhA27ZtcfXqVY3K05fw8HD4+/vjzJkzRilfF5cvX1b7PF+R58+fo0OHDtiyZYtW+Y3dbkZERKB169b45ZdfNNrnCgQFBaFFixa4ePGiznXRVn5+PgIDA0Vp5cuXR/v27UVpI0aMkOSVnw7nbaSobdPX0MF16tTByJEjRWlPnz416jQHALB06VJJ2ldffVXs1++mxND9JMqY0nlzgUWLFmHChAnIyspSO89PP/2EWbNmyZazs7PRu3dv/Prrr2pvIzc3FyNHjsShQ4c0qi8A/PnnnwgICMCVK1c0zhsXF4dx48ZhypQpGv8GMjIy0LNnT2zYsEGj67nTp08jICBAL30d27dvR0BAgFbHoezsbCxevBi9evVCamqqznUpCZSdz+qr3R89erRkOpXdu3cb7RwVeD3tzhdffCFKS0lJwYIFC4xUI92wjVIf26ji17FjR9HykydP8Pjx4yLzZWVlKe0/lO+DV0Z+PRsbG7Rp00atvKamJPWVBAUF6dSHe/78eTRr1gw3b97UY60M48qVK2jfvr1G562CIGDlypUYOnSo3vu+x40bh2+//VbttlEQBHzxxRdYu3atLC0xMREdOnTAvn371C43LS0NvXr10qpNN9T9waKUpN9cYW9K/7Cu4uLi8M477+DXX39Fbm6uRnkFQcDWrVvh7++Pp0+f6lSP/Px8DB8+HN9++61G9+KuX7+OgIAAkzu2v5kTkKnh7t27aNeuncIvpEaNGujTpw+qV68OZ2dnvHz5EpcvX8ahQ4ck6+/duxdZWVk4dOhQkfPQL168GP/8848kvUKFCujcuTPq1q2LMmXKwNbWFunp6UhOTkZYWBhCQkJw4cIFtRvQzZs3K+yw9vDwQJcuXVC/fn14e3vDzs4OGRkZSElJwZMnTxAaGooLFy4YPDghMTERrVu3Fs1HXaBevXpo27YtfHx84OrqCuD13PQXLlzA0aNHkZKSIls3NTUVAwYMwLlz59C0aVOFZXl6euLdd9/FwYMHZWlHjx5FTEwMPD09Na77b7/9Jjkgjxo1SmWemJgY+Pv7y+a2L6x8+fLo27cv6tSpA3d3d8TExODmzZvYv38/4uPjReuePXsWHTt2xPnz52FjY6Nx3U2Vo6OjaN51+ZPHMmXKoGzZsiq34e3trfd6mdL3NmrUKJw8eVK2XKtWLXTr1g21a9eGu7s7kpKScP36dezduxfR0dGivGfOnMEPP/yAzz//XKuyNZWQkID27dvj2bNnkr9Vr14dffv2FbW158+fxx9//CFr737//Xelv+eSJj09Hb169RLtE/Xq1UO3bt1QrVo1uLm5ISYmBg8ePMDu3buLtS43btzAjBkzZDcG7ezs0KFDBwQEBKBs2bKwtLTEs2fP8Pfff0uOW7m5uRgzZgxCQ0Ph4uKiVnk//vgjFi5cKEm3trZGx44d0b59e3h7eyM3NxfPnz/HsWPHEBQUhPz8fOTm5mLIkCGYNm2a7m9cDdbW1qI26M6dO6IbqG5ubqhYsaLeyvv777+xZMkS2bKNjQ3at2+Pdu3ayb6L58+f49KlS2qdRDo6OqJ58+aoU6cOatSoARcXFzg5OSE7OxsJCQm4c+cOTp48ibt374ryPXz4EIMGDcLp06cNPj9tSkoKunXrJgtqMjMzg5+fHzp27IiKFSvC0dERsbGxOHfuHPbv3y+5MbV48WL07NkTLVu2NGi9//zzT0ycOFF2we/i4oLOnTvDz88Pnp6eyM/PR3h4OI4cOYJLly6J8qalpeHDDz/ElStX1P68p06dqvD8zsHBAe+++y78/PxQtmxZZGRkICIiAn/88YesYz4tLQ29e/fGgAEDdHzXmhs1apTCY0CjRo3Qrl071KxZE66urrCyskJKSgoSEhJw79493Lp1C1evXlV5wWXI36uZmRnq16+P+vXro06dOvDw8ICzszMsLCyQkpKCx48f48qVKzh58qSoDjk5Ofjoo4/g4+ODJk2aqF2esdvNiIgItGzZUnIeAQAtWrRA69atUatWLbi5uSE7OxtRUVE4f/48/vzzT9HNi+joaHTv3h3Xrl3TaO5lfTlx4gQiIiJEacOGDZPMf9ukSRP4+PggNDRUlvb7779jxYoVsLOzM0hdTU1GRgbCwsIk6VWrVtVbGfPmzcP27duRkZEhS1u0aBFGjx4tu/YzNH9/f/Tq1Ut0M+zWrVvYunUrhg8fbpQ6KVK2bFlR+5eamopHjx6J1qlWrRocHR0V5q9evbrCdGP0kyhiSufNBQ4cOCC62VqmTBn06NEDTZo0QenSpZGSkoKbN29i165dkrZzyZIl6N27N1q0aIEJEybgzz//lP2tdu3a6NGjB2rUqAFXV1e8evUKQUFB2LNnj6g9zc/Px7hx49CuXTu1b+yfPHkSvXr1UtiJ2rBhQ/Tq1QuVK1eGnZ0doqKicPr0aRw7dkxyE3rlypXIy8vTKNBr8ODBCvu/SpUqhT59+qBhw4bw9PREfHw8QkNDsW/fPtm19t27d3X+vf36668YP368JN3BwQGdOnVC8+bN4eXlBScnJyQlJeHhw4c4fvw4rl27Jlr/jz/+wMiRI7Fnzx6d6lMS3Lp1S5JmZmaGypUr62X7FhYWWLx4Mfr27StLEwQB06dPx7///quXMrQxdepUrF69WtTX87///Q+ffvopqlWrZrR6aYptFNsoUyMfSAK8PjcfO3asynznzp0TnRvK51eH/Hp+fn56O6evW7eu7Dz16dOnovsoVlZWqFu3bpHbUHZ+Jq+k9pUAr/u2mjVrhrp168quG52cnJCbm4ukpCTcu3cPQUFBCA4OFuWLjo5Gv379cO3aNb0FMha38PBwdOrUCUlJSZK/+fj4oFevXqhatSqcnJwQHR2N4OBgHD58WLbv7NixQ6993z/99JMoIKRSpUro2bMn6tWrB3d3dyQmJuLSpUvYvXs3kpOTRXk//fRTdOnSBRUqVMDgwYNF30/Tpk3RtWtXVKlSRfZe/v33Xxw6dEh0fywjIwMfffQRrl69qnZ/lyHvD6pS3L+5wtdwml6/FVC3j6s4+4cN3XeuroyMDKUBXaVLl0afPn3QoEED2fH97t272Lt3L168eCFa986dO/D398eNGzfg7u6uVV1mzZqFbdu2yZYrVKiA7t27o379+ihdujRSU1Nl5T958kSU9/79+/jqq6+M/pCNiHEGQpEy5NQ2mZmZQsOGDSXlubu7y4baVeTVq1fCyJEjFQ458+OPP6osMyMjQzKvsL29vbBx40a1hqHLzMwU/v77b2HIkCFCQECAynWrV68uKsfCwkJYtmyZWsME5+bmCkFBQcLYsWOFatWqFbm+PvTu3Vvyefr5+RU5L1dCQoIwdepUyZBplStXFpKTk5XmO3jwoKS85cuXa1X3GjVqiLZTvnx5ld9nfn6+0K1bN0n5dnZ2wooVK2RDmclLT08XvvjiC4XDkX366adq1bWkTG1TXOULgvbD3Rnze5MfVq/wEPtly5YVdu/erTRvSkqKMHz4cEnZrq6uQnp6ulrl60rR0LYuLi7Chg0blOaJjY0VhgwZIvqctdkPTG1qm8LDmFWuXFk4cuSI0rw5OTlCTk6OJF2bz0HR8KWF96Nhw4YJL168UJr/1KlTgru7u2QbixcvLrJsQXg9H6Gi77BLly4qP8fr168LTZs2VbkfFMfUNvKUDbWuDUXfReH9ol+/fsLTp0+V5s/IyJCkPXnyRHB1dRUmTpwonDp1Su25FM+dOyc0a9ZMUh9FQ1kqo6+pbQrvjy1bthSCg4OV5n3y5InQpEkThfuTOvQ5tU1Bvc3MzIRp06apHP5x165dCqdI+f3339Wq96lTpxQOEfvBBx/IhlZW5OTJk0K1atVU/o6K09mzZyXlVa1aVe0h0l+9eiVs27ZNCAgIEL766qsi19fn71UQBKF9+/ZC586dha1btwoxMTFq5YmNjRUmTZok+b7q1aundrnGbjezsrIUDi/eo0cP4c6dOyrzRkVFSYaxBiA0b95c6flScRo4cKCkLnfv3lW47rJlyyTrajtM+5swtc3OnTsVHrPU/S0IguqpbQp89dVXknUUTWUlr7imthEEQQgNDZUMP1upUqUir6cNObWNPH0MV2+MfpICxXHerAtF+27BVBEWFhbC/PnzFZ6XCcLrKcEUDQHeuXNnYe/evbJlDw8PYdeuXUrr8PDhQ6FmzZqS7SxatEit9xAfH69wCsuKFSsKf/75p9J8z549E7p3767w+zxw4IBaZSv6/MzMzITPP/9c6XVoTk6OsGDBAtFUavLHMXXPNS9duiSZ5tnOzk5YvHixkJSUpDLvqVOnROdOBa+VK1cWWW5Jn9pG0VQmDRo00Ggbqqa2KaBoqHhV+2SB4pjapsCaNWsk6wwaNKjIbZvS1DZso9hGFcUYbVTt2rVF5Q0YMKDIPDNmzJB8N4WXQ0JCVOZPTU2VTMu5cOFClXm07TPXtq9bGfk6GLKvRB/s7e2FESNGCH/99Zfa/c4hISEKpzKaMGGCWvmNPbVNfn6+wuNT2bJlhb179yrNl5KSIkyZMkVpewJoP7VNwfHA3t5e+PXXX5Veh798+VJo3bq1JP/YsWOF5cuXy5arVq0qmRq2sCtXrgienp6S7Wzfvr3I+hcw9P3BAsb8zem7TTZm/7C+++K0+S0IgiB8/PHHkrwWFhbCzJkzlZ4X5ebmCsuWLVM4LWCfPn3UKlf+WGBtbS3bJ52cnIS1a9cq/R1mZ2cr7BOxsLAQnj9/rlb5hvBWBpIsXLhQUpa7u7vac2jNnDlTYQOt6os9cuSIJE9gYKBW9U9LS1P6t5CQEEk533zzjd7L0RdFc6SOHz9eMj+nKhs3bpRsY8mSJUrXz8nJEcqUKSNav379+hrXPSgoSFLurFmzVObZtm2bwgOUqoNxYYo+LzMzM7XmcGcgifYnl8b83uRPYgufxIWHhxeZPz8/X+jSpYskv6rOYH05ffq0pFxHR0fhwoULauWfPHmywveu7n5gaoEkBa9atWoJkZGRWtVFm89B0YlpwUu+A02ZoKAgyUl59erV1crbtWtXSbkDBgxQ64ZiSkqK4Ofnp7T+b0IgScFr0qRJGh37CmRlZWkdGJaRkSH5fipUqKD2zRh9BZIUvHr06KH0xL6w+Ph4yXHc3NxciIiIKDKvPgNJCtpydW80KzqWyN+8VCQ/P1+oU6eOJO/UqVPVKjcqKkphJ68m5wLa+vzzz0VlWVlZCQ8fPtRqW+qcl+r74jUxMVHrvIGBgZLPWtW83IUZu92U77wFVJ9bK6LovHPHjh0abUNXcXFxks6AFi1aKF3/xYsXkuCBdu3aaVV2SQ8kSUlJkXT8AxDeffddjbajTiBJYmKiJGDVzs6uyPnRizOQRBAEYdSoUZL1vv/+e5XbLemBJMboJylQHOfNulB23Dc3Nxf27dtXZP7s7GyhXr16knOG0qVLCwAELy8v4cGDB0VuJywsTNKO1ahRQ6338NFHH0nqX6VKlSJ/W4Lw+txDUVCgh4dHkeediYmJgpubmyTvzz//rFa9d+/erXQecXXONTMzM4XKlSuL8nl6egq3bt1Sq/yC99CgQQPRNkqVKiWkpqaqzFeSA0lCQkIUdqAvW7ZMo+2oE0iiKNC4QYMGRT5kV5yBJDk5OUKtWrUkv9mi+mxMKZCEbRTbKFNsoyZOnCipZ1G/dfmA9v79+4uWiwpSPXr0qOR9Xrp0SWUeUw0kKXgZoq9EH7S9fs7Ly5Oce9vb2wuvXr0qMq+xA0l27NghKb9s2bLC/fv31SqzcMCG/EvbQJKCz+/8+fNF5k9ISJAEgdjb2wsODg4CAMHHx0eIjo4ucjuKju2dOnVS5yMwyv3BAsb8zem7TTZm/7ApBJIouldrbm6udp/tH3/8IQlCBCDs2bOnyLyKzn+B19fSN27cUKt8RedF6t63MYS3LpAkOztb8PLykpR16NAhjbajqIN3xowZStdfuXKlaF07Ozu9PzUjCIJw6NAhSb2M0fGijpycHEkj07VrV622NWbMGNF2ypQpo/KJsWnTpkk+p6tXr2pU5ujRoyXbCAsLU5mn8FOiBS91osYLGzdunGQbQ4YMKTIfA0m0P7k05vemKJDEyspKo/01NDRUso2hQ4dqVH9tKHrKRdVIJPLy8/OV3gwrqYEklpaWwrVr17SqhyDoN5Ckb9++GpWt6Knuotq8hw8fKowK1+TENjIyUnByclL4Ht6UQJIWLVoY5Ul9QRCE6Ohowd7eXlSfP/74Q628+gwkqVy5skadDqtXr5ZsY/369UXm03cgibrBHAVatGghaROKujg9ceKEpFxfX1+NLqqvX7+utMOzOMkfB9TtSNCWvi9edSXf6anOE3jGbjdfvXolGUVx3LhxapddmPyTZY0aNdJqO9r68ccfJe9/1apVKvPIX+OZmZkVeaxTRFFHQrVq1YSGDRvq9FJ3RDv5sjXp1I6Ojhbat28v2Ya5ubnG10vqBJIIguIO1FGjRqncdnEHkjx//lzyZKC7u7vKY1VJDiQxVj9JgeI4b9aFsuP+zJkz1d7Grl27FG4DgNoPJAiC4uvIotql2NhYyUhoFhYWGn2eOTk5Qv369SVl/+9//1OZT77vC3g9AqImFAU0Auqda65du1bSdp07d06j8gVBEB49eiQZMaCom5clNZAkODhYqFixomQb3t7eQkpKikZ1USeQRBAEoWfPnpL1iuroL85AEkEQRKNxFLzeeecdlds2tUAStlFso1QxRht14MABSZmqzicTEhJEozqXK1dO8pBa9+7dVZY5depU0fqurq5F9reYciCJofpKjC0jI0OoUKGCqN6//PJLkfmMHUji7+8vKf/o0aMalauo/xzQLZBk7dq1apevaGRO4PVDtKGhoWpvR/46QJ3+LmPeHxQE4/7mjHXeqIwu/cOmEEjSr18/ST5N+2yXLFki2UarVq2KzKcskOTgwYNqlx0fHy85N/L399eo/sVJPDn0W2Dv3r2ieS8B4N1330XPnj012s6qVatgYWEhSlu7dq1oLqjCCs/VBQAuLi5qzxGmCflygNfzO5qiHTt2iOYsNzMzw88//6zVtubMmSOaezk6OhoXLlxQuv6oUaMkaZs2bVK7vPT0dOzatUuUFhAQoHL+1IsXL0rm/atfvz4mTJigdrnA67nW3NzcRGl79uxROIc96c4Uv7f3339fo3n+6tatiyZNmojS5N+TvkVFRYnmlgeAJk2a4MMPP1R7G7q0CaZq2LBhaNy4sbGrAXNzcyxbtkyjPEOHDpWkFbUfrVu3DoIgiNIWL16s0dy03t7e+PLLL9VevyT67rvvJOcUhuLp6YmuXbuK0oKCggxej7lz58LFxUXt9QcPHiz5zIq7XZPn5OSEuXPnapRH/neUm5urcD76wgrPbVvghx9+EJ33FKVRo0Yatb/6In9eaqrnpMVFfu50dX5bxm43f/nlF6SmpsqWHR0dsXTpUq22NWfOHNHyjRs3EB4ertW2tLFx40bRsrW1NQYPHqwyz4gRI0TLgiBItqOtR48e4ebNmzq95OfO1Yfs7GxER0fjxIkTmDZtGmrXro2TJ09K1vv222/1Ond3YRMmTEClSpVEaZs3b0ZoaGixlKeOcuXKYfLkyaK0V69eYcmSJUaqUfEyVj+JKqZy3lzA1dUVM2fOVHv9Hj16wMbGRpLeqVMndOjQQe3t9O/fX5J27do1lXnWr18vmbP9448/1ujztLS0VHgtVtRc3WvWrBEt29ra4rvvvlO7XACYPXs2ypYtq1Ee4HWbLV/WBx98AD8/P423VbVqVQwbNkyUtn//fo23Y4rS09MRERGBffv2YciQIfD19cXTp09F61hbW2P79u1wdHQsljosWbJE0lZ8/fXXyMrKKpby1NG3b1/4+vqK0v79918cO3bMSDXSDNuo19hGmZZ27dpJfusnTpxQuv6///6L/Px82XLHjh3RqlUrODg4yNJOnz6t8rxCfvuK6lCSlMS+Em3Y2tpiwIABojRj9E1p4u7du5I6dunSBd26ddNoOz/88INe99GaNWtizJgxaq/fr18/henDhg1D3bp11d6O/PEgNzcXt2/fVpnHmPcHlXlbfnPyTKV/WBuRkZE4cOCAKM3T0xPz58/XaDtTp05FjRo1RGkXLlwo8rxGkbZt26JXr15qr+/u7o53331XlHbjxg3RMdGY3rpAkuPHj0vSxo8fr/F2qlWrhi5duojS4uPjle5U8h3n0dHRCAsL07jcoijqoDfVH/yePXtEy+3atUP16tW12laFChVQv359Udrp06eVrl+3bl20aNFClPb777+rfdG6d+9eyc2Rom7QKNr3Pv74Y5iba/YzdHV1xZAhQ0RpOTk5OHXqlEbbIfWY4vf20UcfaZxHfn9/8OCBxtvQxPHjx5GbmytKGzNmjEY3PoHXwSfNmjXTZ9WMavTo0cauAgDgnXfeURn4pkjLli0laffv31eZ599//xUtu7m5oU+fPhqVCwAjR47U+DdXUtSoUQMBAQFGr0NhFy9eNGj5Dg4OeP/99zXK4+bmJql3Ufujvg0aNAjOzs4a5ZFvi4Gi6y1/Q7du3bqSTm51GKP9kT8vvXTpkuTY8CaT30ejoqIkN2rkGbvdlD8/HzBggMb7eQE/Pz+4urqK0lSdn+vTlStXJEFa3bt3LzKYqXfv3pJOo8DAQOTl5em9joYUEREBMzMzhS8bGxuULVsWnTp1wooVK5CQkCDKa2FhgWXLlmHGjBnFVj8bGxssWLBAlJaXl1esZarjq6++gru7uyjtp59+QmRkpJFqVHyM1U+iiqmcNxcYNGiQ6EZWUezs7FCrVi1JuqbvS9GN1aLOHRR9n5988olG5QKvO0B9fHxEabdv38bLly8Vrv/06VPcuXNHlNarVy+UKVNGo3Lt7e0VBrEX5ebNm3j48KEoTZMbKfK6d+8uWr506ZJRAx00sXnzZqXtvoODAypXrox+/fphx44dkhuypUqVwh9//IG2bdsWW/3q1q0rCd6MiIjAL7/8UmxlqkPRwxZffvmlJMjXFLGNeo1tlGm1US4uLpI+PVWBJPJ/69ixI6ysrETtUWpqKi5duqQwf2xsrOTGdceOHTWttskoqX0l2jJ235Sm5K/dAe3OXytWrKjX/fTDDz/UqP+9atWqCgMnDHE8MOb9QUXett+cvJL2Gyzw77//Svpshg8frtF5EQBYWVkpvO+m6LylKPq4f5eammoyfQ9v5l0ZFc6dOydadnBwkERaqWvgwIFFbr+A/A04QRAwePBgPHv2TKuylWnevLmk0/ijjz4y6tNcigiCgLNnz4rStInALqxKlSqi5evXr6tcXz7w49WrV5IRFJSRH73E0dFREjUrT9G+oSzisyia7HukG1P73uzs7BTeiCyKfNBAXl6e6IljfVN0otG3b1+ttqVtPlNjZ2en1c3f4qBNp6Cnp6fkBCwpKUnp+pmZmbh586YorUOHDgqfeipKuXLlTOqJVH1q166d3rcZGRmJ3bt34+uvv8agQYPQtWtX+Pn5oXHjxmjUqJHkFRgYKMpf1I1uffP19YW1tbXG+eTbNVX7Y3HQ5nekKIBLVb0fPXqE2NhYUZp8hLq6fH19DT4iiPz575MnT/DRRx8hPT3doPXQl6ysLPzzzz9YsWIFRo0ahZ49e6Jt27Zo2rSpwt+WouOXqt+XsdvNhIQEScerLufn5ubmklEmijo/15cNGzZI0uRHiFFE0ZNwL168KDFPI+uTjY0NBg4ciFu3buGLL74o9vI++OADNGzYUJR2+PBhoz4Uoejp7oyMDI1HoyoJjNVPoowpnTcX0CbwV74NBIA2bdpotA13d3c4OTmJ0hITE5Wun5eXh8uXL4vSateujXr16mlUbgFF/Rznz59XuK6xrwHlbxZYWFhode1cQL6PKTMzE3fv3tV6e6audOnSmDZtGu7fv2+QG6/z5s2TjLi2aNEig5/TF9amTRv06NFDlHbz5k1s27bNSDVSH9uo/8c2yrTItydBQUGSEWkKKAokUbQNZTf1/vnnH0ngV0kOJCmpfSUF4uPjceDAASxYsADvv/8+unXrBn9/f6V9U4sWLRLl1/d9M32Tb1PMzMy0Pn+WP/boQpvjQcWKFUXL9vb2Go9GWblyZUmaquOBKdwflFfSf3PySlr/sLYUXWsqGjFNHfq672qIPmND0v/cKiYsLS1N8hR+48aNtR46qnnz5pI0ZU/aNGrUCI0bNxY1XsHBwahZsyYGDhyIgQMH4p133tFo2GpF3N3d0atXL9FQPk+ePEHDhg3x3nvvYciQIejcubPWTxbqy927d/Hq1StR2ubNm3HkyBGttynfsMXFxalcf8iQIZg6dSoyMjJkaYGBgUUGhEREREhGkRg4cGCREW7y+0b58uW1GgoRAJo2bQpzc3PR0EbaPOVFRTO1761SpUqwsrLSuGxFkcVJSUnFNlSt/BBu5cqV0/gpjwLFNYy6oTVo0MBkhtOUjzBWl4uLC9LS0mTLqk5m7t27J3m6TX6KJU00bty4RA4NWBRdPhN5e/bswerVq3H69Gmdhr5TdaFXHHTZHwsz9Mm1NvVWdP6lqt6KhgHV9Xek6gkwfRs8eDBmzpwpOdc6evQoRo4cib59+yoMgjY1YWFhWLJkCfbs2aPzfqbq92XsdvPChQuStmPx4sVFDhGuivwIjEWdn+tDRkYGduzYIUorVaqU5KlNZUaMGIH169eL0jZs2KB1EFeBkydPFkvwYHGpW7cuxo8fr9FwxrowNzfHkiVLJENBT58+XekNIUOYOHEifv75Z9GQy4GBgZg6darBPpviZsx+EmVM6by5gDZPR8rfXLWzs4O3t7dW2yk8IqqqY9H9+/clDwzoMsKjsu9T0U1URcccba/lGjVqBAsLC41GhJLv5DUzM9MpICk7O1uSZojjmLG0bdsWEydONFjgcfny5TF58mTRFHrx8fFYsmQJFi9ebJA6KLJkyRL8+eefon1v9uzZGDBggFbBvYbCNur/sY0yLR07dsTChQtly5mZmTh37pxkCqWnT5+KRmypV6+erP9VPhjkxIkTmDdvnqQs+Wvd8uXLKxx5p6QoqX0l//zzD3788UccO3ZMq+kNC+Tm5iI1NbXY+q51Jd9fU6NGDUm7qi59Pjynj+NBpUqVYGmp2a1jRe9d1b5nCvcH5ZXU35y8kto/rC35a01LS0vJQyrqqlSpEjw9PRETE6N0+0WxtbVF+fLlNS5b2f07U/BWBZLEx8dLolLr1Kmj9fZq164tuSmsqnFavXo12rVrJxpmLjMzE7/99ht+++03WFtbo3nz5vD19UXLli0REBCg1Y3X77//HmfOnBE1xHl5edi3bx/27dsHCwsLNG7cGK1atULz5s0REBCgMAq9OD1//lxhmqJ0bcXHx6v8u4uLC/r06YPt27fL0o4dO4aoqCh4eXkpzRcYGCjZj4qa1kYQBMmBUZd9z9HRERUqVBB1aJrixUJJZ4rfm/zw2upSFHyiy8l8UQofbAHodOFWu3ZtXatjEjw9PY1dBRl97Ueq9iFFbbCi6HR1yUeVvyn0sV+8ePECw4YNUzispjYMfZJqiP2xOGhTb0VPNrzJv6OyZcti0aJF+Oyzz0TpMTExWLZsGZYtWwZXV1f4+fmhZcuW8PX1hZ+fn0l1FM2fPx+LFi3S2zDRqn5fxv6+FZ2HP378WOvyFSnq/Fwfdu/eLfmchwwZonYgrr+/P6pVq4ZHjx7J0g4fPoyYmBiTOpZrwsrKSmHQQ25uLpKSkhAZGSm5vrl+/Trat2+PZcuW4fPPPzdIPbt27Yr27duLpvS6cOEC9u/fr9UUT/pgY2OD+fPni6ZgKJh25+DBg0apk74Zu59EEVP8rbm5uWmcR77d0WYbiraj6txB0Wety/epqO1Q9n3KXwNaWlpqPJ1mATs7O1SsWBFPnjxRO4/8cSw3N1cy0peuDHEc0wc3NzfJ08XA69HV4uPjJSPeAa+ncD5x4gT279+P9u3bG6Ka+Oqrr7Bu3TpRv8tPP/2EiRMnoly5cgapgzwfHx8MHz5cNBJxREQEVq9eLTmnNSVso1SXAbCNMhY/Pz/Y29uLRqU8ceKEJJBE2WgkAFC/fn2ULVtWNm3R5cuXkZycLHlQ459//lG6jZKopPWVJCcn46OPPsKuXbv0ts3ifAhSV/K/N1PpqzHW8UDTew+mcH9QXkn7zckr6f3D2pI/7lepUgW2trZab69OnTqic4Y39f6dJkz78T89k5/vGYBk3m5NmJubS05Y5G86F+br64sjR46gdOnSCv+enZ2Nc+fOYfny5Rg4cCDKli2LunXrYs6cORrNq1WtWjWcOHFC6QEoLy8PV69exc8//4zhw4ejcuXKqFKlCqZNm2awUS0McWJb+OlXZeQDQPLy8rBlyxal6wuCgN9++02UVqNGDfj7+6ssJzk5WRKlrsu+B0gP6Kr2PdKOKX5v2oxGYgzyEauKIirVpUteU2LskaAKM8R+pOiYy/1AStf9IjIyEu3atdPbRQLwukPLkEpKuyaPvyP1fPrpp/juu++UPs2SmJiIo0ePYu7cuejSpQvc3NzQpk0brFq1yuhBshMmTMDcuXP1Ote4qotAY3/fpnJ+rittp7VRtX5OTo7KawRT5+3tjRs3bkheISEhePbsGZKTk7Fnzx6F07F+8cUXWL16tcHqumzZMsmc3jNmzNDoiV99Gzp0KBo0aCBKO3TokFGn3dEnY/eTKGJK580F9HHcN9a5gy7fp6IbCcq+T/lrQCcnJ8nvWRNv63FMH3r16qWw3b979y5iYmLw4sULrFq1SvK0ZFJSEnr06IErV64YpJ6urq6YMWOGKM0UphCbP3++5AbEwoULTfqGCtuo/8c2yrRYW1tLpkxSNDWNfCBJp06dRMvvvPOO7P+5ubmS0cLDwsIQHh4uSivpgSQlqa8kOTkZXbp00WsQCWA6N1EVkW/PTKWvpqQcD0yxTSxJvzl5b0L/sLbkf4v6vn+XlZWl0RTdJXk/UuatCiQpPMxegaKmIymKfH5FZRTWsWNH3L9/HzNmzFAaUFLY3bt3sWDBAtSpUwf9+/dXO9K5cePGCAkJwdKlS1GhQoUi1w8PD8eKFSvQtGlTdOzYETdu3FCrHG0punAwhg4dOkhGYyn81IG806dPS57MLGo0EsA09j3SHL837cm/L10+N10/c1Oh6ZCAJZ2iG6/azDNZwJSHEdaFrvvFyJEjRcO/FmjUqBFmzJiB/fv349q1a3j58iWSk5ORnZ0NQRBEL2N31JJyb8rv6PPPP0dISAg++OCDIp8KyM3NRVBQECZNmoRKlSrhiy++EE2pZShbt25VePPc3d0do0ePxsaNG3H27FmEh4cjISEBGRkZkt+WJk8oAsb/vk3l/FwXYWFhOHPmjCitdu3aCoc9V2X48OGSGwuKAlTeFI6OjujXrx8uXLiAadOmSf7+6aef4vLlywapS7NmzSRTjd6/f9+on3/BtDvyvvzySyPURv9M8ZrnbTtv1id9f5+K8ir7PvV5DahN/jfhOGYoXl5emDBhAkJCQiTTrqWnp6Nfv34GC5qYNGmSZPSUwMBA3L171yDlK1Iw7U5h8fHxoml4SDtso95O8gEd169fFwX8CIIguvlpZWWFtm3bqtyGfOCJoilc5Uc9oeIzdepUXLx4UZJeo0YNfPbZZ9i1axcuX76MFy9eICkpCVlZWZLrZ1X3ZEyR/PV7SeyrMaa3uU0sDm9z/7Ahju9v6j08db1VgSSK5unStWNaPr8686C5u7tj0aJFiIqKwl9//YXPP/8cLVq0UHmwEQQBe/fuRaNGjXDs2DG16mZvb4/p06cjIiICp0+fxuzZsxEQEAA7OzuV+f755x+0bNkSmzdvVqscbSiqw4EDBySNly4v+ShkRczMzETDFAOv56dXdOIDSINMLCws1HrC0VT2PdIMvzftyb8vXT43Y9xAJN0piqbX5aQrOTlZl+q8kf744w9JZ4mnpyf++usvXL9+HYsWLULv3r3RuHFjlClTBk5OTgqjok3xqSV67U36HdWqVQtbt27Fy5cvsW3bNowZMwa1a9dW+RRgeno6vv/+ezRp0kQ0LVxxy8nJwfTp0yXpX331FZ4/f47169fjww8/hL+/PypVqgRXV1eFATKa/raM/X0rOj+/ceOGXs/P5Z8c1DdFwQb37t2DmZmZRq8qVapIpvq4e/eu0muEN4WZmRm+//57DBs2TJSek5ODESNGIDs72yD1WLhwoeR49c0332j0FJC+devWTTLVw/nz53HgwAHjVEiPeM3zZtH396kor7LvU5/XgNrklz+ONWrUSK/HMEEQMHLkSJ3ek6lxcXHBoUOH4OPjI0p/9uyZwaZxKZhCrLCCKcSMacaMGZInUn/66Se8ePHCSDV6M7CN+n9vUxslH9CRn58vChy5ffs2oqOjZcu+vr6SG3nyI5TIj2oiP62Nj48PypYtq1O9ST23b9+W3DNxdHTE1q1bcf/+faxYsQIDBgxA8+bN4eXlBWdnZ4X3wUpa35T89XtJ7asxFlO5P/gmeNv7hw1xfH/br2ffqkASRcPdyQ9rp4n8/HxJI6/J/EeWlpbo0qULvvvuO1y6dAnJyck4e/YsFi9ejHbt2il8Cic5ORn9+vXDgwcP1C7HzMwMAQEBWLBgAU6fPo3k5GRcuXIFK1aswLvvvquw0c7Ozsbo0aMlT/Tpi6LRWDR9alNfRo4cKbmJERgYKFkvNTUVe/fuFaV17txZrXlbnZ2dYWFhIUrTZd9TlF/bubcMxZSHolOG35v25IcQ0+VpJmMPH1sS911ToOiYq8uwhaY416+x/f7776JlCwsLHD58GF26dNFoO5wazXS9ib8jFxcXvP/++1i3bh3u3r2L+Ph4HDlyBF9++SXq16+vMM+DBw/QvXt3g93EPn36NKKiokRpkyZNwuLFi4sMyC5M09+Wsb9vUzo/10ZeXl6xBsIDb/aoJIWtXr1aMmrjvXv38MMPPxik/OrVq2Ps2LGitKioKIOVr8zSpUtNbtodfTC1fhLSjb6/T0V5lX2f8teAKSkpkqA8TWh6HSh/HCtJxzBjcnJywpYtWyR9kIGBgQYLoBw2bJhkCrGDBw/i3LlzBilfEVdXV8ycOVOUlp6eXmKf1jUVbKP+39vURjVq1Ejy/gvf9CxqWhvg9UhBtWrVki3fu3cPkZGRAF6fe5w8eVK0fkmf1qYk2blzJ/Lz80VpmzdvxgcffKDR9FHG7pvStP9Xvj0r6X01hlbS+x9MydvePyz/W9T3/TsbGxvY29vrtM2S7q0KJCldurTk4KXLUIn379+XHCTVma5GGRsbG/j7++Orr77CyZMn8fLlSyxdulRyopuWloavv/5a63IsLS3RrFkzfPbZZ/jjjz8QExODNWvWwNvbW7ReXl4evvjiC63LUaVMmTKStFu3bhVLWUWpUqWKZCjPHTt2IDMzU5S2a9cuSTSaOtPaAK+DeUqVKiVK02XfS0tLw9OnT0Vpuux7qsh3Jmh7U70knhCV5O/N2Dw9PUXL9+/f13pb9+7d0yqffGTt27TvmgJF06rdvn1b6+0Z6xhhyuSfwOnatStatGih8Xbkp2wj0/E2/I7c3NzQvXt3LFmyBLdu3cL9+/fxySefSAI5Q0NDDXYTX/63ZW5ujlmzZmm8HU1/W8b+vk3p/FwbR48elQQA6dvOnTvfipHSHB0d8dNPP0nSFy9ebLDOpTlz5sDR0VGUtmzZMsTFxRmkfEWaN2+O/v37i9Lu3buHjRs3GqlG+mHq/SSkGQ8PD0maLt/nnTt3JGnKvk/5a8Dc3Fw8evRIq3IzMjIk185FkT+OJSUlabyNt1Xjxo0xceJEUZogCApHaCsO5ubmWLx4sSTdUOUro2janU2bNhl12p2Sjm3U/3ub2igzMzO88847ojRVgSTKgkCUTW9z/fp1Sd8dA0kMR/762cfHB3379tV4O9r0TSkaVcFQ/b/y1+8hISFaB6eVpOtufSnp/Q+m5G3vH5Y/t3jy5InCqaPVJX9uwWvZtyyQxN7eXhS5CrwerlnbJ4iuXLkiSWvatKlW21KkVKlSmD59Oi5evCgZOufIkSM6/RgKc3R0xMcff4xr165JDoCXL1/Gs2fP9FJOYQ0aNJAMAf7XX3/pvRx1yQeEJCUlYf/+/aI0+VFK3N3d0atXL7XLaNKkiWj5+fPnomH7NBEcHCzpnNPnvleYs7OzaFnbodbCwsL0UR2DK6nfm7HJv6/IyEidPjdtvO37rrFVrFhRMoyotk+05efnKzzmvs2ys7MRExMjSmvTpo3G28nLy8Ply5f1VS3Ss6ZNm0oCKrT9HSUlJWkdmGdINWvWxOrVq/Hbb79J/iY/MlxxkT/3rVmzpsJOjqJcuHBBo/WN3W62bNlSkvbnn39qVb4xKAo0qlu3Lho2bKj1S/7mUUpKCnbt2mWot2RU7733Hvz9/UVpSUlJWLZsmUHK9/T0xOeffy5KS05OxrfffmuQ8pVZtGiRyU27o6uS1k9CqtWsWVMShHX16lWtt6fJ96koXdtrOW32QUUd5iXpOGZss2fPlgzTf/bsWYP11b377ruSh7yMPYWYqU67U5KxjRJ7m9oo+cCOR48eITw8HDk5OaIR0Z2dnZXeAJXfRsHNU/lAFEtLS7Rt21Yf1SY1yF8/a9M3BWh+/QxI+34B7fp/c3JyNA7skt9PdelzedOnUFXE1O4PllTsH5bev8vNzcWNGze02tbTp08lnyevZd+yQBIA8PPzEy2npqZq3UDt3r27yO3rQ61atTB69GhRWnp6utZR08qUKVMG06ZNk6Tr8iSkMra2tpKOyaioKMl8hobSr18/yYlH4bn9Hj16hLNnz4r+/sEHH8DGxkbtMhTtG3v27NGwpq8Zat8DpEM/ahOZGBkZqfX+Kn8DzdBDR5fU783YfH19JWnywVnq2rdvn1b55Pfd8PBwrSLDT58+rVX5JN0PLl++rFVbcOLECa0DkXQlPyqTqQxfr+iJbG2GjT969ChSU1P1USUqBg4ODpLpXo4cOaLV3Ls7d+5Ebm6uvqpW7N5//300atRIlFbU0yn6+r3K/760+W3l5ORodePDmO1mpUqVUL16dUn5mkypaSzR0dH4448/RGne3t64ffs2bty4ofXr2LFjkrJK+ugTmlAUtLFq1SrExsYapPxp06ZJgrh+/fVXow55XL16dXz00UeitBcvXuDHH380ToX0pCT2k5BiFhYWkhsb9+7dQ2hoqFbb0+T7NPY1oKKpELZu3apV+W+jUqVK4bPPPpOkG3IqF0XBijNnzjTqNdiwYcMk5+PGnnanJGMbJfY2tVGK3v/x48dx4cIF0Yh/7du3l/QFK/tbwX0E+UCSli1bSh7KLQ6m2l9kaPq4fr59+7ZWoxM5OjpK9hdt7l1cvnwZGRkZGuVp1aqVJG379u0al52fn48dO3ZonK+kM7X7g4Ym334A2rUhptA/bOy2kPfvit9bF0iiaF6oNWvWaLydJ0+eSDpWSpcuLYl+0pfatWtL0jSdi9GUygFeP+Um75tvvimWsopib2+PQYMGidL++ecfWUSt/GgkgPrT2hRQtO+tXbtWMkJFUZKSkiQnJVZWVmjfvr1G21GX/NNpV69e1bjOa9eu1bp8+RN/Q9/wLKnfm7F16tRJchKxfv16jQM5bty4ofXTKfL7bmpqqsJhT1U5efIkRyTRgXy7CgDfffedxtvRJo++GLsNUsbBwUGSps1w/ytWrNBHdagYyf+O0tPT8csvv2i0jZycHIXTVJg6+fPSos5J9fV7lf99afPb2r59u1bTrBi73ZQ/P8/Pz5c8iWuKNm/eLAmUGjJkCMzNdbvcrV27tuT6LigoSKcp+0qStm3bSoYhT0tLw9KlSw1SvqOjo2RK1+zsbMyePdsg5Sszd+5cydPUS5cuLdHTIZbUfhJSTF/f59mzZxESEiJKa9CggdJRuipWrIi6deuK0g4ePCh5qq8oGRkZWt1cbdmypaRuQUFBkhuMpNxnn30mmWf+8uXLOHLkiEHKVzSF2N27d40axKls2p0vv/zSCLV5M7CN+n9vUxtVuXJlVK1aVZR24sQJtae1AQAXFxc0a9ZMtvzy5UtcvXpVEthlqGltTLW/yND0cf2sbd+UmZkZatasKUrTZlQFbe5ddO7cWXLMXLduncb30n7//Xc8f/5c4/LfBKZ0f9DQFAW7adOGmEL/sLHbwg4dOkgCyrZs2aLxtMS5ublYt26dJL1z58461e9N8NYFkvTp0wfe3t6itCNHjuDo0aMabWfSpEmSzsqPP/5Y4bxs+qCoI1rRvJIlpRwAGD16tGT47qCgIIN1TMqTDwzJz8/Hb7/9Jvu3sIYNG6Jx48Yabb9Fixaik13g9ZO1ml4szZo1SzI3+cCBAyVzfeqL/NBNMTExGkWGPn36FCtXrtS6fPkTMkPP1VZSvzdj8/Lykkz9FBwcjM2bN6u9DUEQMGnSJK3roGjYMU0iw3NycvDVV19pXT4Bffv2lXSUrFu3TjRsaVECAwON2rFi7DZIGRcXF9jb24vS/v77b422sX79epw6dUqPtaLiMHr0aFhbW4vSFixYoNGN7IULF2ocSGcK5M9Lizon1dfv1cvLS7T84MEDhIeHq50/OjpaMiWHuozdbk6bNk0yvOy2bduwc+dOrbZnKIpuMA0bNkwv21a0nbdpVBJFgUSrV6/Gy5cvDVL+2LFjJSPl/P7771oPVasPnp6ekpE8TWHaHV2U1H4SUmzUqFGStnzNmjUazTufm5ur8FqsqOuzcePGiZYzMzMxffp0tcsFXp+3aNPGWFpaKry5P3r0aIONpFTSOTs7Y+rUqZJ0Q45KsmjRIslDKd98843GT4rrU/fu3SXTZJw7dw4HDx40Uo1KNrZRYm9TGyUf4PHvv//KpqdRto48+ZFN5s2bJ2kfDBVIIn/9mZiYiISEBIOUbUrkr59PnDih0QOYJ06c0Ki/WJ58/+/Vq1c1GlUzODgYv//+u8bl2tnZYcSIEaK06OhohSP+K/Py5UuN1n/TmNr9QUOSbz8A7fqwTKF/2Nh9597e3ujTp48oLTo6GvPmzdNoOz/++KOkr7V169Ya3wd+E711gSRWVlaYOHGiJH3EiBFqd7DPmTNHMnSyra0txo8frzTPDz/8IDkxUldycrJkRAwXFxfJnN3A647jvXv3ajV8UE5ODlavXi1KMzc3R7169TTeljrs7Owwa9YsSfrMmTOxatUqrbf7119/qfwulGnVqpXkydfAwED8888/kjnyRo0apVXdFF2Qf/7552pPnbFx40bJd2RmZqZw+FF96datmyTtq6++Qk5OTpF5ExIS0L9/fyQmJmpdvvwQoqdPn9Y4mlBXJfF7MwVTpkyRpE2ePFnhHLaKfP755wgKCtK6fD8/P8kczytXrkRERESRefPz8zFx4sQSOzegqbC2tpZ0lOTn56Nnz55qDQW8a9cujB07triqpxb5NigkJEQy/6uxyA8BeerUKbVv+Pz111+YPHlycVSL9MzDw0PS4Zmeno6OHTuqNf/uDz/8oPHFk75MmzZN6wCWa9euSaYVbNiwoco8+vq9KppPVt2nTl+9eoUePXpo9QQIYPx208vLCxMmTJCkjxo1Cnv37tVqm3l5edi5c6fC8359UDRCSL169YrcX9Q1ZMgQydMtikZAeVO1bt1a0mGfkZGBJUuWGKR8KysrLFy4UJQmCILW+6O+fP7555KAcG2HzzUFxuonoeJRunRpDB8+XJSWm5uLfv36qTValiAIGDNmDG7evClK9/T0xNChQ1XmHT58uGSK0c2bN6v9IMb+/ft1unnwySefoHz58qK0p0+folu3blo/7RsbG4vZs2dr3adX0kyZMgWlSpUSpV27dk2rKfu0UaNGDYVTiJ0/f94g5SujaNqdktzuGxPbqLe3jerQoYNoOS4uDhcvXpQtly9fXuFo6YXJB4nIj5jk6OiIli1b6lhT9chffwLQOAj3TSB//fz48WO1f1PXrl3DkCFDtJqKvICiexfqPtjx9OlTDBw4UK37HIpMnTpVMlLhhg0b8OWXXxYZTBMZGYkOHToYbSpvU2Bq9wcNqUKFCpL7Ftq2H8buHzaFvnNF99mWL1+u9rRRx44dU7gvvs2BXoW9dYEkwOsvX37O9bi4OLRr107ljpWYmIgxY8ZgwYIFkr8tXbpU8gRPYadPn0bnzp1Rr149LFq0SK2OfwAIDQ1Fx44dJTc+Bw0aJHk6FXg9DUT//v1RvXp1zJ49G9euXVOrnIiICPTq1Utyg7djx46SqEB9mjBhgsIhtCdNmoQ+ffpILgqUefLkCZYuXYoGDRqgW7duGj21WZj8qCRhYWGSm+HW1tb44IMPtNr+kCFD8O6774rSMjIy0L17d6xatUrpCUZmZiZmzJiBjz76SHJi9emnnyoceUFf/Pz8JEM/Xrt2DX379pWMsFHYyZMn0apVK9k+Jf+0gSblF5aUlIRBgwZpNW+itkri92YKAgICJJHZKSkp6Ny5s8pI8/j4eAwbNkw2pJqdnZ1W5dvZ2eH9998XpaWmpqJjx44qO8TDwsLQs2dP2bCG2u679NqUKVMk84YmJycjICAAn3zyCYKDg0V/y8vLw+nTpzFgwAAMGjRIdjGnaO5RQ5Bvg/Lz8zFgwACtp1zSp4EDB0rSBg0apLJDMzMzE/Pnz8d7770ne2rH2dm52OpI+rFo0SLJMMDPnz9Ho0aNMGPGDMl5ZXZ2Nv7880906NBBFAypaF7w4rRhwwb4+PigU6dOWLdundrDRh85cgTdunWTHF+L6hjW1++1a9eukqE5d+3ahTFjxqgMZv3777/h6+srK0/b35ax281vv/1WMnd9eno6+vfvjzFjxuDRo0dqbSckJARz5sxBzZo1MXjwYLXP6zW1YcMGSVpR+4omypQpI+mwjo6Oltwwf5MpGpXkf//7H168eGGQ8gcOHIjmzZsbpCx1OTo6Ys6cOcauhl4Zo5+Eis/ixYslNyvDwsLQunVrlaNWRUZG4r333lN4vbZ27doir41cXFzwww8/SNLHjx+Pr776CpmZmQrz5ebmYsmSJRg8eLAsUE+b60BbW1vs2rULNjY2ovTg4GA0adIEq1atUmtki+zsbBw5cgTDhw9HpUqVsHDhwmKb9tnUODk5KbwB98033+h0o08TiqYQM7YWLVpIpt0h7bGNejvbqA4dOsDMzEzp39UZSaRVq1YKp3IoEBAQYLCR0Hx9fSVTaU6bNg0HDx7UOjChJFLUNzVlyhSsXr1a6XEjLy8Pv/zyC9q3by97CEPb6+e+ffvC3d1dlHb48GF89NFHSE9PV5pv37598PX1lY2eoE3/b4UKFRQGGi5btgwtWrTAvn37JHUIDw/HokWLUKdOHVn/tLH6PE2Bqd0fNBQzMzPJ937ixAnMmDFD4ynXjN0/bAp9535+fvjkk08k9Rg2bBi++eYbZGdnK8yXl5eHFStWoHfv3pJ1+vTpIxnp5G1lWfQqxnP16lVJR4Y2PvjgA3zxxReyZWtra2zfvh0tWrQQzdcUGxuLIUOGYN68eejduzeqV68OJycnREdH49KlSzh06BBSUlIk2+/evbva0y+EhoZi1qxZmDVrFipXrozGjRujYcOGKFOmDFxdXWFpaYnk5GSEhYXh7NmzOHfunOSAW6pUqSKfLA0PD8fChQuxcOFCeHl5oUmTJmjUqBG8vb3h6uoKa2trpKamIjw8HOfPn8epU6ckJzg2NjZYvny5Wu9LW2ZmZti6dSv8/f0lB4UDBw7gwIEDaNiwIdq1a4caNWrInopITExEXFwcbt26heDgYL0NlzRs2DDMnDlTNKKLfMBCz549JU9naGLTpk1o1KiRKMo+LS0NkyZNwnfffYe+ffuiTp06cHV1RVxcHG7cuIH9+/crfLK1SZMmCudq1bdFixahd+/eorQjR46gWrVq6N+/P5o3bw43NzfZvnvs2DFcv35dtq6/vz8qVaqEbdu2aVz28OHDMXv2bNFTn3/88Qf++OMPuLm5oUyZMpILMG9vb71HgJfE780U/PDDDzh58qRoVJ/ExESMHDkSCxcuRN++fVGjRg04OjoiOjoaFy5cwJEjR0Rt84IFC7SeImDGjBnYtm0bkpOTZWlhYWFo2LAhevbsiXbt2qFs2bLIzMxEZGQkTp48iZMnT8r2t9KlS2PKlCn4+uuvtfwEyNzcHNu2bUNAQIDoCZv8/HysWbMGa9asgZOTE8qWLYu8vDxERUVJOk66d++Ofv364cKFC6J0+SfEi8N7770Hd3d3UeDcpUuX0Lx5czg5OcHb21vhxaYhhtwfPnw4Fi9eLLqpm5qaigEDBqBJkybo2bMnqlevDisrK8TExCA4OBhHjhxBfHy8bP26deuiZ8+eb8WwkSWZg4MDdu7ciY4dO4o6CLOysrBkyRIsWbIErq6uKFu2LDIyMvDy5UtkZWWJtjF+/Hh4eHiInviS7/QqLgVzX48bNw4+Pj5o3Lgx6tati1KlSsHV1RV5eXl49eoV7t69i+PHjysMuG7Tpg0GDRqkshx9/V7d3Nzw2WefSW6eb9iwAQcOHJD9xtzc3JCYmIjHjx/jyJEjuH37tmxdCwsL/PTTT5IgZXUYu920tbXF/v374evrK3mKZMOGDQgMDESzZs3Qtm1bVK5cGe7u7sjLy0NiYiJiYmJw48YNXL16FZGRkRq/d02lpKRg9+7dojRzc3NJIKmuhg0bhmPHjonSNm7cqHBOZ2XGjBmjl5tixXGeWxRfX19069YNf/75pywtMzMTixYt0ulJMU0sW7YM7du3N0hZ6ho7dix+/PFHhIWFGbsqemHMfhLSP3d3d/z222/o3Lmz6Fr6yZMn6NSpk+xcsXLlyrC1tUVUVBTOnDmDv/76S+GNVEWd/MqMHDkS+/fvx6FDh2RpgiBg6dKl2LBhA/r27YuGDRuidOnSSEhIQGhoKPbu3SsKTuvevTtSU1PVHgW0sFatWmHt2rUYOXKkqD8tNjYWkyZNwtdff422bdvC19cXnp6ecHZ2RmpqKhITExEeHo7g4GBcv37d4COhmpJJkyZhxYoVouk2bt68ib179xokmKJMmTKYOnWqwkBGY1q0aBEOHDjw1oxKVpzYRr2dbVSpUqXQqFEjUZ9xYeoEklhbW6NNmzb466+/tN6Gvnh5eaFr166ic/Po6Gj07t0b1tbWqFChAhwcHCTBM+vXr5dMo16SdejQAQEBAaIb97m5uZgwYQJ++ukn9OnTB3Xr1oWdnR1iY2MREhKCgwcPin5TZcqUwbRp0zSeagp4ff06Z84cfPrpp6L09evX448//kD//v3RqFEjODk5ISEhAffu3cPRo0dFo1oOHjwYUVFRWv2mx40bh/Pnz2Pr1q2i9ODgYPTr1w9WVlYoU6YMnJycEBMTI+qTAwBXV1cEBgaiVq1aonRD9HmaAlO7P2hIo0aNkrRlBX18Xl5ecHd3l0z316tXL8n5kbH7h02l73z58uU4e/YsQkJCZGm5ubmYN28efv31V/Tp0wcNGjSQHd/v3LmDffv2KRwRrEKFCli/fr1e61eiCSbiyZMnAoBieU2ZMkVhmVeuXBE8PT112nbfvn2FjIyMIt/fe++9p5f34uLiIpw+fVppOVOmTNFLOTY2NsKuXbu0/To1lpycLPTr109v37mPj4/WdenevbvKbf/xxx86v99Hjx4JNWrU0Ok9+vv7C69evVK7zLlz50q2oYkPP/xQq3rWqVNHiI2NFUaMGCFKr1Spktplf/PNNxqVqWzbitqZTZs2qV0PY3xvbdu2FeVv27at2nkL27Rpk6QuT5480WpbmgoLCxPKly+v1ec1aNAgIT8/X5I+d+5ctcsPDAzUqmxHR0fh4sWLWn92lSpVEuUZMWKE1p9hAW0+h5MnT0rynTx5UqvydXlPjx49kuRX59WuXTshJSVFWLduneRviYmJWr0PTW3evFnjeiuiz++iwLVr1wR7e3ut9vFy5coJT5480fr4oG37pMvvuTBtjivaHgf02Ybq8v4vX74suLm5afxdDx48WMjJyRFmzZolSnd1ddXqPajLxcVFq31T/lWvXj0hMjJSrTL19XvNzs6W7OPqvszMzIT//e9/ejnvMGa7GRUVJQQEBOjlOwQgdO/eXe2y1bV27VpJOe3bt9d7OWlpaYKjo6OoHEtLSyEqKkrh+vLtkz5f6rR12uQpyuXLlyXbtba2Fp4+fao0j6K2c8uWLVrXoVu3bio/G3WPqfL5OnTooHWddu7cqbJO+jj/U4c+zzEM2U9SoDjOm3Whr+O+LtfAheny+Rw9elRwcHDQ6fucNGmSkJeXp1Gd09PThXfeeUer8mrXri28evVK52vhI0eOaHXepOy1e/duleUVx7m+OhSVq4/f0LJlyyTb9fHxUbkvKDr+PXv2TKvyU1JSimyL1KHo81mwYIFWdRIEQfjkk09U1kmTcz1tsY1iG1WS2ih5X3zxhdL38PLlS7W28f333yvdxq1btzSuky595qGhoRrvw8o+d/n1DNlXoquIiAihTJkyWu27zs7OwtWrV3Vq2/Ly8oSOHTtqVX5AQICQnp6u0286Ly9Pq2tAV1dX4ezZs0JOTo7kbz/++GOR5ep6v6eAvu4/6LIPG+P+oLF/c3l5eUKHDh00el/KjrHG7B8WBP31xQmCbt9LTEyM0KJFC532nTp16ggRERFql6mvNtdUjtOKvJVT2xRo1qwZLly4oFWkqoODA+bNm4fdu3erNeyVPqaH8ff3x/nz5xEQEKB0HU9PT5VDxKmjXr16+PfffzFgwACdtqMJJycn7NmzB7/++ivKlSun07YqVqyo1dOfBUaNGqX0b15eXujSpYvW2y5QtWpVnDt3DgMHDtT4+7KyssKkSZNw/PhxuLm56VwXda1bt04yT21ROnXqhKCgIJQuXVqnsr/++mssXLhQ4XROhlQSvzdTUK1aNZw+fRqtW7fWKN/48eOxbds2ndu0ESNGYOPGjZIIXlWqVq2KoKAgg82r+jaoWrUqbt68iQkTJqg1CoKNjQ1mzZqFY8eOwdHREQkJCaK/m5mZSaaeKC7Dhw/H+vXrDVaeJho3boxjx47By8tLo3y+vr64ePEiKleuXDwVo2LRvHlzhIaGYvDgwWqt7+zsjB9++AHbt2+HpaWl5HckPx+rvul6/mtmZoYRI0YgKChI7akR9PV7tbKywsGDB9GjRw+N8rm6umLXrl0YO3asTuUDxm83y5Yti3/++QfffvutZKhgTdWpU0fhcKu6Ku5pbQrY29tLRufLzc1VOVXfm6Z58+aS30N2djYWLlxosDosWbLEYCMpqWvAgAFv1BOlgGH7Saj4FQztrc1+WqpUKfz6669YuXKlxr89Ozs7HD58WGX/iiL+/v44c+aMXq6Zu3fvjuDgYPTp00ena0pLS0v06NEDDRo00LlOJcmECRNQpkwZUVpoaCh27dplkPIdHR1NcmTQuXPnqpxWgzTDNurta6OUnV/Ur19f0uZouo0yZcqgfv36WtdNG3Xr1sXx48dRvXp1g5ZraipWrIh///1XMqpGUWrVqoXz58/rPP27ubk5Dh06pPH1+wcffIBjx45pPa164fIDAwOxfft2tftB2rdvj8uXL8Pf319y7Q4Uf3+NqTGl+4OGYm5ujj179uhlRFVj9w+bSt+5h4cHTp48iXHjxml0Lwh43Wf2/vvv49y5c6hYsWIx1bBkMq1eGCOoWrUqjh8/jr///hs9e/Ys8mKgatWq+OyzzxAWFoY5c+aofaK6Zs0ahIeH45dffkH//v3Vbgzt7OzQv39/HDp0CGfPnkXdunVVrj9z5ky8fPkSGzduxAcffIAqVaqoVY6VlRW6deuGbdu24caNG5J5rQxl3LhxePz4MdatW4eOHTvC3t6+yDzm5uZo3LgxvvjiC5w8eRLh4eGYNm2a1nXo2bOn0sCH4cOH621YMQ8PD+zcuROXL1/GoEGDirwI8fLywkcffYS7d+9i5cqVBu+Ys7CwwNq1a3H8+HH4+fmpvMhp2LAhtm7dir///lvnGw/A6+945syZiIyMxKpVqzBo0CDUq1cPpUuXNvjnUNK+N1NRtWpVnD17FuvWrUPjxo2Vrmdubo4OHTrg33//xS+//KK339uHH36I0NBQDBgwQDIVUmHe3t5YsGABbt++jYYNG+qlbPp/Li4uWLVqFcLCwrBs2TK0a9cOVapUgZ2dHWxtbVG+fHl06dIF3333HSIiIvDtt9/KAshevnwp2parq6tBbyaNHj0akZGR2LRpE4YNG4bGjRvD09NT54tNfSgY/nH69OlwdXVVuW6zZs2wefNmnDt3TjInNZUMXl5e+P333xESEoK5c+eiVatWqFChAqytrWFnZ4fKlSujZ8+eWL16NZ4+fYpPP/1UdsyW/x3p4xityr179xAcHIwFCxagU6dOas+36unpifHjx+PatWsIDAzUuANFX79XFxcXHDp0CNu2bSuyY9bT0xNffPEF7t+/r9ch343dblpaWmLWrFmIiIjA8uXL4e/vr1Zgr6WlJfz8/DBnzhxcvnwZd+7cwfDhwzUquyh37tzBpUuXRGm2trbFNuT+sGHDJGkbN24slrJMlaLpVTdu3Ijw8HCDlN+gQYNiCRTShZmZmcJ50Us6Q/WTkGE0adIEly9flk2Tp+p6CHh9U2zOnDl49OgRxo0bp3W59vb22LBhA06dOoUuXbqo7MytW7cu1q9fj9OnT8PDw0PrMuVVqVIF+/btQ2hoKMaNG4dq1aqplc/d3R39+vXDmjVr8Pz5cxw+fBg1a9bUW71KAnt7e3z55ZeS9Hnz5ommgy5OH3/8sdrfmaEUTL9A+sM26u1qo9q0aaPwO9YkgLVBgwbw9PSUpHfo0EGnummrVatWsulSxo8fD39/f3h7e8PR0fGtOieqW7curl69ioULFxYZTFGnTh2sXLkSt27dgo+Pj17KLwgQ27lzp8rrdzMzM/j7++Po0aPYunWrXvvqhwwZgkePHmHnzp0YNGgQ6tatCzc3N1haWsLd3R3NmjXDlClTcO7cOfz777+oUaMGAOm1O1D8/TWmyhTuDxqSq6srtm3bhnv37uGbb75Bjx49UK1aNbi5ucHKykqjbRm7f9hU+s7t7e3x66+/IiQkBKNGjSqyPXJ3d8eQIUMQHByMbdu2vXUPgavDTBAKTcZHyM7OxqVLl/Ds2TPExsYiLS0NpUqVgoeHB+rVq6fXk7KoqCiEhYUhPDwcr169QlpaGszNzeHk5ITSpUvDx8cHtWvX1jhySl58fDwePnyIx48fIy4uTjbfsZOTE9zd3VGnTh34+PgUeaJuDNnZ2QgODsbz588RFxeHhIQEWFpayj6jmjVrombNmiZxI09XeXl5uHr1KsLDwxEbG4vk5GS4urrC09MTNWvWNLnI8tjYWAQFBSEqKgoJCQmwsbFBhQoV0KJFC7UDmN4EJe17MxVhYWG4ceMGXrx4gZSUFDg5OaFq1arw9fXVeQSboqSnp+PcuXOIiIhAXFwczMzMUKZMGTRs2BCNGjXSeQQUKh6tW7fG+fPnZctt2rQRzb9KrxW0SaGhoYiLi0Nubi6cnJxQpUoVNGvWTC8jpFHJVb58eURGRsqWhw0bht9++81g5efn5yM8PByPHj1CREQEkpOTkZ6eDhsbGzg7O8PLywsNGjQw2ZFynj59igsXLiA6OhrJycmwtbWFt7c3fHx80KBBA5M7fhRXu5meno6rV6/ixYsXiI+PR2JiImxsbODk5ARPT0/UqlUL1atXN/pIckSkP4bsJ6Hil5aWhosXLyIqKgoxMTHIzs5G6dKl4enpicaNG6NChQrFUm58fDzOnz+PFy9eIC4uDra2tqhQoQKaNm1q0GCB58+f4+bNm4iLi0N8fDwyMjLg6OgIZ2dnVKxYEbVr19b5SVgi0h7bKLZRVPIJgoDbt2/j+vXriIuLQ0ZGBpycnFCpUiU0atTIINf8z549w4ULFxATE4PExETY29ujSpUq8PX11XjUhuK2bt06yaimjx49QtWqVY1UI9PxNt0f1Cf2D/8/QRBw69YtPHz4UNYeODs7w8PDQ/Z5vE1Bf9pgIAkRERGZtOjoaFSsWBHZ2dmytGnTpuH77783Yq2ISpYbN25IRoT6+eefMXHiRCPViIoT200iIiIiIiIi09e7d28cPHhQtly6dGnExsYasUZERP+PYTZERERk0n766SfRzVAACAgIMFJtiEomRVMv8Hf05mK7SURERERERGTa7t27h8OHD4vS2rRpY6TaEBFJcUQSIiIiMlkXLlxA27ZtkZOTI0srV64cwsPDdZ76jehtsXv3bgwcOFCU1rJlS1y8eNFINaLixHaTiIiIiIiIyLRlZWXBz88P165dE6X/+eef6Nq1q5FqRUQkxhFJiIiIqFglJCTg22+/RVxcnEb5Dh06hG7duoluhgLAJ598wpuh9Na5f/8+Vq5ciZSUFI3yrVu3DsOGDZOkT5o0SV9Vo2LAdpOIiIiIiIjI9K1atQqXL1/WKM+LFy/QoUMHSRBJjRo10KVLF31Wj4hIJxyRhIiIiIrVy5cv4eXlBVtbW3Tp0gW9evVCq1atULNmTVhYWEjWPXPmDNasWYOTJ09KtlWvXj1cuXIFtra2hqo+kUm4ePEiWrVqBScnJ/To0QM9e/ZEixYtULVqVZiZmYnWjYiIwMmTJ7Fq1SoEBwdLttWpUyccO3ZMko9MB9tNIiIiIiIiItPXtWtXHDt2DI0aNULv3r3RqVMn1K9fH05OTqL1MjMzceXKFezevRsbN25EWlqa6O/m5uY4efIkp6UlIpPCQBIiIiIqVgU3ROXZ2NjA09MTzs7OyMrKwqtXr/Dq1Sul23FxcUFQUBDq1atXnNUlMkkFgSTy7O3t4eHhAScnJ6SnpyM+Ph5JSUlKt1OuXDlcvXoVZcuWLc7qko7YbhIRERERERGZvoJAksLMzMzg4eEBV1dXWFhYIDExEbGxscjNzVW6nQULFmD27NnFXV0iIo1wfGMiIiIyiqysLDx79kytdatXr47Dhw+jdu3axVwropIlPT0dERERaq3bokULHDhwgEEkJRjbTSIiIiIiIiLTJggCYmJiEBMTU+S6lpaW+PnnnzFu3DgD1IyISDPmxq4AERERvdkcHR3RrVs3WFtba5y3VKlSmDdvHoKDg3kzlN5q3t7eaNu2LczNNT99r1ChAn766SecOXNG4SgXZHrYbhIRERERERGZvo4dO2r1wI65uTn69++Pq1evMoiEiEwWp7YhIiIig0hKSsLp06dx/vx5hISEIDw8HC9fvkRaWhpycnLg7OwMNzc3lC1bFr6+vggICEDHjh3h4OBg7KoTmYzY2FicOnUKFy5cwJ07dxAeHo7Y2FikpaUhLy8PLi4ucHNzQ/ny5eHn54eAgAC88847sLKyMnbVSQtsN4mIiIiIiIhMW35+Pq5evYqgoCBcvXoVjx8/xrNnz5CcnIyMjAzY2NjA3d0d7u7uqF+/PgICAtCpUydUqVLF2FUnIlKJgSREREREREREREREREREREREBIBT2xARERERERERERERERERERHRfxhIQkREREREREREREREREREREQAGEhCRERERERERERERERERERERP9hIAkRERERERERERERERERERERAWAgCRERERERERERERERERERERH9h4EkRERERERERERERERERERERASAgSRERERERERERERERERERERE9B8GkhARERERERERERERERERERERAAaSEBEREREREREREREREREREdF/LI1dAaKS5MmTJ7hx4wZevHiB1NRUeHl5oVKlSvDz84OVlZVR63bt2jU8fPgQkZGRAIBy5cqhZs2aaNy4sV7LSUxMxPnz5xEZGYm4uDiULl0a5cqVg5+fH1xdXfVaFgDk5ubi2rVrCA0NRWxsLLKzs+Ho6Ch7fz4+PrC0ZFNGRERERERERERERERERKQPvPtKpIY9e/ZgxYoVuHDhgsK/u7u7Y9CgQZg/fz5Kly5tsHrl5ORg+fLlWL9+PR49eqRwnerVq2PMmDGYOnWqTsEu169fx/z583H06FFkZ2dL/m5jY4Nu3bph7ty5aNSokdblFHj48CG+++477Ny5E8nJyUrXs7Ozg7+/Pz755BP06dNH53KJiIiIiIiIiIiIiIiIiN5mZoIgCMauBJGpSk1NxUcffYQdO3aotX6ZMmWwefNmdOnSpZhr9jrQYvDgwbh27Zpa6zdt2hQ7duxA9erVNS5ryZIlmDNnDnJycopc19raGgsWLMD06dM1Lgd4PQLJ/PnzsXjxYuTm5qqdb9CgQWp/T8UlMTERp0+fli1XqFABNjY2RqwREREREREREREREdGbLSsrC8+ePZMtt23btlhGUCciepswkIRIiby8PPTq1QtHjx4VpXt4eKBx48ZwcXHBo0ePcP36dRT+GdnY2ODEiRPw9/cvtrq9fPkSvr6+iIiIEKVXr14dPj4+EAQBoaGhklFKqlSpgosXL8LT01PtshYtWoRZs2aJ0uzs7NC8eXN4eXnhxYsXuHLlCjIzM0XrLFu2DF988YVG7ysjIwP9+/eXfOZmZmbw8fFBxYoV4erqitTUVDx+/Bj37t2TBZuYQiDJwYMH0bt3b6PWgYiIiIiIiIiIiIjobXbgwAG89957xq4GEVGJxqltiJT46quvRAENVlZWWLFiBcaOHQtra2tZ+p07dzBmzBjZtDdZWVno3bs3bt++DS8vL73XKz8/H7179xYFkXh5eSEwMBCdO3cWrfvXX3/hww8/xMuXLwEAT548QZ8+fRAUFAQzM7Miyzpy5Ahmz54tShs7diwWLlwomsInNjYWM2fOxPr162VpX375JerXr4+uXbuq9b4EQcDgwYNFn7mtrS2mT5+OsWPHoly5cpI86enpOH78OHbs2CH6ToiIiIiIiIiIiIiIiIiISDsckYRIgcePH6N27dqiqVxURbBmZGSgQ4cOsmASAPj444+xZs0avddty5YtGD58uGzZ3d0dwcHBqFy5ssL1nzx5gqZNmyIhIUGW9vvvv2Pw4MEqy8nLy4OPjw/u378vS/vss8+wYsUKpXk+++wz/Pjjj7LlunXr4tatW7CwsCjiXQG//PILJk6cKFv28vLCP//8gzp16hSZF3g9JY6lpXFj4zgiCRERERERERERERGRcXFEEiIi3TGQhEiBESNG4LfffpMtjxw5Eps2bVKZ58GDB6hfvz6ys7MBAJaWlrh//z6qVq2qt3rl5eWhRo0aePLkiSwtMDAQI0aMUJkvMDAQH374oWy5WrVqePDgAczNzZXm2bRpE0aNGiVbrlWrFm7evAkbGxulebKystCwYUNR8MnmzZtFgS+KPH36FD4+PkhNTQXweiSS4OBg1K1bV2U+U3Pt2jU0bdpUtnzgwAFUr17diDUiIiIiIiIiIiIiInqzhYWFiR7yDA4ORpMmTYxXISKiNwCntiGSk5GRgT179ojSvvzyyyLz1axZE71798auXbsAvB4hY/v27ZKpYXQRFBQkCiIpV64chg4dWmS+YcOGYfbs2YiMjAQAPHr0COfPn4e/v7/SPIUDaYDXo42oCiIBABsbG0yZMgXjx48XbaeoQJKFCxfKgkgAYNasWSUuiASA5POpXr06fHx8jFQbIiIiIiIiIiIiIqK3T1H3MoiIqGjKhyMgeksdO3YM6enpsuVWrVqhdu3aauUtPOoHAOzbt0+vddu/f79oefjw4WpNG2NhYSEJOFFVt/j4eJw9e1a2bG1tjffff1+tOn7wwQewsrKSLZ8+fRqvXr1Sun5KSgq2b98uW3ZwcMCUKVPUKouIiIiIiIiIiIiIiIiIiPSLgSREcv766y/Rcrt27dTO26ZNG1ha/v9AP9evX0d0dLS+qqZT3eTX/fPPP5Wue/z4ceTl5cmWmzZtCicnJ7XKcXZ2Fg0Zl5ubi+PHjytdf+fOnaLRSPr166d2WUREREREREREREREREREpF8MJCGSExISIlpu1aqV2nkdHBxQv359UVpoaKhe6pWVlYWwsDBRmq+vr9r5/fz8RMsPHz5Edna2wnV1+QwUlaXqMzh58qRouVOnThqVRURERERERERERERERERE+sNAEiI5d+/eFS1Xr15do/zVqlUTLd+5c0fnOgHA/fv3RaOEeHp6wtnZWe38zs7OKF26tGw5Ly8PDx48ULiufJ2L8zO4fPmyaLkgaCUjIwPbt29Hr169UK1aNdjZ2cHV1RXVq1fHgAEDsHbtWqSkpGhULyIiIiIiIiIiIiIiIiIiUo2BJESFvHr1Cq9evRKlVaxYUaNtyK//8OFDnesFQDIaiab1UpRHWd10LUvdchITE0VlWVtbo2rVqjh9+jR8fHzwwQcf4PDhw3j8+DEyMzORlJSER48eYc+ePfj4449RpUoVrFy5UqO6ERERERERERERERERERGRcpbGrgCRKUlMTBQt29vbw8HBQaNteHp6ipaTkpJ0rRYAad3ky1GHunXTtSx1y3n58qVo2dvbG/v27cPAgQORn59fZDnx8fGYMmUKrly5gk2bNsHSUn9NWkxMDGJjYzXKIx+AQ0RERERERERERERERERU0jCQhKiQ1NRU0bKdnZ3G25DPo6/pVwxZN13LUrcc+YCV1NRUDB06VBZEUqlSJUyYMAH+/v4oVaoUXr16haCgIPzyyy8IDw+X5du6dSvKlCmD77//XqN6qrJ69WrMmzdPb9sjIiIiIiIiIiIiIiIiIioJGEhCVIh8AIWtra3G25APopDfprYMWTddy1K3HPlAkri4ONn/BwwYgM2bN0u25evri4kTJ2L48OHYvXu3LH358uV477330KZNG43qSkRERERERERERERERERE/8/c2BUgMmVmZmYGyaMNQ9ZN03zqrq9s+prmzZtj+/btSkdCsbW1xfbt29G8eXNR+rfffqtRPYmIiIiIiIiIiIiIiIiISIwjkhAV4ujoKFrOyMjQeBvyeeS3qS1D1s3R0REJCQlal6VJOYp8//33sLRU3TxZWlpixYoVohFI/v77b8TExMDT01Oj+ioyfvx4DBgwQKM8YWFh6N27t85lExEREREREREREREREREZCwNJiAphIMn/pxsrkKRSpUoICAhQqxx/f39UrVoVjx8/lqWdPn1a4wAQRTw9PfUSkEJEREREREREREREREREVJJwahuiQlxcXETL6enpSEtL02gbMTExomVXV1ddqwVAWrfY2FiNt6Fu3XQtS91yFKX7+vpqVFbLli1Fy3fv3tUoPxERERERERERERERERER/T8GkhAVUqpUKbi5uYnSnj59qtE2IiIiRMs1atTQuV6KtiNfjjrUrZuuZalbTqVKlWBjYyNK8/Ly0qgsb29v0XJ8fLxG+YmIiIiIiIiIiIiIiIiI6P8xkIRITp06dUTLYWFhGuUvPM2Kou1pq1atWrCwsJAtx8TEICUlRe38ycnJiIuLky1bWFgoDfAw1GdgYWGBWrVqidLkA0uKIr9+ZmamRvmJiIiIiIiIiIiIiIiIiOj/WRq7AkSmpl69ejh//rxs+cKFC+jZs6daedPS0nDr1i3J9vTBxsYG1apVw4MHD0R169y5s1r5C78n4PUoIcqCNuTrfOHCBY3qeu7cOZXbK6xBgwaizywxMVGjsuTXL1WqlEb5iYiIiIiIiIiICgiCgPz8fAiCYOyqEBGVaGZmZjA3N4eZmZmxq0JERFpgIAmRnK5du2Lt2rWy5VOnTqmd9+zZs8jNzZUtN27cGGXKlNFr3QoHkpw6dUrtQBL599GtWzel63bq1AkWFhbIy8sDAAQHByMlJQVOTk5FlpOSkoJr167Jli0tLdGpUyel67/77rvYunWrbDk0NLTIMgoLCQkRLZcvX16j/ERERERERERE9PbKy8tDWloaUlJSkJaWJusPIyIi/bC2toaTkxOcnJxga2vLwBIiohKCU9sQyenSpQvs7OxkyxcuXMC9e/fUyhsYGCha7tOnjz6rJtneli1b1Lq4zcvLEwVrFFW30qVLw9/fX7acnZ2N7du3q1XHbdu2IScnR7YcEBAAd3d3pev36NFDNDLKlStX8OrVK7XKSkhIwOXLl0Vpbdq0USsvERERERERERG9vfLy8vD8+XM8ePAAkZGRSE5OZhAJEVExyM7ORnx8PMLDw/Ho0SOkp6cbu0pERKQGBpIQybG3t0f//v1FaUuXLi0y34MHD7B//37ZsqWlJd5//3291q1NmzaoUqWKbPn58+eSABFFtm7disjISNlytWrV0Lp1a5V5hg8fLlr+4YcfkJWVpTJPVlYWfvzxR1HaiBEjVOZxcnISfd5ZWVlYtWqVyjwFVq1ahczMTNlypUqV9DaVEBERERERERERvZlycnIQERGBlJQUY1eFiOitkpOTg6dPnzKYhIioBGAgCZEC33zzDaysrGTLgYGBOHTokNL1MzMz8eGHHyI7O1uWNnr0aFSrVk1lOWZmZqJXUdPoWFhYYN68eaK0qVOnIjw8XGme8PBwfPbZZ6K0b7/9Fubmqn/+I0aMQK1atWTL9+/fx8yZM1XmmTFjBu7fvy9brlu3Lj744AOVeQBgwYIFsLa2li0vWrQIFy5cUJnnwoUL+PbbbyXlc1g8IiIiIiIiIiJSJisrC+Hh4UU+MEVERMVDEAQGkxARlQBmgiAIxq4EkSn64osv8P3338uWrayssGLFCowdO1YU9HD37l2MGTMG58+fl6WVKlUKt2/fhpeXl8oy5IMeTp48iXbt2qnMk5+fDz8/P1y6dEmW5uXlhcDAQHTu3Fm07rFjxzBy5Ei8fPlSlubn54egoCC1Ai6OHDmCXr16oXAzMXbsWCxatAilSpWSpcXFxWHmzJlYt26d6L0dPXoUXbt2LbIcAPj6669FgSGOjo5YtmwZxowZIwrqyc3NxYYNG/D5558jNTVVlt6iRQucPXtW9N0YWmhoqGhElJCQEPj4+BitPkREREREREREJPb06VOkpaWJ0szMzGBvbw8nJyfY2dnBwsKCDysREelIEATk5OQgNTUVycnJyMnJEf3dysoK1apV00t7y755IiL9YyAJkRJ5eXno2bMn/vzzT1G6p6cnmjRpAicnJzx+/BjXrl0TBVpYW1vjxIkTaNOmTZFlaBNIAgBRUVHw9fXF06dPRek1atSAj48PBEFAaGgowsLCRH+vXLkyLl68iDJlyhRZRoFFixZh1qxZojQ7Ozu0bNkSZcuWRVRUFC5fvoyMjAzROkuXLsX06dPVLkcQBAwaNAi7d+8Wpbu6usLX1xfu7u549eoVLl68iMTERNE65cqVw8WLF1G+fHm1yysOPFklIiIiIiIiIjJdOTk5kv4ya2trVKhQwagPJxERvekEQUBkZKRkSrEqVarA1tZW5+2zb56ISP8sjV0BIlNlYWGBXbt2YcyYMdi5c6csPSYmBn/99ZfCPJ6enti8ebNaQSS68PLywvHjxzF48GBcv35dlv7w4UM8fPhQYZ4mTZpg586dGgWRAMDMmTNhZmaGuXPnyiKGMzIylE7DY2VlhQULFmgURAK8DqrZsmUL3N3d8b///U+WnpiYqPTzBl6PRLJ//354e3trVB4REREREREREb1dkpKSRMvm5uaoVKkSLC3ZTU5EVJzMzMxQrlw5PHr0SDQySXJysl4CSYiISP/MjV0BIlPm6OiIHTt2YPfu3fD19VW6nru7Oz755BOEhISoPZWLrmrWrIlLly5h8eLFqFq1qtL1qlWrhsWLF+PixYuoXr26VmXNmDEDly5dwnvvvaf06Qxra2u89957uHz5Mr788kutyrGxscGaNWtw4sQJdOrUCRYWFkrXrVevHgIDA3H+/HkGkRARERERERERUZHkA0mcnZ0ZREJEZCBmZmZwdnYWpcmPUEJERKaDU9sQaeDJkye4du0aXrx4gbS0NJQtWxaVKlVC69atjT78ZXBwMB48eIAXL14AALy9vVGzZk00bdpUr+UkJCTg/PnziIyMRHx8PEqVKoVy5crBz88Pbm5uei0rNjYWFy9eRFRUFOLi4uDk5IQyZcrAz8/P6NPYKMLh84iIiIiIiIiITJMgCLh3754orWLFinBwcDBSjYiI3j7p6emIiIgQpdWuXRtmZmY6bZd980RE+sdwayINVKlSBVWqVDF2NRRq2rSp3oNGFHFzc0P37t2LvRwA8PDwQM+ePQ1SFhERERERERERvbny8/MlaVZWVkaoCRHR20vRKFD5+fkqRycnIiLj4NQ2RERERERERERERPRGUzQwt7k5u8eJiAxJUbvLiROIiEwTz5SJiIiIiIiIiIiIiIiIiIiICAADSYiIiIiIiIiIiIiIiIiIiIjoPwwkISIiIiIiIiIiIiIiIiIiIiIADCQhIiIiIiIiIiIiIiIiIiIiov8wkISIiIiIiIiIiIiIiIiIiIiIADCQhIiIiIiIiIiIiIiIiIiIiIj+w0ASIiIiIiIiIiIiIiIiIiIiIgLAQBIiIiIiIiIiIiIiIiIiIiIi+g8DSYiIiIiIiIiIiIiIiIiIiIgIAANJiIiIiIiIiIiIiIiI3jhmZmayV7t27VSu+80334jWP3XqlEnUq6Ro166d6H0RERGVdAwkISIiIiIiIiIiIiIiIiIiIiIADCQhIiIiIiIiIiIiIiKit1R4eLhoNJGRI0cau0pERERGx0ASIiIiIiIiIiIiIiIiIiIiIgLAQBIiIiIiIiIiIiIiIiIiIiIi+g8DSYiIiIiIiIiIiIiIiN5i33zzDQRBkL3atWtn7CqVKKdOnRJ9fkRERCUdA0mIiIiIiIiIiIiIiIiIiIiICAADSYiIiIiIiIiIiIiIiIiIiIjoP5bGrgAREREREREREREREdHbKiUlBXfu3MGDBw8QFxeHtLQ0ODk5wd3dHfXq1UODBg1gYWFh7GqqJAgCLl++jLt37yI6OhpOTk4oV64cWrZsibJly+qtnFevXuHOnTt4+PAhXr16hczMTDg7O6NUqVJo1KgR6tSpAzMzM72VZ0gvXrzAxYsXER0djYSEBLi4uMDDwwPNmzdHlSpV9F7es2fPcOXKFTx//hwZGRkoXbo06tevj2bNmsHcnM+hExG97RhIQkREREREREREREREb71JkyZh1apVsuV169ZhzJgxGm/nnXfewcmTJ2XL58+fR6tWrUTrhISEYOfOnTh27BiuXbuGvLw8pdtzcnLC0KFD8fnnn6Nq1aoa10cd33zzDebNmydbPnnyJNq1a1dkvvz8fKxevRrLli3Ds2fPJH+3sLBAly5dMGfOHLRs2VKrul26dAm7du3C8ePHERISAkEQlK5bqlQpjBkzBp9++mmRASyVK1dGRESEJH3z5s3YvHmz0nybNm3CyJEjRWnt2rXD6dOnZcuq6lhYfn4+fv/9d3z33Xe4efOm0vVq1qyJyZMnY+zYsbCyslJr24UDatq2bYtTp04BeL0/zp07F//++y/y8/Ml+cqUKYNZs2ZhwoQJDCghInqL8QhARER693foS+wJfo7Ac0/wy8kwLP3rHr4+EIK7UcnGrhoREREREREREZFC8kEjGzZs0HgbT548kd2wB4A6depIgkiuXbuG+vXr49tvv8WVK1dUBpEAr0cs+fXXX1GvXj1s3bpV4zoVl8TERAQEBGDSpEkKg0gAIC8vD0ePHoWfnx9++eUXjcvYt28ffH19sWLFCty+fbvIAI34+HgsXboU9erVw/HjxzUuz5CioqLQqlUrDB06VGUQCQA8ePAAEydORP369fHw4UOty1y0aBECAgJw4sQJhUEkABAdHY3Jkyejf//+yM7O1rosIiIq2TgiCRER6d30vbeQmJ4jSW9Z1R11vJyNUCMiIiIiIiIiIu3l5uUjKinT2NV443m52MLSwnjPvzZs2BBNmzZFcHAwAODixYu4c+cO6tatq/Y2Nm7cKAp2GD16tGQd+Rv4ZmZmqFKlCmrUqAFnZ2dYWVkhISEBoaGhePr0qWy9jIwMDBs2DLa2tujfv7+mb0+v0tLS0KlTJ1y9elWUbm9vL5vOJiEhAdeuXUNMTAzy8/MxceJEeHp6alSO/GdlYWGBGjVqoEqVKnB2doaZmRni4+Nx+/ZtvHz5UrZefHw8unfvjtOnT0sCeUxBREQE2rZtKxkRxcnJCS1atICnpydevXqFq1evIj4+Xvb3+/fvw8/PDydOnEDDhg01KvP777/HrFmzZMu1atVCrVq14ODggKioKFy8eBGZmf/fzu3fvx9ff/01li5dquW7JCKikoyBJEREpHcO1pYKA0nSsnKNUBsiIiIiIiIiIt1EJWWizbKTRa9IOjk7vT0quNsbtQ5jxoyRBZIAr0clWb58uVp58/PzERgYKFu2srLC8OHDFa5rZ2eHwYMH47333kOHDh3g6OiocL2QkBDMnz8fu3fvlqWNGjUKAQEBGgdl6NP06dNFQSQ2NjaYM2cOpkyZAgcHB1l6Xl4edu/ejSlTpiAmJgbjxo3TuCxXV1cMHToUPXv2REBAAGxtbRWud+HCBcyaNUs2rVBOTg4GDx6Mhw8fwtraWrJ+UFAQcnNz8fz5c7Rp00aW3q9fP3z//fdK61O6dGmN30Nhubm5GDJkiCiIxNHREQsXLsTYsWNF7y83Nxc7duzAZ599hri4OABAXFwcBg4ciODgYKX7jbzbt2/j7NmzAIDevXtj8eLFqF27tmidhIQETJ06VbQPL1++HJ988gkqV66s5bslIqKSilPbEBGR3jnaKI5TTM1SPUwnERERERERERGRMQ0ZMgT29v8fzLJlyxbk5EgfmFLk2LFjeP78uWy5V69e8PDwkKxXs2ZNREZGYuPGjXjvvfdUBgPUq1cPu3btwty5c2VpKSkpWL16tVp1Kg5Xr17Fr7/+Klu2tLTEnj17MHPmTFEQCfB6BJHBgwfjzJkzslE2NNG2bVtERkbi559/RufOnZUGkQBAq1at8M8//+DDDz+UpT19+hTbt29XuH758uVRuXJllC9fXpTu6OiIypUrK32pG7yhzOrVq3HhwgXZsoODA/7++29MnjxZ8v4sLS0xdOhQnDlzRhTA8uDBA8ybN0/tMl+9eoX8/HxMnz4d+/fvlwSRAICbmxs2bdqE9957T5aWl5en1RRPRERU8jGQhIiI9M7BxkJhOkckISIiIiIiIiIiU+bi4iKaNiY2NhaHDx9WK+/GjRtFy2PGjFG4nrOzM9zc3DSq15w5c9CgQQPZ8rZt2zTKr08rV64UTd8zZcoU9OjRQ2WeWrVqaRX84uHhIQrsKYqZmRlWrVolGq3FmJ+VvPz8fPz444+itCVLlhQ5/U6dOnWwZs0aUdratWuRkpKidtn+/v5YvHhxkestXLhQtPzvv/+qXQYREb05GEhCRER652hrpTA9lYEkRERERERERERk4kaPHi1aVmdEhri4OBw6dEi2XKFCBXTu3FlvdTI3N0efPn1ky2FhYYiNjdXb9tWVmZkpmmbHxsYGM2fOVCtvv3790KhRo2Kq2f+zt7dHt27dZMuXLl1Cfn5+sZerjjNnzuDJkyey5fLly2P8+PFq5e3Xrx+aNWsmW05OTsb+/fvVLnvWrFkwNy/6tqCPj49oKpsbN26oXQYREb05GEhCRER656hkRBIGkhARERERERERkakLCAhAzZo1ZcvHjh1DZGSkyjxbtmxBdna2bPnDDz9U66a9vKysLMTFxSEiIgLh4eGil52dnWjdu3fvarx9XV29ehWZmZmy5S5dusDd3V3t/B988IHe6pKZmYnY2FiFn5WTk5NsvZSUFNGUQ8YUFBQkWh4yZIhG+8nw4cNVbk8ZOzs7vPPOO2qXU6dOHdn/09PTkZqaqnZeIiJ6M1gauwJERPTmcbBWfHjh1DZERERERERERFQSjBo1Cl999RUAIC8vD4GBgZg1a5bS9QtPa2NmZoYPP/xQrXJCQkKwc+dOnD17FqGhoYiLi1O7jgkJCWqvqy9Xr14VLbds2VKj/JquX9ilS5ewe/duXLhwAaGhoUhKSlI7b0JCAipWrKh12foi//n5+flplF9+/StXrqiVr1q1arC2tla7HPmpl5KSkuDo6Kh2fiIiKvkYSEJERHrnaKv48JKayUASIiIiIiIiIip5vFxscXZ6e2NX443n5WJr7CrIjBw5ErNnz0Zu7uv+rE2bNmHmzJkwMzOTrHv58mWEhITIljt27CiaGkSRiIgIfPrppzhw4IDWddQkkEJfoqOjRcs1atTQKH/hkV7UFRISggkTJuDMmTMa5y1gjM9KkZiYGNGypp9H7dq1VW5PGfnAkKJYWYmnLs/JydEoPxERlXwMJCEiIr1ztFESSMIRSYiIiIiIiIioBLK0MEcFd3tjV4MMqEyZMujevTsOHjwIAHj06BFOnz6Ndu3aSdbdsGGDaHn06NEqt3337l106NABUVFROtUxPz9fp/zakB8FxdnZWaP8Li4uGq0fFBSEd999FykpKRrlk2eMz0oR+c9P08/DwcEBlpaWsgCnV69eqZVPm2mWiIjo7cYjBxER6Z2DkkCStGwGkhARERERERERUckgHxAiHzACAOnp6dixY4dsuVSpUujdu7fSbebm5mLw4MGiIBJbW1sMGzYMgYGBuHr1KqKiopCamorc3FwIgiB7bdq0Sfc3pWeKRmjR1/rJyckYOHCgKIjExcUFH3/8MX7//Xdcv34d0dHRSEtLQ15enuizmjt3rkb1MhRBEETLmn5+8nTNT0REpAxHJCEiIr1TGkiSlWfgmhAREREREREREWnn3Xffhbe3N168eAEA2Lt3L1atWiUaRWL37t1ITk6WLQ8dOhQ2NjZKt7lv3z7cunVLtly9enUcO3YMVatWLbI+hcsxFvkpUjSdMiYxMVHtddesWSMKuGnZsiUOHz4MDw+PIvOawmeliLu7u2g5KSkJ3t7eaudPS0uTjUYCaD5lDRERkbo4IgkREemdk5JAkpRMjkhCREREREREREQlg4WFBUaMGCFbzsjIwO+//y5aZ+PGjaLloqa1KZgqp8DatWvVCiIBIAtoMaYyZcqIlh8+fKhR/gcPHqi9buHPyszMDNu3b1criAQwjc9KEU9PT9GyJp8HANy/f1/l9oiIiPSFgSRERKR3ykckYSAJERERERERERGVHKNHjxZNH1J4epuHDx/izJkzsuUWLVqgfv36KrdXOPDCwcEB7dq1U7su58+fV3vd4tKsWTPR8sWLFzXKf+nSJbXXLfxZ1alTR+2AGwC4cOGC2usacnoY+c9P0+9Ufv3mzZvrXCciIiJFGEhCRER652BjoTA9IycPefmCwr8RERERERERERGZmmrVqqFt27ay5atXr8qmptF0NBJAPLWLs7Oz2kEMYWFhOHfunFrrFqdmzZrB1tZWtnzs2DG8evVK7fzbtm1Te93Cn1Xh6YSK8u+//+Lp06dqry8/FVFWVpbaeTXl7+8vWv7999+Rn5+vdv4tW7ao3B4REZG+MJCEiIj0zlHJiCQAkJbNUUmIiIiIiIiIiKjkGDNmjGh5w4YNyMvLw+bNm2VpDg4OGDJkSJHbcnNzk/0/JiYGCQkJatVh+vTpGgUcFBdbW1sMGDBAtpydnY1FixaplXfv3r24ceOG2mUV/qwePnyo1vvPycnBjBkz1C4DAFxdXUXLUVFRGuXXREBAAKpUqSJbfvbsGf73v/+plXf//v24fPmybNnZ2Rm9e/fWdxWJiIgAMJCEiIiKgapAktRMBpIQEREREREREVHJ0a9fP1GwwbZt23DgwAFRwMGAAQPg5ORU5LYaNmwo+39eXh6+++67IvPMnTsX+/fv16zSxWjy5MmikVR++uknHDlyRGWeBw8eYPz48RqVU/iziouLw/r161Wun5eXh48//lgUbKEOW1tbVK5cWbZ85coV0Wgo+mRubo4pU6aI0r788ssi63z//n2MGzdOlPbRRx/B2dlZ73UkIiICGEhCRETFQOWIJFkMJCEiIiIiIiIiopLD1tYWH3zwgWw5Pj4en3zyiWgd+VFLlBk0aJBoefHixZg9ezZSU1Ml696/fx99+/bF/PnzAQAeHh6aVr1YNGvWTPT+c3Nz0b9/fyxevBjp6emidfPy8rBz5060adMGMTExolFGiiL/WU2cOBErV65Edna2ZN0rV67gnXfewaZNmwBo/lm1b99e9v/09HR07doVO3fuREhICJ48eYLw8HDZS9F3pYkJEyagZcuWsuWUlBR06tQJq1evlkyrk5ubi23btsk+vwLVq1fH3LlzdaoHERGRKgwkISIivXNQNSIJA0mIiIiIiIiIiKiEGT16tGg5NjZW9v/atWujdevWam2nQ4cO6Natmyht4cKF8PLyQocOHTB06FD07t0bdevWRe3atWUjkdSpUwezZ8/W8V3oz7Jly9C0aVPZclZWFmbOnAkPDw906NAB77//Prp37w5vb28MHjxYFgTx66+/ql3GiBEj0KBBA9lyTk4OpkyZgrJly6Jbt24YOnQoevbsiSpVqqBFixY4c+YMAKBt27YYO3asRu9n8uTJsLT8/z7NS5cuYfDgwahfvz6qVq2KKlWqyF579uzRaNvyLC0t8fvvv6NChQqytOTkZEyYMAGenp7o3Lkz3n//fXTr1g1eXl4YOnSoaH9zd3fHrl271BoBh4iISFvK7/QRERFpyd7aAmZmgCBI/5aWlWf4ChEREREREREREemgcePGaNKkCa5duyb5m3yQSVG2b9+OTp064erVq7K01NRU/PvvvwrXb9SoEY4cOYLjx49rVuli5ODggBMnTqB79+44f/68LD09PV3h+zA3N8eKFSswaNAgDB48WK0yLC0tcfjwYbzzzjt49OiRLD0hIQF//fWXwjwdO3bE3r17sWLFCo3eT6NGjbB27VqMHz8emZmZGuXVRpUqVXDx4kX06tULwcHBsvTk5GSV33ONGjVw+PBh1KpVq9jrSEREbzeOSEJERHpnZmYGR2vFsYqpWTkGrg0REREREREREZHuFAWMWFlZYfjw4Rptx9XVFUFBQZgzZw7c3d2VrlerVi0sXboUFy9eRLly5TSub3FzdXXF2bNnsXLlSpQvX17hOhYWFujcuTPOnj2LKVOmaFxGxYoVce3aNUyePBn29vZK12vcuDH+97//4dixY3B2dta4HAD48MMPce/ePcyfPx8dO3ZE+fLl4eDgADMzM622VxRvb29cvnwZmzdvFo28okiNGjWwcuVKhISEMIiEiIgMwkwQFD0vTkREmgoNDUW9evVkyyEhIfDx8TFijYzLd9E/eJksjd7/fkBD9G+q+MKSiIiIiIiIiKg45Obm4uHDh6K0GjVqiKayIDKGrKwsXLp0CaGhoUhISICjoyPKli2LOnXqoH79+sauntoEQcClS5dw584dxMTEwMnJCd7e3vD19YWXl5deykhLS8P58+dx//59JCcnw8XFBWXLlkXDhg1RvXp1vZRhTJGRkbh48SKio6ORmJgIJycneHp6onnz5qhataqxq6cXxdUWs2+eiEj/eJZMRETFwsHGQmF6WlaugWtCRERERERERERkmmxsbBAQEICAgABjV0UnZmZm8PX1ha+vb7GV4eDggE6dOqFTp07FVoYxlStXDv369TN2NYiIiABwahsiIiomjrZWCtNTGUhCRET/x96dR8lVl/njfypd3dWd7nQIgUAAWQw7KAhiSAQHR9kUvyDqgKACAwPHDURlEPAni84AKiDqdxwVB1xgABcQHQFR1AEDwS+gEsCQQMIuIQmQdKe36q7fH0iZSt/ba92qDnm9zsk59Xnu/dz7dKvg6X7n+QAAAAAAMGEJkgCQibaUiSSCJAAAAAAAADBxCZIAkInWpuTT0xxtAwAAAAAAABOXIAkAmWgrJAdJTCQBAAAAAACAiUuQBIBMtDWnBEm6BUkAAAAAAABgohIkASATrSkTSTp7BUkAAAAAAABgohIkASAT6Ufb9Ne4EwAAAAAAAGCkBEkAyERrU0NivbPHRBIAAAAAAACYqARJAMhEW3NjYr2jW5AEAAAAAAAAJipBEgAy0VYwkQQAAAAAAADWN4IkAGSitZBPrHf2FqNUKtW4GwAAAAAAAGAkBEkAyERakGSgFNHV11/jbgAAAAAAAICRECQBIBNTUoIkEREd3Y63AQAAAAAAgIlIkASATKRNJImI6OgRJAEAAAAAAICJSJAEgEwMFSTp7HG0DQAAAAAAAExEgiQAZKJtiCDJ6p6+GnYCAAAAAAAAjJQgCQCZaJiUi5bGhsRrJpIAAAAAAADAxCRIAkBm0o636ewp1rgTAAAAAAAAYCQESQDITFsheSJJhyAJAAAAAAAATEiCJABkpq05eSKJIAkAAAAAAABMTIIkAGSmtcnRNgAAAAAAALA+ESQBIDNtBRNJAAAAAAAAYH0iSAJAZlpTgiQmkgAAAAAAAMDEJEgCQGbamk0kAQAAAAAAgPWJIAkAmUk/2qa/xp0AAAAAAAAAIyFIAkBmWpscbQMAAAAAAADrE0ESADLTWmhIrAuSAAAAAAAAwMQkSAJAZqY0J08kWd0tSAIAAAAA9bR06dLI5XLlP8cff3y9W4qIiN/+9rcVfZ133nn1bgkANjiCJABkprWQcrRNryAJAAAAAAAATESCJABkJjVI0lOMUqlU424AAAAAYHgTdVIHAECtCJIAkJkpKUGSvv5S9BQHatwNAAAAAAAAMBxBEgAykzaRJOLlqSQAAAAAAADAxJL+Gz4AGKe2IYMk/TG9rYbNAAAAAABl22677YQ8fvqAAw6YkH0BwIbERBIAMjPURJIOE0kAAAAAAABgwhEkASAzrYWG1GuCJAAAAAAAADDxONoGgMwU8g3R1DApevsHBl3rFCQBAAAAYAO0bNmymD9/fjz77LOxfPnyaGtri0MOOSR23HHH1D0rV66Mhx56KBYtWhQrV66M7u7uaG9vj+nTp8eee+4Zu+yyS+RyuRp+FYOtXr067rjjjnjyySdj5cqVMW3atJg1a1bst99+0dLSUre+li9fHvPmzYunnnoqXnrppZg+fXrsvPPOMWfOnGhsbBzXs1966aX43e9+F0899VSsWrUqZs6cGVtvvXXst99+4342ANSTIAkAmWotNETvmsFBEhNJAAAAAJhItt1223j88ccH1b/73e/Gd7/73dR9V155ZRx//PGJz9lmm21i6dKlERFxxx13xAUXXBC/+c1vor+/v+IZl1122aAgyfz58+P666+P2267LRYsWBClUim1h+nTp8dJJ50Un/jEJ2LzzTcf7kuNiIilS5fGdtttV14fd9xxcdVVV6Xef8ABB8Tvfve78vqVfp5++uk4++yz40c/+lGsWbNm0L7m5uY45ZRT4txzz41p06YN29dvf/vbeOtb31pen3vuuXHeeeel3p/2/X744Yfj7LPPjv/5n/+Jvr6+Qfva29vjU5/6VJxxxhmjDro89thjccYZZ8TPfvazxGdvsskm8aEPfSguuOCCaG1tjauuuipOOOGE8vV1/zsDABONo20AyFRrITmzaCIJAAAAABuKz33uc3HAAQfEr371q0EhkiQ/+clPYt99941LL700HnjggSFDJBERK1asiIsvvjh23333uO2226rV9rB+8YtfxB577BHf+973EkMkERHd3d1x+eWXx9y5c+PJJ5+sSV9XXnllvPGNb4wbb7wxMegREbFq1ao499xz48ADD4wXX3xxxM++9tprY7fddouf/OQnqc9evnx5XHrppbHPPvvEI488MpYvAQDqykQSADLVlhIkMZEEAAAAgA3BV77ylfj85z9fXm+zzTax++67R3t7ezz33HNx//33D9ozMFA54behoSF22GGH2G677aK9vT1yuVysWLEiHnjggfjrX/9avm/FihXxzne+M373u9/FnDlzsvuiIuL3v/99HHnkkdHT0xMREZtttlnstddeMW3atHjxxRdj/vz5sWLFivL9f/nLX+Koo46KO++8MyZNyu7vOf/whz+ME088sRy+eeX7PXXq1Hj++efj7rvvjtWrV1d8Haecckpcd911wz77+uuvjw984AODwkCvfe1rY9ddd43W1tZ45pln4p577omenp54+OGH4//8n/8TH/3oR6v7RQJAxgRJAMiUIAkAAACw3usvRqx6ut5dvPq1bxnRUL9fW9x5551RLBbjqaeeiv33379cf8973hNf/vKXU/dtsskmqdeWLVsWZ5xxRkREzJkzJy677LKYPXt2xT09PT0VgYtXbLTRRvGBD3wg3vWud8Vb3vKWaG5uTnzHXXfdFeecc0785je/iYiIvr6+OProo2PRokXR1NSU/gWP0xFHHBE9PT3x+te/Pi655JJ4+9vfXnG9WCzG17/+9fj0pz9dDl7cddddcfXVV8cHP/jBTHpavnx5HHfccVEqleItb3lLfPnLX4599tmn4p6urq644IIL4qKLLirXrr/++vjYxz5W8Z/7up588sn4l3/5l4oQyR577BH/8R//EXPnzq24d/Xq1XHxxRfHxRdfHAsXLowLLrigSl8hANSGIAkAmXK0DQAAALDeW/V0xOWvr3cXr36n/Tli2jZ1e/1WW22VWG9ra4ttt912TM/s6uqKiIjDDjssfvzjHycGOwqFQmyxxRYVtX/4h3+Ip59+OiZPnjzsO+bMmRO//vWv48QTT4wrr7wyIiKeeOKJuOaaa+L4448fU98jsXz58njrW98aP/vZz6K1tXXQ9Xw+H5/4xCciIuL0008v17/1rW9lFiTp7OyMiIhjjjkmvvvd70Y+P/hnky0tLXHhhRdGV1dXXH755RV9DRUkOeOMM2LVqlXl9Zw5c+KXv/xltLW1Dbp3ypQp8YUvfCFe97rXxfvf//5Yvnz5eL4sAKi57GaHAUAMNZFk+LNgAQAAAGB9t+mmm8Z3v/vdUU0H2XTTTUcUInlFLpeLr3/96zFjxoxy7eqrrx5Vn6O10UYbxbXXXpsYIlnbxz72sdh8883L67vvvrscsMnC9ttvH1dccUViiGRtn/vc5yr+M7n99ttT733mmWfixz/+cXk9efLkuOaaaxJDJGs76qij4qSTThph5wAwcQiSAJApR9sAAAAAsCE7+eSTY+ONN878PZMnT45DDz20vJ4/f34MDAxk9r5TTjmlIriSJp/PxyGHHFJeF4vFeOCBBzLr61Of+lS0tLQMe9/GG29ccSTNM888E8uWLUu890c/+lEUi3//eeaxxx474ik1n/3sZ2PSJL+OA2D94t9cAGTK0TYAAAAAbMiOOOKIqj6vu7s7nn/++Xj88cdj6dKlFX+mTJlSvm/16tXx1FNPVfXda3vnO9854nt32WWXinVaYKMasuhr3rx5Fet/+qd/GvE7tt5665g9e/aI7weAiWDouV4AME5thYbEuokkAAAAALzaNTQ0xOtf//pxPWP+/Pnxwx/+MO6666548MEH46WXXhrx3hdeeCG23nrrcb0/za677jrie6dNm1axHs3XMBptbW3xmte8ZsT3j7SvP/3pTxXrN77xjaPqa5999om77rprVHsAoJ4ESQDIVFtzytE23YIkAAAAALy6TZ06NZqamsa0d8GCBfHRj340/vd//3fM788qsBExOIQxlMbGxop1X19ftduJiNH1FDHyvpYvX17+3N7eHhtttNGo3pNVmAcAsiJIAkCmUo+26RUkAQAAANYT7VtGnPbnenfx6te+Zb07qLq1j5oZjTvvvDPe8Y53xOrVq8f1/oGBgXHtH8qkSZMye/ZYZdXTiy++WP48lv9M29vbq9gNAGRPkASATLWlBUkcbQMAAACsLxryEdO2qXcXbCBWrVoV//RP/1QRIpk6dWocffTRccABB8TOO+8cW2yxRbS1tUVzc3NFeOK8886L888/vx5tv6oVCoUoFl/+eWZvb++o949lDwDUkyAJAJlqbUo52kaQBAAAAAAG+c///M949tlny+vZs2fHz372s9h0002H3btq1aosW9tgTZs2LTo7OyPi5eOCBgYGRjX9ZOXKlVm1BgCZmHhzxwB4VWlrTg6SdPcNRLE/u9GaAAAAALA++ulPf1r+nMvl4pprrhlRiCQi4plnnsmqrQ3aNtv8fSJRb29vPPLII6Pa/8ADD1S7JQDIlCAJAJlKO9omIqKzp7+GnQAAAADA8HK5XF3fv2jRovLnXXbZJV772teOeO9dd92VRUsbvH333bdi/Zvf/GbEe/v7++OOO+6odksAkClBEgAy1TpEkKSj1/E2AAAAAEwshUKhYt3T01PT97/44ovlz1OnTh3xvttvvz2eeOKJDDri7W9/e8X6iiuuiFKpNKK9N910U/z1r3/Noi0AyIwgCQCZai00pF7r7BEkAQAAAGBi2WijjSrWzz77bE3fP23atPLnRYsWxcDA8MdD9/X1xVlnnZVlWxu0gw46KLbbbrvy+r777otvf/vbw+7r6OiIT33qU1m2BgCZECQBIFNTCo2p11Z3C5IAAAAAMLE0NzfHtttuW17/4Q9/qJgSkrU99tij/Hn58uVxxRVXDHl/f39/nHLKKXHPPfdk3doGa9KkSXHOOedU1D72sY/Ftddem7pn+fLl8Y53vCOWLFmSdXsAUHWCJABkqrlxUkxKOVbWRBIAAAAAJqK3vvWt5c9r1qyJQw45JK677rpYsGBBLFmyJJYuXVr+09HRUdV3H3XUURXrj33sY/HVr341ent7B937hz/8If7xH/8xrrzyyoiI2HTTTavaC3934oknxsEHH1xe9/X1xfvf//44+OCD4wc/+EHcf//9sXDhwvjtb38b55xzTuy0005xxx13RC6Xi3/6p3+qY+cAMHr5ejcAwKtbLpeL1kI+cfqIIAkAAAAAE9Gpp54a3//+96NYfPnnV/Pnz4+jjz468d4rr7wyjj/++Kq9+7jjjouvfvWr8ec//zkiXg4snHbaaXHeeefF7NmzY/r06fHSSy/FggULYunSpeV9//AP/xD77bdf/Nu//VvVeqHStddeG4ccckjMnz+/XPvlL38Zv/zlL1P3/H//3/8X2223XVx//fXlWi6X8jfvAGCCMJEEgMxNKSTnFjsESQAAAACYgPbcc8/41re+Fc3NzTV/dz6fj5/97Gcxa9asivoLL7wQt9xyS1x99dXx85//vCJE8va3vz1uuummyOf9/eEsbbTRRnHbbbfFSSedNGwYpKWlJa644oo4//zzo6urq+LalClTsmwTAMZNkASAzLUKkgAAAACwnjnhhBPiL3/5S1xwwQXx9re/PbbaaqtobW2tyTSJrbfeOu6777449dRTY/Lkyan3veENb4hvfvObceutt0Z7e3vmffFyCOTb3/523H///fHpT3869thjj5g+fXoUCoXYZptt4i1veUtccsklsWTJkjjxxBMjIuLFF1+seMbUqVPr0DkAjFyuVCqV6t0EwKvBgw8+GLvvvnt5vWDBgthtt93q2NHEccT//X388ckXB9U/fdCO8bF/3KH2DQEAAAAblGKxGIsWLaqo7bDDDqY3sF7o7OyMefPmxcKFC2PVqlUxderU2HzzzWOPPfaI7bffvt7tMQIf/OAH4wc/+EF5vaH+7Dirfxb72TxA9fl/yQBkri11Ikl/jTsBAAAAgPVLa2trHHjggXHggQfWuxXGoFQqxR133FFet7a2xs4771zHjgBgeIIkAFTfgzdGdL8UUeyOKHbHP3UuiTn5FfGL/n3jwdK25ds6HW0DAAAAALyK3XzzzfH444+X1/vss080NDTUsSMAGJ4gCQDVd+s5EaueKi//T0REPuKxgS0qgiQdgiQAAAAAwKvU6tWr49RTT62ofehDH6pTNwAwcpPq3QAAr0KNzYnl5lxvxVqQBAAAAABYX9x1113xkY98JJ544olh712yZEkccMAB8eijj5Zrm222WRx99NFZtggAVWEiCQDVl08OkhSir2LtaBsAAAAAYH3R09MT3/jGN+Kb3/xmHHDAAXHwwQfHXnvtFZtvvnk0NzfHCy+8EA8//HDceuut8cMf/jD6+ip/HnrllVdGS0tLnboHgJETJAGg+vKFxPK6QRITSQAAAACA9c3AwEDcfvvtcfvtt4/o/oaGhrj00kvj0EMPzbgzAKgOR9sAUH355FS9o20AAAAAgPVVW1tbFArJf4kuzete97q4+eab49RTT82oKwCoPhNJAKi+EU4kcbQNAAAAALC+eOMb3xjPP/983HbbbXHHHXfEn/70p1i6dGk8//zz0dXVFYVCIaZNmxZbbbVVvPnNb46DDjooDjrooMjlcvVuHQBGRZAEgOrLNyeWC1E5kaSzp78W3QAAAAAAVMWUKVPiyCOPjCOPPLLerQBAZhxtA0D1jXAiSUdPMQYGSrXoCAAAAAAAABgBQRIAqq+xJbFcyPUOqq3pM5UEAAAAAAAAJgpBEgCqb4QTSSIiOnuKWXcDAAAAAAAAjJAgCQDVl29OLCcFSToESQAAAAAAAGDCECQBoPpSgiTNMfhom45uQRIAAAAAAACYKARJAKi+tIkkOUfbAAAAAAAAwEQmSAJA9eULiWVH2wAAAAAAAMDEJkgCQPU1tiSWk4626ewVJAEAAACylcvlBtUGBgbq0AnAhivpn7tJ/3wGoP4ESQCovtFMJOkWJAEAAACyNWnS4B+F9/UN/jkFANkpFgf/LDjpn88A1J9/OgNQffnmxHIhl3S0TX/W3QAAAAAbuFwuF01NTRW1VatW1akbgA1TR0dHxbqpqclEEoAJSpAEgOpLC5IkHW3TYyIJAAAAkL2pU6dWrFetWpX4t+MBqL5SqTQowDdlypQ6dQPAcARJAKi+lCBJc9LRNoIkAAAAQA2sGyQZGBiIxx9/PHp7B//FFwCqp1QqxdNPPz3oSLH29vY6dQTAcPL1bgCAV6F8IbH88kSSUkT8fVyhIAkAAABQC42NjdHa2hqdnZ3lWm9vbzz22GMxefLkaGtri8mTJ0dDQ4OjFgDGaWBgIIrFYnR0dMSqVasGhUgaGxujUEj+OTIA9SdIAkD1pUwkaciVIh/9UVzrXz+OtgEAAABqZbPNNosnnnii4kibUqkUnZ2dFQETALKTy+Viiy22ENoDmMAcbQNA9TUmB0kiIpqjclysiSQAAABArRQKhdh22239LXiAOsnlcrH11lvH5MmT690KAEMQJAGg+lImkkREFKJyhKEgCQAAAFBLjY2Nsc0228SUKVPq3QrABqWxsVGIBGA94WgbAKovn/63etYNkjjaBgAAAKi1hoaG2GqrraK/vz86Ozujo6MjOjo6or+/v96tAbyqNDU1xZQpU6K9vT0KhYLjbADWE4IkAFRfviX1UnOuN6L093Vnjx/QAAAAAPXR0NAQ7e3t0d7eHhERpVIpBgYGolQqDbMTgKHkcrmYNGmS4AjAekqQBIDqG8VEEkfbAAAAABNFLpeLhoaGercBAAB1NaneDQDwKpRvTr2UFCTxt3wAAAAAAABgYhAkAaD6hgiSNOd6K9b9A6XoKQ5k3REAAAAAAAAwAoIkAFTfpEkRDU2Jl9adSBLheBsAAAAAAACYKARJAMhGylSSQvQOqnUKkgAAAAAAAMCEIEgCQDbyhcRy0kSS1d2CJAAAAAAAADARCJIAkI18S2K5OWciCQAAAAAAAExUgiQAZGMUE0k6ewVJAAAAAAAAYCIQJAEgG/nmxLKjbQAAAAAAAGDiEiQBIBuNaUGSpKNt+rPuBgAAAAAAABgBQRIAspE2kSSXcLRNj4kkAAAAAAAAMBEIkgCQjXwhsZx0tE2HIAkAAAAAAABMCIIkAGQjZSJJc8LRNoIkAAAAAAAAMDEIkgCQjbSjbRImkjjaBgAAAAAAACYGQRIAspEWJMk52gYAAAAAAAAmKkESALKRLySWCwlH25hIAgAAAAAAABODIAkA2WhsSSw3JxxtYyIJAAAAAAAATAyCJABkYxQTSTp6+rPuBgAAAAAAABgBQRIAspFvTiwXcoMnkjjaBgAAAAAAACYGQRIAspESJGlOmEgiSAIAAAAAAAATgyAJANlIm0gSgyeSrBYkAQAAAAAAgAlBkASAbOQLieWkIElvcSD6+gey7ggAAAAAAAAYhiAJANlobEksN+cGH20T4XgbAAAAAAAAmAgESQDIxigmkkRErO4WJAEAAAAAAIB6EyQBIBv55sRyWpCks1eQBAAAAAAAAOpNkASAbAwZJCkNqjvaBgAAAAAAAOpPkASAbKQESSblStEUg0MjHT39WXcEAAAAAAAADEOQBIBs5Aupl5KOt+noNpEEAAAAAAAA6k2QBIBspEwkiUgOkjjaBgAAAAAAAOpPkASAbDSmB0mac72Dah2CJAAAAAAAAFB3giQAZGPIiSSDgyQmkgAAAAAAAED9CZIAkI18IfVS0tE2JpIAAAAAAABA/QmSAJCNfEvqpeaEiSSCJAAAAAAAAFB/giQAZGOoiSS5wRNJHG0DAAAAAAAA9SdIAkA2crmIhuQwSfLRNv1ZdwQAAAAAAAAMQ5AEgOw0NieWC4lH2wwOlwAAAAAAAAC1JUgCQHbyaUGSpKNtTCQBAAAAAACAehMkASA7+ZSjbXJJQZJi1t0AAAAAAAAAwxAkASA7o5hIslqQBAAAAAAAAOpOkASA7KQESZqjd1DNRBIAAAAAAACoP0ESALIziokka3r7Y2CglHVHAAAAAAAAwBAESQDITr6QWC7kBk8kiYjo7DWVBAAAAAAAAOpJkASA7DS2JJabEyaSRER0ON4GAAAAAAAA6kqQBIDspE0kiZSJJIIkAAAAAAAAUFeCJABkJ9+cWC6kTiTpz7IbAAAAAAAAYBiCJABkJyVI0pwzkQQAAAAAAAAmIkESALKTEiRpnZQcGFndLUgCAAAAAAAA9SRIAkB28oXE8uSUIImJJAAAAAAAAFBfgiQAZKexJbHckhYk6RUkAQAAAAAAgHoSJAEgO2kTSXK9ifUOE0kAAAAAAACgrgRJAMhOvjmxXMglB0Y6ugVJAAAAAAAAoJ4ESQDITlqQJJInknSaSAIAAAAAAAB1JUgCQHZGGSTp6OnPshsAAAAAAABgGIIkAGQnJUjSVOpLrJtIAgAAAAAAAPUlSAJAdvKFxHJjKW0iiSAJAAAAAAAA1JMgCQDZaWxJLOcHehLrgiQAAAAAAABQX4IkAGQnZSJJvtQbEaVBdUfbAAAAAAAAQH0JkgCQnXxz6qVC9A2qmUgCAAAAAAAA9SVIAkB2BEkAAAAAAABgvSJIAkB2hgyS9A6qdfYUo1QafOQNAAAAAAAAUBuCJABkJ19IvVTIDZ5IMlCK6O4byLIjAAAAAAAAYAiCJABkZ5RH20RErO5JrgMAAAAAAADZEyQBIDuN6UGS5oSjbSIiOnv6s+oGAAAAAAAAGIYgCQDZGcNEks6eYlbdAAAAAAAAAMMQJAEgOw1NEZFLvFTIJQdJOgRJAAAAAAAAoG4ESQDITi6XOpWkvSElSNItSAIAAAAAAAD1IkgCQLbyhcTy1MaBxHpnryAJAAAAAAAA1IsgCQDZSplIMqWxP7HuaBsAAAAAAACoH0ESALLVmHa0TXJgpFOQBAAAAAAAAOpGkASAbKVMJGnLp0wk6RYkAQAAAAAAgHoRJAEgW/lCYrktZSJJR09ywAQAAAAAAADIniAJANlKmUjS6mgbAAAAAAAAmHAESQDIVlqQZFJfYr1DkAQAAAAAAADqRpAEgGylBElacmlH2wiSAAAAAAAAQL0IkgCQrXwhsdySMpHE0TYAAAAAAABQP4IkAGSrsSWx3ByOtgEAAAAAAICJRpAEgGylTCRpjt7EuiAJAAAAAAAA1E++3g3A+mTJkiXxxz/+MZ555pno6OiImTNnxjbbbBNz586NxsbGuvZ23333xaJFi+Lpp5+OiIgtt9wydtxxx3jDG95Q1fe8+OKLMW/evHj66adj+fLlsckmm8SWW24Zc+fOjY022qiq7+JVIt+cWG5KmUjiaBsAAAAAAACoH0ESGIEf/ehHcemll8Zdd92VeH3jjTeOo446Ki644ILYZJNNatZXX19fXHLJJXHFFVfEo48+mnjP9ttvHyeddFJ88pOfHFfY5f77748LLrggfvGLX0Rv7+BJEoVCIQ499NA499xzY8899xzVsw844ID43e9+N+berrzyyjj++OPHvJ+MpQZJkieSdPb0Z9kNAAAAAAAAMARH28AQOjo64v3vf3+8733vSw2RRESsXLkyvvGNb8Tuu+8et956a016W7RoUey7775x1llnpYZIIiIWL14cn/nMZ2LOnDmxePHiMb3roosuitmzZ8eNN96YGCKJiOjp6Ykbb7wxZs+eHV/84hfH9B5epVKCJI2l5P8u9fYPRE9RmAQAAAAAAADqwUQSSNHf3x9HHXVU/OIXv6iob7rppvGGN7whpk6dGo8++mjcf//9USqVIiLiueeei8MPPzx+9atfxX777ZdZb3/961/jwAMPjMcff7yivv3228duu+0WpVIpHnzwwYqAyb333hsHHXRQ3H333TFjxowRv+vf//3f45xzzqmotbS0xD777BMzZ86MZ555Jv7whz9Ed3d3RET09vbGmWeeGblcLs4444xxfJW8auQLyeWB5CBJxMtTSQr5hqw6AgAAAAAAAFIIkkCKz3zmMxUhksbGxrj00kvj5JNPjqampnL9oYceipNOOqk8saSnpyeOOOKIeOCBB2LmzJlV72tgYCCOOOKIihDJzJkz46qrroqDDjqo4t5bbrklTjjhhPjrX/8aERFLliyJd7/73XHnnXdGLpcb9l0///nP47Of/WxF7eSTT45/+7d/qzjC5/nnn4+zzz47rrjiinLtzDPPjNe97nVxyCGHjPprXLJkyajur+VxQoxBY0tieeggSTE2bm1KvQ4AAAAAAABkQ5AEEjz22GNx+eWXV9R++MMfxuGHHz7o3l133TV+/etfx9ve9rZymGTFihVx/vnnx3/+539Wvberr7465s+fX15vvPHGMW/evNh2220H3XvIIYfEvHnzYu+9944XXnghIiLmzZsX1113XRx99NFDvqe/vz8+/elPl6etREScfvrpcemllw66d9NNN41vf/vb0dbWFl/5ylciIqJUKsWnPvWpOPDAA6OhYXSTJZK+FtZjKRNJGga6U7d09BSz6gYAAAAAAAAYwqR6NwAT0fnnnx99fX3l9fHHH58YInlFS0tLXHXVVRWTSr7zne/EY489VtW++vv749xzz62oXXrppUMGL7bbbrtB4Y/PfvazMTAwMOS7vve978XChQvL65122ikuvPDCIfdcdNFFsdNOO5XXDz30UFx99dVD7mEDkG9OLE/q70nd0ilIAgAAAAAAAHUhSALr6Orqih/96EcVtTPPPHPYfTvuuGMcccQR5XWxWIxrrrmmqr3deeedFce+bLnllvGBD3xg2H0f/OAHY8sttyyvH3300Zg3b96Qe773ve9VrE8//fQoFJInS7yiUCjEaaedNuRz2AClBElyxZ5obEg+Ymm1IAkAAAAAAADUhSAJrOPWW2+NNWvWlNdz5syJnXfeeUR7TzjhhIr1T37yk6r2dsMNN1SsP/ShD43o2JiGhoZBgZOheluxYkXccccd5XVTU1Mcc8wxI+rx2GOPjcbGxvL6d7/7XaxcuXJEe3mVSgmSRLE7WgvJJ6yZSAIAAAAAAAD1IUgC67jlllsq1gcccMCI9+6///6Rz//9F+P3339/PPfcc9VqbVy9rXvvzTffnHrvbbfdFv39/eX13nvvHVOmTBnRe9rb22OvvfYqr4vFYtx2220j7pNXoSGCJG2CJAAAAAAAADChCJLAOhYsWFCxnjNnzoj3tra2xute97qK2oMPPliVvnp6emLx4sUVtX333XfE++fOnVuxXrRoUfT29ibeO57vQdK7qvU9YD2VTzkSqb83pjQl/2todbcgCQAAAAAAANSDIAms4+GHH65Yb7/99qPaP2vWrIr1Qw89NO6eIiIWLlxYMSVkxowZ0d7ePuL97e3tsckmm5TX/f398cgjjyTeu27Ptf4enHbaafGmN70pZsyYEU1NTbHxxhvHDjvsEO9617vii1/8YmrfTFCNLamXNioMJNY7e/oT6wAAAAAAAEC2BElgLStXroyVK1dW1LbeeutRPWPd+xctWjTuviJi0DSS0faVtCett/G+a7zfg69+9avxhz/8IZ5//vno6+uLF154IRYvXhw///nP48wzz4xddtkljjzyyHj00UdH9VzqJG0iSURs1JgSJOk1kQQAAAAAAADqIV/vBmAiefHFFyvWkydPjtbW1lE9Y8aMGRXrl156abxtRcTg3tZ9z0iMtLfxviur78ErBgYG4oYbbohf//rX8V//9V/xnve8p6rPj4hYtmxZPP/886Pas24Ah7/JN6demtaUHCTp6BEkAQAAAAAAgHoQJIG1dHR0VKxbWtKP5Eiz7p7Vq1ePq6dX1LK38b5rrN+D173udXHooYfGnnvuGdtvv31stNFG0dPTE8uWLYu77rorrrvuunjggQfK969atSqOOuqouOmmm+Id73jHqHoczn/8x3/E+eefX9VnbrCGCJK0NyYfYdPRLUgCAAAAAAAA9SBIAmtZN0DR3Jz+C/A064Yo1n3mWNWyt/G+a7Tfg2OOOSb+7//9v7Hbbrul3vOP//iPcc4558TVV18dH/7wh8vhlP7+/jjqqKPiL3/5S2y55Zaj6pMaGSpIki9G0ilrnSaSAAAAAAAAQF0M/u0dUJbL5WqyZyxq2dto9432/pNPPnnIEMnajj322Pj1r38dkydPLtc6OjpMD5nI8oXUS1PyKRNJBEkAAAAAAACgLkwkgbW0tbVVrLu6ukb9jHX3rPvMsaplb21tbfHCCy+M+V1ZfQ9esc8++8QXvvCF+OQnP1muffe7343LLrssWltbq/KOj3zkI/G+971vVHsWL14cRxxxRFXe/6oyxESSKQ39EdE4qN7ZK0gCAAAAAAAA9SBIAmsRJPl7fSIHSSJeDnqcd955sWrVqoiI6O3tjd/85jdx2GGHVeX5M2bMiBkzZlTlWRu8hsaI3KSI0sCgS20NyYGRjm5BEgAAAAAAAKgHR9vAWqZOnVqxXrNmTXR2do7qGcuWLatYb7TRRuNtKyIG9/b888+P+hkj7W2878rqe7C2QqEQb33rWytqf/7zn6v+Hqogl0udStKaFiTpST7yBgAAAAAAAMiWIAmsZfr06TFt2rSK2hNPPDGqZzz++OMV6x122GHcfSU9Z933jMRIexvvu7L6Hqxr2223rViPJVxDjeQLieXJk5KDJJ09JpIAAAAAAABAPQiSwDp22WWXivXixYtHtf+xxx4b8nljtdNOO0VDQ0N5vWzZsli9evWI969atSqWL19eXjc0NKQGPCbq92BdLS0tFeuxHPdDjeRbEsuTJ/Ul1rv6+qN/oJRlRwAAAAAAAEACQRJYx+67716xvuuuu0a8t7Ozc9DxKus+b6wKhULMmjVrzL3NmzevYr3DDjtEoZA8JWI834OIiN///vdDPq9a1g7GRERssskmmbyHKkiZSNKSSw6SRER0mEoCAAAAAAAANSdIAus45JBDKta//e1vR7z3jjvuiGLx77/8fsMb3hCbbbZZtVobV2/r3nvooYem3nvggQdWTD+59957Rzz9ZPXq1XHfffeV1/l8Pg488MAR9zka8+fPr1hvscUWmbyHKsg3J5abhwiSON4GAAAAAAAAak+QBNZx8MEHVxyZctddd8Vf/vKXEe296qqrKtbvfve7q9naoOd9//vfj/7+/mH39ff3xw9+8IMR97bJJpvEfvvtV1739vbGNddcM6Ier7766ujr+3s44C1veUtsvPHGI9o7Gg888EA88MADFbUDDjig6u+hShpTgiTRm7pFkAQAAAAAAABqT5AE1jF58uR473vfW1G7+OKLh933yCOPxA033FBe5/P5OOaYY6ra2/777x/bbbddef3UU08NCogk+cEPfhBPP/10eT1r1qx485vfPOSeD33oQxXryy67LHp6eobc09PTE1/5ylcqascdd9yw/Y1Wf39/nH766RW17bffPnbdddeqv4sqSZlI0hTpE0lWC5IAAAAAAABAzQmSQILzzjsvGhsby+urrroqbrrpptT7u7u744QTToje3r9PVzjxxBNj1qxZQ74nl8tV/BnuqJqGhoY4//zzK2qf/OQnY+nSpal7li5dOih08YUvfCEmTRr6f/7HHXdc7LTTTuX1woUL4+yzzx5yz1lnnRULFy4sr3fdddc49thjh9zzta99Lbq7u4e8Z229vb3xL//yL/HrX/+6on7uueeO+BnUQb6QWG4s9cakXPIWE0kAAAAAAACg9gRJIMFrX/vaOO200ypq733ve+PrX/96RVgkIuLhhx+Ot73tbTFv3rxybfr06ZkFG4499tiYPXt2eb1y5cqYO3du/PKXvxx076233hpz5syJF154oVybO3duHHXUUcO+p6GhIb785S9HLvf33/Jfeumlccopp8SKFSsq7l2+fHmcfPLJcdlll5VruVwuLrnkkmhoaBjyPaeeempst912ccYZZ8T8+fOjWEwODxSLxfjpT38as2fPjiuvvLLi2tvf/vZhAyvUWcpEklyxJ1qb8onXBEkAAAAAAACg9nKlUqlU7yZgIurv7493vetdcfPNN1fUZ8yYEXvttVdMmTIlHnvssbjvvvti7f8ZNTU1xa9+9avYf//9h33H2iGNiIjf/OY3ccABBwy779lnn4199903nnjiiYr6DjvsELvttluUSqV48MEHY/HixRXXt91227j77rtjs802G/Ydr/j3f//3OOeccypqLS0tMXv27Nh8883j2WefjXvuuSe6uroq7rn44ovjX//1X4d9/rrfg0KhELvttlvMnDkzpk6dGn19fbFs2bK49957o6OjY9D+N77xjXH77bfHlClTRvw1ZeXBBx+M3XffvbxesGBB7LbbbnXsaAK5/riIh24cXH/jP8ecBw6LZ18aPJXmy+/bI96791bZ9wYAAAAAwHrLz+YBqi/5r4ED0dDQENdff32cdNJJcd1115Xry5Yti1tuuSVxz4wZM+K73/3uiEIk4zFz5sy47bbb4uijj47777+/XF+0aFEsWrQocc9ee+0V11133ahCJBERZ599duRyuTj33HOjr68vIiK6urpSj+FpbGyMz3/+8yMKkSTp6emJ++67b9j7crlcfPzjH4+LL744mpuTp10wgaRMJIliT7QWkv9V1NHdl2FDAAAAAAAAQBJH28AQ2tra4tprr40f/vCHse+++6bet/HGG8eHP/zhWLBgQRxyyCE16W3HHXeM+fPnx4UXXhivfe1rU++bNWtWXHjhhXH33XfH9ttvP6Z3nXXWWTF//vw4/PDDo6mpKfGepqamOPzww+Oee+6JM888c8TP/tKXvhTveMc7Yvr06SO6f9NNN42PfvSj8dBDD8Xll18uRLK+yBeS68Xu1CBJZ29/hg0BAAAAAAAASRxtA6OwZMmSuO++++KZZ56Jzs7O2HzzzWObbbaJN7/5zakBi1q5995745FHHolnnnkmIiK22GKL2HHHHWPvvfeu6nteeOGFmDdvXjz99NOxYsWKmD59emy55ZYxd+7cmDZt2rie/dRTT8XChQvjqaeeihUrVkRXV1c0NDTEtGnTYpNNNok999wzZs2aVaWvpPqMzxvCzWdGzP/PwfWd3hnHdp4av1+8YtClDx8wK848ZOcaNAcAAAAAwPrKz+YBqs/RNjAK2223XWy33Xb1biPR3nvvXfXQSJJp06bFO9/5zkyevdVWW8VWW22VybOpsyEmkrSlTSTpKWbYEAAAAAAAAJDE0TYAZC+fcgRRsSf1aJuObkESAAAAAAAAqDVBEgCylxok6UqdSNJhIgkAAAAAAADUnCAJANkbw0SSzl5BEgAAAAAAAKg1QRIAspcvJNeL3UNMJOnPsCEAAAAAAAAgiSAJANlrbEmuF3vSgyTdfRk2BAAAAAAAACQRJAEge2kTSfq60o+2MZEEAAAAAAAAak6QBIDs5ZuT60NMJOnsKWbYEAAAAAAAAJBEkASA7KVNJCl2px9t01uMUqmUYVMAAAAAAADAugRJAMheviW5PtAXrY3Jl0qliDW9jrcBAAAAAACAWhIkASB7aUfbRMSUfHpYxPE2AAAAAAAAUFuCJABkL+1om4hoy6eHRToESQAAAAAAAKCmBEkAyF5jytE2EdE6SZAEAAAAAAAAJgpBEgCyN8REksmCJAAAAAAAADBhCJIAkL18c+qlhv6eaGlsSLzW2dOfVUcAAAAAAABAAkESALI3RJAkit3R1pxPvNRpIgkAAAAAAADUlCAJANkbLkhSSA6SrBYkAQAAAAAAgJoSJAEgew35iFzy8TVR7I7WQtrRNoIkAAAAAAAAUEuCJADURtpUkmJPtDY52gYAAAAAAAAmAkESAGqjMSVI0tcVU5qTgyQdgiQAAAAAAABQU4IkANTGUBNJCilBkm5BEgAAAAAAAKglQRIAaiNfSK4Xu1ODJJ29giQAAAAAAABQS4IkANRGviW5XuyOKWkTSXr6M2wIAAAAAAAAWJcgCQC1MYaJJB3dfRk2BAAAAAAAAKxLkASA2sg3J9eLPelH25hIAgAAAAAAADUlSAJAbTSmBEn6uqKt0JB4qaOnmGFDAAAAAAAAwLoESQCojSEmkrQVGhMvdfYKkgAAAAAAAEAtCZIAUBv5QnK92B2taRNJuotRKpUybAoAAAAAAABYmyAJALWRb0muF3uirZBPvjRQip7iQIZNAQAAAAAAAGsTJAGgNlInknRFa0qQJCKis8fxNgAAAAAAAFArgiQA1Ea+Obk+xESSiIjOnv6MGgIAAAAAAADWJUgCQG2kTiTpHjJIsrqnL6OGAAAAAAAAgHUJkgBQG40tyfW+7mGOtjGRBAAAAAAAAGpFkASA2hhiIklTflI0NST/K6mzp5hhUwAAAAAAAMDaBEkAqI18c3K92BMREW3NyVNJOgRJAAAAAAAAoGYESQCojdQgSVdERLQWGhIvC5IAAAAAAABA7QiSAFAbw0wkaW1KnkjiaBsAAAAAAACoHUESAGojX0iuF7sjImKKo20AAAAAAACg7gRJAKiNxpbk+isTSQomkgAAAAAAAEC9CZIAUBtpE0n6uiIiPUhiIgkAAAAAAADUjiAJALWRb06ul/oj+ovR1pQWJOnPsCkAAAAAAABgbYIkANRG2kSSiIhid7Q1O9oGAAAAAAAA6k2QBIDayLekXyt2px9t0y1IAgAAAAAAALUiSAJAbQw3kaTQkHipw0QSAAAAAAAAqBlBEgBqI9+cfq3YkzqRpLNXkAQAAAAAAABqRZAEgNpoHOJom76uaEsLkphIAgAAAAAAADUjSAJAbQx5tE1PapBkdbcgCQAAAAAAANSKIAkAtTHk0TbdqUfb9BQHotg/kFFTAAAAAAAAwNoESQCojUkNEZMak68Vu1MnkkREdPb0Z9QUAAAAAAAAsDZBEgBqJ20qyTBBko5ex9sAAAAAAABALQiSAFA7+UJyfYijbSIiOroFSQAAAAAAAKAWBEkAqJ3UiSQ9Q08k6REkAQAAAAAAgFoQJAGgdhpTgiR9XdHcOCkaJuUSL3cKkgAAAAAAAEBNCJIAUDtDTCTJ5XLR2tSQeFmQBAAAAAAAAGpDkASA2skXkuvF7oiI1ONtVguSAAAAAAAAQE0IkgBQO/mW5PrfgiStKUESE0kAAAAAAACgNgRJAKid4SaSNAuSAAAAAAAAQD0JkgBQO/nm5HqxJyIcbQMAAAAAAAD1JkgCQO00pgRJ+roiIqK1yUQSAAAAAAAAqCdBEgBqZ5iJJK0pE0k6e/qz6ggAAAAAAABYiyAJALWTLyTXi90RETGlOTlI0mEiCQAAAAAAANSEIAkAtZNvSa6XJ5I0JF7u6BYkAQAAAAAAgFoQJAGgdlInknRFxBBH2/QKkgAAAAAAAEAtCJIAUDv55uT63yaSTEkJkjjaBgAAAAAAAGpDkASA2kmdSNIdEUNMJBEkAQAAAAAAgJoQJAGgdhpbkut9QwdJOroFSQAAAAAAAKAWBEkAqJ1hJpK0pU0k6e2PgYFSVl0BAAAAAAAAfyNIAkDt5JuT68WeiEgPkkRErOnrz6IjAAAAAAAAYC2CJADUTmqQpCsi0o+2iYjo7HG8DQAAAAAAAGRNkASA2hnHRJLV3YIkAAAAAAAAkDVBEgBqJ19Irhe7I0qlaC00pG41kQQAAAAAAACyJ0gCQO00tiTXSwMRA8VobXK0DQAAAAAAANSTIAkAtZM2kSQioq8rJk3KRWtT8lSS1YIkAAAAAAAAkDlBEgBqJ9+cfq3YExERrYXkqSQmkgAAAAAAAED2BEkAqJ2hJpIUuyMiYkpzcpBkVVdfFh0BAAAAAAAAaxEkAaB28i3p1/4WJJna0ph4+aUuE0kAAAAAAAAga4IkANTOCCaSpAdJTCQBAAAAAACArAmSAFA7+eb0a8WeiBAkAQAAAAAAgHoSJAGgdoYKkvR1RYQgCQAAAAAAANSTIAkAtTNpUkRDU/K1YSaSrBIkAQAAAAAAgMwJkgBQW2lTSYrdERHRbiIJAAAAAAAA1I0gCQC1NUyQJG0iyYtdvVl1BAAAAAAAAPyNIAkAtTXGIImJJAAAAAAAAJA9QRIAaitfSK4PEyTp7huInmJ/Vl0BAAAAAAAAIUgCQK01pk0k6YmIiKmTk4MkEaaSAAAAAAAAQNYESQCorbSjbfq6IiJ9IklExCpBEgAAAAAAAMiUIAkAtZUWJHllIskQQRITSQAAAAAAACBbgiQA1Fa+kFwvdkdEREtjQzQ25BJvESQBAAAAAACAbAmSAFBbqRNJXg6S5HK51KkkgiQAAAAAAACQLUESAGprmCBJRPrxNi+tESQBAAAAAACALAmSAFBbqUGSnvLH9IkkxSw6AgAAAAAAAP5GkASA2mpMCZL0dZU/OtoGAAAAAAAA6kOQBIDaGtdEEkESAAAAAAAAyJIgCQC1lS8k14vd5Y+CJAAAAAAAAFAfgiQA1Fa+Jbk+gokkqwRJAAAAAAAAIFOCJADUVupEkq7yx3YTSQAAAAAAAKAuBEkAqK18c3J9BBNJBEkAAAAAAAAgW4IkANRW6kSS7vJHQRIAAAAAAACoD0ESAGqrsSW53jd8kKSrrz96iwNZdAUAAAAAAACEIAkAtTaSiSSTk4MkEaaSAAAAAAAAQJYESQCorXxzcr3YU/6YNpEkQpAEAAAAAAAAsiRIAkBtpQZJuiJKpYgQJAEAAAAAAIB6ESQBoLbSgiQREf29ERHR0tgQjQ25xFtWCZIAAAAAAABAZgRJAKitfCH9WrE7IiJyuVzqVBITSQAAAAAAACA7giQA1FZjS/q1Yk/5Y7sgCQAAAAAAANScIAkAtTXURJK+rvJHE0kAAAAAAACg9gRJAKitfHP6tbUmkgiSAAAAAAAAQO0JkgBQW0NNJCl2lz8KkgAAAAAAAEDtCZIAUFv5lvRrgiQAAAAAAABQV4IkANSWiSQAAAAAAAAwYQmSAFBbuVxEQ0qYpNhT/pgWJFklSAIAAAAAAACZESQBoPYam5PrfV3lj+0mkgAAAAAAAEDNCZIAUHv5lCDJCCaSCJIAAAAAAABAdgRJAKi9fNrRNt3lj2lBkjW9/dHXP5BFVwAAAAAAALDBEyQBoPbyLcn1EQRJIkwlAQAAAAAAgKwIkgBQe+OYSBIhSAIAAAAAAABZESQBoPbyzcl1QRIAAAAAAACoK0ESAGqvMS1I0lP+OLmpIfKTcom3CZIAAAAAAABANgRJAKi9tIkkfV3lj7lcLnUqySpBEgAAAAAAAMiEIAkAtZcvJNfXmkgSkX68jYkkAAAAAAAAkA1BEgBqL20iSbG7YtmeFiRZI0gCAAAAAAAAWRAkAaD2RhgkMZEEAAAAAAAAakuQBIDaG2eQ5EVBEgAAAAAAAMiEIAkAtZcvJNeLPRVLE0kAAAAAAACgtgRJAKi9xpbkel9XxVKQBAAAAAAAAGpLkASA2hvnRJJVgiQAAAAAAACQCUESAGov35xcL3ZXLE0kAQAAAAAAgNoSJAGg9lKDJJUTSdoFSQAAAAAAAKCmBEkAqL3UIElXxTJtIsma3v7o6x+odlcAAAAAAACwwRMkAaD2RjiRZKPJyUGSCFNJAAAAAAAAIAuCJADUXr6QXC92VyzTJpJECJIAAAAAAABAFgRJAKi9xpbkep8gCQAAAAAAANSTIAkAtZc2kaS/J6JUKi8nNzVEflIu8VZBEgAAAAAAAKg+QRIAai/fnH6t2FP+mMvlUqeSrBIkAQAAAAAAgKoTJAGg9oYMknRVLNOCJCaSAAAAAAAAQPUJkgBQeyOcSBIR0Z4WJFkjSAIAAAAAAADVJkgCQO3lC+nXit0VSxNJAAAAAAAAoHYESQCovcaW9GvrTCQRJAEAAAAAAIDaESQBoPaGmkjS11WxFCQBAAAAAACA2hEkAaD28s3p10wkAQAAAAAAgLoRJAGg9hqaIiKXfK3YXbEUJAEAAAAAAIDaESQBoPZyufSpJCMMkqwSJAEAAAAAAICqEyQBoD7yheT6OkGSdhNJAAAAAAAAoGYESQCoj9SJJD0Vy7SJJJ29/dHXP1DtrgAAAAAAAGCDJkgCQH00pgRJ+roqlmlBkgjH2wAAAAAAAEC1CZIAUB8jnUgyOT1I4ngbAAAAAAAAqC5BEgDqI19Irhe7K5ZDTSQRJAEAAAAAAIDqEiQBoD7yLcn1dYIkrU0N0TApl3irIAkAAAAAAABUlyAJAPUxwokkuVwudSqJIAkAAAAAAABUlyAJAPWRb06uF3sGldKCJKsESQAAAAAAAKCqBEkAqI/GtCBJ96BSu4kkAAAAAAAAUBOCJADUR9pEkr7BQRJH2wAAAAAAAEBtCJIAUB/5QnI9YSKJIAkAAAAAAADUhiAJAPWRNpEkMUiST7xVkAQAAAAAAACqS5AEgPoYVZDERBIAAAAAAACoBUESAOojNUjSM6iUHiQpVrMjAAAAAAAA2OAJkgBQH/lCcn0UE0lWmUgCAAAAAAAAVSVIAkB9NLYk1/scbQMAAAAAAAD1IkgCQH2MYiJJe0qQpKOnGMX+gWp2BQAAAAAAABs0QRIA6iPfnFwv9gwqpU0kiYhY1V2sVkcAAAAAAACwwRMkAaA+UoMkIz/aJsLxNgAAAAAAAFBNgiQA1IcgCQAAAAAAAEw4giQA1McogiRthXw0TMol3i5IAgAAAAAAANUjSAJAfeQLyfX+3oiBgYpSLpeL9uZ84u0vrumtdmcAAAAAAACwwRIkAaA+GlvSr43ieJtVJpIAAAAAAABA1QiSAFAfaRNJIkYVJHG0DQAAAAAAAFSPIAkA9ZFvTr9W7BlUahckAQAAAAAAgMwJkgBQH0MGSboGlUwkAQAAAAAAgOwJkgBQH6OcSLLRZEESAAAAAAAAyJogCQD1kS+kXyt2DyqZSAIAAAAAAADZEyQBoD4aW9KvJUwkSQ+SFKvVEQAAAAAAAGzwBEkAqI9J+Yhcyr+G+roGldKCJKtMJAEAAAAAAICqESQBoD5yuYh8c/K1UU0kESQBAAAAAACAahEkAaB+UoMk3YNK7SlBko6eYhT7B6rZFQAAAAAAAGywBEkAqJ9RBEnSJpJERKzqLlarIwAAAAAAANigCZIAUD/5QnJ9lEESx9sAAAAAAABAdQiSAFA/qRNJegaVBEkAAAAAAAAge4IkANRPY0qQpK9rUKmtkI+GSbnE2wVJAAAAAAAAoDoESQCon1FMJMnlctHenE+8XZAEAAAAAAAAqkOQBID6yReS68XuxHLa8TaCJAAAAAAAAFAdgiQA1E++Jbk+yiDJKkESAAAAAAAAqApBEgDqZ5QTSdpNJAEAAAAAAIBMCZIAUD/55uR6sSexnHq0zRpBEgAAAAAAAKgGQRIA6qcxLUgyuqNtTCQBAAAAAACA6hAkAaB+0iaS9AmSAAAAAAAAQD0IkgBQP/lCct1EEgAAAAAAAKgLQRIA6idtIkmxJ7EsSAIAAAAAAADZEiQBoH5SgyRdieW0IMkqQRIAAAAAAACoCkESAOqnShNJVvcUo3+gVK2uAAAAAAAAYIMlSAJA/eQLyfVid2K5PSVIEmEqCQAAAAAAAFSDIAkA9dPYklzvSw6SpE0kiYh4SZAEAAAAAAAAxk2QBID6GeVEkqmTBUkAAAAAAAAgS4IkANRPvjm5XuxJLLc15WNSLnmLIAkAAAAAAACMnyAJAPWTGiRJnkgyaVIu2lOOtxEkAQAAAAAAgPETJAGgftKCJAN9EQP9iZemCpIAAAAAAABAZgRJAKiftCBJROpUEkESAAAAAAAAyI4gCQD1ky+kXyv2JJbTgiSrBEkAAAAAAABg3ARJAKifxpb0a31dieV2E0kAAAAAAAAgM4IkANTPkBNJHG0DAAAAAAAAtSZIAkD95JvTr43yaBtBEgAAAAAAABg/QRIA6mfIIEny0TaCJAAAAAAAAJAdQRIA6sdEEgAAAAAAAJhQBEkAqJ+GfESuIflasTuxLEgCAAAAAAAA2REkAaC+GluS66OcSLK6uxj9A6VqdQUAAAAAAAAbJEESAOorX0iu93UlltOCJBERq7tNJQEAAAAAAIDxECQBoL7yzcn1UU4kiYh4cY0gCQAAAAAAAIyHIAkA9ZUaJOlOLLcPESR5qUuQBAAAAAAAAMYjX+8GYH2yZMmS+OMf/xjPPPNMdHR0xMyZM2ObbbaJuXPnRmNj+i+3a+G+++6LRYsWxdNPPx0REVtuuWXsuOOO8YY3vKGq73nxxRdj3rx58fTTT8fy5ctjk002iS233DLmzp0bG220UVXfxQZilEGSKYV8TMpFDJQGXxMkAQAAAAAAgPERJIER+NGPfhSXXnpp3HXXXYnXN9544zjqqKPiggsuiE022aRmffX19cUll1wSV1xxRTz66KOJ92y//fZx0kknxSc/+clxhV3uv//+uOCCC+IXv/hF9Pb2DrpeKBTi0EMPjXPPPTf23HPPMb8nzZo1a+J1r3tdPPbYYxX14447Lq666qqqv48ayheS6ylBkkmTctHe0ph4jI0gCQAAAAAAAIyPo21gCB0dHfH+978/3ve+96WGSCIiVq5cGd/4xjdi9913j1tvvbUmvS1atCj23XffOOuss1JDJBERixcvjs985jMxZ86cWLx48ZjeddFFF8Xs2bPjxhtvTAyRRET09PTEjTfeGLNnz44vfvGLY3rPUM4555xBIRJeJVInkvSkbpmacryNIAkAAAAAAACMj4kkkKK/vz+OOuqo+MUvflFR33TTTeMNb3hDTJ06NR599NG4//77o1R6+YyN5557Lg4//PD41a9+Ffvtt19mvf31r3+NAw88MB5//PGK+vbbbx+77bZblEqlePDBBysCJvfee28cdNBBcffdd8eMGTNG/K5///d/j3POOaei1tLSEvvss0/MnDkznnnmmfjDH/4Q3d0vT4/o7e2NM888M3K5XJxxxhnj+Cr/7u67746vfvWrVXkWE1BjSpCkryt1iyAJAAAAAAAAZMNEEkjxmc98piJE0tjYGF/72tfiqaeeiltvvTWuv/76uPfee2PBggUxZ86c8n09PT1xxBFHxLPPPptJXwMDA3HEEUdUhEhmzpwZt956ayxatChuvPHG+OlPfxqLFy+Om2++OTbffPPyfUuWLIl3v/vd5eDLcH7+85/HZz/72YraySefHE888UT87ne/i2uvvTb+93//N5544ok46aSTKu4788wz45ZbbhnHV/qy3t7eOPHEE2NgYCAiIqZMmTLuZzLBVHEiySpBEgAAAAAAABgXQRJI8Nhjj8Xll19eUfvhD38YH/vYx6Kpqamivuuuu8avf/3rijDJihUr4vzzz8+kt6uvvjrmz59fXm+88cYxb968OOiggwbde8ghh8S8efNi2rRp5dq8efPiuuuuG/Y9/f398elPf7oidHL66afHN7/5zdhkk00q7t10003j29/+dnziE58o10qlUnzqU5+K/v7+0Xx5g1xwwQXx0EMPRUTENttsE6eccsq4nscElC8k14vdqVvaTSQBAAAAAACATAiSQILzzz8/+vr+/gvp448/Pg4//PDU+1taWuKqq66qCJl85zvficcee6yqffX398e5555bUbv00ktj2223Td2z3XbbxaWXXlpR++xnP1ue8JHme9/7XixcuLC83mmnneLCCy8ccs9FF10UO+20U3n90EMPxdVXXz3knqH86U9/iosvvri8/sY3vhGtra1jfh4TVL4luT5EkMTRNgAAAAAAAJANQRJYR1dXV/zoRz+qqJ155pnD7ttxxx3jiCOOKK+LxWJcc801Ve3tzjvvjCVLlpTXW265ZXzgAx8Ydt8HP/jB2HLLLcvrRx99NObNmzfknu9973sV69NPPz0KhZTJEX9TKBTitNNOG/I5I1UsFuOf//mfo1gsRkTE+9///jj00EPH9CwmuDFMJBEkAQAAAAAAgGwIksA6br311lizZk15PWfOnNh5551HtPeEE06oWP/kJz+pam833HBDxfpDH/pQNDQ0DLuvoaFhUOBkqN5WrFgRd9xxR3nd1NQUxxxzzIh6PPbYY6Ox8e+/5P/d734XK1euHNHetX3pS1+K++67LyJePr7nK1/5yqifwXoi35xcL/akbhEkAQAAAAAAgGwIksA6brnllor1AQccMOK9+++/f+Tz+fL6/vvvj+eee65arY2rt3Xvvfnmm1Pvve2226K/v7+83nvvvWPKlCkjek97e3vstdde5XWxWIzbbrttxH1GRCxcuDDOP//88vqSSy6JGTNmjOoZrEca04IkJpIAAAAAAABArQmSwDoWLFhQsZ4zZ86I97a2tsbrXve6itqDDz5Ylb56enpi8eLFFbV99913xPvnzp1bsV60aFH09vYm3jue70HSu0bzPRgYGIgTTzwxenpenkbxj//4j3H88ceP6v2sZ9ImkvQJkgAAAAAAAECtCZLAOh5++OGK9fbbbz+q/bNmzapYP/TQQ+PuKeLlKR1rTwmZMWNGtLe3j3h/e3t7bLLJJuV1f39/PPLII4n3rttzLb8HX//61+P3v/99RES0tLTEN7/5zVG9m/VQvpBcH8NEktXdxegfKFWjKwAAAAAAANggCZLAWlauXBkrV66sqG299dajesa69y9atGjcfUXEoGkko+0raU9ab+N911i/B0uXLo2zzz67vD733HNHHWJhPZQ2kaTYk7olLUgSEbG621QSAAAAAAAAGCtBEljLiy++WLGePHlytLa2juoZM2bMqFi/9NJL420rIgb3tu57RmKkvY33XWP9HvzLv/xLdHZ2RkTEHnvsEZ/61KdG9V7WU6lBkq7ULUMFSRxvAwAAAAAAAGOXr3cDMJF0dHRUrFtaWkb9jHX3rF69elw9vaKWvY33XWP5HnznO9+JX/3qVxERMWnSpPj2t78d+Xz9/hG1bNmyeP7550e1Z91JLozQGCaStAuSAAAAAAAAQCYESWAt6wYomptTfsE9hHVDFOs+c6xq2dt43zXa78EzzzwTn/70p8vrU089NfbZZ59RvbPa/uM//iPOP//8uvawwcgXkuvF7tQtUwr5yOUiSqXB1wRJAAAAAAAAYOwcbQNDyOVyNdkzFrXsbbT7Rnv/Rz7ykfJxOttss0184QtfGNV+1nONKRNv+tKDJJMm5aK9OXkqiSAJAAAAAAAAjJ0gCaylra2tYt3V1TXqZ6y7Z91njlUtexvvu0bzPbj22mvjpz/9aXn9jW98I1pbW0f1PtZzY5hIEhExNeV4G0ESAAAAAAAAGDtH28BaBEn+Xn/hhRfG/K6Rvmf58uVx6qmnltfvf//749BDDx3Vu7LykY98JN73vveNas/ixYvjiCOOyKahV7N8ytFJpf6I/mJEQ/K/qgRJAAAAAAAAoPoESWAtU6dOrVivWbMmOjs7RzUhY9myZRXrjTbaqBqtDert+eefH/UzRtrb1KlT48knnxzzu0b6nlNPPbX87I033ji+8pWvjOo9WZoxY0bMmDGj3m1sGNKCJBEvTyVpSA4iCZIAAAAAAABA9QmSwFqmT58e06ZNq5jG8cQTT8Quu+wy4mc8/vjjFesddtihKr2t+5x13zMSI+1thx12iAULFoz5XSN5z8KFC+O///u/y+tPfOITsWbNmli6dOmQz37xxRcr1h0dHRV7Jk2aFFtvvfWo+qXOhguSFEYXJFklSAIAAAAAAABjJkgC69hll11i3rx55fXixYtHFSR57LHHBj2vGnbaaadoaGiI/v7+iHh56sfq1atjypQpI9q/atWqWL58eXnd0NCQGiTZZZdd4oYbbiivFy9ePKpeR/I9WPf4m8997nPxuc99blTviYj48Y9/HD/+8Y/L66lTpw4KmzDB5Qvp14rdqZfaTSQBAAAAAACAqptU7wZgotl9990r1nfdddeI93Z2dsaf//znIZ83VoVCIWbNmjXm3tYOx0S8PCWkUEj+Bf54vgcREb///e+HfB5UaGxJv1bsSb3kaBsAAAAAAACoPkESWMchhxxSsf7tb3874r133HFHFIvF8voNb3hDbLbZZtVqbVy9rXvvoYcemnrvgQceGA0NDeX1vffeG6tXrx7Re1avXh333XdfeZ3P5+PAAw8ccZ9sgIY62qavK/WSIAkAAAAAAABUnyAJrOPggw+Olpa/T0i466674i9/+cuI9l511VUV63e/+93VbG3Q877//e+Xj7oZSn9/f/zgBz8YcW+bbLJJ7LfffuV1b29vXHPNNSPq8eqrr46+vr//Iv8tb3lLbLzxxoPu23PPPaNUKo36z7nnnlvxnOOOO67iumNt1kNDHm1jIgkAAAAAAADUkiAJrGPy5Mnx3ve+t6J28cUXD7vvkUceiRtuuKG8zufzccwxx1S1t/333z+222678vqpp54aFBBJ8oMf/CCefvrp8nrWrFnx5je/ecg9H/rQhyrWl112WfT0pP9SPyKip6cnvvKVr1TUjjvuuGH7YwM31ESSYnfqpdQgyRpBEgAAAAAAABgrQRJIcN5550Vj499/SX3VVVfFTTfdlHp/d3d3nHDCCdHb21uunXjiiTFr1qwh35PL5Sr+DHdUTUNDQ5x//vkVtU9+8pOxdOnS1D1Lly6N008/vaL2hS98ISZNGvp//scdd1zstNNO5fXChQvj7LPPHnLPWWedFQsXLiyvd9111zj22GOH3AMxqSFiUnIoJIqjP9pmdU8xBgZK1egMAAAAAAAANjiCJJDgta99bZx22mkVtfe+973x9a9/vSIsEhHx8MMPx9ve9raYN29euTZ9+vRBR7BUy7HHHhuzZ88ur1euXBlz586NX/7yl4PuvfXWW2POnDnxwgsvlGtz586No446atj3NDQ0xJe//OXI5XLl2qWXXhqnnHJKrFixouLe5cuXx8knnxyXXXZZuZbL5eKSSy6JhoaGUX19bKDSppKM4WibUilidXexGl0BAAAAAADABidf7wZgorroooviwQcfjJtvvjkiIvr6+uLjH/94fP7zn4+99torpkyZEo899ljcd999USr9ffpBU1NT3HDDDTFz5sxM+po0aVLccMMNse+++8YTTzwRERHPPvtsHHzwwbHDDjvEbrvtFqVSKR588MFYvHhxxd5tt902fvKTn1SEQ4Zy2GGHxRe+8IU455xzyrVvfetb8f3vfz9mz54dm2++eTz77LNxzz33RFdX5eSIiy66KA455JBxfrVsMPKFiN7Vg+tjONomIuKlrr6YOjn9OgAAAAAAAJBMkARSNDQ0xPXXXx8nnXRSXHfddeX6smXL4pZbbkncM2PGjPjud78b+++/f6a9zZw5M2677bY4+uij4/777y/XFy1aFIsWLUrcs9dee8V1110Xm2222ajedfbZZ0cul4tzzz03+vr6IiKiq6sr9RiexsbG+PznPx//+q//Oqr3sIFrbEmuj2EiScTLQRIAAAAAAABg9BxtA0Noa2uLa6+9Nn74wx/Gvvvum3rfxhtvHB/+8IdjwYIFNZvCseOOO8b8+fPjwgsvjNe+9rWp982aNSsuvPDCuPvuu2P77bcf07vOOuusmD9/fhx++OHR1NSUeE9TU1Mcfvjhcc8998SZZ545pvewAcsXkut9Xcn1iJjSnI+04TqCJAAAAAAAADA2udLaZ3IAQ1qyZEncd9998cwzz0RnZ2dsvvnmsc0228Sb3/zm1IBFrdx7773xyCOPxDPPPBMREVtssUXsuOOOsffee1f1PS+88ELMmzcvnn766VixYkVMnz49ttxyy5g7d25Mmzatqu9a3zz44IOx++67l9cLFiyI3XbbrY4drUe+8eaI5xYMrh98YcScj6Rue/15t8aq7uKg+v89Zq945+uzOV4KAAAAAICJw8/mAarP0TYwCtttt11st9129W4j0d5771310EiSadOmxTvf+c7M38MGJt+cXC92D7lt6uTGxCCJiSQAAAAAAAAwNo62AaD+xhokaWlMrAuSAAAAAAAAwNgIkgBQf/lCcl2QBAAAAAAAAGpKkASA+kudSNIz5DZBEgAAAAAAAKguQRIA6q8xJUjS1zXktrQgySpBEgAAAAAAABgTQRIA6m+ME0naTSQBAAAAAACAqhIkAaD+8oXkerF7yG2OtgEAAAAAAIDqEiQBoP7yLcn1MQZJXuzqHW9HAAAAAAAAsEESJAGg/sY4kWSjlqbE+ktrTCQBAAAAAACAsRAkAaD+8s3J9WLPkNvSJpKs7inGwEBpvF0BAAAAAADABkeQBID6a0wLkoztaJtSKWJ1d3G8XQEAAAAAAMAGR5AEgPpLm0jSN7YgSUTES12OtwEAAAAAAIDREiQBoP7yheT6GCeSRAiSAAAAAAAAwFgIkgBQf2kTSYo9Q26b0pyPXC75miAJAAAAAAAAjJ4gCQD1lxok6Rpy26RJuZhSyCdeEyQBAAAAAACA0RMkAaD+xjiRJCJi6uTk420ESQAAAAAAAGD0BEkAqL98Ible7I4olYbcOrVFkAQAAAAAAACqRZAEgPprbEmulwYi+ocOhAiSAAAAAAAAQPUIkgBQf2kTSSJenkoyBEESAAAAAAAAqB5BEgDqL9+cfq3YM+TWtCDJKkESAAAAAAAAGDVBEgDqb8ggydATSdpNJAEAAAAAAICqESQBoP7GESRxtA0AAAAAAABUjyAJAPWXL6RfEyQBAAAAAACAmhEkAaD+GlvSrxV7htwqSAIAAAAAAADVI0gCQP01DDGRpK9ryK1pQZJV3X0xMFAaT1cAAAAAAACwwREkAaD+Jk2KaGhKvjbGiSSlUsTqnuJ4OwMAAAAAAIANiiAJABNDvjm5XuwecltakCQiYpXjbQAAAAAAAGBUBEkAmBgyCJK8JEgCAAAAAAAAoyJIAsDEMMYgyZRmQRIAAAAAAACoFkESACaGfCG5PkyQpGFSLqY05xOvCZIAAAAAAADA6AiSADAxNKZNJOkZdmva8TaCJAAAAAAAADA6giQATAxpR9v0dQ27VZAEAAAAAAAAqkOQBICJIS1IYiIJAAAAAAAA1IwgCQATQ2qQpHvYrYIkAAAAAAAAUB35ejcAr7j33ntjyZIlUSgUYpdddontt9++3i0BtZQvJNcFSQAAAAAAAKBmBEmouu7u7njmmWfK62222SYaGhpS77/pppvi1FNPjSeffLKiPmfOnPjWt74Vu+66a2a9AhNIBhNJVgmSAAAAAAAAwKg42oaqu+SSS2KHHXaIHXbYId761rfGpEnp/zW7/vrr48gjj4wnn3wySqVSxZ958+bF7Nmz4957761h90DdpAZJeobd2m4iCQAAAAAAAFSFIAlVd+ONN0apVIqIiBNPPDFyuVzifS+88EKccsopMTAwEBFRcV8ul4tcLhednZ1x5JFHRnf38BMJgPVcY0qQpK9r2K2OtgEAAAAAAIDqECShqrq6uuKPf/xjORRy2GGHpd77ta99LV566aXI5XJRKpViiy22iI9//ONx+umnx9Zbb10Oozz11FPx1a9+tSb9A3U0jokkgiQAAAAAAABQHYIkVNUDDzwQ/f39USqVorW1Nfbaa6/Ue3/wgx+UQyQ77bRTLFiwIC6//PK45JJL4oEHHoh99tknIiJKpVJcddVVNfoKgLrJF5LrxeEnEqUFSVZ19cXAQGk8XQEAAAAAAMAGRZCEqlqyZElEvHw0za677pp631/+8pdYvHhx+d4LLrggpk6dWr7e1tYWX/va18rrhQsXxpNPPplR18CEkG9Jro8jSDJQiujoLY6nKwAAAAAAANigCJJQVc8991z588yZM1Pvu+OOOyLi5WkjbW1t8e53v3vQPW9605tiq622Kq///Oc/V7FTYMLJYCJJRMRLaxxvAwAAAAAAACMlSEJVrVmzpvx5ypQpqff9/ve/j4iXp5G87W1vi3w+n3jf7rvvXv78xBNPVKlLYELKNyfXiz3Dbh0ySNIlSAIAAAAAAAAjJUhCVZVKpfLnvr70X97Omzev/Hn//fdPvW/69Onlz6tWrRpnd8CE1pgWJBl+Ikn7EEGSVYIkAAAAAAAAMGKCJFTV2lNI1j7mZm1//etfY/HixeX13LlzU59XLBbLn9cOqQCvQmkTSfqGD5I0TMrFlELyZCMTSQAAAAAAAGDkBEmoqi233DIiXg59PPDAA4n3/OIXvyh/LhQKsddee6U+78UXXyx/bm1trU6TwMSULyTXRzCRJCJ9KokgCQAAAAAAAIycIAlV9frXv778eeXKlXHrrbcOuufKK6+MiIhcLhdvetOborEx/UiKxx57rPx58803r2KnwISTNpGk2DOi7VMFSQAAAAAAAGDcBEmoqlmzZsUOO+wQuVwuSqVSfOQjH4klS5aUr19yySXx+9//vrw+/PDDU5/V0dFRcQTOrFmzsmkamBhSgyRdESM42mqjyYIkAAAAAAAAMF6CJFTdSSedFKVSKXK5XCxZsiR23nnneNOb3hTbbrtt/Ou//mvkcrmIiGhubo4PfOADqc/57W9/G6W//fI4n8/HbrvtVpP+gTpJC5JERPT3Drs9bSLJi4IkAAAAAAAAMGKCJFTdaaedFjvvvHNEvHx8TV9fX9x7773xxBNPlIMhuVwuPvnJT8amm26a+pwbbrihfO8ee+wRhUIh++aB+skP8b/xYvew2x1tAwAAAAAAAOMnSELVNTU1xa233ho777xzOTjyyoSSVz4feeSRcf7556c+o6OjI3784x+X97ztbW/LvnGgvhpb0q/1jT1IskqQBAAAAAAAAEYsX+8GeHV6zWteE3/84x/jv/7rv+Kmm26Kxx9/PCIidt555zjmmGPiyCOPHHL/VVddFatWrSqv3/nOd2baLzABjHMiSbuJJAAAAAAAADBugiRkprGxMU455ZQ45ZRTRr33xBNPjA9+8IPl9dSpU6vZGjAR5ZvTrxV7ht3uaBsAAAAAAAAYP0ESJqSWlpZoaRnimAvg1WfIIMnYj7YRJAEAAAAAAICRm1TvBgAgIjILkqzq6ouBgdJYuwIAAAAAAIANiiAJABNDvpB+bRxBkoFSREdvcaxdAQAAAAAAwAbF0TZMCIsWLYqbbroplixZEoVCIXbZZZd4z3veE9OmTat3a0Ct5HIvTyVJCo0Ue4bdnhYkiYh4aU1ftDenXwcAAAAAAABeJkhC1S1dujRuv/328voDH/hANDU1Jd5bKpXijDPOiMsvvzwGBgYqrn3yk5+Mr371q3H88cdn2S4wkeQLyUGSvq5htw4ZJOnqi9eMpy8AAAAAAADYQAiSUHVf+cpX4mtf+1pEROy9997xz//8z6n3nn322XHppZeW17lcLiJeDph0dHTEiSeeGKVSKU444YRsmwYmhnxzRLw0uD6CiSTtQwRJVnX1jaMpAAAAAAAA2HBMqncDvPr8z//8T5RKpYiIIQMgjzzySHzpS1+KXC5XESB5ZW8ul4tSqRQf//jH4+mnn86+caD+8oXketKUknU0TMrFlEJyPvJFQRIAAAAAAAAYEUESqmr58uXx6KOPltfveMc7Uu+99NJLK46zOeyww+LHP/5x/PSnP40jjzwySqVS5HK56Orqii9+8YuZ9g1MEPmW5PoIgiQRERu3JR+jtaKzd6wdAQAAAAAAwAZFkISqevDBB8ufN91009hmm20S7+vv748f//jH5UkkBx10UNx0003x7ne/O971rnfFj370o/jABz5QnlBy/fXXlyeVAK9i45hIEhGxcWtKkKRj+KNxAAAAAAAAAEESquzxxx+PiJePpdlll11S7/t//+//xYoVK8rhkM9+9rOD7vm3f/u3ctBk2bJl8fDDD2fQMTCh5JuT6yMMkkxvTQ6irDSRBAAAAAAAAEZEkISqWrFiRfnz9OnTU++74447yp9nzpwZb37zmwfd85rXvKYijLJgwYIqdQlMWI1pQZKRTRSZnjqRRJAEAAAAAAAARkKQhKrq6uoqf25tbU29b968eRHx8uSSgw46KPW+HXfcsfz5ueeeq0KHwISWNpGkryu5vo6N21KCJJ2OtgEAAAAAAICRECShqvL5fPnz2qGSdb0SJImI2G+//VLva2trK3/u6OgYZ3fAhJdPPprGRBIAAAAAAACoDUESqqq9vb38+amnnkq85+GHH45ly5aV13PmzEl93tphlIaGhip0CExo+ZbkerF7RNunp0wkWdkpSAIAAAAAAAAjIUhCVb32ta+NiIhSqRR/+tOfort78C9/f/rTn5Y/T5s2LXbZZZfU561cubL8ecqUKVXsFJiQUieSjDBI0pq8/4U1vdE/UBprVwAAAAAAALDBECShqvbcc8/I5XKRy+Wiu7s7/uu//qvierFYjCuuuCIiInK5XOy///5DPu8vf/lL+fNWW21V/YaBiSXfnFwfYZBk45SjbQZKES+uMZUEAAAAAAAAhiNIQlXNmDEj5s6dGxEvTyU588wz4/vf/36sWbMmli5dGkcffXQ89thj5fvf+973pj7rr3/9azz77LPl9Q477JBd48DEkDqRpGdE2zdpS9kfjrcBAAAAAACAkRAkoeo+8YlPRKlUilwuF52dnXH88cfHlClTYtasWXHDDTdELpeLiIiZM2cOGSS55ZZbyp/b2tpip512yrx3oM4aW5LrfV0j2j6ttTH12vIOQRIAAAAAAAAYjiAJVfee97wnjjzyyHKYpFQqlf9ERLl+ySWXRKGQPj3gJz/5SUS8fATOm970pnIABXgVG+dEkkK+IaYU8onXTCQBAAAAAACA4QmSkIlrrrkmTjzxxHJ45BWlUikKhUJcdtllcdRRR6Xuf/LJJ+Pmm28uh0cOPvjgTPsFJoh8c3K92D3iR0xva0qsr+wcWRgFAAAAAAAANmTJf20bxqmpqSm+/e1vx6c//em46aab4vHHH4+IiJ133jmOPPLI2GKLLYbcf/PNN8fuu+9eXr/rXe/KtF9ggqhCkGTj1qZYumLNoLqjbQAAAAAAAGB4giRkaqeddoozzjhj1PtOPvnkOPnkkzPoCJjQqjKRJPl4HEfbAAAAAAAAwPAcbQPAxJEaJBn5sTTTW5OPtlnhaBsAAAAAAAAYliAJABNHY3WOtkmywtE2AAAAAAAAMCxBEgAmjrSJJH2OtgEAAAAAAIBayNe7ATYcfX19ce+998Yf/vCHWLZsWaxcuTJyuVxMmzYtZsyYEfvss0/svffe0djYWO9WgXrJJ4dAor8nolSKyOWGfUT60TaCJAAAAAAAADAcQRIy9+CDD8Zll10W//3f/x3d3UNPFWhubo73v//98YlPfCJ23333GnUITBj5lvRrxZ70o2/WMr0tOUjywpre6B8oRcOk4cMoAAAAAAAAsKFytA2ZGRgYiM9+9rOx5557xpVXXhldXV1RKpWiVCoNuveVeldXV1x55ZWx5557xjnnnBP9/f116Byom7SJJBERxa4RPWLjlIkkpdLLYRIAAAAAAAAgnSAJmejv7493vetdceGFF0Z/f3+USqXI5XKR+9uxFK8ER9YOlqx9fWBgIC666KI47LDDhElgQ5IfYuJIsWdEj5jemh5GWel4GwAAAAAAABiSo23IxEc/+tG4+eabI+LlgMgrgZG99tor5s6dGzvvvHNMnTo1IiJeeumlWLhwYcybNy/uvffeij2//OUv48Mf/nB861vfqtvXAtTQkBNJhj4a6xVpE0kiIpZ39MSOm00ZbVcAAAAAAACwwRAkoeruueee+Na3vlUxfeSwww6Liy66KHbdddch9z788MNx1llnxU033VQOk3znO9+JE088MWbPnl2L9oF6amxJv9Y3siBJU35STGnOx+ru4qBrJpIAAAAAAADA0BxtQ9Wdd955ERHlI2u+9KUvxU033TRsiCQiYpdddokbb7wxLrnkkvJxOBER559/fmb9AhNIFSaSRERs0pb8HEESAAAAAAAAGJogCVXV2dkZt99+e+RyucjlcnHKKafEpz71qVE/5/TTT48Pf/jD5SNxbr/99ujs7MygY2BCyTenXyv2jPgxacfbLO8QJAEAAAAAAIChCJJQVXfeeWf09vZGqVSKhoaG+PznPz/mZ11wwQWRz798+lJfX1/ceeed1WoTmKgamiIil3xtFBNJ0oIkKztHHkYBAAAAAACADZEgCVX19NNPR0RELpeLN73pTTF9+vQxP2v69Onxpje9qbx+6qmnxt0fMMHlculTSUZ1tE1ykGSFiSQAAAAAAAAwJEESqur5558vf956663H/bzXvOY15c/Lly8f9/OA9UC+kFyvwkSSFZ2CJAAAAAAAADAUQRKqqlD4+y+A16xZM+7ndXf//RfHaz8beBVrbEmu93aO+BHTW5P/ebFSkAQAAAAAAACGJEhCVc2YMaP8+aGHHhr38x588MHy50033XTczwPWA80bJdefvm/Ej5ieerRNzxgaAgAAAAAAgA2HIAlVtfPOO0dERKlUikcffTTmz58/5mfdc889sXjx4kHPBl7lttwrub70jhE/Im0iyYtdfVHsHxhLVwAAAAAAALBBECShqvbaa6/YdNNNI5fLRalUio9+9KMVx9OMVHd39//P3p2HR1ne+x//zJJ9JRskYYcAAlqQKoKi1B6ttlWw1lq1am21i7W1alu1eo61p0e7nLq0trZaW7Xi/nNrj0pdqiKrbCoBIYGwhEASsk+Wmczy++OZJDPJM8kkmZls79d13dfMc9/P8mUEVObD99b3vve9zuOcnBwtWrQokqUCGK6mnW4+X/2J5KgK6xZZKeYdSXw+qa6lfaCVAQAAAAAAAAAAAKMeQRJE3GWXXSafzyeLxaJt27bpnHPOUWVlZdjXV1VV6Qtf+IK2bNkiSbJYLLrsssuiVS6A4WbqstBrYXYlCbW1jSTVNrv6WxEAAAAAAAAAAAAwZhAkQcTddtttSktLk2RscbNmzRrNmTNH//mf/6lPPvkk5HW7d+/Wf/3Xf2nOnDl65513ZLFYJEmpqan66U9/GpPaAQwDGYVS1nTztbL3wrrFuOTQQZIah3MgVQEAAAAAAAAAAABjgn2oC8Dok52drccee0xf/vKXO+caGhp011136a677lJmZqaKioqUkZEhi8WihoYG7dmzR/X19ZLUajfKQAABAABJREFU2c3E5/PJZrPpb3/7m3JycoboRwNgSExdJtXu6zlfFl5Hkni7VemJdjW2uXus1dCRBAAAAAAAAAAAAAiJIAmiYuXKlXrooYd07bXXqr29vTMYIkl1dXXatGlTZ8cRSZ1rkjrPjY+P1wMPPKAvfelLMa8fwBCbdrq09bGe87V7pcYKKb2gz1vkpCaYBknY2gYAAAAAAAAAAAAIja1tEDXf+MY3tGHDBi1YsKAzKGKxWDpHoMA5n8+nBQsWaP369br66qtjXjeAYWDqaaHXwuxKkpVivr0NW9sAAAAAAAAAAAAAoREkQVQtWLBAW7Zs0dtvv60rr7xS06dPl8/nMx3Tp0/XlVdeqbfeektbt27VwoULh7p8AEMlbYKUM9t8bf97Yd0iZJCEjiQAAAAAAAAAAABASGxtg5hYvny5li9fLkmqr69XdXW16urq5PP5lJWVpdzcXGVmZg5pjQCGmWnLpGO7e86XhRckyU5NMJ2vcRAkAQAAAAAAAAAAAEIhSIKYy8zMDCs0cuDAAU2fPl2SsfWN2+2OcmUAhpWpy6QP/tJzvv6gVHdAGjel18uzQ3QkqaUjCQAAAAAAAAAAABASW9tgWAvc+gbAGDN1Wei1/Wv6vDw7NdTWNs6BVgQAAAAAAAAAAACMegRJAADDU0q2lDfPfK2s7yBJVoiOJDV0JAEAAAAAAAAAAABCIkgCABi+pp1uPr9/jdRHp6LslATT+fqWdrk93sFWBgAAAAAAAAAAAIxKBEkAAMPXtBDb2zQelmr39XppqK1tJKm2ha4kAAAAAAAAAAAAgBmCJACA4WvKUkkW87Wy93q9NDvE1jaSVMv2NgAAAAAAAAAAAIApgiQAgOEraZyUf4L52v41vV46rrcgiYMgCQAAAAAAAAAAAGCGIAkAYHibGmJ7m7I1ks8X8rI4m1UZSXGma8foSAIAAAAAAAAAAACYIkgCABjepp1hPt9cJR3b0+uloba3qXU4B1sVAAAAAAAAAAAAMCoRJAEADG9TlkgWm/la2Xu9Xpqdah4kqaEjCQAAAAAAAAAAAGCKIAkAYHhLSJMKFpqv9REkyQrRkYQgCQAAAAAAAAAAAGCOIAkAYPibtsx8fv/7ktcb8rLs1ATT+Rq2tgEAAAAAAAAAAABMESQBAAx/U0MESVprpaqdIS/LDtGRpJaOJAAAAAAAAAAAAIApgiQAgOFv8imSNc58bf+akJeFCpKwtQ0AAAAAAAAAAABgzj7UBWDkee+992LynKNHj8bkOQBGgPgUaeKnpYPre66VrZFO+a7pZVkht7YhSAIAAAAAAAAAAACYIUiCflu+fLksFktMnmWxWOTz+WLyLADD3NRl5kGSA+9LXo9ktfVYCtWRpKG1Xe0er+JsNOYCAAAAAAAAAAAAAvENGgbM5/NFfQBAp2nLzOfbGqSjH5kuZaeaB0kkqY7tbQAAAAAAAAAAAIAeCJJgwCwWS9QHAHSaeLJkM9+qRmVrTKezQnQkkaQagiQAAAAAAAAAAABAD2xtg36bPHkyIQ8AsReXKE06WdpvEhrZv0Y69Qc9prOSQwdJagmSAAAAAAAAAAAAAD0QJEG/7d+/f6hLADBWTTvdPEhyYL3kcUu24H+t2W1WZSbHqb6lvcclxxzOaFUJAAAAAAAAAAAAjFhsbQMAGDmmLjOfdzVJR7abLoXa3oaOJAAAAAAAAAAAAEBPBEkAACNH4SIpLtl8rexd0+mclATT+RoHQRIAAAAAAAAAAACgO4IkAICRwx4vTVpsvlZmsuWNQnckqaEjCQAAAAAAAAAAANADQRIAwMgyLcT2Noc2Su6e4ZDs1FBb2zgjWRUAAAAAAAAAAAAwKhAkAQCMLNPOMJ9vb5EOb+kxnR2qIwlb2wAAAAAAAAAAAAA9ECQBAIws+Quk+DTztf09t7cJtbVNLVvbAAAAAAAAAAAAAD0QJAEAjCw2uzRlifla2Xs9prJTE0xPPeZgaxsAAAAAAAAAAACgO4IkAICRZ+oy8/lDm6T2tqCpUFvbNLa55XJ7I10ZAAAAAAAAAAAAMKIRJAEAjDzTQgRJPE6pfFPQVKiOJJJU18L2NgAAAAAAAAAAAEAggiQAgJFnwglSYob5WtmaoMOsEB1JJKnGQZAEAAAAAAAAAAAACESQBAAw8lht0pTTzNf2BwdJxiXHhbxNbTNBEgAAAAAAAAAAACAQQRIAwMgUanub8s2Sq7nz0G6zhgyT1DQ7o1EZAAAAAAAAAAAAMGIRJAEAjExTQwRJvO3SwQ1BU6G2t2FrGwAAAAAAAAAAACAYQRIAwMiUN1dKyjJf67a9TXZqgulpdCQBAAAAAAAAAAAAghEkAQCMTFarNPU087WybkGSEB1JapvpSAIAAAAAAAAAAAAEIkgCABi5pp1uPl+xTXI2dR6ytQ0AAAAAAAAAAAAQHoIkAICRK1SQxOeRDqzvPAy9tQ1BEgAAAAAAAAAAACAQQRIAwMiVM0tKHW++tv+9zrdsbQMAAAAAAAAAAACEhyAJAGDkslikqaeZr5UFBElSzYMkxxzOaFQFAAAAAAAAAAAAjFgESQAAI9vUZebzRz6SWuskSVkhOpI0tbnlcnujVRkAAAAAAAAAAAAw4hAkAQCMbNNOD7Hgkw6skyTlpCaEvLyuhe1tAAAAAAAAAAAAgA4ESQAAI1vWdCm90HytbI1xSoiOJBLb2wAAAAAAAAAAAACBCJIAAEY2iyX09jb7jSDJuOR4WSzmp9Q205EEAAAAAAAAAAAA6ECQBAAw8k0LESSp3CE1H5PNatG4ZPOuJDUOgiQAAAAAAAAAAABAB4IkAICRL1RHEkna/76k0Nvb1NCRBAAAAAAAAAAAAOhEkAQAMPKNmyJlTjFf829vkx0qSOJwRqsqAAAAAAAAAAAAYMQhSAIAGB1CbW9T5g+SpJoHSWrpSAIAAAAAAAAAAAB0IkgCABgdpp5uPn9st9RQztY2AAAAAAAAAAAAQBgIkgAARodQHUkk6f17lZ2SYLrE1jYAAAAAAAAAAABAF4IkAIDRIb1Ayp1jvrblUU2xVpkusbUNAAAAAAAAAAAA0IUgCQBg9Pj0N83nvW6dfOAh06UaB0ESAAAAAAAAAAAAoANBEgDA6LHoSiljsulS4cFXVGQp7zHf5HTL6fZEuzIAAAAAAAAAAABgRCBIAgAYPewJ0vJbTJcs8ukm+3Oma3XN7dGsCgAAAAAAAAAAABgxCJIAAEaXEy6WcmaZLp1j+0AnWPb2mD/mcEa7KgAAAAAAAAAAAGBEIEgCABhdbHbpM7eFXP6R/dkec7XNrmhWBAAAAAAAAAAAAIwYBEkAAKPP3BVS/gLTpdNtH2uJtThorqaZjiQAAAAAAAAAAACARJAEADAaWSzSZ/8z5LLRlcTXeVzjoCMJAAAAAAAAAAAAIBEkAQCMVjM+K0051XRpkbVEn7Vu7TxmaxsAAAAAAAAAAADAQJAEADA6WSzSmb13JbHIK4mOJAAAAAAAAAAAAEAHgiQAgNFryhKp6GzTpeOsh3SedYMkqYaOJAAAAAAAAAAAAIAkgiQAgNHuzNtDLt1gf052uVXT7IxhQQAAAAAAAAAAAMDwRZAEADC65X9KmneB6dI0a6W+bHtPtXQkAQAAAAAAAAAAACQRJAEAjAWfuV2y2EyXrre/IIfDEeOCAAAAAAAAAAAAgOGJIAkAYPTLmSktuNR0Kd9Sq5Xu1+R0e2JcFAAAAAAAAAAAADD8ECQBAIwNZ9wsrzXedOl79pdVV1sT44IAAAAAAAAAAACA4YcgCQBgbMicJNfCr5suZVkcsmx8MLb1AAAAAAAAAAAAAMMQQRIAwJgRf8aP1OxLMF3L/vDPUkttjCsCAAAAAAAAAAAAhheCJACAMcOaPl5PW79oumZ3N0vv3xvjigAAAAAAAAAAAIDhhSAJAGBM+WfahWrwJZsvbnpIaqyIbUEAAAAAAAAAAADAMEKQBAAwpiSkZulP7vPNF91t0nu/iW1BAAAAAAAAAAAAwDBCkAQAMKZkpyboUc/ZqvJlmp+w9XGpdl9MawIAAAAAAAAAAACGC4IkAIAxJTslXq1K1O/dK81P8Lqld34Z05oAAAAAAAAAAACA4YIgCQBgTMlOSZAkPe05U4e8ueYnffSsVLkzhlUBAAAAAAAAAAAAwwNBEgDAmJKVGi9Japdd97kvDHGWT3r7F7ErCgAAAAAAAAAAABgmCJIAAMaU7JT4zvcvek9TibfQ/MTd/yeVb45RVQAAAAAAAAAAAMDwQJAEADCmBAZJvLLqt+6LQp/85s8kny/6RQEAAAAAAAAAAADDBEESAMCYkp0aH3T8uvckfeSdZn7y/jXS5kdiUBUAAAAAAAAAAAAwPBAkAQCMKdkpCd1mLPqN++LQF6y+XTpWGtWaAAAAAAAAAAAAgOGCIAkAYEzJSIqTzWoJmlvjPV7rPXPNL3C3Si9cI3naY1AdAAAAAAAAAAAAMLQIkgAAxhSr1aJxyXHdZi263X2VPNbu3Ur8KrZKa34b9doAAAAAAAAAAACAoUaQBAAw5vTc3kba6yvUhpk/DH3Ru7+WyjdHrygAAAAAAAAAAABgGCBIAgAYc7JS4k3n12SulGacaX6RzyO98C3J1Ry9wgAAAAAAAAAAAIAhRpAEADDmZKeaB0lqmtulFX+QEjPNL6zdK/3rP6NXGAAAAAAAAAAAADDECJIAAMac7BAdSWqbXVJ6gfTFe0NfvPkRqeSNKFUGAAAAAAAAAAAADC2CJACAMSc7NcF0/lizy3gz/0vSCReHvsHL35Oaa6JQGQAAAAAAAAAAADC0CJIAAMacrJAdSZxdB+f+WkqfaH4DR6X0z+slny8K1QEAAAAAAAAAAABDhyAJAGDMCbm1jcPVdZCUKV3wYOib7PqH9OFTkS0MAAAAAAAAAAAAGGIESQAAY06orW2aXR61tXu6JqadLi25LvSNXv2JVHcgwtUBAAAAAAAAAAAAQ4cgCQBgzAm1tY0k1TS7gifO/E8pb675ya4m6cXvSF6P+ToAAAAAAAAAAAAwwhAkAQCMOTmpvQRJHM7gibhE6UsPSdY48wsOrpPWPxDB6gAAAAAAAAAAAIChQ5AEADDmpCfGyWa1mK716EgiSROOl868PfQN3/pv6ejHEaoOAAAAAAAAAAAAGDoESQAAY47VatG4ZPOuJLUOkyCJJC39vjR5qfmat1164VtSe1uEKgQAAAAAAAAAAACGBkESAMCYFGp7m5pmp+m8rDbpgj9J8Wnm61U7pbf/O0LVAQAAAAAAAAAAAEODIAkAYEzKSgkVJAnRkUSSxk2Rzv1V6PX1f5DK3htkZQAAAAAAAAAAAMDQIUgCABiTslMTTOdrQm1t02HBpdJx54VY9EkvfldqaxhccQAAAAAAAAAAAMAQIUgCABiTskN0JKntrSOJJFks0hfvl1LyzNcby6VXfzLI6gAAAAAAAAAAAIChQZAEADAmhdzaxuHs++KUbGnFH0Kvf/S09PHzA6wMAAAAAAAAAAAAGDoESQAAY1J2aoggSV8dSTrMOlv69DdCr7/0Xan0rQFUBgAAAAAAAAAAAAwdgiQAgDFpwFvbBDr7F1LWDPM1j0t6+lKpbM0AqgMAAAAAAAAAAACGBkESAMCYlJ2aYDrf4vKo1eUJ7ybxKdKXHpIsNvN1d5v05MXSwY0DrBIAAAAAAAAAAACILYIkAIAxKStERxJJqml2hn+jiZ+Wlt8aer29WVr1Zenw1n5UBwAAAAAAAAAAAAwNgiQAgDEp1NY2klTj6Mf2NpJ0+o+kT38z9LqzUfr7BdLRj/t3XwAAAAAAAAAAACDGCJIAAMak9MQ42a0W07Xa5n4GSSwW6fP/Ky38Wuhz2uqlx1dIVZ/0794AAAAAAAAAAABADBEkAQCMSVarReNCdCWp6W+QxLihdN7vpOMvCn1OS430+PlSzd7+3x8AAAAAAAAAAACIAYIkAIAxK9T2NjUO58BuaLVJK/8kHXd+6HMcldJj50l1+wf2DAAAAAAAAAAAACCKCJIAAMas7FTzIEm/t7YJZLNLFz4izTo39DmNh6XHzpcaygf+HAAAAAAAAAAAACAKCJIAAMasrJQE0/ljjkEESSTJHi9d9Kg048zQ59QfMMIkTUcH9ywAAAAAAAAAAAAgggiSAADGrFBb29Q2D3Brm0BxidLFq6Spy0KfU7tXenyF1Hxs8M8DAAAAAAAAAAAAIsA+1AUAI0lZWZm2b9+uiooKORwO5efna8qUKVq6dKni4uKGtLatW7eqpKREhw8fliQVFhZq1qxZWrhwYUSfU19fr3Xr1unw4cM6duyYcnJyVFhYqKVLlyozM3PQ96+trdUnn3yiQ4cOqbKyUs3NzZKkjIwMjR8/XgsXLtT06dMH/RxACh0kqRnM1jaB4pOlS56WnviSdGij+TnVn0iPr5SufEVKzorMcwEAAAAAAAAAAIABIkgChOH555/XPffco/Xr15uuZ2Vl6eKLL9bPf/5z5eTkxKyu9vZ2/fa3v9Vf/vIX7d271/ScmTNn6uqrr9aNN944qLDLtm3b9POf/1yvvvqqXK6eX7InJCTo3HPP1R133KEFCxaEfV+Hw6EHHnhA69ev1wcffKAjR470ec3EiRN1xRVX6Ac/+IHGjx/fnx8GECQ71Xxrm5rBbm0TKCFVuuw5o/NIxTbzcyo/lp64ULriJSkxI3LPBgAAAAAAAAAAAPqJrW2AXjgcDl1yySW66KKLQoZIJKOLxoMPPqj58+dr9erVMamtpKREp5xyim699daQIRJJKi0t1S233KIlS5aotLR0QM/65S9/qcWLF+ull14yDZFIktPp1EsvvaTFixfr17/+ddj3Pnr0qG699Va98sorYYVIJKm8vFx33XWXZs+erUcffTTsZwHdZYXc2iaCQRLJCId87QVp/PGhz6nYKq26SHI6IvtsAAAAAAAAAAAAoB/oSAKE4PF4dPHFF+vVV18Nms/NzdXChQuVkZGhvXv3atu2bfL5fJKkyspKrVixQm+++aZOO+20qNV29OhRnXXWWTpw4EDQ/MyZMzVv3jz5fD4VFxcHBUy2bNmis88+Wxs2bFBeXl7Yz7rrrrt02223Bc0lJSXppJNOUn5+vioqKvTBBx+ora1NkuRyuXTzzTfLYrHoxz/+8YB+fFlZWSoqKtKECROUmpoqp9Opo0eP6sMPP1RTU1PneQ0NDbrqqqtUU1Ojm266aUDPwtiWk2oeJGlt96jF5VZyfAT/NZmcZXQcefQLxnY2Zg5tlJ76qnTpM1J8SuSeDQAAAAAAAAAAAISJjiRACLfccktQiCQuLk6///3vVV5ertWrV+vZZ5/Vli1btGPHDi1ZsqTzPKfTqZUrV4bdXaO/vF6vVq5cGRQiyc/P1+rVq1VSUqKXXnpJL7/8skpLS/Xaa69pwoQJneeVlZXpggsu6Ay+9OWf//ynbr/99qC5b33rWzp48KDeffddPf3003rvvfd08OBBXX311UHn3XzzzXr99dfDek5eXp6uuuoqPfnkk9q/f79qamq0YcMGvfTSS3riiSf03HPPac2aNaqpqdHzzz+vGTNmBF3/k5/8RBs3bgzrWUCgUB1JpAhvb9MhJUe64mUpa3roc/avkf56jlR/KPLPBwAAAAAAAAAAAPpAkAQwsW/fPt1///1Bc88995yuu+46xccHf/E8d+5cvfXWW0FhkpqaGt15551RqW3VqlVBoYmsrCytW7dOZ599do9zzznnHK1bt07jxo3rnFu3bp2eeeaZPp/j8Xj0ox/9KCh0csMNN+jPf/6zcnJygs7Nzc3Vww8/rB/+8Iedcz6fTzfddJM8Hk+vz5k2bZqOHDmiv/71r7rkkks0ZcqUkOfGxcXpwgsv1AcffKD58+d3znu9Xv3sZz/r88cEdJedkhByrSbS29t0SJsgXfkPKXNy6HOOfiQ9tFzavzY6NQAAAAAAAAAAAAAhECQBTNx5551qb2/vPP7617+uFStWhDw/KSlJjz76aFDI5JFHHtG+ffsiWpfH49Edd9wRNHfPPfdo6tSpIa+ZNm2a7rnnnqC522+/XV6vt9dnPf7449q9e3fn8ezZs3X33Xf3es0vf/lLzZ49u/N4586dWrVqVa/X2Gw2Wa39+61o3LhxPYI+b775ZtC2N0A40pPsslstpmu1zc7oPThjohEmSS8MfU7LMenx86UPHoleHQAAAAAAAAAAAEA3BEmAblpbW/X8888Hzd188819Xjdr1iytXLmy89jtduvJJ5+MaG3vv/++ysrKOo8LCwv1ta99rc/rLr/8chUWdn1hvXfvXq1bt67Xax5//PGg4xtuuEEJCaG7N0hSQkKCrr/++l7vEynLly9XUlJS57Hb7Q7a7gcIh8ViCbm9TVS2tgk0bqoRJkkdH/ocr1v6vxulf/xQcke5HgAAAAAAAAAAAEAESYAeVq9erZaWls7jJUuWaM6cOWFde9VVVwUdv/DCCxGt7cUXXww6vuKKK2Sz2fq8zmaz9Qic9FZbTU2N1qxZ03kcHx+vSy+9NKwaL7vsMsXFxXUev/vuu6qtrQ3r2v6wWq3KzMwMmqMjCQYiO9U8IBW1rW2CHj7DCJNkTOr9vC1/M7qTOKqjXxMAAAAAAAAAAADGNIIkQDevv/560PHy5cvDvnbZsmWy2+2dx9u2bVNlZWWkShtUbd3Pfe2110Ke+8Ybb8jj8XQeL1q0SGlpaWE9Jz09XSeeeGLnsdvt1htvvBF2neFqaWlRdXXwl+oFBQURfw5Gv+wQHUlqYxEkkaTc2dI1/5YmL+39vIPrpYeWSxXbY1EVAAAAAAAAAAAAxiiCJEA3O3bsCDpesmRJ2NempKTo+OOPD5orLi6OSF1Op1OlpaVBc6ecckrY1y9dGvwldUlJiVwu8y/KB/MZmD0rUp9BoKeeekput7vzeNq0aZoyZUrEn4PRL9TWNsccztgVkZorXfGy9Olv9n5eY7n013Okj5/v/TwAAAAAAAAAAABggAiSAN3s2rUr6HjmzJn9un7GjBlBxzt37hx0TZK0e/fuoC4heXl5Sk9PD/v69PR05eTkdB57PB7t2bPH9NzuNQ+Xz6DD2rVr9aMf/ShorvsxEK7s1CHuSNLBHi998R7pi/dKVnvo89yt0v/7pvTmzySvJ/R5AAAAAAAAAAAAwAAQJAEC1NbWqra2Nmhu8uTJ/bpH9/NLSkoGXZekHt1I+luX2TWhahvssyL9GTidTpWXl+sf//iHvva1r+n0009XfX195/p5552n7373u4N6BsauId/aprtPf0O68h9Sck7v571/r/TUV6W2htjUBQAAAAAAAAAAgDGhl7/yDIw9geEESUpOTlZKSkq/7pGXlxd03NAQmS95u9fW/TnhCLe2wT5rsJ/BggUL9OGHH/Z5nsVi0bXXXqt77rlHFoulX8/oS1VVlaqrq/t1TfcADkaG7NQE0/kaxxAFSSRpylLpW+9Iz1wmHenl10LJv6SHPytd8pSUUxSz8gAAAAAAAAAAADB6ESQBAjgcjqDjpKSkft+j+zVNTU2DqqlDLGsb7LOi9Rl0iI+P1zXXXKNrr71Wc+fOjei9O/zxj3/UnXfeGZV7Y3jJCtGRpKbZKZ/PF/GQUtgyJ0lXvS698n1px/Ohz6spkR4+U7rwEWnW2bGrDwAAAAAAAAAAAKMSW9sAAboHKBITE/t9j+4hiu73HKhY1jbYZ0XrM+jgcrn0xBNP6L777tPevXsjem+MPaG2tmlr96rF5YlxNd3EJ0sX/kX6jzsl9RJocTZKT35FWnOP5PXGrDwAAAAAAAAAAACMPgRJgF4MpBNBrLoXxLK2/l432M/g1VdfVVlZWef46KOP9K9//Uu/+MUvdNxxx0kytst5+OGHdcIJJ+jRRx8d1PMwtoXa2kaSapuHcHubDhaLdNoPpUuflRIyejnRJ711p7Tqy1LT0VhVBwAAAAAAAAAAgFGGrW2AAKmpqUHHra2t/b5H92u633OgYllbamqq6urqBvyswX4GBQUFPeaOP/54nXXWWbrtttv08MMP6wc/+IHa2trU0tKib3zjG7Jarbriiiv69ZzeXHvttbrooov6dU1paalWrlwZsRoQG6G2tpGkYw6nJmUlx7CaXsw6W7rmLempS4ztbELZ+5b04FJpxR+k2efGrj4AAAAAAAAAAACMCgRJgAAESbrmhzJI0pdrrrlG48eP14oVKyRJPp9P1157rT772c+qsLAwIs/Iy8tTXl5eRO6F4S090a44m0XtHl+PtWHRkSRQTpERJvl/10glq0Of11IjPfVV6dPflM7+hbFFDgAAAAAAAAAAABAGtrYBAmRkBG8b0dLSoubm5n7do6qqKug4MzNzsGVJ6llbdXV1v+8Rbm2DfVa0PoNA559/vi644ILO4+bmZv3xj3+M+HMw+lkslpBdSWqGW5BEkhIzpEuekk67se9zNz8iPbRcOvJR1MsCAAAAAAAAAADA6ECQBAiQnZ2tcePGBc0dPHiwX/c4cOBA0HFRUdGg6zK7T/fnhCPc2gb7rGh9Bt1dcsklQcevv/56VJ6D0S8rJcF0vsYxDIMkkmS1Sf9xh/Tlv0pxfXQbObZbevhMad3vJa83NvUBAAAAAAAAAABgxCJIAnRz3HHHBR2Xlpb26/p9+/b1er+Bmj17tmw2W+dxVVWVmpqawr6+sbFRx44d6zy22WwhAx7D9TPobvbs2UHH/a0T6JCTat6RpLbZGeNK+mn+hdK310j5C3o/z9su/et26YkLpMYjMSkNAAAAAAAAAAAAIxNBEqCb+fPnBx2vX78+7Gubm5v10UfBW0h0v99AJSQkaMaMGQOubd26dUHHRUVFSkgw78IwmM9AktauXdvr/SIlLi4u6NjpHOZf+mPYCrm1zXDtSBIoZ6b0zTek026QZOn93H3vSA8ulXb9MxaVAQAAAAAAAAAAYAQiSAJ0c8455wQdv/POO2Ffu2bNGrnd7s7jhQsXavz48ZEqbVC1dT/33HPPDXnuWWedFdT9ZMuWLWF3P2lqatLWrVs7j+12u84666yw6+yP8vLyoONIftYYW7JDbW3TPAKCJJJkj5f+42fSlf+Q0gt7P7e1VnrmMukf10uu5piUBwAAAAAAAAAAgJGDIAnQzec+9zklJSV1Hq9fv16ffPJJWNc++uijQccXXHBBJEvrcb+///3v8ng8fV7n8Xj0xBNPhF1bTk6OTjvttM5jl8ulJ598MqwaV61apfb29s7j008/XVlZWWFd21//+te/go5DbdUD9CU75NY2IyRI0mHaMuk770tzV/R97pZHpT+fIVVsj3ZVAAAAAAAAAAAAGEEIkgDdJCcn68tf/nLQ3K9+9as+r9uzZ49efPHFzmO73a5LL700orUtW7ZM06ZN6zwuLy/vERAx88QTT+jw4cOdxzNmzNCpp57a6zVXXHFF0PG9997b59YxTqdT9913X9DclVde2Wd9A3HkyBE99NBDQXMrVoTx5TlgIjvk1jYjcLuk5CzposekFX+Q4lJ6P7emRPrLf0jv3yd5vTEpDwAAAAAAAAAAAMMbQRLAxM9+9jPFxcV1Hj/66KN65ZVXQp7f1tamq666Si5XV/eCb37zm5oxY0avz7FYLEGjr61qbDab7rzzzqC5G2+8Ufv37w95zf79+3XDDTcEzf3iF7+Q1dr7L/8rr7xSs2fP7jzevXu3fvrTn/Z6za233qrdu3d3Hs+dO1eXXXZZyPObm5t1zz33qLW1tdf7dlddXa0vfOELamxs7JzLysrSJZdc0q/7AB2yQgVJml3y+XwxriYCLBZp4dek76yRChf1fq63XXrzDunx86WG8t7PBQAAAAAAAAAAwKhHkAQwMX36dF1//fVBc1/+8pf1wAMPBIVFJGnXrl367Gc/q3Xr1nXOZWdn64477ohKbZdddpkWL17ceVxbW6ulS5f22OZFklavXq0lS5aorq6uc27p0qW6+OKL+3yOzWbT//7v/8pisXTO3XPPPfr2t7+tmpqaoHOPHTumb33rW7r33ns75ywWi37729/KZrOFfEZ7e7tuuukmTZ8+XTfeeKPWr1/f4/MNVFlZqd/+9rc67rjjtG3btqC13/zmN8rJyenzxwWYCbW1jdPtVbOr7+2jhq3sGdI3VkvLfiTJ0vu5+9dIDy6Vil/s/TwAAAAAAAAAAACMahbfiPyr1kD0eTwenXfeeXrttdeC5vPy8nTiiScqLS1N+/bt09atW4M6FsTHx+vNN9/UsmXL+nxGYEhDkv79739r+fLlfV535MgRnXLKKTp48GDQfFFRkebNmyefz6fi4mKVlpYGrU+dOlUbNmzQ+PHj+3xGh7vuuku33XZb0FxSUpIWL16sCRMm6MiRI9q0aVOPriK/+tWv9JOf/KTXe9fX12vcuHFBc/Hx8Zo7d67y8/OVmZkpn8+nhoYG7dmzR/v27TPtDvE///M/fXZLiYXi4mLNnz+/83jHjh2aN2/eEFaEcO0/1qzl//uO6dp7P/6MJmcnx7agaNi/Vnrx21LDob7P/dSl0rm/khLTo18XAAAAAAAAAAwCfzYPAJFnH+oCgOHKZrPp2Wef1dVXX61nnnmmc76qqkqvv/666TV5eXl67LHHwgqRDEZ+fr7eeOMNffWrXw3qzFFSUqKSkhLTa0488UQ988wz/QqRSNJPf/pTWSwW3XHHHWpvb5cktba2htyGJy4uTv/93//dZ4gkFJfLpe3bt2v79u19njtx4kT97ne/0wUXXDCgZwEdskJ0JJGkqqa20REkmXqq9J33pX/eIBW/0Pu5Hz4pHVgrfelhafLi3s8FAAAAAAAAAADAqMLWNkAvUlNT9fTTT+u5557TKaecEvK8rKwsffe739WOHTt0zjnnxKS2WbNmaePGjbr77rs1ffr0kOfNmDFDd999tzZs2KCZM2cO6Fm33nqrNm7cqBUrVig+3vwL9/j4eK1YsUKbNm3SzTffHNZ909PT9corr+jaa6/V3LlzZbX2/VuS3W7XsmXL9NBDD2nXrl2ESBARaQl2pcSbb8P0ydGmGFcTRUmZ0pf/Kl3wZyk+rfdz6w9IfztH+vddkscdk/IAAAAAAAAAAAAw9NjaBuiHsrIybd26VRUVFWpubtaECRM0ZcoUnXrqqSEDFrGyZcsW7dmzRxUVFZKkgoICzZo1S4sWLYroc+rq6rRu3TodPnxYNTU1ys7OVmFhoZYuXdpjm5r+ampq0s6dO7V//34dPXpUzc3NkozASUZGhmbPnq0TTjhBiYmJkfihRBzt80a2Lz+4TpsP1PWYv+Tkybr7S8cPQUVRVlsmvXCNVP5B3+cWflr60kNS9ozo1wUAAAAAAAAA/cCfzQNA5BEkAYAI4T9WR7Y7Xt6hx9Yf6DF/wsQMvXLdaUNQUQx43NKa/5Xe/bXk8/R+blyKdO6vpIVfkyyW2NQHAAAAAAAAAH3gz+YBIPLY2gYAAEnzCjJM5z852qR2jzfG1cSIzS4tv0X6xuvSuKm9n9veLL1ynfTs5VJLbUzKAwAAAAAAAAAAQOwRJAEAQNLcgnTTeZfbq73VjhhXE2OTTpa+87604Gt9n7vrH9KDS6W9/45+XQAAAAAAAAAAAIg5giQAAEiaNT5NcTbzLVuKDzfGuJohkJAmrfyDdNFjUmJm7+c2HZH+vlJ6/adSe1ssqgMAAAAAAAAAAECMECQBAEBSvN2qorw007XiijEQJOkwb6X03XXStDP6PnfDH6SHz5TKt0S9LAAAAAAAAAAAAMQGQRIAAPzmhdjepriiIcaVDLGMQunyl6SzfyHZ4ns/t6pY+suZRqBk2yqpvTUmJQIAAAAAAAAAACA6CJIAAOAXKkiy80ijfD5fjKsZYlartPT70jVvS7lz+j7/8Bbp5Wul386RVt8m1eyNfo0AAAAAAAAAAACIOIIkAAD4zS/MMJ1vanPrUO0Y7bQx4XjpW+9IJ387vPPb6qX1D0i/P1F6fKW06x+Sxx3FAgEAAAAAAAAAABBJBEkAAPA7Lj9dFov52o6xtr1NoLgk6fO/li57XkrJC/+6ff+WnvmadN/x0ju/khqPRK9GAAAAAAAAAAAARARBEgAA/FIS7JqWnWK6VjyWgyQdis6Srl0vzf58/65rqpDeuUu6d570zOXSvnelsbZVEAAAAAAAAAAAwAhBkAQAgABzC9JN54srGmNcyTCVkiN99Unpq09J007v37U+j7TrFenx86UHTpLW/1FqI6ADAAAAAAAAAAAwnBAkAQAgwLyCDNN5giQBLBZpzuelK/8hfe8DafF3pQTzzy2kmhJp9a3SvfOlN+6Qmo5Gp1YAAAAAAAAAAAD0C0ESAAACzAvRkaS6yamqprYYVzMC5M6Szv2ldNMu6fzfS/mf6t/1zkZp7X3SfcdLr/xAqtkblTIBAAAAAAAAAAAQHoIkAAAECBUkkehK0qv4FOnEK6RvvStd/ba04DLJnhj+9R6XtPUx6feLpGevkA5vjV6tAAAAAAAAAAAACIkgCQAAAbJTEzQh3TwAsZMgSd8sFmniImnlH6Ubd0ln/4+UNaMfN/BJO1+WHv6M9Nh5Uulbks8XtXIBAAAAAAAAAAAQjCAJAADdzC8070pSXNEQ40pGuOQsael10nWbpctflOZ8UbLYwr++7D3piS9Jfz5d2vH/JI87erUCAAAAAAAAAABAEkESAAB6mFuQYTq/4zAdSQbEapVmnCl9dZX0w4+l038sJY0L//qjH0nPf0N6YJG06WGpvTV6tQIAAAAAAAAAAIxxBEkAAOhmXoF5R5KDtS1qbGuPcTWjTEahdObt0g93SJ+7W0qfGP61dfulV38k3Ttfeu83UhsdYgAAAAAAAAAAACKNIAkAAN2ECpJI0s4KupJEREKqtORa6frt0so/SbnHhX9tyzHp7V9I9y+QNjwouV3RqhIAAAAAAAAAAGDMIUgCAEA3hZlJykiKM10rJkgSWbY4acEl0nfXSZc8I01eEv61rbXS67dIfzhJ2vH/JK83enUCAAAAAAAAAACMEQRJAADoxmKxhOxKUlzBdipRYbVKs8+RvvG69I1/SbM/H/61dful578h/eVMqWxN1EoEAAAAAAAAAAAYCwiSAABgIlSQhK1tYmDyYumSp6RrN0qfulSy2sO7rmKb9NgXpVUXSZU7o1sjAAAAAAAAAADAKEWQBAAAE/MKMkznS6ocamv3xLiaMSpvjnTBg9L1H0qnfE+KSwnvupJ/SX86VXr5e1LD4ejWCAAAAAAAAAAAMMoQJAEAwMT8QvOOJB6vT3sqm2JczRiXMVE65y7phh3S6T+R4pL7vsbnlbY9If3+ROnNO6U2tiQCAAAAAAAAAAAIB0ESAABMTMtJVVKczXRtx2G2txkSyVnSmbdJP9gmLbpKspj/8wnibpPev0e6f4G04U+S2xX1MgEAAAAAAAAAAEYygiQAAJiwWS2ak59mulZcQXeLIZU2QTrvPuna9dLsL4R3TWut9PrN0h9Oknb8P8nrjWqJAAAAAAAAAAAAIxVBEgAAQphXYL69TXEFHUmGhdzZ0iVPSle9Lk08Kbxr6vZLz39D+tNp0sfPS15PVEsEAAAAAAAAAAAYaQiSAAAQwryCDNP5T442yuP1xbgahDRlifTNN6SvPC5lTQ/vmqpi6f99U3rgJGnr39nyBgAAAAAAAAAAwI8gCQAAIYTqSNLW7tW+akeMq0GvLBZp7grpe5ukz/+vlJwT3nW1e6VXrpN+f6K06WGpvTW6dQIAAAAAAAAAAAxzBEkAAAhh1vg02awW0zW2txmmbHHSyddIP9gmnf4TKS45vOsaDkmv/ki67wRp7e8kJ0EhAAAAAAAAAAAwNhEkAQAghMQ4m4ryUk3XiisaYlwN+iUxXTrzNun7W6UTr5QsYf4nT3OV9MZ/SvfNl975ldRaF906AQAAAAAAAAAAhhmCJAAA9GJeQYbpPB1JRoj0fOn830nfXS8dd74k8w4zPbTWSe/cJd17vPTGHZKjOqplAgAAAAAAAAAADBcESQAA6MW8gnTT+R2HG+Tz+WJcDQYsb4508d+l722UTviqZLGFd52rSVp7n3Tf8dJrN0sNh6NaJgAAAAAAAAAAwFAjSAIAQC9CBUka29wqr2uNcTUYtNzZ0pf+LH1/i7ToKskWH9517lZp45+k+z8l/fNGAiUAAAAAAAAAAGDUIkgCAEAv5oYIkkhsbzOiZU2TzrtPuv5D6ZRrJXtSeNd526XNj0i/W2h0KGk6GtUyAQAAAAAAAAAAYo0gCQAAvUhLjNOU7GTTtZ0VDTGuBhGXXiCdc7d0ww5p2U1SQujgUBCPs6tDyerbJEd1dOsEAAAAAAAAAACIEYIkAAD0IdT2NnQkGUVScqTP/pf0w4+lz9wuJWWFd527TVr/gHT/CdIbd0jNNdGtEwAAAAAAAAAAIMoIkgAA0Id5BRmm8wRJRqGkTOmMHxuBkrP/R0qdEN517S3S2vuMQMnbv5Ba66JZJQAAAAAAAAAAQNQQJAEAoA9zQ3QkOdrYphqHM8bVICYSUqWl10nXfyh94R4pY1J417kc0nu/ke47QXrnl1Ib2x8BAAAAAAAAAICRhSAJAAB9CLW1jURXklEvLlE66ZvS97cagZK0gvCuczZK79xtBEre+1/J2RTdOgEAAAAAAAAAACKEIAkAAH3IS0tUXlqC6dqOCjpOjAn2eCNQ8oNt0rm/llLHh3ddW7309n8bgZL375NcLdGsEgAAAAAAAAAAYNAIkgAAEIZQXUnoSDLGxCVKi79tbHlz9v9IyTnhXddaK715h/S7BdKmhyW3K6plAgAAAAAAAAAADBRBEgAAwjCvIMN0fidBkrEpLklaep30w4+k//iZlDQuvOscldKrP5IeWCRtf1LyeqJaJgAAAAAAAAAAQH8RJAEAIAyhOpKUHWuWw+mOcTUYNuJTpNNukK7/SPrM7VKieeCoh/qD0kvflf64RNr5suTzRbdOAAAAAAAAAACAMBEkAQAgDKE6kkjSriN0JRnzEtOlM35sBErOuEVKMA8e9XBst/TsFdJDy6XSNwmUAAAAAAAAAACAIUeQBACAMEzKSlJaot10rfhwQ4yrwbCVlCl95lbp+g+lZTdJcSnhXXdku/TEhdKjX5AObohmhQAAAAAAAAAAAL0iSAIAQBgsFovm5pt3mSiuoCMJuknOkj77X9IPP5KWXCfZEsK77sBa6a+fk1Z9RTryUXRrBAAAAAAAAAAAMEGQBACAMIXa3mYHQRKEkpIjfe5/pB9skxZdJVnNu9r0ULJa+vMy6bmrpGOl0a0RAAAAAAAAAAAgAEESAADCNL/QvCNJSWWTnG5PjKvBiJJRKJ13n/S9TdLxX5FkCe+64hekP5wsvfQ9qWJ7FAsEAAAAAAAAAAAwECQBACBMoTqSuL0+lVQ6YlwNRqTsGdKFD0vfXSvN/kJ41/g80vYnpIfOkP58uvTBX6TW+qiWCQAAAAAAAAAAxi6CJAAAhGlGbooS7Ob/6iyuaIhxNRjRxs+TLnlSuvotafry8K878qH0fzdJv50jvfgd6cA6yeeLWpkAAAAAAAAAAGDsIUgCAECY7Dar5kxIM10rrmiMcTUYFSZ+WrriZemKV6TCT4d/nbtV+vAp6W/nSg98Wlp7v+Soil6dAAAAAAAAAABgzCBIAgBAP8wNsb0NQRIMyvQzpKvflL76lJQ3t3/X1pRKb/yXdM9x0jOXSyVvSl5PdOoEAAAAAAAAAACjHkESAAD6YV5Buun8riON8njZYgSDYLFIcz4vfWet9KW/SOOm9e96r1va9Yq06kLpvhOkf98t1R+MTq0AAAAAAAAAAGDUsg91AQAAjCShgiQtLo/21zRrRm5qjCvCqGO1SidcJM1bKe18Wdr6uFT2bv/u0VguvftL6d1fSZNOliaeZGyjM/EkKb3QCK0AAAAAAAAAAACYIEgCAEA/zJmQLqtFMms+suNwA0ESRI4tTjr+y8aoLZO2PSFtXyU1HenHTXzSoY3G6JCWLxUu8odLTpIKFkjxKZGuHgAAAAAAAAAAjFAESQAA6IekeJtm5qVqT6Wjx9rOikatWFA4BFVh1MuaJn32P6Xlt0qlbxhdSvaslnye/t+r6Yj0yT+NIUkWmzR+rhEqKfR3LcmeaXRGAQAAAAAAAAAAYw5BEgAA+mleQYZpkKS4onEIqsGYYrNLs881RuMR6cMnjVBJ3f6B39PnkY5+bIzNfzXmEjOMriVTlkrTPyPlLzCeDQAAAAAAAAAARj2+EQAAoJ/mFaTrxW2He8wXVzTI5/PJYrEMQVUYc9LzpWU3SafeIB143wiU7HxF8jgHf++2Bmnv28Z4+xdSQoY0bZk0fbkxsmdK/DwHAAAAAAAAAGBUIkgCAEA/zS1IN52va2nXkYY2FWQmxbgijGlWqzTtdGOcWyt99Ky09TGpamfknuFsCN4OJ72wK1Qy7QwpbXzkngUAAAAAAAAAAIYUQRIAAPppXn5GyLXiikaCJBg6yVnSKd+RFn9bqiyWDq6XyjdLhzdLNaWRe07jYWn7KmNIUu5xXcGSqadKCWmRexYAAAAAAAAAAIgpgiQAAPRTRnKcJo5LUnlda4+14ooGnTWX7gwYYhaLNGG+MU6+xphrqZUOb5XKPzDG4c3GFjaRUL3LGBsflKx2qfDT0qyzpXkXSFnTI/MMAAAAAAAAAAAQEwRJAAAYgHkF6SGCJI1DUA0QhuQsqeg/jCFJXq9Uu9cfLNlsvFYWSz7P4J7jdUuHNhjjrZ9L+Quk+V+S5q6Uxk0Z7I8CAAAAAAAAAABEGUESAAAGYF5BhlYXV/aYLz4coQ4PQLRZrVJOkTEWXGrMuVqkI9ulQxulsjXSgXWSu2dgql+ObDfGG/9ldCqZd4E0b6WUMXFw9wUAAAAAAAAAAFFBkAQAgAGYV5BuOl/R0Ka6ZpfGpcTHuCIgAuKTpSlLjXHaDZLbKR3aJO17xxgVWyWfd+D3P7zZGP+6TZq0WJr3JWnuCik9P1I/AgAAAAAAAAAAMEgESQAAGID5hRkh14orGnVaUU4MqwGixJ4gTVtmjM/+p9RaL+1/vytYUlMy8Hsf2miM128xgivzLjBCJal5ESoeAAAAAAAAAAAMBEESAAAGIC8tQTmp8TrmcPVYK65oIEiC0SkpUzrui8aQpIZyad+7XcGS5qoB3NQnHVhrjNd+Ik05VZrxGWMbnMITpYS0yNUPAAAAAAAAAAD6RJAEAIABsFgsmluQoff2VPdYK65oHIKKgCGQMVFaeJkxfD6papdU+oZU/JKxDU5/+bzS/jXGkCRZpNw50sRFRrBk4qel3OMkG/8JCwAAAAAAAABAtPCn8AAADNC8gvQQQZKGIagGGGIWizR+rjFOvV6qLZN2viTteEE6+tEAb+qTqncZY9sTxlRcilSwwAiVdIRL0gsi9IMAAAAAAAAAAAAESQAAGKB5Bemm8/uONavF5VZyPP+axRiWNU067QZj1OyVil8wOpVU7hjcfdubu7bC6ZBW4O9askjKmyflzZEyJhnhFgAAAAAAAAAA0C98wwUAwADNK8gwnff5pF1HmrRoyrgYVwQMU9kzpNN/bIzqPVLxi0awpPqTyNy/qULaVSHt+kfXXHyqlDNLyjvO2B6n4zVjIgETAAAAAAAAAAB6QZAEAIABmpKVrNQEuxxOd4+14ooGgiSAmdxZ0vKbjVG1y9j6pvgFqaY0ss9xOaSKrcYIFJ8q5c6Wco8zOpfkziFgAgAAAAAAAABAAIIkAAAMkNVq0XH5afpgf12PteLDjUNQETDC5B0nnXmb9JmfSpXF0u5XpUObpMObpdaev64iwuWQDm8xRqD4VCmnyOhi0vk6S8qaLtkTolMLAAAAAAAAAADDEEESAAAGYV5BhnmQ5EjDEFQDjFAWizRhvjEkY3+o2n1G2KN8sxEsOfKR5G2PXg0uh1SxzRhBtVmlzCk9AyY5s6SU7OjVAwAAAAAAAADAECFIAgDAIMwrSDed3320SQdrWjQ5OznGFQGjgMUiZc8wxglfMebcTunox13BkvLNUl1Z9GvxeY3n1JVJJauD15KyjEBJ3hxp2hnSjDOlpMzo1wQAAAAAAAAAQBQRJAEAYBDmFWSYzrd7fPrW3zfrhWuXKjmef90Cg2ZPkCZ+2hgdmmv829Rslg5vlap2SY3lsauptVY6tMEYWx6VLDZp8ilS0VlS0dlS3lwjFAMAAAAAAAAAwAjCN1sAAAxC0fhUZafEq6bZ1WPtk6NN+vFzH+mBSxfKwpfJQOSlZEuzzjZGh7ZGqXq3VL1LqvpEqvaPxsPRr8fnkQ6sNcabP5PSC/2hks9J006XElKjXwMAAAAAAAAAAINEkAQAgEGIs1n1jdOm6Terd5uu/9/HRzT3nXR97zMzY1wZMEYlpkuTTjJGoLYGI2BStSs4aNJUEb1aGg8bnUq2PCrZ4qUppxqdSmZ9zti2BwAAAAAAAACAYYggCQAAg/Tt06drbekxrdtbY7r+v//arePy03TmnPExrgxAp8QMadLJxgjUWm8ES2pKpGN7pGP+19oyo8NIpHhc0r5/G2P1rVLWdCNUMvkUKT7V2LrHnhgw/Mdx/mNbPNvkAAAAAAAAAABigiAJAACDZLdZ9cClJ+r8B95XeV1rj3WfT7r+qe166bpTNSOXrS2AYSUpU5q82BiB3C6pbr8/XBIQMDlWIjkbBv/c2n3Sxj8ZI1zdgyZp+dKE46X8E6QJJ0h5xxnzAAAAAAAAAAAMAkESAAAiICslXg9d/mld+OA6tbb37GLQ5HTrW49v1kvfO1VpiXFDUCGAfrHHS7mzjBHI55McVV0dTCq2S6VvGtvYRJu7zRgd6g9IhzZ0HVvjpLw50oRPdYVLJsyXEtKiXxsAAAAAAAAAYNQgSAIAQITMLUjXby46Qdc9uc10fW91s254ZrseuvzTslrZogIYkSwWKW28MaaeZsz5fFLVTqnkX1LJG9LBDZHdFidc3nbp6MfG2N5ZsJQ9wwiVdIRL8j8lpeTEvj4AAAAAAAAAwIhAkAQAgAj64gkFKq5o1IPv7DVdf3NXle57c49uPHt2jCsDEDUWizR+njFOu0FqrZf2/Vva8y+p9A2puXoIi/NJNaXGKH6hazq90NgWZ8IJ/tfjpXFTjR8LAAAAAAAAAGBMI0gCAECE/ejs2dp1pFHv7Db/8vh3b5dqbkG6zpmfH+PKAMREUqY07wJjeL3Ske1Gp5KS1dLhrZJ8Q1ygjK14Gg9Le17vmktI7wqVdIzc44xtfgAAAAAAAAAAYwZBEgAAIsxmtej+ry7Uyj+sVdmxZtNzbnz2Q03LSdXsCWkxrg5ATFmtUuGJxlh+s+Solva+Je1ZLR3eLLU2SO42yeMc6kolZ6N0YK0xOljjpNw5Rqgk39+9ZPw8KWnc0NUJAAAAAAAAAIgqgiQAAERBRlKcHr5ikVb+YZ0cTneP9RaXR9c8vlmvXHeqMpP52/7AmJGaK33qq8YI5PUaYRJ3m+R2Su2txqu7LWAEzDsbpcpi6ciHUtVOYz0avO1S5cfG+PDJgB/HBCl3thEyyZ0t5R1nvE/Oik4dAAAAAAAAAICYIUgCAECUzMxL070XL9A1j282XT9Y26LvP7VNf/v6SbLbrDGuDsCwYrVK1iQpLqn/13rc0rE90tGPjGDJkY+M987GyNfZwXHUGGXvBs+n5PrDJXMCgiZzjAANAAAAAAAAAGBEIEgCAEAUnTV3vG74j1m69809putrSo7pN6t369bPHxfjygCMGja7NH6uMTo6nXi9Uv3+rlDJEX/IpLkqurU0Vxtj/5rg+eRsI1CSM8sYuf7X9IlGiAYAAAAAAAAAMGwQJAEAIMq+f+ZM7TzSoNXFlabrf35vn+YWpGvFgsIYVwZg1LJapazpxpi3smu+6WhXuOTox8ao3Rv9elpqpANrjRHIniTlzOwKmOQUSTmzpewZA+vOAgAAAAAAAAAYNIIkAABEmdVq0W+/skBlf1yrPZUO03N+8vxHmpGbqvmFGTGuDsCYkjbBGLPO7ppzNkmVxf5gib97SdUuyeOMfj3u1q5ASxCLlDk5OGCSPdMIxqTl08UEAAAAAAAAAKKIIAkAADGQmmDXQ5d/Wuc/8L4a29w91p1ur7799y16+bpTlZOaMAQVAhizEtKkyacYo4OnXTpWEtC5xB8waauPUVE+qf6AMUrfCF6yJ0rjpklZ0/xdV6Z1dV9Jn2hs9QMAAAAAAAAAGDD+lBUAgBiZmpOi3196oq762yZ5fT3XD9e36tpVW/W7ry7UhIzE2BcIAB1scdL4ucb41FeNOZ9PaqyQju2Wqj6Rqj+RqndL1buktobY1eZuM55ZvavnmtUuZU7pCpZkTTNCJ0mZRgAlLlmK87/aE43tc6y22NUOAAAAAAAAACMAQRIAAGLojFm5uvmcObr7tU9M1zeV1erUX72t/zguT5ctnqLTZubIarXEuEoAMGGxSBmFxphxZte8zyc5qvzhjt1dAZOqXVJrbWxr9Lql2r3GCJct3giUdIZLAsImcclSeoE0boo0bqoRUhk3VUoaZ3weAAAAAAAAADAKESQBACDGvnX6dBVXNOqVDytM1z1en1YXV2p1caUmZyXr0sWTddGiicpmyxsAw5HFIqWNN8b05cFrjmp/sMQfLjm2x9gyp8n8978h4XEZoz9dVRLS/aGSwIBJx/vJRjAFAAAAAAAAAEYogiQAAMSYxWLRry48QaVVDu080tjruQdrW/TL1z7RPf/ao3PmT9Bliyfr5GlZsvA34QGMBKm5xpi2LHje2WQESo6VGFvldARMavZK3vahqbU/nI1S5cfGMJM63giVjJtqbK3TscVO1jQpJZduJgAAAAAAAACGNYIkAAAMgaR4mx66YpHOf2CtaptdfZ7v8nj1yocVeuXDCs3MS9VliyfrSwsnKiM5LgbVAkCEJaRJhScaI5DHLdUfCO5ecmyPVFMitdYNTa0D4ag0xqGNPdfiU/2hkqk9QybpEyUb/4sGAAAAAAAAYGhZfD6fb6iLAIDRoLi4WPPnz+883rFjh+bNmzeEFWEk+Li8Qd9dtUXlda39vjYxzqrzTijQZadM0acmZtClBMDo1lon1ZZJtfuM17qO9/uM0MZoYLUbW+NkTDLeWyySLF2vUs85S+C8JHuSlDVdyp0l5cySsmey1Q4AAAAAYFTjz+YBIPL4624AAAyh4ydm6NXrl+m5zeVatfGA9lU3h31tW7tXz20p13NbyjWvIF23nnucTivKiWK1ADCEksZJheN6djGRJKdDqtvfFSyp3ecPmpRJjYclnzfm5Q6I191Vf8RYpHFTpJzZXeGSjvdJ4yL4HAAAAAAAAACjBR1JACBCSD1jsHw+n9bvq9GqjQe1esdRub39+1e0xSLd+5UFWrmwMEoVAsAI5PNJHpfU3moMd2vX+1BzHccttcZWO3UHpPqDksc51D+ayErJk3JnSzlFRrgkp0gaN1XKmCjZE4a6OgAAAAAAwsKfzQNA5NGRBACAYcJisWjpjBwtnZGj6iannt18SE9tOhj2tjc+n3T/WyVasaCAbW4AoIPFYoQi7AlSUubA7+P1So6j/lCJP1xSt7/rfeNhSSMso99cZYz9a7otWKS0fClzkrHVTtCYMvCgic8nudskZ5PU1ig5G4z3FquUnC0l5xivNv43FQAAAAAAABhK/AkdAADDUG5agr73mZn6zhkz9F5JtVZtOKi3P6lUX01Kyo4160hDmwoyk2JTKACMFVarlF5gjClLeq67nVJDuREu6RxlUq3/1eWIbb2D4pOaKoxxaKP5KWn5wQGThHTJ2RgQEul43xA8723v+/GJmUagJCXHHy7JCnjfMe9/jU81gihWm2SxGa9WuzFHqBIAAAAAAAAYEIIkAAAMYzarRZ+ZnafPzM5TRX2rnv7gkJ754KAqG0Nvr1BS5SBIAgCxZk+QsmcYozufT2o+JtXu84dLyoJfm6tjX+9gNR0xRqigyWC01Rujdu/g7mOxdoVLOl8DQicpOdKE46X8BVL+p6T8E6SEtAj8AAAAAAAAAICRjSAJAAAjREFmkm48a5Z+cOZMvfVJlX783IdqbHP3OK+0yqEzZuUOQYUAAFMWi5Saa4zJi3uuO5uMDia1ZUbYpLVOks8IoHS++gXNmbw6qqRje6SaveF1/xjNfF5jhPocmqukqp3SR8/4JyxS9kwjVFKwwB8wOUFKzIhRwQAAAAAAAMDwQJAEAIARxm6z6nPzJujx9fu1trSmx3ppVdMQVAUAGLCENKMzxoTjI3dPT7sRTqnebQRLju3pej+ittmJJZ9UU2KMHc93TWdN7+paUuB/TRoXwcf6JFez1FortdRILbVGmKjFf9xaa7x3OaS0CdKkxdKUpVLmFLbvAQAAAAAAQFQQJAEAYIQqykszDZKUVPIFIQCMebY4KafIGIF8Pqmxome4pHq30aEDPdXuM0bxC11ziRmSLUGyxUs2u/813vjce7wPmLPapLbGrqBIR3jE4wq/ni2PGq/phUagZMpSacqpUs4sgiUAAAAAAACICIIkAACMUDPzUk3nS6oc8vl8svBlEgCgO4tFyig0xozPBK+5WqSGQ1L9QfMRlaCJRZKvz7OGnbaGoa5AajwsffycMSQpOVuavMQIlUxZanS4sdqGtkYAAAAAAACMSARJAAAYoYpCBEkaWttV7XAqLy0xxhUBAEa0+GQpd7YxzLhapIZyf7DkgPHaETzxuKSEdGMkBr6mBc91n49PlbxuozNH8zGp5ZjRoaO5JuC9/zXwvc8T289mJGipkT75pzEkKT5NmuzfBmfyUikpU3I6jC1yXM3+0dT1vsea/7i9TbInSPEpxohLNv65xSf7j1O6ve84TvWfm2L8845PMbqyEHQFAAAAAAAY9giSAAAwQhWNTwu5VlrpIEgCAIis+GQpd5YxIskaL6VNMEY4vF6prd7YGsbdZoRKvB7J5/W/+o+9bv97b8A5/le3U6raKR3ZLh35cHh0GIk0V5NU+qYxhgur3R80SfWPlIDjFCkhYN6eaPzz8rQbw9suedz+V1fA+3bjn7XH1fVekjImSuOmSuOmSVnTjNeUHIIsAAAAAAAAYSBIAgDACJWVEq/slHjVNLt6rJVWO7R0Zs4QVAUAQJRZrVJyljEiweeT6vZ3hUoqthvvW+sic3908bqN0M5QBXfiU/3hkqld4ZKO9xmTJFvc0NQFAAAAAAAwzBAkAQBgBJuRl6qastoe8yWVjiGoBgCAEchiMYIEWdOkeRcYcz6fsW1PR6ikI2DSciwWBUlJ44ygTFKWlJxtvLfapPItUlVxDGoYpVwOqXKHMbqz2KTMSVLmZGNboLhEKS7J2J7Hnmi8xnW8Jkn2JP96wJzFZnRGcbf5h1NqbzVeO45DvVptUuaU4KBLcjYdVAAAAAAAwJAgSAIAwAhWlJeqTWZBkqqmIagGAIBRwmIxAgWZk6W55xtzPp/UeFiq/kRytXRtpeJxBb/v2G4l1Hp8ij8g0i0o0jGXmGGECkJpqZUObpAOrpMOrDMCLj5PTD6WUc3nMTrT1O0f6kq6xKf5gyVTArqoTDU6qWRMkuzxA7+3z+ffBsgi2fijIQAAAAAAEIw/LQAAYAQryks1nS+toiMJAAARZbFIGRONMZSSs6Q5nzeGJDkdUvkm6cB6I1hS/oHkcQ5tjYgMV5NU+bExurNYpfSJUtZUKTknIKzklNwu49Xj6nrvdnWFmtz+NfmMe1njurqqdHRh6fG+W0eWUN1a7IHndJuzxdFhBQAAAACAEYIgCQAAI1jR+DTT+WMOl+qaXRqXMoi/qQoAAIa/hFRpxpnGkIyQwOGtXR1LDm40AgmdLFJ8qnFdfIp/pPpHwHHHuj3J2H6lvUVyNXeN9hZjqxhXs9GhxeXoOqe9ZUg+ijHF55UaDhpjsLztkrPBGNFksXYFSxJSpcRMowNPUma39xn+48zgtcSMwXVhAQAAAAAAYSNIAgDACBaqI4kklVY7dFJKVgyrAQAAQ86eIE1ZYoxlN0lej9RcLVlsRjAkLin6XSG83oBQSUD4xOkICJ80+987At4HBlVaJavd6GJhizO6ZtjiAubiu953rHW8b2/p2qamtsyoAUPP5+36Z95ybGD3iEv2h1ESjJ8D3V97zCUY4ZOOV3uSFJ/cdZ+4pK5fF3H+1+7robaa6tgeyNNuvHaM7sdej5Q0TkrJZRshAAAAAMCIwf/BAgAwguWmJSg90a7GNnePtZJKh06aSpAEAIAxzWqT0ibE+JlWo+NEQujAa8z4fEaQprbMHy4p87/3Hzsqh7pC9Ed7S+w73tgTjSEFh0V8nv7dx2KVUvKk9HwpLd/4dRn06h/JWf0Pe3nckrtVam/revV5pPQCo5MLAAAAAAD9RJAEAIARzGKxaGZeqrYerO+xVlLV1PMCAACAscRikVLzjDF5cc91V3NX55K6MiN00vllfLdhNtfeYh4osMYZ3SzsCf4gQqhX/3tnU1fYhQ4qw4u7zRiD5fNKjqPG0LbQ59nipdQJRsAkNc+4rr21a4upwLCIu81Y87aHvl9avpQzS8qdI+V2vM6RUnIG/2OSjI4rjiqpsUJqPGz8XI5PkRLTpYQM/2ualJAem45IAAAAAICIIEgCAMAIV5SXZhokKa1yxL4YAACAkSQ+RRo/zxgD5Wn3f5nv7gqGhNoOpS8dHVQ6wy37u7qn1Jb5QwgY1TwuqeGgMSKh6Ygxyt4Nnk/KCg6XdIRN0gu6wh5ul/FzriMk0ljR833T0fC7s1jtRqAkMd3/mmG8JqQZc4mZ0ripUvYMKXumlJxN8AQAAAAAhghBEgAARrii8eZt40sqCZIAAABEnS3OGJEQ2EFl0sk9110tUv2BrmBJ/QGjS4UtwQiw2OL9r3HBc7Z4yR4fMOdf7+h20d7SretKS3DXFdO5tp7XeVyR+RwQfa210sF1xggUnyZlTpJaaoxOI/JF7plet/Hc1trwzk/IkLKnG6GSrBn+gMkM431SZuTqAgAAAAD0QJAEAIARbmaeeZDkaGObGtvalZ4YoS82AAAAMLTik6W844wxHHk9PbdhaW/peexslFrrpbYGqa0++H1bQ9dxuJ0uEDmuJqlq51BXYXA2SBXbjNFdcnZAwGS6lDnV6DAUlyjZ/dtKxSV1bSEVOE+XEwAAAADoE0ESAABGuKLxaSHX9lY5tHDyuBhWAwAAgDHLapMSUo0xWD6f5HL0DJl0dD5xO41thTxO/3tXt1ensTWLxz/cbf4gS7NxD1eLP9ziD7pgZGmpMcahjf2/tjNc4g+W2BIkizVgWLoddx8B61absWWP1d7tfbdji63nuj1eiks26uh8DXyfEnxsT5Ss1p4/Hq/X//Pc/2vB9NdBwKvFFhy06fgc7En+wE2i0cWIwA0AAAAwphEkAQBghCvISFRKvE3Nrp5/Y7OEIAkAAABGIotFSkgzhiZF91leb8DWPYFBk+aurXwsVskaZwQAbPaA93H+gIB/i6PAoIAtzgjENFdLTUelpgr/6xHjtdF/3FxlbDOE2HC3GaOtfqgr6T+7P2widYVFvO1ReJDFPGASnyolpksJ6VJiRrf3Gf736cHv49PMAzAAAAAAhjWCJAAAjHAWi0Uz8lL1UXlDj7XSKscQVAQAAACMIFZrQCeV3MjfP228NGF+6HWP2x826RY0aakxOkN0dI2ISwr4Uj/w1WTO2y4d2yNV75GqP/G//0RqrYv8j6+DPVFKyjICOM4mwjHR4G41RtT5ujr2aLA/ZyxGqCQlR8oolNInShkTA94XSumFRuhkILxe49eKo1JyHJUcVcavH0dV16+h+IAuL/HJRpeX+BT/a3LAfLd1G9vEAgAAYOwiSAIAwCgwM0SQpKSyaQiqAQAAABA2m11KzzdGJGVOlmb+R9exzyc1HzMCJYHhkuo9xhfwvYlPNb7sT8/3vxb4R2HXa9K4ru1QOrYmamuUnI0Brw3djgNemyqkmn2Si/+HGV18krPBGLV7Q5+WkG78PMooNIImHSGTtAmSq7krHBIUFqk03vt6dueMCIvNH+RK7NoSyZ4YsDWQv2NLj+N4SYFbA/kC3vq6PaTbscUqpeQav67SCoxfc6kT/PccAl6P0ZnJ3Rb82u4PNbW3SV63Ebqxxfm7M8V3dW6yxXdb63ae1cY2SgAAAMMUQRIAAEaBorw00/kSOpIAAAAAkIwva1NzjTFtWfBaa31XsKSlVkrODg6K9LdbRNDWRIXhX+fzGcGA2r1SzV6pptT/fp/x6m7rXx0YOZyNUnWjVL1rqCvp4vP4t7hqHupKjHBJWr4/YJLfFeYKnEvMMLY8cjb5Q1tN3UZjiPdNRvCrMyDSZgRE3K3G/aLJFm8EhzInS5lTur1OllLHszUSAADAECFIAgDAKFCUl2o6X17XqhaXW8nx/CsfAAAAQAhJmdKkk40xlCwWYyugtPHSlKXBa16vv2vJ3oCgif998zHJ7TS++GZLHYxGzdXGOPpR6HMstuh1Z4kWj0uq3WcMM7Z4KWNSV7AkMGiSkmP8evd5jc4pPk/Aq38+aM7Tdb7FZnSaiUsytjKyJxqvHduIsa0RAAAAQRIAAEaDovHmQRJJ2lfdrPmFGTGsBgAAAAAizGo1OhdkTJSmn2F+js8nedqNjgqdW3E4u7bgMJv3tBvXdXzBbDpCrHvdXa9Bw9P7sccteZz+DhAtXZ0gXM3RCwLY4o3h9fg7u3TfYgUj3kgLkYTD4zLCYr1tixQNVruxTVFct9GxlZE9wb9NT0LX9j32hK5fZ6HmpK5QS+fvHx3vA8IuHb9v+Dp+7/Aa94tPMQIv8clSXIpRU8d7s7nALZF8PuP3vfaW4C2KOrcpMjl2O41nJmYa25cljTOCh0njjDnbCP56yeuVmqukhsNSY7nx6myUkrKMLaXS/CM1j2ARAGDMGsH/pgcAAB0mjktWvN0ql7vn374rqWoiSAIAAABg9LNYjC9O7fGS+rkdz3DhaQ8IlwSETALnJOMLbHu8/9X/ZXXQa0LAF97xxmfTweczvqDv2MYkcCsTtzNge5NuQZyOLVHaGvxbpzRKbY1dr20NRkAGGOm8bsnVZIyRzGo3gidet/HrOdIBsvi0gHBJZlfYJNF/bLH5T/QZv+90vnafU7c5+QMxKVJ8qhGM6XgfF/A+3h+cCfz9TTLu0VZvhEMayruCIg3lUmPHa4XkbQ/jB2kxwiRpE7rCJWn5wWGTtHwpOatnHQAAjHAESQAAGAVsVotm5KZq15HGHmsllY4hqAgAAAAA0G+2OMmWISVG8S8DWCxdIZNIczsDwiX+wElbvfGlbeDf/G8olxyViuwX2xZju5PUCcb2SCm5RleH9haj20t7i+Rqkdqb/a8t0e0CAww1r9v4tRgtHWGbhoPRe0afLP5gib9bi9UmNR4xfp1HhM/4vcpRKR35MPRp1jjj9+3EDCkxXUpI73qfmOk/9s91f5+QZtRtsRmvVnvXe8IpAIAhRJAEAIBRoigvRJCkiiAJAAAAACAG7AlSaq4x+uJ2SU1HujoEdHYL6AiclEutdUZ3lbTxXQGRVP/7ji4Bqf65lNz+b7XRsR1S93BJ5/YfHVsiObt1bOnl2G3SlSXoy2BL6LX2NuMzaTrS1X1mOLMnGV96e9v920QRykGs+SSXwxhDydsutRwzRiRZrAGhEn/IxGoNeN8xb5XO/bU063ORfT4AYEwjSAIAwChRlJdqOl9KkAQAAAAAMNzY46VxU4wRiscd3b+VH7gdUtK46DxjIHw+o5tL0xGjm0vTEaPLQlNFwGuF1Fzd973iUoyOB91HYkbX+45uDvZEY6uQuCQjJBKXGPDqXwt87f7PxevtCpV0vJq+dxvbK7Uck+oPGqPugP/9gaEPBQDDhc9rjHC24WlvjX49AIAxhSAJAACjRNF48yDJgZpmtbV7lBhnM10HAAAAAGBY6m+HkdHCYpGSMo2Rd1zo89wuyXHUCJc4m6SE1OCwSHxabD9Dq1WyDnLbJJ/P6ETTETDpMQYSNLEEbx3i9QdZgNHEyp/7AQAia4z+lzgAAKPPzLw003mvT9pf06w5E9JjXBEAAAAAAIgae7yUOdkYo4XFIiVnGaNgQc/1jqCJqzk4HGKx9jzufG/S0cbrMTo4tLcGbGXUYmwv1N7i37LIZM7jNDqruJ1GGCVwuF29z0nBNVrtXduS9NiqxBb83u30b73UYmzF1N7a9d7rHvznbu/oOpNsdKCJS5Zs8UZop7Xe+MzD6YoxEtkTpfRC4+dcc7URzPKYbFE13FkIkgAAIosgCQAAo8SU7GTZrRa5vb4eayWVDoIkAAAAAABgZAsMmgyG1ebv4GLe3XVEcbuMQInLH3bpfO8PnFjjusIh3cMiHQESq7X3Z/h8RnintU5qqzdeO0e347Z6Y2smn88f4rEEhHks3eYCXiXjvc/nD800d726HMYWL/1lsUlp+VJGoZQx0QiMZEwMfp+cHRw26ggrNR01tpFq8nf9aQocRyVH5cBqihYrX/cBACKLf7MAADBKxNmsmpaTopKqni1ezeYAAAAAAAAwwtnjjZE0LnrPsFgCgjeTovecUHw+o0OMq8UIlXSGTPzvO4bbKaXmSukTjfBI6oT+b+8UGFYaPzf0eR630cGkqUJqPia1NUrOBiNE09YoORu7vfcfOxsHsD1TGPoKAwEA0E8ESQAAGEWKxqeahkZKq5qGoBoAAAAAAABgkCwWKS7JGCnZQ12NwWaX0vON0V8etxEocTYa4Rifx9iiyOsNeO8xmfcf+zzGXOd7j5R7XOR/jACAMY0gCQAAo8jMvDRJR3vMl1TSkQQAAAAAAAAYcjZ7ZLZoAgAgiuh1BQDAKFKUZ763b9mxZrV7htG+rQAAAAAAAAAAABiWCJIAADCKFI03D5K4vT4dqGmJcTUAAAAAAAAAAAAYaQiSAAAwikzLSZHVYr5WWtUU22IAAAAAAAAAAAAw4hAkAQBgFEmw2zQlO8V0raTSEeNqAAAAAAAAAAAAMNIQJAEAYJSZmWe+vU1JFUESAAAAAAAAAAAA9I4gCQAAo0wRQRIAAAAAAAAAAAAMEEESAABGmaLx5kGSvdUOeby+GFcDAAAAAAAAAACAkYQgCQAAo0xRXprpvMvtVXldS4yrAQAAAAAAAAAAwEhCkAQAgFFmem5KyLWSSra3AQAAAAAAAAAAQGgESQAAGGWS4+2aOC7JdK2kiiAJAAAAAAAAAAAAQiNIAgDAKFSUl2o6X1LVFONKAAAAAAAAAAAAMJIQJAEAYBQqGp9mOl9KRxIAAAAAAAAAAAD0giAJAACj0MwQHUlKqxzyen0xrgYAAAAAAAAAAAAjBUESAABGoVBBkhaXR0ca22JcDQAAAAAAAAAAAEYKgiQAAIxCoYIkklRS2RTDSgAAAAAAAAAAADCSECQBAGAUSk+M04T0RNO10ipHjKsBAAAAAAAAAADASEGQBACAUapovHlXkpJKgiQAAAAAAAAAAAAwR5AEAIBRKtT2NiVVbG0DAAAAAAAAAAAAcwRJAAAYpYry0kznS6oc8vl8Ma4GAAAAAAAAAAAAIwFBEgAARqlQHUma2tyqbnLGuBoAAAAAAAAAAACMBARJAAAYpYpCBEkkoysJAAAAAAAAAAAA0B1BEgAARqlxKfHKSY03XSupbIpxNQAAAAAAAAAAABgJCJIAADCKhdreho4kAAAAAAAAAAAAMEOQBACAUawoL810niAJAAAAAAAAAAAAzBAkAQBgFCsab96RpJQgCQAAAAAAAAAAAEwQJAEAYBSbmWseJKltdqnG4YxxNQAAAAAAAAAAABjuCJIAADCKzQzRkUSiKwkAAAAAAAAAAAB6IkgCAMAolpuaoIykONO1EoIkAAAAAAAAAAAA6IYgCQAAo5jFYlFRnnlXEjqSAAAAAAAAAAAAoDuCJAAAjHJFIba3KalqinElAAAAAAAAAAAAGO4IkgAAMMrNzEsznS+ppCMJAAAAAAAAAAAAghEkAQBglJsZYmubqianGlrbY1wNAAAAAAAAAAAAhjOCJAAAjHJFIYIkklRaRVcSAAAAAAAAAAAAdCFIAgDAKJefkaiUeJvpWmlVU4yrAQAAAAAAAAAAwHBGkAQAgFHOYrFo5vg007WSSjqSAAAAAAAAAAAAoAtBEgAAxoBQ29uUsLUNAAAAAAAAAAAAAhAkAQBgDJgZIkhSSpAEAAAAAAAAAAAAAQiSAAAwBoTqSHK4vlXNTneMqwEAAAAAAAAAAMBwRZAEAIAxoCgvLeTa3mq6kgAAAAAAAAAAAMBAkAQAgDGgcFySEuPM/7VfUkmQBAAAAAAAAAAAAAaCJAAAjAE2q0Uzcs23tympIkgCAAAAAAAAAAAAA0ESAADGiKI88yBJaVVTjCsBAAAAAAAAAADAcEWQBACAMWJmiCAJHUkAAAAAAAAAAADQgSAJAABjxMy8NNP5Q7Utamv3xLgaAAAAAAAAAAAADEcESQAAGCOKxpt3JPH6pH3VzTGuBgAAAAAAAAAAAMMRQRIAAMaIKVnJirNZTNdKqppiXA0AAAAAAAAAAACGI4IkAACMEXabVdNzzLuSlFY5YlwNAAAAAAAAAAAAhiOCJAAAjCEzQ2xvU1JJkAQAAAAAAAAAAAAESQAAGFNm5oYIkrC1DQAAAAAAAAAAAESQBACAMaUoREeSAzUtcrm9Ma4GAAAAAAAAAAAAww1BEgAAxpCivDTTebfXpwM1zTGuBgAAAAAAAAAAAMMNQRIAAMaQqTnJslktpmslVY4YVwMAAAAAAAAAAIDhhiAJAABjSILdpinZyaZrJZUESQAAAAAAAAAAAMY6giQAAIwxRXmppvMlVU0xrgQAAAAAAAAAAADDDUESAADGmJkhgiR7KgmSAAAAAAAAAAAAjHUESQAAGGOK8tJM5/dUOrSzojHG1QAAAAAAAAAAAGA4IUgCAMAYc8LEjJBrf3l/XwwrAQAAAAAAAAAAwHBDkAQAgDFmem6qFk7ONF37x4cVqmxsi21BAAAAAAAAAAAAGDYIkgAAMAZds2y66Xy7x6fH1u2PbTEAAAAAAAAAAAAYNgiSAAAwBn1u3gRNykoyXVu18aBaXO4YVwQAAAAAAAAAAIDhgCAJAABjkM1q0VVLp5muNbS267nN5TGuCAAAAAAAAAAAAMMBQRIAAMaor5w0SWmJdtO1R94vk8fri3FFAAAAAAAAAAAAGGoESQAAGKNSE+y6dPFk07WDtS16Y+fRGFcEAAAAAAAAAACAoUaQBACAMezrS6fKbrWYrv1lTVmMq8FY1ury6IWt5Xp28yGV17UMdTkAAAAAAAAAAIxZ5v3sAQDAmJCfkaTzPlWgF7cd7rG2+UCdth2s08LJ44agMowlB2ta9KUH1+qYwyVJSoqz6d6LP6Vz5ucPcWUAAAAAAAAAAIw9dCQBAGCM++Zp00Ku0ZUE0ebz+fTDZ7Z1hkgkqbXdo9tfKlaz0z2ElQEAAAAAAAAAMDYRJAEAYIybX5ihJdOzTdde23FEh2rZZgTRs/lAnbYerO8xf8zh1JqS6tgXBAAAAAAAAADAGEeQBAAA6JrTzbuSeH3SX9fSlQTR8+ja/SHX3t1DkAQAAAAAAAAAgFgjSAIAALR8Vp5m5KaYrj37wSE1tLbHuCKMBRX1rXq9+GjI9Xd3V8vn88WwIgAAAAAAAAAAQJAEAADIarXo6mXTTdeaXR49velgjCvqyeX26oWt5frJ8x/q3jf2qL7FNdQlYZAeX39AHm/ooEhFQ5v2VjtiWBEAAAAAAAAAACBIAgAAJEkXLCxUdkq86dqj6/ar3eONcUVdfD6fvv/UVt347Id6dnO57n+rRJ+77z1VNrYNWU0YnFaXR0+FEVB6Zzfb2wAAAAAAAAAAEEv2oS4AGEnKysq0fft2VVRUyOFwKD8/X1OmTNHSpUsVFxc3pLVt3bpVJSUlOnz4sCSpsLBQs2bN0sKFCyP6nPr6eq1bt06HDx/WsWPHlJOTo8LCQi1dulSZmZmDvn9ra6t27dqlTz75RNXV1XI4HEpNTVVWVpbmz5+v448/XnY7v3UB0ZAYZ9PlS6bovjdLeqwdaWjT/310RCsXFg5BZdL/fXxEq4srg+YqG53687v79F/nzR2SmjA4L247HNaWSe/uqQ7ZLQcAAAAAAAAAAEQe38YCYXj++ed1zz33aP369abrWVlZuvjii/Xzn/9cOTk5Maurvb1dv/3tb/WXv/xFe/fuNT1n5syZuvrqq3XjjTcOKuyybds2/fznP9err74ql6vndhIJCQk699xzdccdd2jBggX9uvfWrVv10ksv6e2339amTZvU3h76i8WUlBRdfPHFuv7663XCCSf094cBoA9fO2WK/vjOXrncPbuPPLxmn1YsKJDFYol5XY+u3W86//YnlQRJRiCfz6dH15WFde6mslq1tXuUGGeLclUAAAAAAAAAAEBiaxugVw6HQ5dccokuuuiikCESSaqtrdWDDz6o+fPna/Xq1TGpraSkRKeccopuvfXWkCESSSotLdUtt9yiJUuWqLS0dEDP+uUvf6nFixfrpZdeMg2RSJLT6dRLL72kxYsX69e//nVY921ra9OMGTO0aNEi/fd//7fWrl3ba4hEkpqbm/XXv/5VJ554om655ZY+zwfQPzmpCbrwRPOuI8UVjVq/rybGFUk7Kxq1+UCd6dr+mhbVNZv/voTha21pjfZUOsI61+n2asMQ/LwDAAAAAAAAAGCsoiMJEILH49HFF1+sV199NWg+NzdXCxcuVEZGhvbu3att27bJ5/NJkiorK7VixQq9+eabOu2006JW29GjR3XWWWfpwIEDQfMzZ87UvHnz5PP5VFxcHBQw2bJli84++2xt2LBBeXl5YT/rrrvu0m233RY0l5SUpJNOOkn5+fmqqKjQBx98oLa2NkmSy+XSzTffLIvFoh//+Me93tvtdmvfvn095i0Wi2bPnq3JkycrJydHDodDO3bsCDrX4/HoV7/6lUpKSvTMM8+w3Q0QQd88bbqe2nTIdO0va8q0dEbsOi9J0t837O91/cPyei2fHf7vaxh6f1sbXjeSDu/uqeafMQAAAAAAAAAAMUJHEiCEW265JShEEhcXp9///vcqLy/X6tWr9eyzz2rLli3asWOHlixZ0nme0+nUypUrdeTIkajU5fV6tXLlyqAQSX5+vlavXq2SkhK99NJLevnll1VaWqrXXntNEyZM6DyvrKxMF1xwQWfwpS///Oc/dfvttwfNfetb39LBgwf17rvv6umnn9Z7772ngwcP6uqrrw467+abb9brr78e9o/LZrPp3HPP1dNPP62qqirt2rVLq1ev1qpVq/Tyyy9r79692rx5s04//fSg61544QX97Gc/C/s5APo2My9VZ84x/9L+7U+qVFoVXieJSGhoadeL2w73es6HhxpiVE3/vbmzUhf8ca0W/vxfuvjP6/VRef1QlzTk9h9r1tu7q/p1zbt7qqNUDQAAAAAAAAAA6I4gCWBi3759uv/++4PmnnvuOV133XWKj48Pmp87d67eeuutoDBJTU2N7rzzzqjUtmrVKm3cuLHzOCsrS+vWrdPZZ5/d49xzzjlH69at07hx4zrn1q1bp2eeeabP53g8Hv3oRz8KCp3ccMMN+vOf/6ycnOBuBLm5uXr44Yf1wx/+sHPO5/Pppptuksfj6fU5CQkJ+t73vqf9+/fr1Vdf1cUXX9zj/h0WLVqkt99+W5dccknQ/G9+85se3VkADM7Vy6aFXHvk/f51kxiM57YcUlu7t9dzPhyG4Yy2do9++uLHuvrxzdp2sF51Le3aWFaryx/ZpKMNbUNd3pB6bP1+hcozfnnRRNP5fdXNOlTbEsWqAAAAAAAAAABAB4IkgIk777xT7e3tncdf//rXtWLFipDnJyUl6dFHHw0KmTzyyCOm27YMhsfj0R133BE0d88992jq1Kkhr5k2bZruueeeoLnbb79dXm/vX8w+/vjj2r17d+fx7Nmzdffdd/d6zS9/+UvNnj2783jnzp1atWpVyPMTExNVWlqqBx54QBMnmn952J3NZtMjjzyiSZMmdc65XC49++yzYV0PIDxLpmdrXkG66doLW8tV43BGvQav16cnNvQdEtt+qD7sTkuxsPtok85/4H09ufFgj7WG1nb96d29JleNDU1t7Xpuc7np2pwJafrJ52abrknSeyV0JQEAAAAAAAAAIBYIkgDdtLa26vnnnw+au/nmm/u8btasWVq5cmXnsdvt1pNPPhnR2t5//32VlXV1AigsLNTXvva1Pq+7/PLLVVhY2Hm8d+9erVu3rtdrHn/88aDjG264QQkJCb1ek5CQoOuvv77X+wSy2+1hB0gCJSUl6aqrrgqa+/e//93v+wAIzWKxhOxK4nR79fcwAh6Dtab0mPbX9N2ForbZpfK61qjX0xefzwi+nP/A+9pTGXr7n/+3pVwtLncMKxs+nt9SLofT/Md+1alTlZeeqLn55gGmd3cTJAEAAAAAAAAAIBYIkgDdrF69Wi0tXV9cLlmyRHPmzAnr2u7hhhdeeCGitb344otBx1dccYVsNluf19lsth6Bk95qq6mp0Zo1azqP4+Pjdemll4ZV42WXXaa4uLjO43fffVe1tbVhXdsfCxcuDDquqKiI+DOAse6LJxRoQnqi6drf1x9QW3vvW1cN1uPr9od97vZD9VGrIxwNLe26dtVW3f7SDjndvXd8anK69fL2sfd7ltfr02Mh/pmOS47TigVG4PGM2bmm56zbW6N2T++fLQAAAAAAAAAAGDyCJEA3r7/+etDx8uXLw7522bJlstvtncfbtm1TZWVlpEobVG3dz33ttddCnvvGG2/I4+n6gnjRokVKS0sL6znp6ek68cQTO4/dbrfeeOONsOsMV+DnLBnb2wCIrDibVV8/darpWk2zSy9uOxy1Zx+qbdHbu6vCPn8ogySb99fq879bo9d2HA37mic2HBhW2/HEwr93V4XsMHPp4slKjDOCkacXmQdJHE63th6oi1p9AAAAAAAAAADAQJAE6GbHjh1Bx0uWLAn72pSUFB1//PFBc8XFxRGpy+l0qrS0NGjulFNOCfv6pUuXBh2XlJSEDF8M5jMwe1akPoNA3T+L/Pz8iD8DgHTJyZOVEm/e+eiR98vk9UYnDPHExgPqT87iwyEIkni8Pv3+rRJd/NAGHa7v39Y6xRWNQ95FpTdVTW06VNsipztyXWf+tna/6bzdatHlp0ztPF40ZVzIn3Pv7mF7GwAAAAAAAAAAoo0gCdDNrl27go5nzpzZr+tnzJgRdLxz585B1yRJu3fvDuoSkpeXp/T09LCvT09PV05OTuexx+PRnj17TM/tXvNw+QwCPf/880HHJ598csSfAUDKSIrTV06aZLpWWuWIyhf7be0ePfvBoX5ds6OiIabbnhxtaNNlf9mg376xR54Bhmme2HAwwlUNXrvHq9te/Fin3PWWlv363/rcve9pw76aQd93T2WT3i89Zrp27vH5mpDRtYVSvN2qpTNzTM99r4QgCQAAAAAAAAAA0UaQBAhQW1ur2traoLnJkyf36x7dzy8pKRl0XVLPDhz9rcvsmlC1DfZZ0foMOnzwwQdau3Zt0NwFF1wQ0WcA6PKNU6fJajFfe3jNvog/758fHVFdS7vp2ldDhFra2r3aU9kU8VrMvLmzUufe/5427Kvt9bx4u1U/XzFPy4rMQxH//KhCdc3Da1uuJzYc0KqNB9WRjdlf06IrHtmkdXvNQyDhCtWNRJKuMtk+6fRZ5tvb7DjcqOom56BqAQAAAAAAAAAAvbMPdQHAcFJfXx90nJycrJSUlH7dIy8vL+i4oaFhsGVJ6llb9+eEI9zaBvusaH0GktTe3q5vf/vbQXPLli2LeEeSqqoqVVf372++dw/gAKPFpKxknTN/gl79+GiPtXV7a1Rc0aB5BRkRe97j6/ebzqcl2HXrucfphW2H5XL37D6y/VB9ROvozun26O5XP9Gj68zrCzQjN0W/v+REzS1I1/j0RK0p6RnEcLq9en5Lua45fXoUqh2Yv6wp6zHn8nj1rce36KlrTtHxE/v/+da3uPTitnLTtU9NytSJk8f1mD+jyDxIIklrSqr1pRMn9rsOAAAAAAAAAAAQHoIkQACHwxF0nJSU1O97dL+mqSkyf0M+lrUN9lnR+gwk6cc//rG2bdvWeRwXF6ff/e53Ebt/hz/+8Y+68847I35fYKS6etl00yCJZIQP7r14QUSes/1QvT4qNw+fXbhoojKS4zSvIF3bDtb3WP/wUL0uWzwlInV0t7faoe8/uU07jzT2ee5XT5qk/zpvrpLjjf/M+uycPOVnJOpIQ1uPc1dtPKBvnjZN1lAtX2LoUG2LDte3mq45nG59/W+b9Nx3lmh6bmq/7vv0B4fU1m6+7dBVS6eazk/OTtb0nBTtO9bcY+29PQRJAAAAAAAAAACIJra2AQJ0D1AkJib2+x7dQxTd7zlQsaxtsM+K1mfw17/+Vffff3/Q3M9+9jMtWLAgIvcHENqJk8dp0ZSenSMk6R8fVuhIg3kAob9CdSORpMuXGCGRT03MNF3/8FDkuh8Feu3jIzrv9+/3GSJJS7Dr95cs1C8vPKEzRCJJdptVl5xsvkXY/poWrR3ktjGRsmFfTa/rNc0uXf7Ipn79s3Z7vHo8RAeXvLQEff74/JDXhtre5r2SY/J27L0DAAAAAAAAAAAijiAJ0AuLpf9/Q3wg1wxELGvr73XR+Axef/11fec73wma++IXv6hbb7014s8CYO6aZdNM591en3731uC3dqptdumfHx0xXTttZo5m+DthLJycaXrOnqomOZzuQdcR6JOjjbruqW1qcXl6PW/BpEy9ev0ynfepAtP1r540SfYQXUee2HBg0HVGwsay2j7POVzfqise2aS6ZldY9/zXzkpVmHRikaSvnTJF8fbQ/yl6RoggSW2zSzsqohMaAgAAAAAAAAAAbG0DBElNDW7X39ra/79h3/2a7vccqFjWlpqaqrq6ugE/K9Kfwdq1a3XhhReqvb29c+60007TM888E7XgzrXXXquLLrqoX9eUlpZq5cqVUakHGA7OmjtBk7OSdbC2pcfaU5sOavG0LK1cWDjg+z/zwSG53OZboHR0I5FCdyTx+aSPyxu0ZEb2gGvo7smNB+XppfuFxSJ954wZuvGsWYqzhQ5F5KUn6ux54023B3pjZ6WONLQqP6P/W5ZFUl8dSTqUVDl01aMfaNXVi5WS0Pt/Sv5tbZnpfLzNqksXm3dp6bB4epbi7VbTnxPv7anWCSF+HgAAAAAAAAAAgMEhSAIEIEjSNT9cgiRbtmzRF77wBbW0dH1xffLJJ+v//u//lJycPOD79iUvL095eXlRuz8wEtmsFn3j1Kn62T92mq7f8sJHmj0hTcflp/f73h6vL2RnjsLMJH12TtevxynZycpMjlN9S3uPcz8sr49okOTdPdUh13LTEnTvVxbotKKcsO71tcVTTIMkXp/01KZDuvGsWQOuc7AO1baovC783+u3H6rXd57YokeuPClkV5Edhxv0wf4607XzFxQo5/+zd9fhUV35G8DfmUkm7u4uaIIlaPBCKRVaWtpS6u7t7nYr27Xqbt1lKxQrdaOluJMECQRJIAkh7u42M78/+EEJOWcyk8zEeD/Ps88299y5c0gmMzf3fO/7tbfS+xy2agvEBbtid1bX1j87Msrx4KwIg+dLRERERERERERERESGY2sbovM4OTl1+rqpqQmNjY1GHaOsrKzT187Ozr2dFoCucysvly9uyhg6t94+l6m+B0eOHMEll1yC2to/WhiMGTMGGzZsgKOj8QvVRNR7100IgLejtXCspV2Le1cdRG1z1wKP7mw9UYbCGnEhw43xgbA4L+1DoVBIU0lS82uMfm6Z3MpG5FZ2TV8BgIRID6x/ZJrBRSQAMCnMDaEedsKxtfvy0K4Rp7H0BUPa2lxoV2YFHv/6sDSx5TNJGgkA3DYl2KDnkLW3Scmr6dHrjIiIiIiIiIiIiIiIusdCEqLzuLm5wcXFpdO2vLw8o46Rm9v5jvqICNPcMX3hcS58HkMYOrfePpcpvgdpaWmYM2cOqqr+WNwcOXIkNm7caLLiHCIynq3aAm8siYVKKW4rlVvZhD99fRhaPe1gRFYk5gi3q1VKXD8hoMv2mABn4f6HTVhIslOSRqJQAG9cF9NtokbXxylwU3yQcKysvhWb00qNnqOpJEva2gS52SLcU54qte5IMf7583HodJ1/3uX1rViXWix8TFyIK0b4OgnHLjQ9SlxIotHqsFeQVEJERERERERERERERL3HQhKiCwwbNqzT11lZWUY9Pjs7W+/xeioqKgoqlerc12VlZaivrzf48XV1daio+GPRTaVSSQs8+vt7cPLkScyePbtTEkp0dDQ2b94Md3fD7/4nIvOYFOaGv86Pko5vTi/D+9sNf9/ILm/ArkxxUcBlo33gJijYiA0QFyIU17agtK7F4OfWZ0eGeE4jfZ2EczLENeP8YW0pPv1aKWnt0xeSTosLSWZEemDF7XHwdRKn0ABn5v3m5sxO21Yn56JNkrByu4FpJAAQ4WkvTcDZmWl8MhcREREREREREREREXWPhSREFxg5cmSnrxMTEw1+bGNjI44cOaL3eD1lZWWFsLCwHs9t7969nb6OiIiAlZV4IbQ33wMA2LNnj97j6ZOVlYVZs2ahpKTk3LaIiAhs3boVXl5eRs2DiMznrmmhWDDKWzr+2qYM7JAkelxIXwHFskniBA9ZaxvANO1t2jq0SDwlLiRJiOx5QZuTjSWuiPEVju09VYlT5Q09PnZPFVQ3Ib9K3FYoPtQNvs42WHFHPFxsLaXHeGtLJpb/fyubtg4tViWJ07z8nG0wZ5jh7+UKhULa3mbHyfIuSShERERERERERERERNR7LCQhusD8+fM7fb19+3aDH7tr1y50dHSc+3rMmDEmLX7ozdwu3PfSSy+V7jt37txO6ScHDx40OP2kvr4eKSkp5762sLDA3LlzDXrs6dOnMWvWLBQVFZ3bFhoaiq1bt8LHx8egYxBR31AoFPjv4hiEedgJx3U64JG1h5Bf1aT3OE1tHfj2YIFwbKSfI8ZIWti42VshwNVGOGaK9jYpedVobNMIxxIixIUNhlo2MVg6tlpSgGFOydlV0rG4EFcAQLinPZbfFgc7tUq67z9/ScNPhwvx69EiVDS0Cve5eVIQLFTGnX7K2tsU1bYgq6zvC2+IiIiIiIiIiIiIiIY6FpIQXWDevHmwsfljcTIxMREnTpww6LHLly/v9PWiRYtMObUux1u5ciU0GvFC5/k0Gg1WrVpl8Nzc3d0xderUc1+3tbVhzZo1Bs1x9erVaG9vP/d1QkICXF1du31cXl4eZs2ahfz8/HPbgoKCsHXrVvj7+xv03ETUt+ytLPDRsnHS4oKapnbct/ogWtrl71M/HipCfUuHcOzmScFQKBTSx8pSSVILaqSPMdROSZqKvZUFxga59OrYo/ydEOMvbs3z7cF8NEsKWMwlWdLWJtLLHu7ntfCJCXDGxzePh1pPIcifvk7FqxsyhGM2lipcPyHQ6PlNCXeHSil+HRiaekNERERERERERERERIZjIQnRBWxtbbF48eJO2/7zn/90+7iMjAz88MMP5762sLDAjTfeaNK5TZs2DSEhIee+Ligo6FIgIrJq1SoUFhae+zosLAxTpkzR+5ibb76509dvvPEGWlvFd5if1draijfffLPTtltuuaXb+RUVFWH27NnIyck5t83Pzw9bt25FUJC4rQURDQzhng545doY6fixwjr8/adjwhYkOp0OKxJzhI9ztpW3gDkrVpJWciS/Flpt71qe7MwUFyhMCnODpZGJGiJLJ4rf2+paOvDLkSLhmLkkSRJJ4kPcumybEu6Ot66PhaSuAx1aHQprxG1yrh7rByc97XFknGwspT9rFpIQEREREREREREREZkeC0mIBP75z3/C0vKPxa7ly5fj559/lu7f0tKC2267DW1tbee23XHHHQgLC9P7PAqFotP/umtVo1Kp8K9//avTtscff7xTAcaFcnJy8Nhjj3Xa9vzzz0Op1P/rf8sttyAqKurc1ydPnsTTTz+t9zFPPfUUTp48ee7r4cOHY+nSpXofU1ZWhtmzZyMrK+vcNh8fH2zbtg2hoaF6H0tEA8OCUT64O0H++/r1gQKs3Z/fZfuB3GqcKBG3zbpufACsLeVtVAB5IUl9aweyKxr1PlafioZWHCusE44lRPaurc1Zl4/2haO1hXBsVVKuSZ7DEEU1zciTtB+aGNq1kAQALh3lgxcWjTL6uW6bEmz0Y86aLvm+J5+u6vMEFyIiIiIiIiIiIiKioY6FJEQCoaGheOSRRzptW7x4Md59991OxSIAkJ6ejtmzZ2Pv3r3ntrm5ueEf//iHWea2dOlSxMfHn/u6qqoKkydPxsaNG7vsu2HDBkyaNAnV1dXntk2ePBlLlizp9nlUKhVeffXVTm0lXn/9ddxzzz2orOzcBqGiogJ333033njjjXPbFAoFXnvtNahU8oXgmpoazJ07t1PrIDs7O3z66aewtLRETk6OUf8jov7zxLwoTAyVt7H6x0/HkZpf02nbF3tzhPsqFMBN8d2nEY3wdZK2PDl8wXMZY3dmhXRseoRpCkls1CpcOz5AOHakoBZHTNCexxCytjYAEK/n53lDXCD+Mi9KOn6haRHuCPd0MGpu55MVkrR1aPX+G4iIiIiIiIiIiIiIyHjiW2GJCC+//DKOHz+O9evXAwDa29vx0EMP4bnnnsPYsWPh4OCA7OxspKSkdGrZoFar8cMPP8DHx8cs81Iqlfjhhx8wceJE5OXlAQCKi4sxb948REREYMSIEdDpdDh+/HinlA8ACA4Oxvfff9+pOESfhQsX4vnnn8czzzxzbtvHH3+MlStXIj4+Ht7e3iguLsa+ffvQ3Ny5lcHLL7+M+fPn6z3+4cOHceTIkU7bGhsbsWDBAoPmdyFR6wwi6hsWKiXeuWEsLn9nN0rqWrqMt2m0uG/VQax7eBpc7dQoq2vB78dKhMeaEemBQDfbbp/TRq1ClJcD0oq7poek5tdg8Th/4/8hAHZK2qUEu9kaNC9DLY0PxKe7TwvHViXl4r+LnU32XDJJp8RtbSI87eFub6X3sffPCENVY5v033C+26eEdLuPPiP9nOBia4nqpvYuYzsyyjEjyrNXxyciIiIiIiIiIiIioj8wkYRIQqVS4euvv+6S3lFWVobff/8d33zzDQ4ePNipeMHT0xM//fQTpk2bZta5+fj4YNOmTRgzZkyn7ZmZmfjxxx/x008/dSkiGTt2LDZt2gQvLy+jnuvpp5/Giy++2KnVT3NzM7Zv3461a9dix44dnYpILC0t8fLLL+OJJ57owb+MiAYzDwcrvLd0LCxV4mK1otoWPPzlIWi0Ony5Lx8dWnHx182Tgg1+zhhJe5vUHiZ6aLU67JQkkpiqrc1ZoR72mBIubh/zc2oRagVFE6aWJEnz0JdGcpZCocAzC4bh6jF+evcLdbeTJooYSqVUYJokDWaHpPCHiIiIiIiIiIiIiIh6hoUkRHrY29tj7dq1+OabbzBx4kTpfq6urrjvvvtw7NixblM4TCUyMhLJycl46aWXEBoaKt0vLCwML730EpKSkhAeHt6j53rqqaeQnJyMK6+8Emq1WriPWq3GlVdeiX379uGvf/1rj56HiAa/cUEu+PvC4dLx3VkV+M/vJ7BmX65wPNDV1qiigzGSQpL04jq0tGsMPs65x5XUoaKhVTiWYKK2NueTtfBpadfi25QCkz/f+Yprm5Fb2SQcmxgqLnC5kFKpwH8Wj8bsaHkiyC2Tg6GUtCAyhux1kV3eiPwq8b+DiIiIiIiIiIiIiIiMx9Y2RAZYvHgxFi9ejNOnTyMlJQVFRUVobGyEt7c3goKCMGXKFGmBhT69bcViaWmJJ598Ek8++SQOHjyIjIwMFBUVAQB8fX0RGRmJcePG9eo5zhozZgx+/PFHVFdXY+/evSgsLERlZSXc3Nzg5+eHyZMnw8XFxahjzpgxg+1oiIagmyYG4VB+Db5PKRSOf7wzW/rYZRODjCo6kCWStGt0SCuuw9hA496XdmaI00gsVQpMCjOsuMIYc4Z7wcvRCqV1XYtXVifn4vYpwQa3IzNWcra4rQ0AxIcY/m+1VCnx3tKxWPZpMvbnVHca83O2wTU9bDF0oWmR7tKxnZnlWCopyiEiIiIiIiIiIiIiIuOwkITICCEhIQgJCenvaQiNGzfOZEUj+ri4uOCyyy4z+/MQ0eClUCjwwlWjkF5cj/TiOoMfZ2WhxLXjjSs6CPe0h51ahca2rukjqfk1RheS7MgoE24fF+QCOyvTnzZZqpS4fkIg3tqS2WUsu7wRiacqMTlcXkDRG0nZ4rY24Z728HCwMupY1pYqfHrrBPzjp+P44dCZAqJQDzu8tWQM7E30ffN0sMZwH0ekCV5TO06ykISIiIiIiIiIiIiIyFTY2oaIiIhMzkatwoc3jYWjteFFBFfG+sLZ1rh0J5VSgVH+TsKx1Pwao47V2NqBg7nVwrEEI9rtGOuGuECoJCksq5LFLYBMQVZIEh/i2qPjOVpb4o0lsdj3zGxsfjwBmx+bLv3Z9NT0KPHPYe+pSrRrtCZ9LiIiIiIiIiIiIiKiixULSYiIiMgsgtzs8Ob1sQbvf/Ok4B49j6y9zWEjC0kST1WiXSNut5UQYb5CEm8na8wZ5ikc23C8FKV1LSZ/zpLaFuRUNgnHJob2roWPp4M1wj0djGpRZCjZz6GhtQMpkiIgIiIiIiIiIiIiIiIyDgtJiIiIyGxmRXvhkdkR3e43JtAZI/16ll4R6+8s3J5T2YSapjaDj7Mzs1y43d1ejeE+jj2ZmsGWTQwWbtdodVi7L9/kz5d8WpxGAgDxoT1LJOkL44JcYKdWCcd2ZIh/fkREREREQ0VbhxYHc6txtKAWOp24CJ6IiIiIiMgUWEhCREREZvXI7AjMkLQkOeuWHqaRAEBsoLN0LLWg1uDj7JQUIkyL8DBLusb5Joe5IcTdTjj25b48dJi4bYusrU2ohx08HaxN+lympLZQYnK4u3CMhSRERERENJQdK6zFJW/swDUf7MXl7+7G1R/sRUmt6dMLiYiIiIiIABaSEBERkZkplQq8uSQWAa42wnE3OzUuHeXd4+N7O1rD08FKOHY4r8agY+RVNklbvSREigsXTEmpVGBpfKBwrKSuBVtOlJn0+ZKyq4Tbe9vWpi8kRIqLko4X1aG8vrWPZ0NEREREZH4dGi3+/E1qp79ZDuXV4Llf0/pxVkRERERENJSxkISIiIjMztlWjQ+WjoOVRddTjwdmhsPKQtyuxBAKhQIxAc7CsdSCGoOOsUPS1gY4k0jSFxaP8xd+fwBgVVKuyZ6ntK4FpysahWODoZBkup6fxy49P0ciIiIiosEq+XQVTpTUd9n+65FiFlMTEREREZFZsJCEiIiI+sRIPyesuSsePk5nWqeoLZS4bUowbpsS3Otjx8oKSfJrDOodLmtrM8LXEe724rQTU3O2VePyGF/h2K7MCmnxh7FkbW0AYGKIq0mew5wC3WwRKmkDxPY2RERERDQUbU4vlY5tO2na9EIiIiIiIiKAhSRERETUh8YFuWL3X2dh75OzkPzUbPzj8hFQKBS9Pq6skKSysQ0F1c16H9uu0SLxlLi4QtZGxVxumhgkHVuTbJpUEllbm1B3O3g6WpvkOcxN9nPZlVkBrbb7wqGhoK6l/aL5txIRERFdzHQ6Hbaky4tFtpm4DSYRERERERHAQhIiIiLqYyqlAr7ONnCxU5vsmKP8naRjh/Nr9D42JbcaDa0dwrGEPmprc1aMvxNG+jkKx745WICWdk2vnyNZkkgSPwja2pw1XVJIUtXYhmNFtX08m761J6sCiz/Yi5h/bcSY5zbh1Q0nDUrdISIiIqLBKausAXlVTdLxXZkVaOvQ9uGMiIiIiIjoYsBCEiIiIhr0HK0tEeYhbneS2k0hyc5McTsUO7UK44Jcejs1oygUCiyTpJLUNLXj+5TCXh2/rK4F2ZIWORNDB35bm7PiQ12hthCfxu44OTTb21Q3tuHP36Ri6SfJOJBbDZ0OqG1ux7vbsvDetqz+nh4RERERmclmPWkkANDQ2oEDOeLUQSIiIiIiop5iIQkRERENCbEB4qKP1IIavY/bmVEh3D4pzE1arGBOl8f4wsHaQjj2/vYstGt6frdh0mn5BeaJgyiRxFZtgbhgceGLrDBosNLpdPg5tQhz39iBbw8WCPd5Z2sWimr0t3AiIiIiosFpS3ppt/tsZXsbIiIiIiIyMRaSEBER0ZAQGyBub3O0sFZafFHZ0CpthZIgaZ9ibrZqCywe5y8cK6huxo+Hep5KkiRpaxPibgcvR+seH7c/yNrbpOTVoLa5vY9nYx6FNc2444sDePjLQ6hoaJPu19qhxasbT/bhzIiIiIioL1Q1tiElr7rb/baeZCEJERERERGZFgtJiIiIaEiICXAWbm9p1yKjtF44tjurAjqd+HgJEf1TSAIAd04LhaVKIRx7b1sWOnqYSiIrJBlMbW3Omh4l/vlotDrszRKnzAwWGq0Oy/ecxiWv7zD47tIfDhXiWKG4KIqIiIiIBqdtJ8qglfy9cr7s8kbkSFpYEhERERER9QQLSYiIiGhIiPZ2lLaiSc0XL7DvyBC3QQl0tUWwu53J5mYsP2cbaSpJTmUTfjlSZPQxy+pbkF0uvrg8mNranBXhaQ9vSYrKuqPFvWoB1J9OltTjmg/24p+/pKGxTWPw43Q64KX16dDJKqOIiIiIaNDZcqL7tjZnsb0NERERERGZEgtJiIiIaEhQWygxwtdROJaaX9Nlm06nw65McXJFQqS7KafWI/dND4dKKU4leXdrFjSG3Jp4nuTsKulYfMjgKyRRKBTS9ja/HinG9P9uw8c7T6GuZXC0uWlp1+D1jSex8J1dOCx4vRpiT1YltkuKo4jMKa+yCbmVjdAa+b5EREREcm0dWuzMMDxpbxvb2xARERERkQmxkISIiIiGjBh/Z+F20cJ8enE9yutbhfv3Z1ubswLdbLFojJ9w7FR5I347WmzU8WRtbYLdbOHtJE72GOhk7W0AoKi2BS/+dgKTX9qK59elobCmuQ9nZpx9p6uw4O1deHtrFto1+hfiLZQKLBjlLR1/+bcTRhcZEfVUTVMblnyUiIRXtmH6K9ux6P09yKts6u9pERERDQnJpyvR0Nph+P7ZVWg0Yn8iIiIiIiJ9WEhCREREQ0ZsgLNwe0ZZfZeLsDszxckNFkoFJoUNjISOB2aGQxJKgne2Zhp197+skGQwtrU5a0q4uzS15ayG1g58svs0Ev67DQ99eQhHCmr6ZnIGqGtpxzM/HMV1HyVK2w6dL8bfCb88NBXv3TgWcSGuwn1Oltbj24P5pp4qURc6nQ73r05B8uk/0o5SC2rx0NpDTCYhIiIygS3p4oQR2flvm0aL3VmGJ5gQERERERHpw0ISIiIiGjJkhSQ6HXCssLbTtp2SFiBjg1zgYG1p6qn1SIi7Ha6I8RWOZZQ2YGNaiUHHKa9vxSlJocJgLiRxsrHEdeMDDNpXo9Xhl9QiXPHuHlz3USI2p5X262L3huMlmPv6DqxOzut2XxtLFZ5dOBzf3z8Fw3wcoVAo8MyCYdL9X9uYgaY23o1K5rX+WAn2nupaoJaaX8NFLCIiol7S6XTYnF4qHJs/whvu9lbCsW0n2N6GiIiIiIhMg4UkRERENGQEudnCyUZcBHJ+e5umtg4cyKkW7jc9sv/b2pzvwVnhUEhCN97akgWdrvtiiOTT4jQSAIgPFSdbDBbPLhyGxeP8jXrMvtNVuHPFAcx5YwdWJ+eipV1jptmJfbTjFO5ZeRCldeLWSuebHumBjY8l4I6pIZ3uPo0JcJYWGZXVt+J/O0+bbL5EF2pp1+DF39Kl4yuTcvtwNkRERENPRmkDCqrFrRnnDPfETEmLx60nygz6+2Coa+vQ4psD+Xj2x2P4aMcpFlkTEREREfUAC0mIiIhoyFAoFIiRpJKknldIkpRdiTaNVrhfQsTAKiQJ93TAZaN8hGPpxXXYLIm8Pp+srU2Qmy18nGx6Nb/+Zqu2wKvXxmDTYwm4fkIA1BaGn95mlzfimR+OYfLLW/H6pgxUNbaZcaZnFNY049WNJ7vdz9VOjbeuj8Xy2yYgwNVWuM9f5kVBrRL/ez/aeQpl9S29miuRzGd7TksXtwBgS3opCmvk40RERKSfLI1EqQBmRHpiVrSncLysvhXHi+rMObUBT6vV4Y4v9uMv3x7ByqRcvLT+BC59a1eXVqdERERERKQfC0mIiIhoSJG1tzm/kGRnhrjtgpudGiN8Hc0wq955cFa4dOydrZnd3nWYnF0l3D4xZPC2tblQhJcDXr5mNPb8dRYenh0BF1vD2xNVNbbh7S2ZuOSNHZ1eJ+awdl8e2jX6f15Xj/HD5sen48pYPyhkcTQAAlxtccvkIOFYU5sGb27O7NVciUTK6lvw3tYsvftodcCXBrRtIiIiIrEtkkKS8UGucLFTY2qEOyxV4vPErRd5e5uvD+RjV2bnv/dyK5vwv53Z/TQjIiIiIqLBiYUkRERENKTEBjgJtxfVtqCs7kxCw86McuE+UyPcoVTKF+77S7S3I+aP8BaOHSmoxXbJvwcAKhpakVnWIBybGDa429qIeDhY4fG5kdj75Gw8f9VIhLrbGfzYioY2PP71YWi05okDb9do8dX+fOm4v4sNVtweh9eXxMLVTm3QMR+cGSFt5/TV/nxkldX3aK5EMq9tyEBjW/ftoNbuz0Nbhzj5iYiIiOQqGlpxSFLcPHvYmSQSB2tLxIWIz+Uv5kISnU6H5XtzhGO/HS3u28kQEREREQ1yLCQhIiKiIWW0v7N07HB+DfKrmpBd0SgcH2htbc730Gx5KsnbW+SpJLI0EgCIH0KJJBeyUatw08QgbH58Ov5383jphfYLnSpvNNvF9y3ppSirbxWOXTvOHxsfS0BCpHGvQSdbSzwkSazRaHV4ef0Jo+dJJHOssBZfH5QXQ52voqEN649xwYaIiMhY206UQRY4OHuY17n/nhklbm+TWlCDygbxOedQtz+nGidKxIXUmWUNKK5l6z0iIiIiIkOxkISIiIiGFHd7K/i72AjHUgtqsDNTnt4xLdLdXNPqtRG+TpgzTHyx+FBeDfZkVQrHkrLF2wNdbeHrLP4+DSVKpQJzh3vh63sm4ecHp+DyGF+oukmdWb73tFnmslrS6sPaUom/LRwOW7VFj467bFIQAlzFP8vN6WXS1wCRMXQ6HZ5blyZd2BJZlZRrvgkRERENUVvSxUXNwW62CPP4I21vVrT4bwOdDth+Uv43z1C2IjFH7/iFLW+IiIiIiEiOhSREREQ05MQGOAu3p+bXYofkouowH0d4OlibcVa999CsCOnYW1syhKkkyafFRQQTQ4deW5vujPZ3xjs3jMGOv8zAnVNDYG8lLtzYk1WJjFLTtoTJrWyUXri+IsZX2p7GEFYWKjwxL1o6/uJv6dCaqV0PXTw2HC9B8ml5wpHI/pxqpBfXmWlGREREQ09rhwa7JIXvs4d5QaH4oyA61MMewW62wn23nrz42tuU1bXg92MlevfZzUISIiIiIiKDsZCEiIiIhhx5IUkNEk+JCysSBnAayVkxAc6YESVufbI/pxpJF7SxqWhoRUZpg3D/odzWpjv+Lrb428LhWHlHnHQfWW/1nlqzT5xGAgBL44N6ffyFo30QI3ndHymoxS9Hinr9HHTxau3Q4IXf0qXjf5obKR1jKgkREZHhkrKr0NimEY7NFqQTzpSkkuw8WY52jdakcxvo1u7PR0c3xdO7sypYYE1EREREZCAWkhAREdGQI1tQr2/tQH1rh3BseoS4QGOg0ZdK8s7WzE5f79OTHhB/ESaSXGhMoIu06Oj7lALUNrWb5HlaOzT45kCBcGyknyNG+zv1+jkUCgWeWTBMOv7f30+ipV28KEHUnc/35CC/qlk4NiXcDQ/OCseEYBfh+A+HClHfYprfJSKiwaS6sQ3/+OkYrvlgL574NhVZZeLiXqLzbUkvFW53sLbAhOCu5++y9jb1rR04kFNt0rkNZB0aLdZI2kier6qxDWlMSyMiIiIiMggLSYiIiGjIGenrBJVS0f2O/8/GUoVxkkXQgWZckAumhovTU/aeqsT+nD+KR5KyxekrAa428HcRx2BfbG6bEizc3tKuxVcHur8YbYjfj5WgqrFNOHZjXFCniPLeiAtxxdzhXsKxwprmbnvGE4mU17fi3a1ZwjGlAvjbZcOhUChw00Rxsk5TmwbfpxSac4pERANOS7sGV72/B18k5uJgbjW+PlCAxR/uNXnrPBpadDodtqSLW9LMiPKEparrZdy4EFfYqVXCx2y7iNrbbEorRUldi0H77pS0DiIiIiIios5YSEJERERDjo1ahSgvB4P3nxTmBisL8QXYgejh2fJUkre3/JFKkpwtTiSZeBG3tbnQpSN94OFgJRz7Ym8uOkwQCS67O9LeygJXxPr2+vjne/LSaGkR1btbs1DTJC5oIZJ5fdNJNEiSnK6PC8QwH0cAwPyR3nC3Vwv3W5mUC52OMfJEdPFYuy8PuZVNnbbVNLXjkbWH0drBhDASO1FSj8IacQLYHEFbGwCwslBhaoS4yHzriYunkGRFouGt9HZlVJhxJkREREREQwcLSYiIiGhIkrW3EUmQXHwdqOJCXBEfIm5NsyuzAofyqlHZ0IqTkrte40NZSHKW2kKJm+LFSQqFNc3YLLkr1FBZZfVIlrQYumqML+ytLHp1/AuFedjjhrgA4VhdSwfekSRLEIkcL6rF2v35wjEHKws8Pjfy3NdWFiosmSB+7WWVNSBJUthGRDQUrT9WItyeXlyHNzZlCseIZG1tVEoFZkSKC0kAeXubrLIG5Fc1CceGkszSeiRKkhhFDuRWoalNXCRLRERERER/YCEJERERDUmxAU4G75sQ6WHGmZjHI3pSSd7ZmoV9kuIFANIilIvVjfGBUAuiwgFg+d7TvTr2aj292m+MExew9NajcyKlBSorEnOQVzn0FxSo93Q6HZ5blwZZkMhDs8Phbt85zefG+CDIuoqtSjL8TmEiosGstqkdB3KrpeMf7TyFZCMWveniIStgHh/kAidbS+njZkbJi0wuhlQSY88x2jU6aXIjERERERH9gYUkRERENCTFBrgYtJ+/iw1C3O3MPBvTmxTmhvFB4n/j1hNl+HxPjnDM38UGAa62ZpzZ4OPhYIWFMT7CsaTsKqQX1/XouC3tGnx3sEA4NibQGcN9HXt03O6421vh3umhwrF2jQ7/2XDCLM9LQ8vGtFJpikiQmy1umRzcZbufsw1mD/MSPmbD8RKU1rWYcopERAPS9owyaLTydl46HfD416mob2nvw1nRQFde34rUghrh2BzJZ+tZno7WGOknPq8c6oUkDa0d+C6lUDgWE+AMNztx272dmeXmnBYRERER0ZDAQhIiIiIaksI97WGrVnW7X0KkBxQKyS30A5hCocBDelJJ9uWIF4Ansq2N0G2TQ6RjX+zN6dEx1x0pRl2LODZ7qaSdjqncMTUU3o7WwrFfjxTjUJ78Tmmi1g4NXvwtXTr+9IJhsLIQv78umyh+bXdodfhynzyhh4hoqDCkLV5hTTP++XNaH8yGBottJ8qkKWCzh8kTR86aJUklScyuHNJtXH44VIiGVvG/75ZJQZgqaWG6K7PCnNMiIiIiIhoSWEhCREREQ5JKqcAov+7b2yREDL62NmclRLgjJsDZqMewrY3YKH8njJMkvPxwqBBVjW1GH3N1sjhm29HaAgtHixNQTMVGrcLjl0RKx1/8LR062WoFXfS+2JuDXEkLpEmhbrhkuPzO6Knh7gh2E6cefbkvD+0arUnmSEQ0ELVrtNh+0rAEiO9SCrD+aLGZZ0SDxeb0UuH2UHc7hHrYd/v4WZLUkrYOLfZkDc1WSjqdDisTc4RjrnZqLBjlg2mSv/WyyhpQVNNsxtkREREREQ1+LCQhIiKiISs20FnvuEqpwOTwwZvQoVAo8PCscKMew0QSuVsFrToAoLVDi7X7jUtSSCuqw6G8GuHYNeP8YW3ZfVpOb10z1h/R3g7Csf051diYJl6woItbRUMr3tmSJRxTKIBnFw7Xm+KkVCpwkySVpLSuFZv5uiOiIWz/6SrUS9LIRJ764SjK2PbrotfSrpEmZBiSRgIAo/2c4G4vbuMyVNvb7DtdhYzSBuHYdeMDYG2pwjRJIgkA7DZTKklLuwZZZQ16W1wREREREQ0GLCQhIiKiISvW31nv+NhAZzhaW/bNZMxkVrQnRviKe6JfyM/ZBgGu4qQAAuaP9Ja2g1mZmIsOI5IU1uwTp5EAwNL4QKPn1hMqpQJPLRgmHX/pt3Sk5FUzmYQ6eX1TBuolEfHXTwjAcAPeb64dFwBrS/GfmiuT5L8bRESDnSFtbc5X09SOv3x7hJ/FF7nE7Eo0t2uEY7MlSSMXUioVmB4pLjrZfrJsSL7GVkjOKRSKP863vRytEeUlLqzemVlu8jl9uvs0JrywGXNe34FR/9yA348xdYiIiIiIBi8WkhAREdGQ1V3bl8Hc1uYshUKBh2dHGLQv00j0s1QpsWySOEmhuLbF4ASPhtYO/JBSKByLD3FFuKf4YrY5TI/0kN6JmVPZhKvf34vZr+/A+9uzUFLLO6IvdunFdVi7T5y+Y29lgcfnRhl0HCdbS1wR4ysc23uqElll9T2eIxHRQKXT6bDlhPhcIdrbAUpJmNOOjHKsYpHdRW2LpK2Nk40lxktaL4rMihYXkhTXtiC9eGh99pbVtWDDsRLh2Oxoz07F87Jz4d1ZFSZNDdmVWY7nf007l0rU1KbB/atTsDfLPMknRERERETmxkISIiIiGrJ8nKzh6WAlHU+IHPyFJAAwd5iXtIXJ+eJDXftgNoPb9RMCoLYQnyIv35Nj0DF+PlyExjbxXaVLJS0/zOmpS4dBTycSZJc34r+/n8Skl7dg2afJ+OlwIVokd8XS0KXT6fD8r2mQrac8OCscHnreTy+0bGKwdGxVknGtooiIBoNT5Q3IrWwSjt05LRQPzpS3I3zht3ScKhe36KChTafTYaskyWZGlAcsVIZfup0W6Q4LScXStpNDq73Nmn156JCctCybFNzp62mSv/lqmtpxvKjWZHN6Z2sWLgx+0eqAv/10DK0dPLcmItNp12hxIKfKqNRUIiKinmAhCREREQ1ZCoVCmkriYmuJkX5OfTshM1EqFXhoVvepJJOYSNItN3srXClJUtiXU4VjhfovNut0OqxOFt9V7GqnxrwRhsWTm9JwX0dcM9a/2/10OmBXZgUeWXsYE17YjKe+P4qDuWx9c7HYnF6GPVmVwrFAV1vcNiXYqOON8ndCrOT997uDBWiUtM8hIhqsNqWJF+oVCmBmlAcemh2B0f7ic8+Wdi0e++ow2rkgdNFJK65DkSQVztC2Nmc5WltifLA4wUSWejIYtWu0WJMsLkoNdrPFtPDOCSRxwa7SQvFdmaZJC8kub8C+01WSsUZ8suu0SZ6HiAgADufXYPGHiRjz3Cbct+ogvtyXh8Ka5v6eFhERDUEsJCEiIqIhTdbOZUaUJ1SyjPFB6NKR3ojwtJeO+znbwN/Fpg9nNHjdMjlYOrZ8b47exx4pqMXxojrh2LXj/WFloerFzHruT5dEwsHKwuD961s68OW+PFzzwV7Mfm0H3tuWheJaXpgaqto6tHjh1zTp+NMLonv02l0mSeCpb+3AT4eLjD4eEdFAJluoHxfoAjd7K1iqlHhjSSysLcWX4o4U1OKdLZnmnCINQFskaSQWSgWm9yA9cXa0uPjkUH4NqhrbjD7eQLQprRRl9a3CsZsmBkF5wd94NmoV4oLFyYw7M8pNMqevDuTrHX9nayYKqsWJRURExjr73lXf0oH1x0rw1PdHMeXlrbji3d28EYSIiEyKhSREREQ0pF0/IQBudupO29QqJR6e3X2Cx2CiVCrw4Cx5ZHp8iCsU+vqb0Dkj/ZykF5t/PlyEigbxhWsA0jQSALgxLrDXc+spHycbfHTzOHg7Whv92OyKRryy4SQmv7wVyz5Nxi+pRdCasJ889b8ViTnIkbRjiA9xxbwR3j067mWjfeBsayl9Tl7kJKK+UFjTjEfWHsKkl7bg0rd24dcjxSZ/jqrGNqTkVQvHzk+VCPOwxzMLhkmP8+62LOlxaGiSFSBNCHaFk434M1SfmdGewu06HbAjY2i0t1mRmCPcbm2pxLXjAoRjUyPchdtT8qrR0MuUtHaNFt8dLNC7T0u7Fv/+RV60S0RkjB2SIjgPeyte9yEiIpNiIQkRERENaXZWFvj2vsmYGeWBAFcbjA9ywao74xHibtffUzO5haN9ESr5d00JF188JTFZG482jRZr94mjtGub2/FzqjhlYVqEO4Lc+vc1NznMHdv/MgPv3TgWM6M8YGwgz9nWNw99eQg3/C8JLe3s9T4UVDa04i3JHfAKBfDswuE9vhhpbanCkvHiBZ0TJfU4mMvFUiIyr9YODe5Yvh8/HS5CcW0L0ovr8MCaFGw/adoF9W0nyiCrsZwzrPPC/k0Tg6RJE1od8NhXh9n+6yJRVteC1AJx28TZw8QFId0J87BDoKutcGzrCdOkb/SnjNJ6JGWLW8hcGeMHJ0kB6zRJIUm7RofkbHFrP0NtSS9DRUP3aS8b00qx7cTQKOYhov5T1diGo5KWuwk9SLIiIiLSh4UkRERENOSFuNvh89visOuJWfj2vsmICxGnTQx2KqUCf798eJcCgQBXG1we49s/kxqk5g73gq+TOL1jZVIu2jXaLtt/PFSIlvau2wFgaXz/pZGcz9pShctG++Dz2+KQ9NRsPHVptN6WSDLJp6vw0Y5sM8yQ+to7W7NQ3yJesLxuXABG+jn16vhL44Mgq0NZmSRP8CEiMoUfUgpxoqS+y/YXfk03aSrSZkmqRKCrLcIv+JxVKBR4ZfFoaWJTbmUTnv813WRzo4Frq56igjnDxC1quqNQKDBLkkqy42QZOgTnsIPJykT5ucOySeKWegAwzNsR7vZq4diuzIpezemr/eIic5F//HycxdhE1Cu7MsshO4VhIQkREZkaC0mIiIiIhpAZUZ5454axGO7jCA8HK8wZ5oW1d0+C2oKnfcawUCmxbFKwcKy0rhXrj5V02qbT6aRtbTwdrDrF2g8Uno7WuGd6GDY+loCfH5yCmycFGRWhvnZ/HlvcDHL6otjt1Cr8aV5kr58j0M0WMyQXNH87Wqy3VRQRUW99uT9fuD2zrEGaamCs1g4Ndkoi5ucM8xKmOnk6WuOlRaOkx/xyXx42p4mLU2jo2JwuLiQJ87BDcC/SE2XtbepaOpCSV9Pj4/a3+pZ2fJ8iPm8ZE+ist/hVqVRgqiShcVdmz5NaimqapS0mRPKqmvDB9lM9fj4iop0Z4uK3AFcbBLuJE6mIiIh6iisKREREREPMZaN98Nsj07D/mTn45Jbx8HO26e8pDUrXTwiAlaQAZ/me052+PpBbjYzSBulxLFUD97RboVBgtL8z/n3lSOx7ZjbeXzoWs6I9oeqm901xbQtS8tiaZDA7WliLekn7hAdmhcPTQZzKYyzZHcLtGh2+kizykmlklzfg1s/3YcrLW3H1+3twIMc0C+dEg0F6cR1S82uk4yuTckzyPMnZVWhsEycMXNjW5nyXjvLBNWP9peNPfn+ExXZDWEu7Bruz5AVIvREf4gobS5VwTF8KykD346FC6e/azXrSSM6aFiEubD1V3ojCmuYezenbgwXStlYyH+w4hdzKxh49HxFd3HQ6nbT4LSHCo8ctSYmIiGQG7hVtIiIiIqJ+5GKnxqIxfsKxlLyaTotTa5LFkdZKBbAkbmC0tTGElYUKC0b54LNbJyDxyVl4ekE0Ir3krW/WHSnuw9mRqSVlVwq321iqcPuUEJM9z/RIT/i7iAva1iTnQcNkG7NIza/Bwnd2Y/vJchTWNCMlrwZLPk7C6QouXplDWlEdUvKqUd/S3t9Tof/XXaHahuOlKK7t2eLx+bZI2to4WFtgQjftFP9xxXBpwW9FQxue/O6oSVvw0MCx91SFtCVib5PsrC1VmBohTt/YemJwJt3odDqskLS1cbNTY8Eon26PMU3yPQGA3T1IJdFq5QWxEZ72uFHS2rKtQ4t//Hycv9tEZLQTJfUoqxcXmbKtDRERmQMLSYiIiIiIJG6dEiwd+2JvDgCgqrENvx4VF1TMjPIctIkwno7WuDshDBseTcBsSUT6b0eLWQQwiCWeEheSjA92gbXkTuaeUCkVuGmi+E7hwprmQX139ECVVXYmiaTpgju3NVodPhzCkfptHVokZ1fiy315yK9q6pPnbGrrwJKPErHg7V24+v29mPXaDqPaHJB5tLRrpC0wztJodfhSUghqKJ1OJ21PMj3So9tEMkdrS7x+XQxkNxBvTi/F1weY3DQUyV43zraWGBvo3Ovjz5Kcu2WUNqCgum/eH00pKbsKmWXi9L8lEwJgZdH9eYunozWivR2EYzszxa0i9NlzqkKaZLJkQgCemBcFVzu1cHz7yXJsOD44i3qIqP/IWulZKBWYHObWx7MhIqKLAQtJiIiIiIgkor0dMSlUfEHmlyNFKKtvwXcHC9DWIb6jdOnEwZNGIqNQKHB5jK9wrKy+FfvZKmNQauvQ4kCOuDXRRMlrvjeuGx8AtaRV1Mok8R3G1DPFtc24+dNkVDeJkzGSTosLiAa75jYNbv4sGUs+TsJT3x/FtP9u69KGzBz+/tNxJJ/+432wvL4Vd31xAHuyjF+U7Evrjxbj3pUHcecXB/Dy+hNYd6QIpysaoR0ixYHrjxWjrkXcuut8a/blSz/DDXGipF66kDx3uGGpEvGhbrg7IVQ6/q9f0tgGY4jR6XTYKikkmRnlCQsTtEScGSVvq7RtEBZwylpRKRWQJn+IyFJJ9mRVGF0cvVaSRmKpUuDqsf5wtlXjyfnR0sf/+5fjaGrr/n2KiOgsWbHy2CAXOFhb9vFsiIjoYsBCEiIiIiIiPWSpJO0aHVYn5WHNPvHdzH7ONpgeKb+IP5jMGe4FK0kRwLojRX08GzKFo4U1aG7XCMfMUUjiaqfGQkns/M6McuSw3YpJVDe2Ydmn+1BU2yLdJ7eyCVWNbX04q77x73XHkZTdubDtX+vSkJInLpgyhbzKJmHqRZtGi7tXHMCRghqzPXdvfLjjFO5bnYLfj5dgc3opPtxxCg+uOYSZr27H6H9txHUfJeJfvxzHdwcLcLKkHh2anhda9Jcv9xmW4lHR0Ir1x3repk3W1kalVGCGEecAj8+NxDAfR+FYU5sGC9/ejQfWpOCHQwWoHoK/vxeb40V1KKkTv0/PHmaac0dvJ2sMl7ymBlsSWEltizS9Y1a0F/xdbA0+1rQIceuHmqZ2HCusNfg4VY1t2Hi8RDh2yQjvc0kki8f5SxNmimpb8PaWLIOfk4gubk1tHdIbAaazrQ0REZkJC0mIiIiIiPSYM8xL2p7mgx2ncFqyAH79hAColJKs+kHG3spCemfr+qMlg3KR8WIna2tjq1ZhtL+TWZ5z2SRxexsAWMVUkl5rauvA7V/sR5Yk+v98qfk15p9QH8oqq8dXgjvDdTrg/W3mW6T7ZHc2ZDewN7ZpcOvnhv08+lJaUR1e2XBSOt7Q2oF9p6vw+Z4c/OmbVMx7cydG/GMDrnxvD57+4SjWJOfhSEENWiSFaAPBqfIG7DtteFrWysSev//I2pOMD3KBk63hdwZbWajw5pJYaXJTfWsHfj1SjMe+SsW45zfhug8T8dGOU8gqq4dONzRSZC4mmyUFSBZKBRJMuBgoa2+z91QlmtsG7u/whb7clydNC7lZz7mFSFyIq/T3bFem4W3Jvk8pQLtGPKfrJwSc+2+lUoHnrhoJ2Z8En+zKRlZZvcHPS0QXr6TsSrRJ/u5OkBTJERER9RYLSYiIiIiI9FApFbhlsvgitSwOX6VUYMl5F5GHgoUx4jSJysa2Tm0dqOd2Z1bg2g/3YvZr2/HcujSzpkYkZosLSSYEu8LSBJH6IrEBzhjpJ747+puDBYNqUWugaevQ4t5VKTiUV2PQ/oeGWCHJqxsypAUdm9PLcLLE9It0VY1t+PqA/tSLqsY23PxpMoprxa1P+ppWq8MzPx41un1Da4cWqfk1WJOch6d/OIor3t2Dkf/YgKve24NfjxQPuEKGryXtJmQO5FbjeJHhSQRnldW3IFWSOjNnmGFtbc4X5e2AJ+ZFdbufVgfsy6nCS+tPYM7rOzHj1e349y9p2JtVgXYWdg4KWyQFSPGhrnA0YWuCmZJCktYOLRKzB3b7rbPaNVp8KUn/C3G3w9RwcasaGWtLFeJDXIVjOzMN+57odDph8SJwJpFwSljnOY3wdcLNk4KF+3dodXj2x+MD7n2UzEOr1fFnTT22M0P8HuVmp8YIX/HfWERERL3FQhIiIiIiom4sGR8IG0uVwftfMtwLno7WZpxR35sV7Sn9HgyV9jbtGi2Ka5txOL8GG4+XYMPxEuRXNfXJc69MysVNnyZjf041TpU34tPdp/HA6hSzXGxu7dDgYK44FtkcbW3OUigUWDZRXJRV29yOX4bI66ivabU6/OXbVOyU9EwXOTyECkkO59fgd0l7gbM+3HHK5M+7IjEHLe3dL9oX1bZg2af7BkQ7kjX78gwuNupOh1aHw/k1eGBNCr5LKTTJMU2hrUOLbw92bTcEQJouBvQslWTbiTLI3qLnDDe+kAQAbp8Sgslhxr0P51Y24bM9p3HjJ8kY+9wmPMgWOANaaV0LjkpaqMyO7tnrRiY2wPlci5ULyYpZzOFYYS2+3JeHFYk5SDxVadRrc8PxEpTVtwrHlsYHQtmD9L9pEeLik5TcajS0dnT7+JS8GmRK0qaWTAgQzunxSyLh4WAlfExidiV+TuU50FBW19KOB9akIPbfGzHl5a14Z0smC0rIaLJz/akR7j16LyQiIjKERX9PgIiIiIhooHOytcTVY/2wOll8R+SFbowPNPOM+p6t2gKzhnni1yPFXcbWHyvBv68cabYki95q69CivKEVZXUtKK1rRXn9mf8vO/f/Z7ZVNrZ1WRS0UCrw6JwIPDAzHAqFeS7QbT9Zhn/8dKzL9sTsSmw7WYZZJl5YSs2vlS6ATzJyAdNYV8T44YVf01HX0nWh5vM9OVg81p8XQo2g0+nw73Vp+OmwcQtQqfk10Ol0ZntN9xWdTof/rD/R7X4/pxbh8bmRCHC1NcnzNrdpsMKIwoOssgbctnw/Vt8ZDzur/rkMU1bfgv/83v33qiee/zUNl8f4wMrC8IJLc9mcXopKySL1g7PC8cOhQmHbmx8PF+KpS4cZ1Y5G1tYm1MMOIe52Bh/nfEqlAq9eG4Or3tsjXTzXp76lA+uOFGPdkWIoFcD0SA88tWAYIr0cejQfMj19BRw9SbLRR6VUYEakB74/1LXY60whlPk/B97bloXXNp7skhrl5WiFaG9HRHs7IMrbAdHejgjztOvyPiJ7r7W2VOLacT1L/5sW4QGg6/thh1aHpFOV3RaCfbVf/PeAUgEsHucvHHO0tsQzC4bh0a8OC8df+DUds6I94WDCRBoaGNo6tLjpk2QcKThTQFbX0oHXNmXAQqXEfTPC+nl2NFjkVzUhW9JSl21tiIjInFhIQkRERERkgFsnBxtUSBLkZtsl0nqouHy0j7CQpKapHXuyKjAjShyh3h/Siurw+qYMHMqrli4qGqJDq8OrGzPQrtHhsbmRJpzhGSdK6vDgmkPSthyb0kpNXkiSeErc1sbeygIjzRyLbKNW4drxAfh09+kuY+nFdfgupQDXjh9abaHM6b1tWVi+N0c6bmOpQnN715ZBtc3tOF3RiFAPezPOzvx2ZVZI2zSdT6PV4X+7svHvK0ea5Hm/TSkwuvXU4fwa3LvqID69ZQLUFn1fdPf8unTUCwq4AMDB2gINrR3SdI3u1DS1Y1dGRY9TOExJ1gLDVq3C5TG+cLC2EBaStLRr8c3BfNw5LdSg52lp12C3pA1Gb4sBfJ1t8O29k/Hm5gxsTCs1KCFBRKsDtp0sx9HCWvz28LQhl5Q2WG1JLxVuj/C0R6CbaYrdzjcz2lNYSFJU24KTpfWI9jbf5/7XB/LxyoaTwrHSulaU1pVjx3l32FsoFQj1sEPU/xeYuNmphb+vAHBVrJ9RhV/ni/Z2gLu9FSoauhZr7cos1/teVt/Sjl9Su54LA2cKt3z1JB9dGeuLtfvzkJTd9d9UVt+KNzZl4u+XDzfgX0CDycc7T50rIjnfm5szcM1YP743k0F2ZsqTB6dFDs1rD0RENDAMzFsGiYiIiIgGmAgvB4P6sN8Y17OY7cFgRpQn7NSy9jbii+r9Ib+qCdd8sFfvnenGemtLJj4TFD/0Rll9C+5YfkDvIuGW9DJoZVUmPZQkWXifEOwCiz5IlVmqJ7HnP7+fRH1Lu9nnMBSsSc7DqxszpOMutpZYe/dEyN6OBnt7G61Wh/9uMDxh46v9+SjvQcLDhTRaHT7ZlS0cs7eyQIy/k/SxuzIr8KdvUk3+O92dnRnl0rYJVhZK/PrQNBz75zx8d98k/OuKEbhuvD+G+zjCUmX4Z9lAaHGWX9WE3Vni4o4rYnxhb2WBeSO84SlpL7EyKdfgn03iqUphkRYAzI7ufVFloJstXl8Si5Rn52LlHXG4dXIw/F3kC9T6VDS04e2tmb2eE/VebXO79DU628RpJGclRHpAJfkg2HrCfO1tjhfV4tkfu6at6dOh1SGjtAG/pBbhlQ0n8eT3R6X7LpskbpVnCIVCgQRJe5tdkgKxs9YdKZb+7i+ZoD+RUKFQ4LkrR8JC8vP4IjEH6cV1eo9Bg8vpika8vTVLONbaocX7203feo+Gph0nxYUkw30c4enAYiQiIjIfFpIQERERERno1snBesfVKqU00noosLZUYa7kLs0Nx0vQ1iFul9LX3tuWJb3I3xv/XpeGbw7km+RYzW0a3LXiIAprmvXuV1bfiuNFpltUaGnX4GBetXDM3G1tzgr1sMf8Ed7CsYqGVry7TXzBnf6w/mgx/vajfIHNVq3C57fFISbAWdrSYrAXkvx6tBjHCg3/3Wjt0GL53t4Xg204XoLcyibh2A1xAVh+WxzCPeVJL7+kFuGfvxyHrqfxH0ZqadfgWUHrrLMenh2BQDdb2FlZYFyQK26ZHIz/Lo7Bb49Mw7F/zcO6h6bi5atHYdnEIIwJdIaVJE1lU1opWszwvmuMbw7kS1NVlkw4k3RkqVJK28/lVjbpveP3fJskqRLOtpYYF+Ri0DEMobZQYlqEB/55xQjsemImNj6WgCfmR2FckAuM6Ujy9YEClNW1mGxe1DPfpxSgVXKuNGeYeVLdnGzkr8lVibkoqzf966K2uR33r06R/lt7a1yQC0b4yov2DDFVUkiSXdGI/CrxezwArN0vPg90t1djtgE/wwgvB9wxNUQ4ptHq8OyPx/q82JDMQ6fT4W8/HtX799GafXkortX/twBRu0aLvZJEyYRItrUhIiLzYiEJEREREZGBZkZ7ItBVHjs+f6Q33OzFdzoPFZeN9hVur2/pwC4DF+DMqaVdI2y/Yyp//e4Ifj/Wu+NrtTr86ZvDSDVwIX+zZMGyJw7n10gvaE8M7ZtCEgB4akE01JL0k892n8ZpSQ9wAvZmVeCRtYel7ZAsVQp8eNM4xAY4A8C5/7/QYC4kaddo8dpGcbsE4EyrFpEVibm9SrzR6XT4aKc4jcRCqcBtU0LgYqfGyjvi4KenvcGKxFy8taVvEiLe25YlLXyJ8LTHXXpauVhZqDDSzwnXxwXiuatG4of7p+DDm8YJ921s02C75G7ZvtCh0eLrAwXCsWhvh06/BzfGBUoTAVYm5nb7XDqdDlvTxUkOM6M8zZbspFAoEOnlgPtnhOO7+ybjwDNz8Oq1Mbh0pLc0Leystg4tPjFxqhYZR6fTYWWS+PXl4WCFMYGmK0C60CxJSk5RbQvu/OIAmtp61j5JRKfT4c/fpErfd0xh2cSep5GcpS9lUJYak15cJz13u2acPywN/N1/eHYEfJzECQIHcqvxXYr4vYwGlx8OFWJPlv72e20dWry/jakkpN+hvBppgmUC29oQEZGZsZCEiIiIiMhAKqUCN+uJ0tbXsmOoSIh0ly7SDoT2NttOlKFeT6uY3tLqgIe/PIzd3USf6/PqxpP47WiJwfubMno+UXI3m4O1Ra/v7jVGkJsd7pgmviO3XaPDC7+m9dlcBpOjBbW4a8UBtGnExUAKBfDadbGd7k6UFZKkF9f1e4JET319IB85kkXKeSO88PCsCOFYfUsHVifn9fh5952uki4iXhHrC9//Lx7xcbLBijvi4Gqnlh7rzc2Z+GJvTo/nYoissnp8uEO+QPXi1aOgliSMyEwJd4eTjaVwrD/b2+zIKEeJJHHj+gkBUJwX3+HpaI15I8WpSFtPlulNIwCA40V10ucyJJHAVNzsrbB4nD8+uGkcUv7+Rwsc2Wf0qqRcVJuo3RsZb09WJbLLxUWS108IkLafMYVLhntJE2yOFNTikbWHoTFRCsZHO7OxKc10BbAXcrNT49JR4t9fY3g6WiPaW5zYJSuM/kqSRgIAS8YHGPzcdlYWeHbhcOn4y+tPoLaJbf4Gs+rGNjz/a7pB+67dn4eCavMUXnVotFiRmIP7Vh3E8+vSWKg9SO3MEL8n2apVGB/k2sezISKiiw0LSYiIiIiIjHDdhADhItpwH0fEhQz9CzlWFipcMlx8AX8gtDb44VCh3nG1hRIBrjYYF+SCBaO8cevkYDwxPwqvXhuDlXfEYcOjCTj897l4+epR0mO0abS4e+UBpEhaxOjz9YF8o/uhHy2sRamJWhIkZosLSeJDXM26iCXywMxweDqIE3w2p5dhh+Si6cXqdEUjbv18Hxrb5L9j/7x8BK6I6ZwaFBvoLNy3XaMzadukvtLcpsFbm8VpHkoF8Jd5UbghPlBa7PDp7tM9fp+SpZEAwN0JnZM9wjzssfy2CXqTIv75y3H8dFj/e1ZP6XQ6PPPDMbRrxIvDS8YHYEKw8Z9ZagultDXVlvQykyYbGOPLfeIFXisLJRaN6dpy7pZJwcL9dbozBRf6yFKiLJSKfouYt7JQnWuB8+icSOE+TW0aLDdz8RLJrUzKEW5XKRXSdkumEuphj8Vj5a0XN6WV4gUDF731STxVif/+fkI6ftkoHyQ/PRtf3B6Hpy6NxtVj/DDcx1GaUCZy+9QQWFnoT+AxlOz3dXdmRZfCmpZ2jfQcMy7EFaEe8pZmIpeO9MY0SXudysY2vLJR/n2kge+l9emoMrBwr12jw3tmaOuo0erwwJoU/P2n41h/rASf7D6Nq97bM6gT6S5WsrZ7k0LdjC4IJiIiMhY/aYiIiIiIjOBobYnXro3pdNHbycYSry+J6XTH81C2MMZHuL2htaNfF/9rm9qlrRWuHuOH1L9fgpPPzceuJ2bhu/sm4/2l4/DPK0bg/hnhWDzOH9MiPBDl7QBnWzWujwvE0wuipc/V1KbBrZ/tQ3qx4Qvxe09V4Onvj0rH3e3l6QWmSCVpadfgcF6NcKwv29qcZW9lgb/Ol3+Pn1uXhnZJ8sbFprSuBcs+TUalnkWJh2dH4JbJwV22R3g6SIsZBuNiwud7T6OsvlU4tnicP8I9HWBvZYFbJOlR5fWtPWobkFlaL/09nB7pgWhvxy7bR/s74+Obx0sXSXU64E9fp5rlffPbgwVIPl0lHHO1U+PJS+W/e92RfQY0t2tMmqBkqNK6Fmw7KX7eBaN84GTbtahoQrCLNI3gqwP5eouNZIUkE0Pd4GgtLmDqSzfEBUjTcJbvzZHG45P5FNU0S1M65g7zgo+TvBWWqfzryhEY7S9PHvtsz+lepSSV1rXgoS8PSduuhXrY4T+LR8PL0RrTIz1wz/QwvL4kFr89Mg3H/z0Pmx5LwNs3jMEDM8MwZ5insD3Y3OFeuCdB3o7LWLJCjrqWDhwpqOm0bcPxEtQ2i1NCrp9geBrJWQqFAv++cqT082F1cl6XOdDgkJRdKW21JvPNgQLkmbgd1Bd7c7DheOf3ndrmdjywOqXPEm8aWztQ39IOrYkSjy5GVY1tOFpYKxzrr+JVIiK6uIjzLomIiIiISGrOcC/sfGImNqaVwFKlPLNQJbn7fSiaEnamtYHogvq6I8WYJ7lb3dx+O1YsbfmxZEKAcDFRn7sTwlDb3I73JL3L61o6sOzTffj23kkIdrfTe6xT5Q24b1UKOiQXUu3UKqy4PR6PrD2EzLKGLuNb0stwQ1zv7lhOya2Wfn/6o5AEABaN8cOKpFxhu5CssgasTMzF7VPFLXAuFqV1Lbj5030oqG6W7rM0PhCPzRG3c1EpFRjl74Sk7K5FBYOtkKS2qR0fShJ91BZKPHJeEsOtU0Lw8a5stLR3fc1/tCMbS8YHwMKIu+A/1pNGom9hc0q4O968PhYPrEmBTvDr36HV4d6VB7H6rniMDXQxeD76VDW24cXf5OkCzywYBhc9bXe6MynUDW52amFh07rUYiwc7St4lPl8e7BA2pZDtsCrUCiwbFIQnvnhWJexmqZ2/JxahOsErSpKaltwrFBcQNiXbW30sVVb4I6pIXhlw8kuY7XN7ViVlIt7p4f1w8wuXmuS86QFFvpaJpqSrdoCn9wyHove24vCGvHnyb9+OQ5/FxvMHuZl1LHbNVo8uCYFFQ3iIj8bSxU+vGkc7K3El6EtVUpEeDkgwssBOC9Vq76lHRml9Sivb4OPkzViJK3aempCsCusLJRo7ej6ObE7swJjzntPlrW1cbC2wKUjxcV13Qlxt8M900PxztauaRQ6HfC3H4/hh/un9HliHPVca4cGT/8gLxof4esoTIPr0OrwztZMvHJtjEnmkV/VJPwMAIDCmmb89bsj+OCmsWa7CWLf6So89f0RnDqvnZe9lcWZ/1mf+X+H////M/9tCXtrCzicN+5kY4mxQS7S942Lxa7McuH5I3CmkJmIiMjcmEhCRERERNQD3k7WuHlSMG6Ik7dQGKr0tzYoRbOe1hvmJIsc93O26VELBwD48yVRWDZRvshT0dCKpZ8ko6RW3nqmqrENty/fL72TVakA3rlxDIb7OkoXb3Znlfe6bZCsrY2TjSWG+3RNU+gLSqUC/7x8uHT8jc0ZqJQsTA1lbR1arD9ajNuX78fkl7fiZGm9dN8Fo7zx7ytH6l0MiA0QFygczje+PVN/+mDHKdS1iNMUbp4Y1OkOdlc7Na6fIC6+yqtqwm/HSgx+3pLaFvwoaUEzys8Jk8L0F2ItGOWDF66St8tqbtfg9uX7kann52yMl35LR7XkbuNJoW64eqxfr45voVJi/kjxZ8C2k2V9mnih1eqwdn+ecCzU3U5vy7mrYv3gYC1eoFqZmAudYOVmywlxqgQAzDFy8d2cbpoYBAfJ4tsnu3re3omM19ahlb5Gwzzsun3/MCVPB2t8ftsE6WtDqwMeXHMIxyR3v8v89/cT2J8j/zx5+ZpRiPQSJwDp42BtiXFBrpg/0tvkRSQAYG2pkr5H7MqsOPffeZVN2HtKfA51VawfbPS0MOvO/TPC4e8iTqQ5UlCLjccN/6yi/vfB9lPIPq944nzBbrb46p5JCHAV/7y/P1SI0xXixxpDp9Phqe+PolnP+/zvx0uwKln8vtRbSdmVWPpJUqciEuBMcmVJXQuyyhpwOL8GuzIrsP5YCb45WIDP9pzG21sy8cJv6Xjq+6N46MtDuPmzfRj73CZ8vNO4lqBDzc6MCuH2QFfbbm9kICIiMgUWkhARERERkdFkrQ2a2vqntUFhTTP2Sdo4XBHrC2UP7+ZUKBT41xUjcFWs/A77wppm3PRpsrAXemuHBvesPIBcPXHVf184HLOizyxAyu5ob2nXIlGyiGGoJEkhSVyIa4+/P6YwJtBFurBd39KB1zdl9PGM+s+xwlr88+fjiH9xM+5bnYKtJ8qkSQsAMCXcDW8sie32buVYyQJcflWzWQp1tp8sw9zXdyD62fW48r092JMlvghujJLaFny+57RwzMHKAg/MDO+y/a6EUFhIvjcfbD8lLBQQ+XzvabRrxPvenRBq0B29N8YH4s+XRErHa5rasezTfb0uJknOrsQ3B8WR+mqVEs8v0l90ZChZ6khrhxabJS08zGHvqUrkV4nTFZZMCND7b7WzssDicf7CsaOFtcLEni3p4s+3KC8HBLjadj/hPuJkY4mbJ4uLICsaWvH1AXG6Apne+mPFqGgQtyVbNjGoz9siRno54IObxknfG88WthVJUksutP5oMf63S/zeDJxJXLkytnfFa+aUECG+oz8lrxr1LWcK8vT9vizpQVub89moVfjn5SOk46uSc3t1fOo7p8ob8L4kxRAAXlg0CvZWFnh4ljhBTqPV4Z0tmb2ex7cHC7DbgPOu59alIU2QjtIbpysace+qg9JzJmO1dWjx4m8nsP5osUmON9jodDrszBS3P0yIFLfmIiIiMjUWkhARERERkdEmhbrBVdIaYd2Roj6eDfCTJC0AONM+pTeUSgVeuTYGc/S0Lcgqa8Ctn+87t+gAnLn49+R3R/XepXvLpCDcOuWP1i1jA13gLGnBszm954uzzW0aaRuTSf3U1uZ8f50fDVvJHb1f7ssz+YXugaSyoRWf7j6NS9/ahYXv7MbyvTnSNInzjfJzwkfLxsPKovs7occEOkvHTN3eJqusHnd+cQCZZQ1oadciNb8GN32ajE92ZRtcuCHy1pZMYfsB4Ewxh6hVi5+zjXQBM724DjsyxBfnz1ff0o41SeK7dv1dbHCpJJlD5IGZ4bhtSrB0vKSuBQve3oXXN57sUWJEW4cWz/zYtVXLWffOCEOYh73RxxWJC3GFh4OVcKwvPwO+lCQ9WKoUuEZSJHI+fYlTKxM7L+A2tXVIF+cGSlub890+JQQ2luL3h492ZKNd0uqMTOvC19FZtmoVrjbgNWoOUyPc8eIieUpSWX0rbl++v9M5jUh2eQP+8u0R6XhsgDOeuWxYj+fZF6ZJFmM7tDokZVehQ6PFNwfFhSQj/Rwx0s+p13OYM9xLeo65J6sSOSZIqTCXDo0WW0+U4pfUIpTX912CXEu7BltPlGLj8ZJ+S0I8n06nwzM/HJW2kLx6jB+mhJ95rS0a44dgN3Hh4Y+HC5ElaHFpqLL6Fjy3Ls2gfds6tHjwyxQ0tZkmRaymqQ13LN+PGgPOYY31wm/paO3o/59zX0svrpf+XsmK4IiIiEyNhSRERERERGQ0fa0Ntp4oQ2MftjbQ6XT4UdLWZpiPY4/i1C9kqVLi3RvHIl5Pm4QjBbW484sD5xaA39maJW23AwAzojzw7MLObV1USgVmRokXE7aeKOvxQvzB3Grp3YF9Gasv4+VoLUyUAM5E7f/rl+O9KkIYaNo1WmxKK8XdKw4g/sUteG5dGtKLDS+WCXW3w/LbJhjcN97L0Ro+TtbCMVMXkry/7RQ6LkhR0emA539Nx9M/HOvR4nV2eYP0jnB3eyvcPjVEOAYA904Plc91e/dx6V/uy0O95P3szqkhsFAZfllFoVDg2cuG6y1ua9fo8PbWLFz61i6jk1w+3nlKugAV4m6H+2eEGXU8fVRKBS4bJU6m2pFRLm3lZUqVDa3Stg9zh3vB3V5c6HK+UA97TIsQLySvO1KMivMSe3ZnVqBNUswka0vWn9zsrXBDnLi9U2FNs/Rzk0wnragOB3LFxaRXjfGDo3X/tUa8bkIAHpgpf084UVKP+1enSN+zm9o6cN+qFGkrKxdbS7y3dKxBxY79KcrLQVoUtyuzHDsyylFaJ17IXSJpn9YTD88Wp1QAZz6HBqKKhlZc9vZu3L78AB768hBmvLINq/sgQeVgbjXmvL4Dty8/gLtXHsSU/2zF0QLj2jGZ2rcHC5CULU5GdLa17FRQZaFS4pE54p+3VnemcLan/vHTcWkLQJHs8kb8/afjPX6+s9o6tLhvVQqyzVT0VFDdLC3KG8pkaSQWSsWA+PuNiIguDiwkISIiIiKiHlk4WryI2Nqh7VV6hrHSi+uRUSpePNXXksZY1pYqfHLLeIz2l999mny6Cg+sTsH3KQV6W7JEezvgnRvGCBehZ0WLC0mKa1uQZkSxwfkSs8UL0i62logyQaGNKdwxNQSBktYQyaersP6YeMF4MDlZUo/n16Vh0ktbcNeKA9iYVtql6KI7YR52WHFHHNwMWCQ/n6y9jSkLSTRaHbbrSfn4cl8ebv18H2qNvFv1tY0Z0hY/D88Oh52egpoILwdcMly8yL/vdBUO5ooXfoAzCyOf7c4RjjnbWuK6HrQ0UCoV+O/i0ZgZpf9O0tMVjVj6STIe/+qwQe2Hcisb8c7WLOn4c1eOhLUknaKnZJ8B7RqdtMDDlH44VCgtkLveiAXemycFC7e3abT4av8fBUyytjZudmrp71d/uyshBJYqeXsnfa2zqPdWJskXPm+eJE/D6St/mhuFy2Pk50m7Mivw95+OdSnkPJO+cAwnJa24FArgrevHwM/ZxqTzNQeFQiEtJtuVWYG1+8VFjNaWSlyh53tnrNH+zhjp5ygc++ZgwYBMY3jx1/ROr4HGNg2e+eEY3tycYbbi320ny7D0kyQUVP/ReqmqsQ33rT7Yb9+jqsY2vPhbunT86QXDupyzXRHjh1APO+H+644U4WSJ8W3ufj9WLD1X1tdB69uDBfjhkLglniF0Oh2e/fEYEiUtNE3lna1ZqGkStwkbqnZKzqnHBrnAoR8LEYmI6OLCQhIiIiIiIuqR+BA3Pa0N+q6XtaytjUIBXGHCQhIAcLC2xPLb4hDhKW8PseVEGR7/OlU67m5vhU9vnSC9AJgQ6QELpfiK71bJQmZ3Ek+JL+7Gh7hBKXmuvmZtqdIbgf/Cr+k9avcxEORXNWHpJ0mY9+ZOfLL7NCoajL8QHupuhycvjcavD0+Dv4u44EYf2UJ3an4NtCZaTE4tqEFVo/5/256sSiz6YA9yKw27a/VIQQ1+PSp+Pwl0tTWoYOA+PUkcH+hJJfkltQgldS3CsZsnBsFWbVgizIUsVUq8v3QcxgW5dLvv94cKMfv1Hfh6f750YU6n0+FvPx6Ttv65KtYXUyULpb0xNtBFmnRj7s8AnU4nvUvfz9kGU8MN//fOivaULnivSc5Dh0YLrVaHLSfE77+zoj2hGiDvoxfycbLBYkn7lOyKRqw/1nef1Reb2uZ2aepLXLAror3FRQN9SalU4JXFozFez3vRl/vy8eGO7E7bVifn6U1ce3R2JBIiB0/bBVmLiNMVjdgiKYxeMMoHTjamXchdGi8uLqpqbMOG431XoG2I2uZ2/CJpY/bm5kz88+fjJju3OOunw4W464sDaGnv+llXUN2Mnw73fWtNAHj+1zRpS8L4EFdcK3gPVikVeHROpPAxOh3w1hZ5MbpIbVM7ntWTLPKnuZFYMEreiu9vPxzD6R6mifxvVza+kqTGAcC0CHesuTMeHy0bh9eujcG/rhiBv8yLwr3Tw7A0PhBXxvpidrQn4kJcMdzHUZomVtvcjnf1FMwONU1tHTggaY86fRC9vxIR0eDHQhIiIiIiIuoRlVKBBZL2NjtOlqOuxfytDTRanfTC8cQQN/g4mf5uWFc7NVbeEQ9/F+OPbW2pxKe3jNd7l66TjSUmBItb6GyWLGTq09jagSOSyO+JofJWPf3hkuFemBIujmourGnGxzuzhWMDWU5FI656bw/2ZBl/p6a9lQVuiAvAd/dNxpY/Tce908N6nCohKySpa+nAaQOLOrqz3cDXZ3b5me9JsgF3r76y4aR07E+XREJt0f1ljTGBLpgUKn5dbU4vE975q9PppK83Kwslbp4c3O3z6mOjVuGzWycYtBhQ09SOJ747giUfJwlb1/xypBi7MsWpQ042lvjbBS20TEWpp73NnqwKVHdTVNQbB3Krcapc/LpdMiHAqAI5lVKBpRPlLWC2nCjDkcLaTm1uzjcQ29qc797pYZB9O97bdmpItQ0bSL5PKUCzpPjxpgGQRnKWtaUKH988HsFu8gLF//x+Auv+v2ggNb8G//4lTbrvjCgPPDRL3KpuoJqip/BMVgthTOqRoa6I8ZW2rFutJ92mP2w7USZNhAKALxJz8djXh3vUzk54vL05ePSrw3pT3P63M9vkxSvd2ZtVge9TxEVVapUSL149CgpJHMjCUT6I9BIXpv92tARpRYanEL7wWxrK68WfUcN8HHHP9DC8dPVo6d8ujW0aPLgmxehUlw3HS/DS+hPS8QhPe7y3dCwmh7tj3ghvXDPOH7dMDsYDM8Px5KXReGHRKLx1/Rh8eusEfH3PJPz2yDT89shU2KrF57orEnORX9Vk1BwHq6TsSrRJfn9YSEJERH2JhSRERERERNRjCyWx3m0aLTanmf/uyeTTldLEgEVj/Mz2vN5O1lh1R7z0rjmZN66LRYwBLRBmDxO3t0nNr5FeKJY5kFstvfA+Kcz0KQW9oVAo8PeFI6R397+/PQtFNc3CsYGosqEVt36+D5VGLKgrFMCUcDe8uSQW+5+Zg5euHo1xQS7ShQhDjfJ3kn5fD+fV9OrYZ207KW9rc6Hqpnbc9Gkyvj0oj1Pfk1UhLZCI9nbA5aMNTxzSl0ry4Y6uqSTbM8qlbRsWj/M3+ndfxMnGEstvm4DXr4uBq5262/33na7CpW/txOubMs6l89Q2t+td1H3y0miTzFVG9hnQodXhdzO2t5GlkSgVwLXjxQkc+iwZHyAtSlqZmCv9PFOrlNK2GANFkJudtH1JenEdtp3sWdIVyel0OmlbG3d7K8wfIU8G6A+udmp8flscXGzlCRuPf52KLemluH91inRx08/ZBm9cFztgks4M5eFgheE+hifEhLrbYUJw94lSxrKzssCVkiS95NNVwkLC/vK7Ae0GfzpchLtXHEBzW8/T5HQ6Hd7cnIF//Hwc3dW8ZZY1YHtG372ftbRr8MyPx6Tj988MQ5iHPMFQqVTgMUkqCQC8udmwVJLdmRX4+oD4XEqpAP57zWhYqpRwsrHE2zeMkaYeHi+qw0u/yYtCLnSssBaPrj0s/bm42qnx2a0T4GhkCxZPB2vckyA+Z2vTaPFfPQXGQ8nODPH5r5ud2qj3KyIiot5iIQkREREREfXYuEAXeDv2T2sDANLYeLWFEvP1RDibQrC7HVbeEQdHa8PaW/x1fjQuldy9fyF9d7gbu+gna2vjaqeW3gnZn6K8HXBTvPhO35Z2LV7Wc+fjQNLcpsHtXxxATqVhd04Gutri8bmR2PXETKy+cyKuGuMHG8kdmT1hq7ZApJeDcOxwfk2vj19W34KjheLkG5l2jQ5//iYV//39RJe7iHU6Hf7zu/xn/df50UYtVk6LcMdIP/GF959Ti7rc4frxDnEaiUIB3Dkt1ODn7Y5CocDVY/2x5fHpuM6AAoh2jQ5vb8nEgrd2Ye+pCryy4YQ0KWN8kAuWjA8w2VxFYvydEOAqvsN5naTtQW/VNrfjN0m7o5lRnj1KonKzt8LC0eL3591ZFfhaEts/KcwNdpIEgYHk/hnyhIh3t2YxlcTE9mRVIluSmHNjnLxoqT+FuNvh45vHQ60Sz62tQ4s7vjiAQkkxp1qlxPtLx8LFgKK4gWhapOEFYUsmBPS6uFPmRsn5DyAvoOtrzW0a7MgwrHB028lyLPs0GbXNxqcUarU6/PPn43hzc6bBj+nL5Lr3t2VJ28GEutvpLWA9a94IbwyTFAVsTCvFUUmi4FlNbR148vsj0vG7EkIxyt/p3NdjA13w53lR0v2X783BJgNuBCipbcGdXxyQpi6pVUp8vGwcAlyNb8UIAHclhMBT0j71l9QipJrgvHWgk/2OTYtwH3TFekRENLgNvL9ciIiIiIho0FAqFVggKY7YlVmOWknPcFNoaddg/VHxHZFzhnkafQdcTwzzccTy2+OkEcxnLRkfgHunG774HOJuh1B3O+HYlnTjkl6SJO1DJoa6mm0hpLcemxsJZ8md0T+nFmF/TlUfz8g4HRotHvoypdsL3bZqFRaP88dXd0/E9j/PwMOzI+Dv0rOL7oaQtbcxRSHJDj1pJLJCg7Pe334K969O6XTX8u/HSqQtmeJCXDEjyrhYb4VCgfumixfTNVodPtn1x+LTkYIaJEp+b+YN90aI5HezN1zs1Pjv4hisvXsiQj26P352RSNu/F8yViWJFxYtlAq8sGiU2RcbFAoFLhslvoM+8VSl0QlKhvj5cCFa2sWJCNfH9bzdxM2TgqVjZZJ/x5zhA7utzVlR3g64RDLXlLwaJGUP7PfUwWZlUo5wu0qpwA16CgX624RgV7x6XUyPHvuPK4YblLg2UE0LN+wzxUJ5pvjPXEb4Okm/j9+lFJxLo+pPOzPLpQUEIgdyq7Hko0SUSRIERdo6tHj0q8P4ItG4lj5J2VU4UlBj1GN6IqusHh8I0szOemHRKFhZdF8MfCaVJEI6/kY3qSSvbcxAQbW4uCvYzVaYeHL3tFAk6GmN8pdvU/Wm/zW1deDOFfuliZAA8N/FozFe0qbTELZqCzw+V57W8sJv6UO6ADK/qklapKTvZ0dERGQOLCQhIiIiIqJeWRgjLiRp1+iwwYytDbaeKEN9a4dw7MpY87W1udDYQBd8vEx+F+/kMDc8d9VIo4s2ZO1tdmVWGNzDvKG1Q5oSMSnUzaj59CVnWzX+pOcC8r9+Od4lwWKg0Ol0+PvPx7E5XZ4cM8zHEa8sHo39z8zBq9fGID7UrU/uLhwjWZxKL67r9eLUdkkhSaiHHdY9OA1TwvW/3n4/XoLrPkpEaV0LOjRavLJRHl3+1/lRPSqCmj9SXgSydn/+uWSPj/Tc0Xy3EQVhPTEx1A3rH5mGx+ZESt9TDHHntFBEeYsTaExNluSh1QG/HzNtMpVOp8OX+8TpIJ4OVphpZIHR+WIDnDH6vDu3DTE7Wvw+PRA9MFOeSvLetqw+nIlYu0aLpraOQb84WFTTLL2jf+4wrx4l5vSlK2J88Rc9iQUiV4/xw429KOIaCMYHu8DKgKSYOcO84CFJSjCVpZLvZU1TO9ab+D21JzYY0NbmQidK6rH4w0TkGZDS1tTWgbtWHMDPqT1LtdL3GW4KWq0OT39/DO0a8XvVteP8MSnM8HPsucO9pIlpW0+U4VBetXAsJa8an+05LT3uS1ePhrVl12IWpVKB16+Lkb6Oa5ra8cjaQ+gQtLDSanV4/KtUHCuskz7vw7PCcZUJ2oteOz5Ampy473SV3vPswU5f4s+0CBaSEBFR32IhCRERERER9cqYAGf4OUtaG0jaD5iCrK2Nk42l0WkFvTU1wl3YdzzSyx4fLB3Xoxj7WdHiu8eb2jQG3z2+/3QVNJKCC2MucveHG+ICESVpxXKssA7fHBQvJhuitUODtg5xokFvvb/9FNYky+Pno70d8PU9E3Ht+IA+b4kRG+gs3N6h1eF4kXFtac7XrtFiZ6b4ovfMKE842Vpi+W1xeiP7AeBoYS2uem8PXl5/QtoWYs4wL4wL6tldriqlAvckiAtBWju0WL4nB3mVTVgved+KC3bF2ECXHj23MawsVHhkTgTWPzqtRwVf/i42eGS2/A5nUxvh64hgN3GSzi8mbnF2tLAWacXiBaxrx/vDohfFN4D+VJILDfdxhK/ks28giglwxrQIcfuO3VkVJkkm6onmNg1e+i0dY/69CcP/vgHjn9+M25fvx1ubM7H9ZBmqG9tM/pztGi0yS+vxS2oR3tqciY92nMKJEvnCqDHWJOdBVud486QgkzyHud0/I8zgtljR3g54YdGoAZtwZihrSxXiDXi/XRJn3nZhwJkCbQfJ+YG+84u+0K7RYrMkGS8uxFWaJgcAeVVNuObDvUiXvIcDQE1TG276JFnvQrpCAby4aBTiQsTnAuuPFndpV2dK3xzMxz5JMp6rnRpPLxhm1PEUCoXe9I03BK19Wjs0+Ou3RyCru7sxPlDveb67vRXeXBIL2a/t/pxqvLWl6/O+svEkftdzk8DC0T54TM+/xRgqpQJPXSr/Xr68Pl1Y7DIU7JS8/of7OJq9kI2IiOhCLCQhIiIiIqJeUSgUuExyR/qerApUmWERqKapDdtOiu9EWzDKx6A4aVObP9Ib3903GfNGeGG0vxPumBqC7+6bDCc9F9X1GR/sAkdr8ULCVgPb28ja2rjbWyHMQ3yX30BhoVLi75cPl46/suEk6lq6b52k0+mQU9GIr/fn409fp2Lqf7Yi6m+/I+ZfG/HMD0dR02S61+f3KQV4ZYM8ScPXyRrLb4uDQx+0XRIJ87CHvWRx6lBeTY+Pm5JbjfoWcTrQzKgziQ2WKiVeuGoknl04XLpwAQDFtS34ZLf4DluFAnhivnF3yl9o0Vg/eDmKL8J/kZiDN7dkSBeB75YUoZhLmIc91twVj1evjYGLEe8jz105EjbdtNsyJYVCgYWjxe1t9udUodSIdgbdkaWRAMCS8b1PRFg42sfg7/UcSWrUQDbQUkkO5lZjwdu78NHObDT8f8JYZWMbtp4owxubM3Dr5/sx5rlNmP7KNjz85SF8sisbB3KqOrXB0ken06GwphlbT5Tig+2n8OjaQ5j/5k6M+PsGzH1jJx768hDe2JyBl9afwMK3d3dqcdUTbR1arN0vXugP87Ab8AWcZykUCjy/aCSmhosLj86yt7LA+0vH9un7jTklSAqtzvJxskZCH6QB2KotsGisONFhf041MkrrzT4HmaTsStRJPu/vmx6Gr++ZJP2MBYDy+lYs+SgRBwSFGKV1LVjyURJS9JyPqFVKvHfjWNwYHygtDNXqgE8l5xG9VdHQihd/OyEd/9tlw+Bipzb6uDOjPKUtjXZmlHf5fr2/7RQyyxqE+3s7WuPJS6O7fc4p4e54YIb8M+HdbVnYm1Vx7uuvD+Tjg+3ydj6xAc549doYkxaVzYjykCbanSpvxNr9PS8qH6jaNVrsPSX++41tbYiIqD+wkISIiIiIiHpN1tpAo9Xh9x5EYHfnt6Ml0kjpRSaIU+6pmABnfLRsPH5+cCqeXTi8VwUDliolpkeJFyo3p5cZFP+fKCkkmRjqOijuHp4S7o55I8TJLBUNbXh3a9eFT61Wh5Ml9ViZmIMH16Qg/sUtmPHqdjzx3RF8l1Jwrpd8c7sGq5PzMPeNndgsaUNgjF2Z5Xji2yPScUdrCyy/PQ7eTta9fq6eUikV0tYdvUkj2CZpa2OrVmFCyB8JHgqFAndMDcEnN4+HXQ8WHq8e449ISUqNoawsVLhzqnjxqb6lA9+niJOOwjzsMKsf2pgoFAosHuePLX+agcXj/Lvdf8Eob8zsh3nKWpzpdMBvJkqmamztwM+HxT+fqeHuCJSkohjD2lKF6yYYljgwZ7j4vWkgiw9xxfggcarOprRSk6VydKe1Q4P//H4C1364F6crxOlD58utbMLPqUV4/td0LP4wESP/uQEL3tqFp74/iq/25+FESR2qGtuQnF2JFYk5ePqHo1j8wV6M/tdGTHl5K25ffgD/+f0EfjxchBMl9WgT3MXeodXh+V/TsVxPq4jurD9WjIoGcXHisolBg+Jz9yxLlRLv3zRW2loCAF69djRCB3hRqjG6axlx7fgAqPqgDR0AvQle/ZlKIjunt7eywORwN0R6OeDbeydL28gBQF1LB276NLlTQXhORSOu+WAvTuopkrFVq/DZrROwYNSZz5uZUZ4I9xS//r7an2+WNKPn16WhtllcxDwl3K3Hf4N0n0qSce6/T5bU4/3t8sK/568aCUcD//54dE6E9DNBpwMe+eowKhpakZRdiWd+OCo9jp+zDf5383hhK53eUCjOpJLI3jrf3JxxrghxqDiUVyP9N01nIQkREfUDFpIQEREREVGvjfJzQqCreBFv3ZGe9TjXR9bWxs/ZRnpBdDCS3fFeWNOMjFLxnYhn1bW041ihuF3JYLkrGgCeWTBc2hro8z2nkVlaj6MFtfhkVzbuXnEA457fhHlv7sSzPx3HuiPFKKtv1Xv88vpW3LniAB776nCP00nSiupw36oUdEiiLNQqJT6+eXyviyBMIVZyx2uvCklOiNOBpoS7C9OBZg/zwrf3TYavEUU1apUSj801TbuWG+ID4WRjXJHX3QmhUPbRAqKIq50ar14bgzV3xSNUskBnb2WBf1w+oo9ndkaUl4N0QW+didrb/HqkGI2SJIrrTdhu4qb4IL2pOQDg6WCFkb7ioqyBTKFQ6E0l0Xe3uamkFdXhynf34IPtp6TpP93RaHVIK67Dl/vy8NfvjmL+m7sw9rlNWPJxEv7+03GsSc7DAT1JSfr885c0fH2gZ3e5r0zMFW63VatwtQGFYAONo7UlPrt1grCVwt0JoZg/UlxANlhFetnDU9I2QqEAru3Dn2G0tyPGSc5nv0spMDiVx5Q0Wh02HBcX3s6M9jz3eR/gaotv7p2EEb6O0mO1tGtx1xcH8NPhQhwvqsXiD/eeK/QVcba1xJq7JmLqeakxSqUCd00LEe7f3K7BqiTx72NP7cosx4+HxX/TqC2UeP6q3rV4Sohwl/4NsyerEknZldBodXjiuyPSYvrLY3yNKnK0UCnx1g1jpOdE5fWtuH9VCu5ddVD6nHZqFT69dbzZWq6M9HPColhxgU5FQxs+3mH+z62+JGtrY6dWSd8TiIiIzImFJERERERE1GtnWhuIFxSSsitR3s1ivjEKqpukvcmviPXt18VeU5se6QHZP0fWo/6s/aerpIt0E0MHTyFJoJutdKGgXaPDJW/uxOXv7sbzv6ZjY1opqpu6b3cj8sOhwh6lkxTWNOO25fv03hH52nUxA+Z7LiskKahuRkWD8b+nhTXN0juI9SV4DPNxxI8PTpFGuV/opolB8HfpfeIEcKbg4pZJQQbv7+Fghav6MenofJPD3PHbI9PwyOwIqFV/XNKxsVTh/aVj4eXYP4k3+j4DDuZWo6hGvkBoqC8lLUNc7dSYa8J0kABXW8zuJtVl9jDPQftZMyPKQ7rA+0tqEXIMSAjpiQ6NFu9ty8KV7+3GiZL+a81hiCe/O2J0EWxaUR0O5FYLxxaN8TM4IWCg8XexxXf3TkZ8iCtUSgWcbS3x2JxIPDm/+9YZg41CoZCmkkwNd0eApGDaXG6ME6eS1Ld0mKVIuzuH8qql5wnzR3h3+trd3gpf3j0RcSGu0uN1aHV49KvDuPbDRGmSD3CmpdC3904Snr9cNcZPWsDwRWIOWtpNU3DT0NqBp/Ukcjw0M1xvCoshuksleX1TBj7fcxqpksJfF1tL/ENPS0gZP2cb/HfxaOn4vpwq1EjOrZUK4N0bxyLaW140ZAp/mhclLSr/eFc2SmpN10Kvv+3MFBeSTApzk34PiIiIzImfPkREREREZBKXSRYRtTrg92OmuSMdAH6S3A0I9G9bG3NwtlVjfJD4IvxWSQrEWYmS/tqeDlbSRIOB6v4Z4fByFC8UGNDhx2DGppPUNrXj1s/2obROXoDxzIJhuDzG13ST7KXYQGfp2OG8GqOPt/2k/HU4I0p/BLengzW+unui9L3jLDu1Cg/MDDN6bvrcMjkY1paGXRK5bUqwMFmlv1hbqvDY3Ejs/utMvLhoFF5YNBIbHk1AQj9Hni8cLX+d/9rLVJKTJfU4JHl9Xj3Gz+Q/n2WTgvWOz44efG1tztKXSqLVAR+a4e7uU+UNWPxhIl7ZcFJ6V/tAotUBj649jC3dFGyeb6We9INlRhSuDUSBbrb46p5JOP6veUj521w8Midi0BZSdefWycFdEokUCuC+Gab9DDLEZaN9pEkRa/b1fXubDcfFbW3UFkrh572jtSVW3B6HOcPk75c6HdCkJ10l1MMO3943GeGe4kQ3KwsVbp0cLByraGjDD5IEQ2M9vy4N+VXigshwT3vcM900r49JYW6IlxTf7DtdhZfXn5A+9u+XD4e7fc9SQeaN8JZ+H/V5duHwPmmn5+dsg9uniIvKW9q1eH3TSbPPoS9UNrTiqCRNsr/P8YiI6OLFQhIiIiIiIjKJ4T6O0gKFX0zU2kCn00nb2gzzcRwQrUNMbbakvU1KXjWq9PR/TzotLiSZFObWq+jt/mBnZYEnL+27u58NSSdp7dDgrpUHkFkmbzF025Rg3ClJU+kvng7W8HO2EY71pL3NthPiOyejvR3g4yR+nvNZW6rwzvVj8PAsebuNuxJC4dbDxREZN3srXD9BfLf3+ezUKiyNH5iLwJ6O1rgxPhBL44MQ6Na3d8qLhHvaI9pb/B7c27vnv9SzaGrKtjZnTQt3R7Dke2ptqcSUcHfh2GAxb4Q3Qj3En9ffpRSguLb3CTIAoNXq8Pme07js7V0Gvb8sGuOHv8yLwiXDvaQtRkxJpacYokOrw32rU7A3q6Lb49Q2t0vPTeKCXc1+t35fsbZUDdkCkrNG+Tvh31eOhK36THGavZUF/nbZcEwO6/vfeWtLFa4eKy6QPpRXg/Tiuj6bi06nw++SQpKECHfYWVkIx6wtVfjwprG4ZqzxbYFG+Tnhm3smSc9ZzropPujcz+tC/9uVDW1Pe2j9v01ppVi7X97u6sVFo0yWFKFQKPCYnlQSWQvFGVEeuErS/sVQT14ajeE+hr9XLZsY1KPik566f2YYXGzFhVXfHCzAiZK++30wl91ZFdIC+QRJWhIREZG5sZCEiIiIiIhMQl9rg/05VSit633scFpxnXThftGYgZP6YEqyQhKdDtgmSSWpbWrH8SLxBdWB0mLFWFfG+GGMnjSN7igUZ4qdbp0cjFsmBXW54/hC+tJJtFodHv86FftOi1ssAcClI73xt8uGD8iiHVl7G2MLSVo7NNgjWWQ15g5VpVKBxy+JwhtLYjq1awGAEb6OuDsh1Kh5GequhFBYdLMoen1coPSOcOpKlr6TWlCLvMqmHh0zv6oJ36cUCMcmBLtI71TvDaVSIU0lSYjwgI1k0XKwUCkVuH+GuHirXaPDxzuze/0cBdVNuOnTZPzrlzS0tGv17utmp8ZHy8bhjSWxeGBmOD6+eTz2PTMHSU/Nxoc3jcN9M8IwOcwN9pLFakP4OlljZpQH7p0ehjeWxOC3h6ch7d/z8MyCYdLHtHVoceeKAzgoaVlz1vcpBWiWtNAY7GkkF6NlE4OQ+NRsbH58OvY8OQt3TO2/gtCl8fKCxzXJfZdKklZcJ03kmHdBW5sLWaiUeGXxaKO+j5PD3PDl3RMNKiJ1srXEkgnigsLs8kZs6SbBT5/KhlY89f0R6fgNcQF62/f0xMRQN0wJN/xc3U6twguLRvX6fNPaUoV3bxwjLco537QId/zj8r49x3W0tsTDsyOEYzod8NJv8rSWwWJHhrg4O9DVFsGDLE2SiIiGjp7/BUZERERERHSBhTG+eHtrVpftOt2Z1ga39/JivKytjUIBXBEztNranBXmYY8gN1vkChZht54owzXjut7lmXy6UnpH26RBWkiiVCrwj8tH4Kr39hi0v4VSgVH+TogLcUV8iCvGBbl2Kgi4ItYPf/kmFdkVjXqP88OhQuzOqsCLi0Zh7vAz8ewvrU/X26pjfJAL3lgSq/du9/4UG+CMX492nX9qfg20Wp3Bd5zvO10lXTydGWV81PmiMf4Y6euE97efQmFNM8YGuuCuaSGwVZvn0oWfsw2ujPXDd5IiBQulotfvWRebhaN98MoGccT8uqNF0uIFmfL6Viz7NBl1LR3CcUNSZXpqaXwgvjmQjxMl9ee2qZQK/OmSKLM9Z1+6MtYXb27OQEF118XhL/fl4YGZ4T1qk6DT6fDNwQL8+5c0NLSKf27nmzfCCy8sGiV8Lm8na8x38sb8kWcWqrVaHbIrGpGaX4PUghqk5tcgrbiuU7scJxtLRHk7IMrLAVHeDoj2dkCEl4O0IOyuhFA0tHbgrS2ZwvGmNg1u/XwfvrxrIkb6OQn/vbK2Nu72Vt0ustPA5GRjOSCKCMM9HRAX7Ip9OV0LV388VIinFkSb7TPyfBuOixPaVEqF3tY1ZymVCvztsmFwtVNLPyPOmj/CG29eHwtrS8ML9u6YGoIVibnQCBI7Pt556tz5mzF0Oh2e+v4oKhrE6X/+LjZ4Wk8hWm88NicSe7ISDdr3r5dGd5vaYqhQD3s8f9VIPP51qnSfCE97vLd0LCxUfX9/8tL4IHyxNwc5gr+JdmSUY3dmBaZGDM7EMJ1Oh12Z4uLs6WxrQ0RE/YiFJEREREREZDKRXg6I8LQXpob8erR3hSQarQ4/HRZHx08KdYO3k3WPjz2QKRQKzIr2xOd7crqM7cgoR1uHtkukdlK2OCnD29EaQQOgBUZPxQY444GZYXhv26kuY1YWSowJdEZcyJn+8mMCnfUurowLcsFvj0zD65sy8L9d2dLCG+DMYvZdKw7gqlhfhHva43+7Tkv3DfOwwye3jDdqAaSvxUqSXepbO5Bd0WBwwoOsrY2DtQXG9jA9JsLLAW8sie3RY3vi3umh0kKSy2N8TbY4c7EIcrPDKD8nHC2s7TK2LrXYqEKS2uZ23PzZPuGCEXDmdbZglDgFyxSsLVVYflsc3t2Wif2nq+HpaIU/XRKFKEn7nsHGUqXEPdPD8OyPx7qMtbRr8fme0/jLPONaipXVt+Dp749ic3r3CQAO1hb41xUjsGiMn8F3tSuVCoR72iPc0/5cEWVrhwaZpQ1oadfA38UWXo5WRt8l/+icCDS2duCT3eL39vqWDtz82T58fc/ELu+Pe7IqkV0uLki8MS7AZC0v6OK1dGKgsJCkvrUDv6QWYYkZC+rO2nBM3NZmYqgrXOzUBh1DoVDggZnhcLFV45kfjwrPu5aMD8CLV48yuhDX38UWl43ywc+pXQvO9+dUIyWvGmMDXYw65rcHC7BR0uJQoQBevy4WDtbmKTYaH+yKhEgP7JQkVJzbL8gFN5m4/d7VY/2xJ6tSeG7kaqfGp7dMgKOZ/t3dUVso8cT8aNy/OkU4/uJv6Vj30NRB2YIrvbge5fWtwrEEFpIQEVE/YiEJERERERGZ1MLRvnhjc0aX7Qdzq1FU0wzfHi7MJmdXorROfIGtt33BB7rZ0V7CQpKG1g7sz6nClPDOd98lZlcKjzMpzG1Atloxxp8viUK4pz1+O1oCleKP1JHR/k6wsjCueMPaUoWnFwzDvBHeBqWT/ChJxDnLw8EKy2+Lg7OtYYsq/WWkrxNUSoXwzt1DeTUGF5JsPyleLE6I9OiXO1V7IsLLAfNHeOP3450XyZQKmK2lzlC3cLSPsJAkrbgO2eUNCPWw7/YYzW0a3PnFfqQXi1t0AcCtk4PN3mLG28kaz181yqzP0Z+uHeePt7dkChevVuzNxd0JYbBVq1Dd1Ibqxvb///82VDed+e+qxrZO206VNaDegBSSaRHu+O/i0fBx6n2hlpWFSpgUYgyFQoFnLhuGxjYNvtwnbhdS1diGpZ8k45t7JiPwvILMlUk5wv1VSgVu0NOWhMhQ80d6w8XWEtVN7V3G1iTnmb2QJLu8ASdL64VjPUncuTH+TMu4P3+Tei7VTKEA7psehr/Mi+rxeerdCaHCQhIA+N/ObHxw0ziDj5Vf1YR//ZKm97lM3dLmQo/PjdRbSKK2UOI/i0ebpWji31eOQEZpfafPcju1Ch8vG9fp/a8/XDrSG2MDnZGSV9NlLK24Dj8cKhSmNQ50OzPFP2sLpQKTwgZnmiQREQ0NLCQhIiIiIiKTWhjjIywkAc60t7mrh4uzP0rSSNQWSswfNbSj4+NCXGFvZSFsE7A5vbRTIUl1Y5t08XWwtrU5n0KhwKIx/lg0xnQXiY1JJ5GxU6vw+a0TEOA68BNfbNQqRHk5IE3wOjmcX4Nrxwd0e4ycikZp4U1P2tr0p+cXjcTx4lrkV/3R4uPJS6MxzMexH2c1eF022gcvrT8hHFt3pBgPz47Q+/h2jRYPrEnB/pxq6T4Tgl3wwEzj2uRQV9aWKtw1LQQv/tb151Xf2oG4FzajtUNrsuezsVTh6cuG4ab4wAFX1KhQKPD8VSPR3NYhLRosrWvF0k+T8M09k+HtZI2immZskiQWzB3mZZJCGSIrCxUWj/MXpqGlFtTiWGFtr4up9JG1tQGAS4b37Pz7stE+mBDsgm8OFqC1XYP5I30w3Ld3n7kj/ZwwOcwNe091Lab+/XgJcioaEexu1+1xNFod/vRNqrQ1V7S3Ax6fG9mruRoiNsAZs6M9seWEuGj3kdkRCDOgMLMn7KwssPbuifhg+ykczK1GgKsN7p0eZlAhqLmdLfy75gNx65/XNp7EZaN9BnQyoIisaGhckAvsrbiER0RE/Wdw3CJERERERESDRpiHvXQBdt0R/YkOMi3tGqw/Ko7VnjPMs98ilvuK2kKJhEhxz+8t6WXQnVf5kHxa3NYGACYOgUISczmbTvLtvZMRasBCw/lUSgXev2mcWRdyTE3W3uZwfo1Bj98mSSMBBl8vd3d7K2x6bDpeuzYGzy4cjvWPTMPdCWH9Pa1By9/FFmMkr6/uPgO0Wh2e+PYItkoWzgBgmI8jPrllwqBbJBqolsYHwclG/BlqyiKS8UEuWP/INCybGDTgikjOUikVePXaGFwy3Eu6T35VM5Z+koSKhlasSc6DINgJAHDzJNO2m6CL2w1x8tSR1cniFB1T2XBcfP49JtC5V20lPR2t8cDMcDx+SVSvi0jOkiWJ6XTAJ7uzDTrGp7uzsU9yLq1WKfHGklijE/B66jFJwcpwH0ezp6bZWVngz/Oi8OXdE/HfxTEDoojkrHFBrrh0pLiIqai2BZ/tkbegHIga/z9hUoRtbYiIqL+xkISIiIiIiExu4Wgf4fbUglpkSOKx9dl6okwal3/lEG9rc9bsaPHCVl5VE06VN5z7OknS1sbP2QYBrrw7ujtn00nuTgiFoWudL109atAVT8QGOAu3nyipR3ObptvHbzspvnMyxt8JHg5WvZlav7C2VOGacf64Y2oIk0hMYOFoX+H2jNIG6WeATqfDv9el4YdD4vQpAAh2s8WK2+OkhQ9kPDsrC9w2Jdhsx1erlHjq0mh8dc8kg9IA+puFSol3bhyDaRHi4k0AOFXeiGWf7sPa/eIF/HBPe7YiIJMK9bCXpsr9fLhQmp7RW8W1zdIC0/k9aGtjbtMjPRDlJW7P982BAlQ2iFtknnWipA6vbhCnKgLA45dE9uk5wkg/J/ztsmGdtvm72OC9pWNhOUhaCJrLE/OjYSFp6/PBtlPd/qwHkqTsSrRrxFWJg+3vCyIiGnou7jMOIiIiIiIyC1khCQBc8/5efHewoFOKRndkC4tONpaYEXVxXGCbEeUhLWzYkv7H3fuyQpKJoW4D9i7wgcaYdJLH5kTiOgNawQw0YySFJBqtDseKavU+tqmtQ/o6mzHI2tqQeVw2ykf6frUuVZxK8s7WLCzfmyM9ppejFVbeET8oC5UGulsnB8NObfo77Ef4OuKXh6binulhUEkW/AYiKwsVPl42HhOCXaT7pBfXoaKhTTg2kFNXaPC6MV6cStLYpsFPkvaPvbVRT1ubeQOwkEShUEhbaLZ2aLEyKVf62NYODR77KhVtGnESU1ywK+6aZt4UEJE7p4Xixwem4NE5EfjPNaPw4wNTEDIIivLMLcTdDjdNFCc/1bd24O0tmX08o56TtbVxs1NjOIubiYion7GQhIiIiIiITC7IzQ6jJG0+6ls78KdvUnHfqhSD7haraWrDdkkbjQWjfPosXrq/udlbYWygeFHrbP/0yoZWnCgR3+0/MdTVbHMbqrpLJ7l+QgAenh3e9xMzgTAPezhIeq4fzqvR+9jEU5Vok7S8mBnNQhICvJ2sMSFI/J6z7khxl0LClYk5eH2T/C5wJxtLrLg9HgGutiadJ53hbKvGLZODTXY8G0sVHp4Vjh/un4Iob3E6wEBno1bh01snYLS/cS3LbNUqLBp7cSSlUd+aN8IbbnZq4dia5DyjCrQN9fsxcVubaG+HAZswdEWML7wcxQWHKxJzpalrb27ORHpxnXDMTq3Ca9fF9FtBXGyAMx6dE4klEwLhbs9iyrMenh0hPZddnZyH7PMSGweynZkVwu0JkR5QDqIiTCIiGppYSEJERERERGZxRYy4tcFZvx8vwbw3d2FzmvxuRwD49WixNO530ZiLa7FmlmSR/mBuNWqa2pAs6ekOgDH7PXR+OsmkUDcoFIC9lQX+fEkknr9q5KC961ypVGB0gHiBVBZjf9Y2SWGXm50aoyUFZHTxWRgjTqbKrmhE2nmLdT8dLsTffz4uPY6NpQqf3Tph0BYkDBYPz47A+CB5AsdZ9lYWCHC1QYy/E6ZHemDRGD/cNiUYf5p75j3xfzePx8Fn5+DxS6Kgthjclx0drS3xxW1x0lYZIovG+MHRmq2XyPTUFkosHu8vHDteVIcjBfrTxIxV1diGfTni88qBmEZyltpCidumhAjHqhrb8G1KQZft+3Oq8OGOU9Jj/uPyESxkHIBc7dS4b2aYcKxDq8MzPxxDXmVTH8/KOPlVTThd0SgcS4iUt1gjIiLqK+KSTSIiIiIiol66Pi4An+85jaLaFuk+FQ2tuHPFASwZH4BnLx8Oe8FdZT8dErdB8HO2MWjRayiZPcwTr2w42WW7RqvDjoxyHMytFj4uwNUG/i68AN4b44Jc8OXdE9HSroGVhXLQFpCcLzbAGXuyurao0VdIotPpsO2EOIJ7Ou+cpPNcOtIH//z5OLSCOsBfjxRjhK8Ttp0sw5++ToXsRnpLlQIfLRuHcRfZe31/sLZU4at7JmHbiTLkVDbCRq2Ci636zP/sLOFqq4azrXrQF4cYy8VOjZV3xuG6DxORY8CC5LJJ4lYLRKZwY1wgPtqRLRxbk5yHGEnbup7YnF4KjegNHAO7kAQ40wbo3a1ZaGjt6DL26a5s3BgXeC5dpKG1A49/fVj6OTRnmBeulRTwUP+7fUoIViXmCv/eTMyuxIxXt2H+SG/cOS1UmuzYn3ZI2toAwLSIi6N9KxERDWwX119/RERERETUZxysLfH1vZMMWgD86kA+5r+5E/suSNQoqG6S3g15ZazvRbdoHeXlAD9nG+HY5vQyJJ7qWhQAABNDmEZiKtaWqiFRRAIAsQHi383CmmaU1YsLwLLKGlBY0ywcm8G2NnQeDwcrTAwVv/esO1KMAzlVuG/VQXRIFioVCuCNJbFIiORCSl9RKRWYM9wLd04LxdL4ICwY5YNJYW6I9naEp6P1RVdEcpangzVW3RkPXydrvfvFBbsi2tuxj2ZFF6MgNztMixCnFPycWoS6lnaTPdcGSVubQFdbDPMZ2AlRjtaWuCEuQDiWU9mETWl//NueX5eG/CrxeY2bnRovXzNqyJz3DUXWlir8eV6UdFyrA347WoKr39+Laz7Yi9+PFUsLpPrDTkkhyQhfR7YxIiKiAeHi/AuQiIiIiIj6hL+LLb6+ZxL+Mi8Klir9F2ELqpux5ONEvPRbOlraz/Qv/+mwOI0EAK66yNraAIBCocDsYeLF+i3ppcgsE/cCZ1sbEonVc+fy4bwa4XZZWxulAkiQLG7Rxeuy0eL2NnlVTbjp02S0tGulj33+qpFYOFp/izSivuLvYovVd03Uu7DHNBLqCzfGBQq3N7dr8OOhQpM8R0NrB3ZlVQjH5o/0HhSFFbdNCYGFpOD8o53Z0Ol02JxWirX786XHeOnqUVzMHwSuivXDCN/ui/gO5lbj3lUpmPXadnyxNwdNbV0Ta/pSu0aLvZKbAFhES0REAwULSYiIiIiIyKxUSgUemBmOHx+Ygigv/Xcw6nRnLu5e+e4eHC+qlV4QH+7jiMhujjVUzR7mJdze1KaRPkaWCkAXNw8HK2nCjay9jaytzdhAFzjbqk01NRoiLh3pc659wIX0FZH8ZV4UlsZzUZ4GlhB3O6y+Mx7OtpZdxoLcbAd8uw8aGuYM94KHg7i4YU1yHnSyHi1G2H6yDG0d4vfowfI693W2weUx4mLEQ3k12HC8FE9+f0T6+OvG++OSQfJvvdgplQr84/IRMDSoMreyCf/4+TgmvbQV//39BMrq5G1YzSklt1rYfgkAEtjWhoiIBggWkhARERERUZ8Y4euEnx+agnsSQtHdjYwnS+txxbt7pAkbV425eO9Sjw9xha1aZfD+QW628JUUCxDFBjoLt4sKSepa2rFf0mpqJtvakICrnRqTjUxEunNqCO6fEWamGRH1TpS3A1bdEY9Qd7tz29zt1XjvxrEXbesf6luWKiWuG+8vHDtRUo8USaKYMX6XtLXxdLDCGD1pZgPNXdNCpWMPrElBRUObcMzfxQbPLhxurmmRGcSFuOLdG8calSBT29yO97efwpT/bMWfv0nFiZI6M86ws7K6Fry5OVM4ZqdWGdQaloiIqC/wLxwiIiIiIuozVhYqPLVgGL66exL8XfQXN8j6VysUwBUxF19bm7OsLVWYZkQLkUlMIyE9ZAtCRwpqu/wO7smsQIfk93JGFO+cJLHLjWhPc81Yfzy9YNigaJtAF6+Rfk747ZFpWHF7HD65eTx2/3UWRvo59fe06CJy/YRAaVH2muS8Xh27pV2DbSfEbewuGeEFpaGxDwPAcF9H6Tmzvr8zXr8uFg7WXZOHaGBbMMoHu/86E/+9ZjQiPO0Nfly7RodvDxZg/pu7sOzTZOzKLDdJso+ITqfDT4cLMfeNnUjMFre1mRTmzsJEIiIaMPiJREREREREfS4uxBW/P5qA6ycEGP3YSaFu8HayNsOsBo/Z0eL2NiJsa0P6xEoKSRpaO3CqvHMi0LaT4oUlL0crDPfpvjc9XZzmjfCGpar7hcc5w7zwn2tGDapFSrp4WVuqkBDpgTnDvWBtaXhKGJEpBLjaSltfrDtShNqm9h4fe++pCjRK2iXOH+HT4+P2l7sT5Kkkwv2nhSIuxNVMsyFzs7ZU4boJAdj4WAI+v20CpoQb93fQrswKLPt0H675YC92Z1aYtKCkvL4V9646iEfWHkZts/x3dHqk4TcMEBERmRsLSYiIiIiIqF/YW1ng5WtG45Obx8PdXm3w466KvXjTSM6aEW14+sMkI9tK0MVlpJ8TLCQL94fPi8fX6XTYdrJcuN/MKE8mSJCUk60lpkkWPM+KD3HFuzeOgYWKl6mIiAyxND5QuL21Q4vvDxX0+LiytjZONpaIDx18BRZTw90NLnaN9nbA45dEmnlG1BcUCgVmRnli9Z0T8evDU3H1GD/p+a5ISl4Nbvo0GUs+SkKSJDnEGOuOFOGSN3Zgw/FSvftZWSgxf+TgK9giIqKhi3+hExERERFRv5oz3AsbHk3A/BHe3e6rtlBi/qju9xvqPB2sEWNAj/pQdzt4OV7c6S2kn7WlCtE+DsKxQ/k15/77eFEdyutbhfvNiPI0x9RoCFk4Wr4oMtLPEZ/cMp6pDkRERpgV7QlvyTne6uQ8dGi0Rh+zQ6PFpjTxQvfsYZ6wHITFfgqFwqBUErVKiTeWxMLKgp9FQ80IXye8viQWu/46E/dMD4WDtYXBj92XU4XrP07Cjf9LwoGcKqOfu7KhFQ+sTsGDaw6hupukIEuVAq9cGwMPByujn4eIiMhcBt/ZHxERERERDTlu9lb44KaxeP26GK6UzG8AADSUSURBVDhYyS/uzRnmCUf2LAcAzI7ufvE+nm1tyACy9jaHzysk2S5pa2OpUhgdG04Xn3kjvOEraEkW6m6H5bfFwYHv60RERrFQKXGdpEVkVlkD7l+dgpZ2cYsamX05VdLFbkMKvgeqy0b7CD+Dzvf4JZEYxjZ9Q5qPkw2eunQYEp+ajb8vHA5/FxuDH7v3VCUWf5iImz/bh0N51QY95vdjxbjkjZ349Whxt/uO8HXEzw9OxRUxvgbPiYiIqC+wkISIiIiIiAYEhUKBq8f64/fHEjBZ0I5FqQDunR7WDzMbmGYP676QhG1tyBCxAS7C7Rml9Whq6wAAaVubCcGuLAKgbtlZWeC162I73WUbF+yKlXfGw92ed94SEfXE9RMCIOvWsTGtFLcv34+G1g6Dj7dR0nbDxlKFhEjD2yoONJYqJW6fGiIdjwt2xV3Tuk8toaHB3soCt08NwfY/z8B7N441KOXxrJ0Z5Vj0/l7cvnw/jhbUCvepbmzDw18ewr2rUlDZ2Kb3eBZKBR6bE4kfH5jCQiYiIhqQDM/xIiIiIiIi6gN+zjZYdUc8Vibl4sMdp1Bc2wJXOzX+vnA4Rvs79/f0BozhPo7wdrRGSV2LdJ+Jg7CXPfU9WSKJRqvDscI6RHjaS+++nMm2NmSgSWFu2PXETBzIqYadlQox/s5QylZAiYioW77ONpgV7YXN6eICkL2nKrH0f0n4/LY4uNqp9R5Lq9Xh92MlwrGZ0R6Dvv3Y9XGBeGdrFmqbOyeu2KlVeO26GKj4eXTRsVApcdloHywY5Y0DudV4b1sWtksKpy+09UQZtp4owyXDvfDY3D/SbDalleKp74+iokHcDvJ80d4OePXaGIz0c+rVv4OIiMicWEhCREREREQDjlKpwC2Tg3HTxCDUNLXB2VbNC7wXUCgUmDXME2uS84TjYR528HTQH+NNBJxpL+JgbYH6lq53LR/Or0ZxbTO0OvFjZ0YP3juUqe9ZW6owNcK9v6dBRDRkPLtwGPaeqkBTm7iNTWpBLa77KBEr74iDj5O8lceRwlppcfK8QdzW5ix7Kwu8fcMY3Pb5vnPnNDaWKrx9wxgEuNr27+SoXykUCkwIdsXy2+JwMLcab2zKwO6sCoMeuzGtFBvTSnHZKB9YqhT48XBRt49RKRV4YEYYHpwVAbUFGwYQEdHAxkISIiIiIiIasFRKBdzY9kBqjp5CEra1IUMplQrEBjhjV2bXi+aH82ugVokvcvu72CDMw97c0yMiIiKJIDc7fHF7HG5fvl9YEAoAWWUNWPzBmWKSUMnntiyNxFKlwMzooZE+Nj3SA5sen45fUotgq1ZhZpQnIrwc+ntaNICMC3LBqjvjkZxdidc3ZSD5dJVBj/v1aLFB+0V62eO1a2Mxyp8pJERENDiw5JGIiIiIiGiQmhzmDmtL8Z91E0NZSEKGk7W3ScmtwY4Mccz3zChPKBRMCiIiIupPE4JdsfbuiXC3l7evKaxpxnUfJeJ4UW2XMZ1Ohw3HxYUkU8Ld4WhtabK59rcwD3s8OicSdyeEsYiEpOJD3bD27olYc2c8xgW59Pp4SgVw/4ww/PLQVBaREBHRoMJCEiIiIiIiokHK2lKF6ycEdtnu42SN2dFe/TAjGqxkhSQldS2obmoXjrGtDRER0cAwwtcJ39w7GX7O8vY1FQ1tuP6jJOy7IGUhs6wBpysahY8ZCm1tiHpCoVBgcrg7vr13ElbcHocYyblyd8I87PD9/VPwxPxoWFmoTDtJIiIiM2MhCRERERER0SD2+CWRmBrufu5rL0crvHptDGzUvFBJhpMVkshYWSgxKdS9+x2JiIioT4S42+G7+yYjwlPedq6+tQPLPk3G1hOl57bJ2tooFMDc4SxMpoubQqFAQqQHfrx/Mj67dTxG+jka9DilArgnIRS/PjzN6PNsIiKigcKivydAREREREREPedobYlVd8bjeFEtWju0iPRygL0V/9Qj47jZWyHA1Qb5Vc0G7T8pzI3FSkRERAOMt5M1vr5nEm5dvh+p+TXCfVo7tLh7xUG8dl0Mroz1k7a1mRDsCnd7KzPOlmjwUCgUmBXthZlRntiYVoo3NmXgREm9cN9Qdzu8cm2MSdriEBER9SdeXSQiIiIiIhoCRviy3zb1TmyAi8GFJDOjPM08GyIiIuoJFzs1Vt8Zj3tWHsCerErhPh1aHR796jBOltTjeFGdcJ/5bGtD1IVCocC8Ed6YO8wL64+V4M3NGcgsawAAWKoUuHlSMP4yLwrWliy4JiKiwY+FJERERERERESE2ABn/JJaZNC+LCQhIiIauOytLPDZrRPwyJeH8bskcUSnA97ffkp6jEtGsK0NkYxSqcBlo31w6UhvZJU3IKeiEfEhbnCytezvqREREZmMsr8nQERERERERET9z9D+7aEedgh0szXvZIiIiKhXrCxUePfGMbhuvL/Rjx3l5wR/F37WE3VHqVQg0ssBl4zwZhEJERENOSwkISIiIiIiIiKM8HWEpUrR7X5MIyEiIhocLFRK/Oea0bg7IdSox80fybY2RERERBc7FpIQEREREREREawtVRjm49jtfiwkISIiGjwUCgWeujQaT8yPMvgx80awkISIiIjoYsdCEiIiIiIiIiIC0H17G1u1ChNCXPpmMkRERGQSCoUC988IxwuLRkLRTfhYmIcdwj3t+2ZiRERERDRgsZCEiIiIiIiIiAB0X0gyJdwdVhaqvpkMERERmdTS+CC8ff0YWCjl1SRsa0NEREREAAtJiIiIiIiIiOj/dVdIMiuabW2IiIgGs8tjfPHJLeNhbdl1aUClVOCqWL9+mBURERERDTQsJCEiIiIiIiIiAECIux2cbCyl4zOiPPpwNkRERGQOM6I8sfrOeLjZqTttv39GGCK8HPppVkREREQ0kFj09wSIiIiIiIiIaGBQKBSICXDGzozyLmPR3g7wcbLph1kRERGRqY0LcsX2v8zA1wcKUN/SjkmhbogPdevvaRERERHRAMFCEiIiIiIiIiI6Z0akh7CQZP5I736YDREREZmLg7Ul7pga0t/TICIiIqIBiK1tiIiIiIiIiOicG+MD4e/SOXnE18kad00L7acZERERERERERFRX2IiCRERERERERGdY22pwq8PT8N727KQU9H4f+3deZTXdb0/8NfAzLAMO7Io6rAHKiiIyeLWBUVN09I0d9DkZpqkdSO73gPYcii5lUsYal5sURRTolLKMVcEjUWEQAVxWEXAAQwYBga+vz8avj+/s8B3Zr40LI/HOZzj6z3vbe459xXz5TmfTxzbqnHcdFaXyGvgIwQAAACAw4FPgQAAAIAUzRvlxPfO71nX1wAAAACgDni1DQAAAAAAAAAAESFIAgAAAAAAAABAGUESAAAAAAAAAAAiQpAEAAAAAAAAAIAygiQAAAAAAAAAAESEIAkAAAAAAAAAAGUESQAAAAAAAAAAiAhBEgAAAAAAAAAAygiSAAAAAAAAAAAQEYIkAAAAAAAAAACUESQBAAAAAAAAACAiBEkAAAAAAAAAACgjSAIAAAAAAAAAQEQIkgAAAAAAAAAAUEaQBAAAAAAAAACAiBAkAQAAAAAAAACgjCAJAAAAAAAAAAARIUgCAAAAAAAAAEAZQRIAAAAAAAAAACJCkAQAAAAAAAAAgDKCJAAAAAAAAAAARIQgCQAAAAAAAAAAZQRJAAAAAAAAAACICEESAAAAAAAAAADKCJIAAAAAAAAAABARgiQAAAAAAAAAAJQRJAEAAAAAAAAAICIESQAAAAAAAAAAKCNIAgAAAAAAAABARAiSAAAAAAAAAABQRpAEAAAAAAAAAICIECQBAAAAAAAAAKCMIAkAAAAAAAAAABEhSAIAAAAAAAAAQBlBEgAAAAAAAAAAIkKQBAAAAAAAAACAMoIkAAAAAAAAAABEhCAJAAAAAAAAAABlBEkAAAAAAAAAAIgIQRIAAAAAAAAAAMoIkgAAAAAAAAAAEBGCJAAAAAAAAAAAlBEkAQAAAAAAAAAgIgRJAAAAAAAAAAAoI0gCAAAAAAAAAEBECJIAAAAAAAAAAFAmu64vAHCoKCkpSamXLl1aRzcBAAAAAIDDQ/nP4st/Vg9A9QmSAGTIypUrU+qLL764bi4CAAAAAACHqZUrV0bfvn3r+hoABzWvtgEAAAAAAAAAICIESQAAAAAAAAAAKJOVSCQSdX0JgEPBpk2b4uWXX07WxxxzTDRo0KAOb1R7S5cuTXlFz9SpU6Nr1651dyGASuhVwMFArwIOdPoUcDDQq4DKlJSUpLx6/swzz4wWLVrU3YUADgHZdX0BgENFixYt4qKLLqrra+xXXbt2jeOPP76urwGwV3oVcDDQq4ADnT4FHAz0KmCPvn371vUVAA4pXm0DAAAAAAAAAEBECJIAAAAAAAAAAFBGkAQAAAAAAAAAgIgQJAEAAAAAAAAAoIwgCQAAAAAAAAAAESFIAgAAAAAAAABAGUESAAAAAAAAAAAiQpAEAAAAAAAAAIAygiQAAAAAAAAAAESEIAkAAAAAAAAAAGUESQAAAAAAAAAAiIiI7Lq+AAAHrjZt2sTo0aNTaoADjV4FHAz0KuBAp08BBwO9CgDg3yMrkUgk6voSAAAAAAAAAADUPa+2AQAAAAAAAAAgIgRJAAAAAAAAAAAoI0gCAAAAAAAAAEBECJIAAAAAAAAAAFBGkAQAAAAAAAAAgIgQJAEAAAAAAAAAoIwgCQAAAAAAAAAAESFIAgAAAAAAAABAGUESAAAAAAAAAAAiQpAEAAAAAAAAAIAygiQAAAAAAAAAAESEIAkAAAAAAAAAAGUESQAAAAAAAAAAiIiI7Lq+AAAHrg8++CDeeuutWLNmTWzZsiWOPPLIyM/Pj4EDB0ZOTk5dXw84BOzatSuWLl0aixYtijVr1sTmzZujQYMG0bJly+jSpUv069cv8vLyMnrmzp07Y8aMGbFixYr48MMPo0mTJnHUUUdFnz59omPHjhk9C6C69Cggk955552YP39+rFq1KoqLi6Nhw4bRtm3b6Nq1a5x44om1+nuWfgXUVHFxcbz11luxePHi2LhxY2zfvj2aNWsWbdu2jb59+0bXrl0jKyur1ufoUwAANSdIAkAFTz31VPz0pz+NmTNnVvr1Vq1axeWXXx533XVXHHHEEf/m2wEHuxUrVsTTTz8dBQUF8eqrr8Ynn3xS5dz69evH2WefHbfcckt8/vOfr9W569evj9GjR8cTTzwRRUVFlc4ZOHBg3H777XHJJZfU6izg8PCVr3wlnnjiiZSx/Pz8KCwsrPZeehSQKZs2bYp77rknHnnkkVixYkWV8+rXrx8nnXRSXHrppfHd73437f31K6CmZs6cGT//+c9j6tSpsWPHjirndejQIW644YYYOXJktGrVqtrn6FMAALWXlUgkEnV9CQAODFu2bIkbb7wxJk+enNb8du3axaOPPhpDhw7dzzcDDhVXXnllPP744zVae8EFF8TDDz8c7dq1q/ba5557LoYNGxbr1q1La/5VV10VEydOzPjTUIBDx7Rp0+Kiiy6qMF6TIIkeBWTKlClT4qabboqPP/447TXt2rWLtWvXpjVXvwJqorS0NL75zW/GhAkTojr/HNGuXbuYNGlSnHvuuWmv0acAADJDkASAiPjX6yW+8IUvxLPPPpsy3qZNm+jTp080b9483n///Zg3b17KD/0NGjSIgoKCOO200/7dVwYOQv369Ys5c+ZUGO/QoUN069Yt2rVrF6WlpbFs2bKYP39+7N69O2Ve9+7d4+WXX4727dunfeZLL70UQ4cOTfmNt6ysrOjbt2907tw5Nm3aFPPmzYsNGzakrLvwwgtj6tSpUa9evWp+l8ChbtOmTXH88cfHmjVrKnytukESPQrIlLFjx8aYMWMqjB977LHRvXv3aNOmTWzfvj0+/PDDWLBgQWzdujUi0g+S6FdATSQSibjsssviqaeeqvC1Hj16RM+ePaNRo0axfv36mD17dmzcuDFlTm5ubvzhD39IK0yiTwEAZFACABKJxLe//e1ERCT/5OTkJO67775ESUlJyrx//OMfiQEDBqTMbd26dWLNmjV1dHPgYHLyyScne0efPn0S9913X2Lp0qWVzl21alVixIgRKf0mIhKnnXZaYvfu3Wmdt3LlykTLli1T1g8aNCixaNGilHnbt29P3HPPPYmcnJyUuXfccUetv2fg0HP99dcn+0TTpk1T+kZ+fn7a++hRQKaMHz++wt+ZrrjiisTbb79d6fxdu3YlXnvttcRtt92WOO644/a5v34F1NSDDz5YoT+dccYZiQULFlSYu3PnzsQjjzySaN68ecr8tm3bJjZt2rTXc/QpAIDM8kQSAGLZsmXRo0eP2LlzZ3Js6tSplT6uPSKiuLg4Bg8eHDNnzkyO/ed//mf88pe/3O93BQ5up5xySrRr1y7GjBkT/fr1S2vNhAkT4uabb04Ze/zxx+MrX/nKPtfecMMN8cgjjyTrgQMHxgsvvBANGzasdP7UqVPji1/8YrJu0KBBvPvuu5Gfn5/WXYFDX0FBQZx99tkREZGdnR1333133HbbbcmvV+eJJHoUkAnz58+Pfv36RWlpaURE5OTkxGOPPRaXXnppWutLS0sjOzt7r3P0K6CmOnXqlPJ3ozPOOCMKCgoiJyenyjWzZ8+O0047LUpKSpJj48aNi1GjRlW5Rp8CAMgsz2oDIMaOHZsSIhk2bFiVIZKIiEaNGsWkSZMiNzc3OfarX/0qli1btl/vCRz8pkyZEn/605/SDpFERHz961+PSy65JGXsN7/5zT7XLVmyJB599NFknZubG5MmTaryg8SIiIsvvjiuu+66ZF1SUhJjx45N+67AoW3r1q1x4403Juvbb789TjrppBrtpUcBmVBaWhrXX399MkQSETFx4sS0QyQRsc8QiX4F1NSCBQsqBGzvvffevYZIIv71StRP/50rIuKPf/xjlfP1KQCAzBMkATjMFRcXV3hP7d5+w2OP7t27x8UXX5ysS0tL47HHHsv09YBDTMeOHWu0rvwTSV588cV9rnnsscdi165dyfpLX/pSdOvWbZ/ryvfAJ598MrZv357mTYFD2R133JH8x5DOnTvHmDFjaryXHgVkwpQpU2Lu3LnJevDgwTF8+PCMnqFfATVV/heOjjnmmDjxxBPTWlv+F5yWLFlS5Vx9CgAg8wRJAA5zf/nLX2Lbtm3JesCAAdGjR4+01pb/gPLpp5/O6N0A9ujTp09KXVxcHJs2bdrrmmeeeSalTvcfVXr27Bmnnnpqst66dWv89a9/Te+iwCHr9ddfj1/84hfJeuLEidGoUaMa76dHAZkwceLElPp73/texs/Qr4Ca2rp1a0p99NFHp732mGOOSak3btxY5Vx9CgAg8wRJAA5z06dPT6nPOuustNeefvrpKY9BnjdvXnz00UeZuhpAUmWPXN+xY0eV89euXRvz589PWT9o0KC0zyvfC5977rm01wKHnpKSkrj++utj9+7dERFx3XXXxZAhQ2q8nx4FZMLSpUvj5ZdfTtYdO3aMz33ucxk9Q78CaqN9+/YpdXWe9lF+bqtWrSqdp08BAOwfgiQAh7mFCxem1AMGDEh7bV5eXvTq1Stl7B//+EdG7gXwaUuXLk2ps7Oz44gjjqhyfvne1rt378jLy0v7vIEDB6bUehsc3saMGRPvvvtuRES0adMm/vd//7dW++lRQCaUf9Xf4MGDIysrK6Nn6FdAbZxyyinRoEGDZL148eIoLi5Oa+2cOXMq7FUZfQoAYP8QJAE4zC1evDil7tq1a7XWd+nSJaVetGhRre8EUN5TTz2VUvfr1y/q1av6r7Lle5HeBtTU3LlzY/z48cn65z//ebRu3bpWe+pRQCa8+eabKfWeXwpIJBJRUFAQw4cPj+OOOy6aN28eeXl5kZ+fH0OGDIlx48ZFYWFhWmfoV0BtNG3aNK699tpkvX379vjVr361z3W7du2K+++/P2Xsuuuuq3SuPgUAsH8IkgAcxoqKiqKoqChl7Nhjj63WHuXnL1mypNb3Avi0LVu2VPiw8Ytf/OJe15R/gkl1e1t+fn5K/fHHH+/1ndzAoam0tDSuv/76KC0tjYiIc889N6688spa76tHAZkwe/bslLpnz55RWFgYQ4YMibPPPjsmTZoUixcvjk8++SS2bdsWK1asiBdeeCHuuOOO6N69e9x8882xbdu2vZ6hXwG1NW7cuOjYsWOy/s53vhMFBQVVzt+5c2eMGDEi5s2blxz7j//4j7jkkksqna9PAQDsH4IkAIexTZs2pdSNGzeu1uM/IyLatm2bUm/evLm21wJIcccdd8TatWuTdYsWLeKrX/3qXteU72/le9W+NGnSJBo2bJgypr/B4WfcuHExf/78iPjXK/0eeOCBjOyrRwGZ8OGHH6bU27Zti1NOOSX+9re/7XPtzp07Y8KECXHaaadV2OfT9Cugtlq1ahUvvvhi9OnTJyIiiouLY+jQoXH55ZfHlClTYsGCBbF06dKYNWtW/OxnP4tevXrFI488klz/2c9+Np566qkqX92lTwEA7B/ZdX0BAOrOli1bUupGjRpVe4/ya/75z3/W6k4An/bMM89UeKTxD3/4w2jVqtVe12Wqv23fvj1Z629weFm0aFH84Ac/SNbf//73U36btjb0KCATyv/j6fDhw2PDhg0R8a/w29e+9rU477zz4uijj46tW7fG/Pnz45FHHonXXnstuWbevHlxySWXxMsvvxw5OTkVztCvgEzo2LFjvPHGGzFp0qR48MEHY86cOfHkk0/Gk08+WeWa1q1bx+233x7/9V//VWl/2kOfAgDYPzyRBOAwVv6H7fK/gZGO8j+gl98ToKbmz5+f8j7tiIhzzjknbrrppn2u1d+A2ti9e3fccMMNUVJSEhERJ598ctx6660Z21+PAmqrpKQk2aP2WLVqVUREHHfccbF48eIYP358DB48OD7zmc9E3759Y/jw4fHqq6/G+PHjU9bNnDkzfvzjH1d6jn4FZMquXbti165d0aBBgyqfLrLHMcccE+PHj4/bb799ryGSCH0KAGB/ESQBIGlfP8hnag3AvqxYsSI+//nPp3yAl5+fH7/97W//bb1Kf4PD1z333BOzZs2KiIjs7Ox4+OGHo379+vvtPD0KqK5du3ZVOt68efOYPn16HHPMMVWu/da3vhW33XZbytjPfvaztP7hVL8CamLGjBnRs2fPuOmmm2LGjBmxe/fuvc5fuXJlDB8+PI499th4+OGHq3WWPgUAkBmCJACHsSZNmqTUxcXF1d6j/JryewJU17p16+Lss8+O1atXJ8fat28fzz//fLRp0yatPfQ3oKaWLVsWd955Z7K+/fbb46STTsroGXoUUFuNGzeOevUqfqx3++237zVEssf3v//9aN68ebIuKiqK5557rsI8/QqorRdeeCGGDBkShYWFybEOHTrEuHHjYt68ebFp06bYsWNHrF27NqZPnx7XXXddZGdnR0TE+vXr48Ybb4wRI0ZEIpGodH99CgBg/xAkATiM+WEbONAUFRXFkCFD4r333kuOHXHEEVFQUBDdunVLex/9DaiJRCIRN954Y2zbti0iIjp37hxjxozJ+Dl6FJAJeXl5FcbKvxZwb2u/9KUvpYy99NJLFebpV0BtrF+/Pq644orYvn17cuzCCy+MRYsWxahRo+Kkk06K5s2bR05OTrRr1y6GDh0akyZNildffTVat26dXPPQQw/FT37yk0rP0KcAAPYPQRKAw9infwMtImLbtm2xdevWau2xbt26lLpFixa1vRZwmNq8eXOcc845sWDBguRYy5Yt4/nnn4/jjz++WnuV72/r16+v1votW7ZU+DBRf4ND30MPPRR/+9vfkvXEiROjUaNGGT9HjwIyofz/37dr1y46duyY9vr+/fun1IsXL64wR78CauOnP/1pSt/o0aNHPPnkk9GsWbO9ruvfv3888cQTKWNjx46t8BlUhD4FALC/ZNf1BQCoO61bt46WLVvGxo0bk2MrVqyInj17pr3H8uXLU+rqPDEAYI9//vOfce6558acOXOSY82aNYvp06fX6JUS5XtR+V61L+Xnt2rVKlq2bFntewAHl9GjRyf/+/zzz4+uXbumPIa9MmvXrk2pS0tLK6w56qijIjc3N1nrUUAmdO/ePVauXJmsjzzyyGqtP+qoo1Lqjz/+uMIc/QqojSlTpqTUo0aNioYNG6a1dvDgwXH66afHq6++GhH/emrI5MmT49Zbb02Zp08BAOwfgiQAh7mePXvG66+/nqyXLl1arSDJsmXLKuwHUB1bt26N888/P2bNmpUca9KkSTz33HPx2c9+tkZ7lu9FS5curdb68r3tuOOOq9E9gIPLp38b9dlnn41OnTpVe4/Vq1dXWDdv3ryUUJweBWTC8ccfHy+88EKybtCgQbXWl5//6VdP7KFfATW1devWeP/991PGBg8eXK09hgwZkgySRES88cYbFeboUwAA+4dX2wAc5k444YSUeubMmWmv3bp1a7z99tt73Q9gb4qLi+OCCy6I1157LTnWuHHj+POf/xwDBw6s8b7le9Hbb78d27ZtS3v9jBkz9rofQG3oUUAm9O7dO6XetGlTtdaXn9+6desKc/QroKYq60nt27ev1h7l52/YsKHCHH0KAGD/ECQBOMyde+65KfVLL72U9tpXX301SktLk3WfPn2iXbt2mboacIjbvn17fOELX0jpOw0bNoxp06bFGWecUau9jzzyyJR/XCktLU0Jq+xL+V543nnn1eo+AJ+mRwGZcN5550VWVlayXrZsWaVPFanKwoULU+qjjz66whz9CqipFi1aVBjbunVrtfbYsmVLSt2kSZMKc/QpAID9Q5AE4DA3dOjQaNSoUbKeOXNmvPPOO2mtnTRpUkr9xS9+MZNXAw5hO3bsiC996UtRUFCQHGvQoEFMnTq12o87rkr5nvR///d/aa175513Uh6ZnJeXF+ecc05G7gQc2DZt2hSJRKJaf1588cWUPfLz8yvM+fRrbfbQo4DaOuqoo2LAgAHJeufOnSmvutmX6dOnp9Snn356pfP0K6Am8vLyolmzZilj8+bNq9Yec+bMSamreqKJPgUAkHmCJACHucaNG8ell16aMvbjH/94n+vee++9eOaZZ5J1dnZ2XHnllRm/H3DoKS0tjcsuuyyee+655FhOTk489dRTMXTo0Iydc9VVV0X9+vWT9dNPPx1LlizZ57ryPfCyyy6Lhg0bZuxeABF6FJAZw4cPT6l/+tOfprXu1VdfjTfffDNZ16tXL84///xK5+pXQE2dddZZKfWDDz6Y9tq1a9fGtGnTUsaqCrzpUwAAmSdIAkCMGTMmcnJykvWkSZMq/LD+adu3b4/hw4fHjh07kmM33HBDdOnSZb/eEzj47dq1K6666qr4wx/+kBzLzs6OJ554Ii644IKMntWtW7e47rrrkvWOHTti2LBhe33k+x/+8IeUpy3l5ubG6NGjM3ovgAg9CsiM4cOHR8+ePZP13/72t32GSdatW1chgHLZZZdV+fOcfgXU1OWXX55SP/HEE/Hb3/52n+tKSkrimmuuSXm1TZMmTar8xQN9CgAg8wRJAIjOnTvHyJEjU8YuvfTSuP/++1PCIhERixcvjsGDB8frr7+eHGvdurUftoG0XH/99fHkk0+mjP3oRz+KPn36RGFhYbX+7O1DwT3Gjh0bLVu2TNavv/56DBkypMIrvEpKSuK+++6LL3/5yynj3/rWtyI/P78W3zFA1fQooLbq168f99xzT9Sr9/8/4vvWt74VI0eOjI0bN1aYX1BQEIMGDYr3338/OdayZcv40Y9+tNdz9CugJr7yla/EiSeemKwTiURce+21MXLkyPjwww8rXfPiiy9G//79U16DGhExatSolD5Unj4FAJBZWYlEIlHXlwCg7u3atSsuvPDClFdNRES0bds2+vbtG02bNo1ly5bF3Llz49P/05GbmxsFBQVVPl4U4NOysrIytteLL75Y4VHJlXnppZdi6NChKcG4rKysOPnkk6Nz586xefPmmDt3bqxfvz5l3QUXXBBTp05NeUQyQHkvvfRSfO5zn0vW+fn5UVhYWK31ehRQW/fff3984xvfSBnLycmJ/v37R4cOHaK4uDjeeuutWL58ecqc3NzcmDZtWlqvF9SvgJpYunRpDBo0KNatW5cyXq9evejdu3d07tw5GjVqFEVFRTFv3rxYu3ZthT3OP//8mDp1asrTdCujTwEAZI4gCQBJW7Zsia9+9avxxBNPpDW/bdu28eijj8a55567n28GHCrqIkgSEfHss8/GsGHDKnxgWJUrrrgiHnroocjLy6vFDYHDQW2DJBF6FJAZDzzwQHz729+Obdu2pTW/Xbt28fTTT8fAgQPTPkO/AmrinXfeiWuuuSZmz55drXVZWVlx4403xs9//vNo1KhRWmv0KQCAzPBqGwCSmjRpEpMnT44pU6ZE//79q5zXqlWruOmmm2LhwoVCJMBB4fzzz4+FCxfG1772tb0+Drl///7x1FNPxWOPPeaDRODfRo8CMuGmm26Kt99+O66++upo2rRplfPat28fY8aMiXfffbdaIZII/QqomR49esTMmTPj0UcfjQEDBuzzFwwaNWoUV111Vbz++usxceLEtEMkEfoUAECmeCIJAFX64IMPYu7cubFmzZrYunVrtG/fPvLz82PQoEGRm5tb19cDqJEdO3bEjBkzYvny5bF27drIy8uLDh06RJ8+faJTp051fT3gMKdHAZlQXFwcM2bMiFWrVsXatWsjNzc32rRpEyeeeGL07t07I2foV0BNbd68OWbPnh0ffPBBbNq0KUpKSqJp06bRsmXLOOGEE6JXr16RnZ1d63P0KQCAmhMkAQAAAAAAAAAgIrzaBgAAAAAAAACAMoIkAAAAAAAAAABEhCAJAAAAAAAAAABlBEkAAAAAAAAAAIgIQRIAAAAAAAAAAMoIkgAAAAAAAAAAEBGCJAAAAAAAAAAAlBEkAQAAAAAAAAAgIgRJAAAAAAAAAAAoI0gCAAAAAAAAAEBECJIAAAAAAAAAAFBGkAQAAAAAAAAAgIgQJAEAAAAAAAAAoIwgCQAAAAAAAAAAESFIAgAAAAAAAABAGUESAAAAAAAAAAAiQpAEAAAAAAAAAIAygiQAAAAAAAAAAESEIAkAAAAAAAAAAGUESQAAAAAAAAAAiAhBEgAAAAAAAAAAygiSAAAAAAAAAAAQEYIkAAAAAAAAAACUESQBAAAAAAAAACAiBEkAAAAADgqFhYWRlZWV/DNs2LC6vhIAAABwCBIkAQAAAKrUsWPHlPBCbf5MnTq1rr8dAAAAAPZBkAQAAAAAAAAAgIgQJAEAAAAAAAAAoEx2XV8AAAAAOHg8/vjj0b9//xqtbdu2bYZvAwAAAECmCZIAAAAAaWvfvn107Nixrq8BAAAAwH7i1TYAAAAAAAAAAESEIAkAAAAAAAAAAGW82gYAAAA4oBUWFsbcuXNj9erVUVxcHO3bt4/evXvHSSedlJH916xZE7NmzYqPPvooNm7cGM2bN482bdrEKaecEp06dcrIGRERH3/8ccyaNSvWrl0bGzZsiEQiES1atIguXbrEiSeeGG3btq31Ge+9917Mnz8/Vq1aFaWlpdGmTZs4+eSTo1evXhn4DgAAAIDDgSAJAAAAUKc6duwYy5cvj4iI/Pz8KCwsjIiI6dOnx7hx4+KVV16JRCJRYV2XLl3izjvvjGHDhlX7zN27d8fjjz8ed999d8yfP7/Ked27d49bb701RowYETk5OdU+Z+fOnTFp0qSYMGFCzJ8/v9LvY49evXrF5ZdfHjfccEO0b9++Wuf86U9/ih/+8Icxa9asSr/euXPnuOuuu+Kqq66q1r4AAADA4cerbQAAAIADzh133BHnnXdevPzyy1WGL95///0YPnx4nHfeebFt27a09/7www9jwIABcfXVV+81RBLxryd83HLLLdGrV69YsmRJtb6HN954I7p37x4jRoyIt956a68hkoiIBQsWxJ133hm//OUv0z5j165d8Y1vfCMuvPDCKkMkERHLli2Lq6++Om655ZZ93gMAAAA4vHkiCQAAAHBAGT9+fIwbNy5ZH3vssdGrV69o0qRJrF69Ot54443YuXNn8uvTp0+Pz3/+8/GXv/wlcnNz97r38uXL48wzz0w+AWWPpk2bxmc/+9lo27ZtFBUVxezZs+Pjjz9Ofv3dd9+NgQMHRkFBQZx44on7/B4mT54cw4YNi5KSkpTxBg0axMknnxzt27ePBg0aRFFRUSxatChWrly5zz0rM3LkyPjFL34RERFZWVnRu3fv6Ny5czRo0CCWL18ef//736O0tDQ5/xe/+EUcf/zxcdNNN9XoPAAAAODQJ0gCAAAAHDA2bNgQ//3f/x0REV27do0JEybE2WefnTKnqKgoxo4dG/fdd1/y6RovvfRS3HXXXfGDH/ygyr1LS0vjiiuuSAmRNGnSJH74wx/GiBEjomHDhilzJ0+eHLfddlts2LAhebfLLrss5syZE02aNKnynDfeeKNCiOTYY4+NsWPHxuWXXx6NGjWqsGbVqlXx+9//Ph588MG9/Z8nxZ///Ofk3b761a/G6NGj4+ijj06Zs3r16hgxYkQ8++yzybHvfve7ce2110ZeXl7aZwEAAACHj6yE55kCAAAAVejYsWNK8OLxxx+P/v37V3ufxo0bR9u2bdM6IyKiR48e8corr0SbNm2q3PO+++6LW2+9NVlnZ2fHwoUL4zOf+Uyl8++9994YOXJkss7Ly4vnn38+BgwYUOUZixcvjjPOOCMZ2IiI+Pa3vx133313pfN37NgR3bt3T/l+Bg0aFNOmTYtWrVpVec4eiUQi1q1bF+3atavwtcLCwujUqVOF8QkTJuz1CSOlpaXRv3//mDNnTnLs4YcfjhtuuGGf9wEAAAAOP4IkAAAAQJUqC3nUxEUXXRRTp05N64z69evHm2++GX379k1r32nTpiXrb3zjG3HvvfdWmLd79+7o2rVrfPDBB8mx++67L2655ZZ9nvH73/8+Lr300mTdrFmzWLVqVTRt2rTC3IceeihGjBiRrDt06BDz58+P1q1b7/OcfaksSHLllVfG7373u32u/fOf/xwXXHBBtdcBAAAAh596dX0BAAAAgE+7+OKL0wqRRESFV9n8+te/jt27d1eY98orr6SESI4++uj4+te/ntYZl1xySfTr1y9Zf/LJJ/HMM89UOveBBx6ocL9MhEiq8j//8z9pzTvnnHMiNzc3Wc+bN29/XQkAAAA4yAmSAAAAAAeUK6+8Mu25vXr1ihNOOCFZb968ORYuXFhh3muvvZZSX3HFFVGvXvofi1x77bV73S8ioqioKN56661k3bx587jiiivSPqO6OnfuHD169Ehrbk5OTnTp0iVZr1u3bn9dCwAAADjICZIAAAAAaXvxxRcjkUhU+09Vr7WpzKmnnlqtO5Wf//e//73CnNmzZ6fUAwcOrNYZ5edXdsbMmTPj028Q7t+/fzRo0KBa51THcccdV635LVu2TP735s2bM30dAAAA4BAhSAIAAAAcMBo3bhwdOnSo1ppu3bql1JU9baP8WPfu3at1Rvknf1R2xocffphSH3/88dU6o7o+HQxJR05OTvK/S0tLM30dAAAA4BAhSAIAAAAcMJo1a1btNc2bN0+pi4qKKszZuHHjXtfsS15eXmRnZ+/1jI8//jilrm7Qo7qq82oeAAAAgHT5xAEAAAA4YGRlZe2XPT79yplMnJPO+kx8LwAAAAD/boIkAAAAwAFj8+bNtV5T2ZNAWrVqVatztm7dmvI6mMrOOOKII1Lqyp5aAgAAAHCgEyQBAAAADhjbtm2L1atXV2vNkiVLUuq2bdtWmFN+7L333qvWGe++++4+zzjyyCNT6kWLFlXrDAAAAIADgSAJAAAAcECZNWtWtea/8cYbKfUpp5xSYU6/fv1S6tdff71aZ5SfX9kZAwYMiHr1/v9HLTNnzowdO3ZU6xwAAACAuiZIAgAAABxQHn/88bTnLliwIBYuXJismzdvHieccEKFeaeddlqFM3bv3p32Ob/5zW/2ul/Ev15307dv32S9efPmmDx5ctpnAAAAABwIBEkAAACAA8rUqVNj7ty5ac298847U+prrrkm5akge5xxxhnRqVOnZL1y5cqYOHFiWmc888wz8eabbybrZs2axcUXX1zp3JtvvrnC/TZu3JjWOQAAAAAHAkESAAAA4ICya9euuOqqq2LDhg17nXf//ffHtGnTknX9+vUrBDn2qFevXowcOTJlbNSoUSkBkcq8++678bWvfS1l7MYbb4xmzZpVOv/qq6+OLl26JOuVK1fGxRdfnHaYJJFIxEcffZTWXAAAAID9QZAEAAAASNvatWujsLCwRn/WrVu3z/3z8vIiJycn3nnnnRg4cGAUFBRUmFNUVBTf/OY349Zbb00ZHzVqVPTo0aPKvW+++eY49dRTk/U///nPOPvss2PChAlRUlKSMre0tDR+97vfxemnn55y765du8bo0aOrPCM7OzsmT54cDRs2TI698sor0bdv3/j1r38d27dvr3TdqlWr4t57741evXrFAw88UOX+AAAAAPtbViKRSNT1JQAAAIADU8eOHWP58uUZ2euiiy6KqVOn7vWM/Pz8+PrXvx6jRo1Kfj0/Pz969+4deXl5sXr16pg1a1bs3LkzZY8zzzwz/vrXv0Zubu5e7/DBBx/EmWeeGStXrkwZb9asWZx66qlxxBFHxMaNG2P27NkVnojSqlWrKCgoiD59+uzze50yZUpcc801FQIqDRs2jJNPPjnat28fubm5UVRUFIsXL44VK1Yk54wePTrGjBlTYc/CwsKU1/Ncd911MWnSpH3eZY+zzjorXn755WTtIyEAAACgMtl1fQEAAACAT/vOd74T69evj/Hjx0dExPLly/caZhk6dGg8/fTT+wyRRER06tQpZs2aFV/4whdizpw5yfFPPvkknn/++SrXdevWLf74xz/GZz7zmbS+hy9/+ctx9NFHx+WXX54SWtm+fXvMmDEjrT0AAAAA6oJX2wAAAAAHnLvvvjumTZsWgwYNqnJOly5d4pFHHonp06dH48aN0977qKOOijfffDMeffTR6N27917nduvWLe69995YuHBh2iGSPQYMGBBLliyJe++9N4477ri9zs3Kyoq+ffvGT37yk7jllluqdQ4AAABAJnm1DQAAAFCnyr/aprCwMOXrH3zwQcyZMyfWrFkTxcXF0b59++jdu3dar5hJx57X5Xz00UexadOmaNq0abRt2zZOOeWU6Ny5c0bOiIhYs2ZNzJo1K9atWxdFRUWRnZ0dLVq0iC5dusRJJ50UrVu3zthZAAAAADUlSAIAAADUqX0FSQAAAAD49/FqGwAAAAAAAAAAIkKQBAAAAAAAAACAMoIkAAAAAAAAAABEhCAJAAAAAAAAAABlBEkAAAAAAAAAAIgIQRIAAAAAAAAAAMoIkgAAAAAAAAAAEBERWYlEIlHXlwAAAAAAAAAAoO55IgkAAAAAAAAAABEhSAIAAAAAAAAAQBlBEgAAAAAAAAAAIkKQBAAAAAAAAACAMoIkAAAAAAAAAABEhCAJAAAAAAAAAABlBEkAAAAAAAAAAIgIQRIAAAAAAAAAAMoIkgAAAAAAAAAAEBGCJAAAAAAAAAAAlBEkAQAAAAAAAAAgIgRJAAAAAAAAAAAoI0gCAAAAAAAAAEBECJIAAAAAAAAAAFBGkAQAAAAAAAAAgIgQJAEAAAAAAAAAoIwgCQAAAAAAAAAAESFIAgAAAAAAAABAGUESAAAAAAAAAAAiQpAEAAAAAAAAAIAygiQAAAAAAAAAAESEIAkAAAAAAAAAAGUESQAAAAAAAAAAiAhBEgAAAAAAAAAAygiSAAAAAAAAAAAQEYIkAAAAAAAAAACUESQBAAAAAAAAACAiBEkAAAAAAAAAACjz/wAe7hv4Ql7R/AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1950x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.show_curves([curves], [modelo_final], 'AERNN')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data...\n",
      "\n",
      "Setup finishied. Starting training...\n",
      "(0.02 min) Epoch 1/300 -- Iteration 38 - Batch 38/3851 - Train loss: 1.12005163  - Train acc: 0.2997 - Val loss: 0.00000000\n",
      "(0.03 min) Epoch 1/300 -- Iteration 76 - Batch 76/3851 - Train loss: 1.10491521  - Train acc: 0.3315 - Val loss: 0.00000000\n",
      "(0.04 min) Epoch 1/300 -- Iteration 114 - Batch 114/3851 - Train loss: 1.08652822  - Train acc: 0.3740 - Val loss: 0.00000000\n",
      "(0.04 min) Epoch 1/300 -- Iteration 152 - Batch 152/3851 - Train loss: 1.06700349  - Train acc: 0.4144 - Val loss: 0.00000000\n",
      "(0.05 min) Epoch 1/300 -- Iteration 190 - Batch 190/3851 - Train loss: 1.04535439  - Train acc: 0.4472 - Val loss: 0.00000000\n",
      "(0.06 min) Epoch 1/300 -- Iteration 228 - Batch 228/3851 - Train loss: 1.01763238  - Train acc: 0.4810 - Val loss: 0.00000000\n",
      "(0.07 min) Epoch 1/300 -- Iteration 266 - Batch 266/3851 - Train loss: 0.99021266  - Train acc: 0.5075 - Val loss: 0.00000000\n",
      "(0.08 min) Epoch 1/300 -- Iteration 304 - Batch 304/3851 - Train loss: 0.96371917  - Train acc: 0.5303 - Val loss: 0.00000000\n",
      "(0.09 min) Epoch 1/300 -- Iteration 342 - Batch 342/3851 - Train loss: 0.93830386  - Train acc: 0.5521 - Val loss: 0.00000000\n",
      "(0.09 min) Epoch 1/300 -- Iteration 380 - Batch 380/3851 - Train loss: 0.91448102  - Train acc: 0.5722 - Val loss: 0.00000000\n",
      "(0.10 min) Epoch 1/300 -- Iteration 418 - Batch 418/3851 - Train loss: 0.89123300  - Train acc: 0.5902 - Val loss: 0.00000000\n",
      "(0.11 min) Epoch 1/300 -- Iteration 456 - Batch 456/3851 - Train loss: 0.86883036  - Train acc: 0.6065 - Val loss: 0.00000000\n",
      "(0.12 min) Epoch 1/300 -- Iteration 494 - Batch 494/3851 - Train loss: 0.84652679  - Train acc: 0.6215 - Val loss: 0.00000000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 532 - Batch 532/3851 - Train loss: 0.82496250  - Train acc: 0.6352 - Val loss: 0.00000000\n",
      "(0.13 min) Epoch 1/300 -- Iteration 570 - Batch 570/3851 - Train loss: 0.80515080  - Train acc: 0.6475 - Val loss: 0.00000000\n",
      "(0.14 min) Epoch 1/300 -- Iteration 608 - Batch 608/3851 - Train loss: 0.78692874  - Train acc: 0.6585 - Val loss: 0.00000000\n",
      "(0.15 min) Epoch 1/300 -- Iteration 646 - Batch 646/3851 - Train loss: 0.76976485  - Train acc: 0.6684 - Val loss: 0.00000000\n",
      "(0.16 min) Epoch 1/300 -- Iteration 684 - Batch 684/3851 - Train loss: 0.75245321  - Train acc: 0.6783 - Val loss: 0.00000000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 722 - Batch 722/3851 - Train loss: 0.73735872  - Train acc: 0.6862 - Val loss: 0.00000000\n",
      "(0.17 min) Epoch 1/300 -- Iteration 760 - Batch 760/3851 - Train loss: 0.72221066  - Train acc: 0.6942 - Val loss: 0.00000000\n",
      "(0.18 min) Epoch 1/300 -- Iteration 798 - Batch 798/3851 - Train loss: 0.70872603  - Train acc: 0.7009 - Val loss: 0.00000000\n",
      "(0.19 min) Epoch 1/300 -- Iteration 836 - Batch 836/3851 - Train loss: 0.69590434  - Train acc: 0.7079 - Val loss: 0.00000000\n",
      "(0.20 min) Epoch 1/300 -- Iteration 874 - Batch 874/3851 - Train loss: 0.68340972  - Train acc: 0.7143 - Val loss: 0.00000000\n",
      "(0.21 min) Epoch 1/300 -- Iteration 912 - Batch 912/3851 - Train loss: 0.67156579  - Train acc: 0.7200 - Val loss: 0.00000000\n",
      "(0.21 min) Epoch 1/300 -- Iteration 950 - Batch 950/3851 - Train loss: 0.66043900  - Train acc: 0.7256 - Val loss: 0.00000000\n",
      "(0.22 min) Epoch 1/300 -- Iteration 988 - Batch 988/3851 - Train loss: 0.64996328  - Train acc: 0.7306 - Val loss: 0.00000000\n",
      "(0.23 min) Epoch 1/300 -- Iteration 1026 - Batch 1026/3851 - Train loss: 0.63982048  - Train acc: 0.7355 - Val loss: 0.00000000\n",
      "(0.24 min) Epoch 1/300 -- Iteration 1064 - Batch 1064/3851 - Train loss: 0.63032129  - Train acc: 0.7400 - Val loss: 0.00000000\n",
      "(0.25 min) Epoch 1/300 -- Iteration 1102 - Batch 1102/3851 - Train loss: 0.62189923  - Train acc: 0.7440 - Val loss: 0.00000000\n",
      "(0.25 min) Epoch 1/300 -- Iteration 1140 - Batch 1140/3851 - Train loss: 0.61345624  - Train acc: 0.7480 - Val loss: 0.00000000\n",
      "(0.26 min) Epoch 1/300 -- Iteration 1178 - Batch 1178/3851 - Train loss: 0.60521287  - Train acc: 0.7517 - Val loss: 0.00000000\n",
      "(0.27 min) Epoch 1/300 -- Iteration 1216 - Batch 1216/3851 - Train loss: 0.59783037  - Train acc: 0.7551 - Val loss: 0.00000000\n",
      "(0.28 min) Epoch 1/300 -- Iteration 1254 - Batch 1254/3851 - Train loss: 0.59064720  - Train acc: 0.7583 - Val loss: 0.00000000\n",
      "(0.29 min) Epoch 1/300 -- Iteration 1292 - Batch 1292/3851 - Train loss: 0.58349368  - Train acc: 0.7614 - Val loss: 0.00000000\n",
      "(0.29 min) Epoch 1/300 -- Iteration 1330 - Batch 1330/3851 - Train loss: 0.57661189  - Train acc: 0.7648 - Val loss: 0.00000000\n",
      "(0.30 min) Epoch 1/300 -- Iteration 1368 - Batch 1368/3851 - Train loss: 0.57048226  - Train acc: 0.7675 - Val loss: 0.00000000\n",
      "(0.31 min) Epoch 1/300 -- Iteration 1406 - Batch 1406/3851 - Train loss: 0.56399678  - Train acc: 0.7704 - Val loss: 0.00000000\n",
      "(0.32 min) Epoch 1/300 -- Iteration 1444 - Batch 1444/3851 - Train loss: 0.55843848  - Train acc: 0.7729 - Val loss: 0.00000000\n",
      "(0.32 min) Epoch 1/300 -- Iteration 1482 - Batch 1482/3851 - Train loss: 0.55286709  - Train acc: 0.7754 - Val loss: 0.00000000\n",
      "(0.33 min) Epoch 1/300 -- Iteration 1520 - Batch 1520/3851 - Train loss: 0.54744538  - Train acc: 0.7778 - Val loss: 0.00000000\n",
      "(0.34 min) Epoch 1/300 -- Iteration 1558 - Batch 1558/3851 - Train loss: 0.54235832  - Train acc: 0.7801 - Val loss: 0.00000000\n",
      "(0.35 min) Epoch 1/300 -- Iteration 1596 - Batch 1596/3851 - Train loss: 0.53727556  - Train acc: 0.7823 - Val loss: 0.00000000\n",
      "(0.36 min) Epoch 1/300 -- Iteration 1634 - Batch 1634/3851 - Train loss: 0.53238431  - Train acc: 0.7844 - Val loss: 0.00000000\n",
      "(0.36 min) Epoch 1/300 -- Iteration 1672 - Batch 1672/3851 - Train loss: 0.52780119  - Train acc: 0.7864 - Val loss: 0.00000000\n",
      "(0.37 min) Epoch 1/300 -- Iteration 1710 - Batch 1710/3851 - Train loss: 0.52336726  - Train acc: 0.7883 - Val loss: 0.00000000\n",
      "(0.38 min) Epoch 1/300 -- Iteration 1748 - Batch 1748/3851 - Train loss: 0.51923916  - Train acc: 0.7902 - Val loss: 0.00000000\n",
      "(0.39 min) Epoch 1/300 -- Iteration 1786 - Batch 1786/3851 - Train loss: 0.51494426  - Train acc: 0.7920 - Val loss: 0.00000000\n",
      "(0.40 min) Epoch 1/300 -- Iteration 1824 - Batch 1824/3851 - Train loss: 0.51096931  - Train acc: 0.7938 - Val loss: 0.00000000\n",
      "(0.40 min) Epoch 1/300 -- Iteration 1862 - Batch 1862/3851 - Train loss: 0.50692935  - Train acc: 0.7957 - Val loss: 0.00000000\n",
      "(0.41 min) Epoch 1/300 -- Iteration 1900 - Batch 1900/3851 - Train loss: 0.50336782  - Train acc: 0.7972 - Val loss: 0.00000000\n",
      "(0.42 min) Epoch 1/300 -- Iteration 1938 - Batch 1938/3851 - Train loss: 0.49982388  - Train acc: 0.7987 - Val loss: 0.00000000\n",
      "(0.43 min) Epoch 1/300 -- Iteration 1976 - Batch 1976/3851 - Train loss: 0.49645872  - Train acc: 0.8002 - Val loss: 0.00000000\n",
      "(0.44 min) Epoch 1/300 -- Iteration 2014 - Batch 2014/3851 - Train loss: 0.49287817  - Train acc: 0.8018 - Val loss: 0.00000000\n",
      "(0.44 min) Epoch 1/300 -- Iteration 2052 - Batch 2052/3851 - Train loss: 0.48955772  - Train acc: 0.8032 - Val loss: 0.00000000\n",
      "(0.45 min) Epoch 1/300 -- Iteration 2090 - Batch 2090/3851 - Train loss: 0.48660716  - Train acc: 0.8045 - Val loss: 0.00000000\n",
      "(0.46 min) Epoch 1/300 -- Iteration 2128 - Batch 2128/3851 - Train loss: 0.48349515  - Train acc: 0.8059 - Val loss: 0.00000000\n",
      "(0.47 min) Epoch 1/300 -- Iteration 2166 - Batch 2166/3851 - Train loss: 0.48045837  - Train acc: 0.8072 - Val loss: 0.00000000\n",
      "(0.48 min) Epoch 1/300 -- Iteration 2204 - Batch 2204/3851 - Train loss: 0.47768252  - Train acc: 0.8084 - Val loss: 0.00000000\n",
      "(0.48 min) Epoch 1/300 -- Iteration 2242 - Batch 2242/3851 - Train loss: 0.47476451  - Train acc: 0.8096 - Val loss: 0.00000000\n",
      "(0.49 min) Epoch 1/300 -- Iteration 2280 - Batch 2280/3851 - Train loss: 0.47187552  - Train acc: 0.8108 - Val loss: 0.00000000\n",
      "(0.50 min) Epoch 1/300 -- Iteration 2318 - Batch 2318/3851 - Train loss: 0.46910999  - Train acc: 0.8120 - Val loss: 0.00000000\n",
      "(0.51 min) Epoch 1/300 -- Iteration 2356 - Batch 2356/3851 - Train loss: 0.46651614  - Train acc: 0.8132 - Val loss: 0.00000000\n",
      "(0.51 min) Epoch 1/300 -- Iteration 2394 - Batch 2394/3851 - Train loss: 0.46407812  - Train acc: 0.8142 - Val loss: 0.00000000\n",
      "(0.52 min) Epoch 1/300 -- Iteration 2432 - Batch 2432/3851 - Train loss: 0.46161772  - Train acc: 0.8153 - Val loss: 0.00000000\n",
      "(0.53 min) Epoch 1/300 -- Iteration 2470 - Batch 2470/3851 - Train loss: 0.45906197  - Train acc: 0.8164 - Val loss: 0.00000000\n",
      "(0.54 min) Epoch 1/300 -- Iteration 2508 - Batch 2508/3851 - Train loss: 0.45637616  - Train acc: 0.8176 - Val loss: 0.00000000\n",
      "(0.55 min) Epoch 1/300 -- Iteration 2546 - Batch 2546/3851 - Train loss: 0.45419942  - Train acc: 0.8185 - Val loss: 0.00000000\n",
      "(0.55 min) Epoch 1/300 -- Iteration 2584 - Batch 2584/3851 - Train loss: 0.45174777  - Train acc: 0.8196 - Val loss: 0.00000000\n",
      "(0.56 min) Epoch 1/300 -- Iteration 2622 - Batch 2622/3851 - Train loss: 0.44947975  - Train acc: 0.8205 - Val loss: 0.00000000\n",
      "(0.57 min) Epoch 1/300 -- Iteration 2660 - Batch 2660/3851 - Train loss: 0.44756843  - Train acc: 0.8213 - Val loss: 0.00000000\n",
      "(0.58 min) Epoch 1/300 -- Iteration 2698 - Batch 2698/3851 - Train loss: 0.44553144  - Train acc: 0.8222 - Val loss: 0.00000000\n",
      "(0.59 min) Epoch 1/300 -- Iteration 2736 - Batch 2736/3851 - Train loss: 0.44350138  - Train acc: 0.8230 - Val loss: 0.00000000\n",
      "(0.59 min) Epoch 1/300 -- Iteration 2774 - Batch 2774/3851 - Train loss: 0.44137622  - Train acc: 0.8239 - Val loss: 0.00000000\n",
      "(0.60 min) Epoch 1/300 -- Iteration 2812 - Batch 2812/3851 - Train loss: 0.43929448  - Train acc: 0.8248 - Val loss: 0.00000000\n",
      "(0.61 min) Epoch 1/300 -- Iteration 2850 - Batch 2850/3851 - Train loss: 0.43745114  - Train acc: 0.8256 - Val loss: 0.00000000\n",
      "(0.62 min) Epoch 1/300 -- Iteration 2888 - Batch 2888/3851 - Train loss: 0.43585164  - Train acc: 0.8263 - Val loss: 0.00000000\n",
      "(0.62 min) Epoch 1/300 -- Iteration 2926 - Batch 2926/3851 - Train loss: 0.43404647  - Train acc: 0.8271 - Val loss: 0.00000000\n",
      "(0.63 min) Epoch 1/300 -- Iteration 2964 - Batch 2964/3851 - Train loss: 0.43225145  - Train acc: 0.8278 - Val loss: 0.00000000\n",
      "(0.64 min) Epoch 1/300 -- Iteration 3002 - Batch 3002/3851 - Train loss: 0.43028125  - Train acc: 0.8287 - Val loss: 0.00000000\n",
      "(0.65 min) Epoch 1/300 -- Iteration 3040 - Batch 3040/3851 - Train loss: 0.42851523  - Train acc: 0.8294 - Val loss: 0.00000000\n",
      "(0.66 min) Epoch 1/300 -- Iteration 3078 - Batch 3078/3851 - Train loss: 0.42663128  - Train acc: 0.8302 - Val loss: 0.00000000\n",
      "(0.66 min) Epoch 1/300 -- Iteration 3116 - Batch 3116/3851 - Train loss: 0.42482591  - Train acc: 0.8310 - Val loss: 0.00000000\n",
      "(0.67 min) Epoch 1/300 -- Iteration 3154 - Batch 3154/3851 - Train loss: 0.42322216  - Train acc: 0.8317 - Val loss: 0.00000000\n",
      "(0.68 min) Epoch 1/300 -- Iteration 3192 - Batch 3192/3851 - Train loss: 0.42164409  - Train acc: 0.8324 - Val loss: 0.00000000\n",
      "(0.69 min) Epoch 1/300 -- Iteration 3230 - Batch 3230/3851 - Train loss: 0.42000676  - Train acc: 0.8331 - Val loss: 0.00000000\n",
      "(0.70 min) Epoch 1/300 -- Iteration 3268 - Batch 3268/3851 - Train loss: 0.41868482  - Train acc: 0.8336 - Val loss: 0.00000000\n",
      "(0.70 min) Epoch 1/300 -- Iteration 3306 - Batch 3306/3851 - Train loss: 0.41710542  - Train acc: 0.8343 - Val loss: 0.00000000\n",
      "(0.71 min) Epoch 1/300 -- Iteration 3344 - Batch 3344/3851 - Train loss: 0.41552232  - Train acc: 0.8350 - Val loss: 0.00000000\n",
      "(0.72 min) Epoch 1/300 -- Iteration 3382 - Batch 3382/3851 - Train loss: 0.41405173  - Train acc: 0.8356 - Val loss: 0.00000000\n",
      "(0.73 min) Epoch 1/300 -- Iteration 3420 - Batch 3420/3851 - Train loss: 0.41280618  - Train acc: 0.8361 - Val loss: 0.00000000\n",
      "(0.73 min) Epoch 1/300 -- Iteration 3458 - Batch 3458/3851 - Train loss: 0.41116445  - Train acc: 0.8368 - Val loss: 0.00000000\n",
      "(0.74 min) Epoch 1/300 -- Iteration 3496 - Batch 3496/3851 - Train loss: 0.40979829  - Train acc: 0.8374 - Val loss: 0.00000000\n",
      "(0.75 min) Epoch 1/300 -- Iteration 3534 - Batch 3534/3851 - Train loss: 0.40829335  - Train acc: 0.8380 - Val loss: 0.00000000\n",
      "(0.76 min) Epoch 1/300 -- Iteration 3572 - Batch 3572/3851 - Train loss: 0.40688213  - Train acc: 0.8386 - Val loss: 0.00000000\n",
      "(0.77 min) Epoch 1/300 -- Iteration 3610 - Batch 3610/3851 - Train loss: 0.40538908  - Train acc: 0.8392 - Val loss: 0.00000000\n",
      "(0.77 min) Epoch 1/300 -- Iteration 3648 - Batch 3648/3851 - Train loss: 0.40411970  - Train acc: 0.8398 - Val loss: 0.00000000\n",
      "(0.78 min) Epoch 1/300 -- Iteration 3686 - Batch 3686/3851 - Train loss: 0.40280604  - Train acc: 0.8404 - Val loss: 0.00000000\n",
      "(0.79 min) Epoch 1/300 -- Iteration 3724 - Batch 3724/3851 - Train loss: 0.40152664  - Train acc: 0.8409 - Val loss: 0.00000000\n",
      "(0.80 min) Epoch 1/300 -- Iteration 3762 - Batch 3762/3851 - Train loss: 0.40016095  - Train acc: 0.8415 - Val loss: 0.00000000\n",
      "(0.81 min) Epoch 1/300 -- Iteration 3800 - Batch 3800/3851 - Train loss: 0.39883175  - Train acc: 0.8421 - Val loss: 0.00000000\n",
      "(0.81 min) Epoch 1/300 -- Iteration 3838 - Batch 3838/3851 - Train loss: 0.39775126  - Train acc: 0.8426 - Val loss: 0.00000000\n",
      "(0.82 min) Epoch 1/300 -- Iteration 3851 - Batch 3850/3851 - Train loss: 0.39734201  - Train acc: 0.8428 - Val loss: 0.52196938 - Val acc: 0.7950\n",
      "(0.83 min) Epoch 2/300 -- Iteration 3889 - Batch 38/3851 - Train loss: 0.26291474  - Train acc: 0.9000 - Val loss: 0.52196938\n",
      "(0.83 min) Epoch 2/300 -- Iteration 3927 - Batch 76/3851 - Train loss: 0.26684225  - Train acc: 0.9005 - Val loss: 0.52196938\n",
      "(0.84 min) Epoch 2/300 -- Iteration 3965 - Batch 114/3851 - Train loss: 0.27761426  - Train acc: 0.8955 - Val loss: 0.52196938\n",
      "(0.85 min) Epoch 2/300 -- Iteration 4003 - Batch 152/3851 - Train loss: 0.27866011  - Train acc: 0.8942 - Val loss: 0.52196938\n",
      "(0.86 min) Epoch 2/300 -- Iteration 4041 - Batch 190/3851 - Train loss: 0.27665625  - Train acc: 0.8952 - Val loss: 0.52196938\n",
      "(0.87 min) Epoch 2/300 -- Iteration 4079 - Batch 228/3851 - Train loss: 0.27588703  - Train acc: 0.8953 - Val loss: 0.52196938\n",
      "(0.87 min) Epoch 2/300 -- Iteration 4117 - Batch 266/3851 - Train loss: 0.27607171  - Train acc: 0.8948 - Val loss: 0.52196938\n",
      "(0.88 min) Epoch 2/300 -- Iteration 4155 - Batch 304/3851 - Train loss: 0.27639281  - Train acc: 0.8948 - Val loss: 0.52196938\n",
      "(0.89 min) Epoch 2/300 -- Iteration 4193 - Batch 342/3851 - Train loss: 0.27660652  - Train acc: 0.8944 - Val loss: 0.52196938\n",
      "(0.90 min) Epoch 2/300 -- Iteration 4231 - Batch 380/3851 - Train loss: 0.27574199  - Train acc: 0.8950 - Val loss: 0.52196938\n",
      "(0.90 min) Epoch 2/300 -- Iteration 4269 - Batch 418/3851 - Train loss: 0.27526276  - Train acc: 0.8955 - Val loss: 0.52196938\n",
      "(0.91 min) Epoch 2/300 -- Iteration 4307 - Batch 456/3851 - Train loss: 0.27517931  - Train acc: 0.8956 - Val loss: 0.52196938\n",
      "(0.92 min) Epoch 2/300 -- Iteration 4345 - Batch 494/3851 - Train loss: 0.27432617  - Train acc: 0.8958 - Val loss: 0.52196938\n",
      "(0.93 min) Epoch 2/300 -- Iteration 4383 - Batch 532/3851 - Train loss: 0.27302510  - Train acc: 0.8965 - Val loss: 0.52196938\n",
      "(0.94 min) Epoch 2/300 -- Iteration 4421 - Batch 570/3851 - Train loss: 0.27283656  - Train acc: 0.8967 - Val loss: 0.52196938\n",
      "(0.94 min) Epoch 2/300 -- Iteration 4459 - Batch 608/3851 - Train loss: 0.27275838  - Train acc: 0.8969 - Val loss: 0.52196938\n",
      "(0.95 min) Epoch 2/300 -- Iteration 4497 - Batch 646/3851 - Train loss: 0.27252981  - Train acc: 0.8968 - Val loss: 0.52196938\n",
      "(0.96 min) Epoch 2/300 -- Iteration 4535 - Batch 684/3851 - Train loss: 0.27277800  - Train acc: 0.8967 - Val loss: 0.52196938\n",
      "(0.97 min) Epoch 2/300 -- Iteration 4573 - Batch 722/3851 - Train loss: 0.27170459  - Train acc: 0.8972 - Val loss: 0.52196938\n",
      "(0.98 min) Epoch 2/300 -- Iteration 4611 - Batch 760/3851 - Train loss: 0.27128377  - Train acc: 0.8975 - Val loss: 0.52196938\n",
      "(0.98 min) Epoch 2/300 -- Iteration 4649 - Batch 798/3851 - Train loss: 0.27037891  - Train acc: 0.8979 - Val loss: 0.52196938\n",
      "(0.99 min) Epoch 2/300 -- Iteration 4687 - Batch 836/3851 - Train loss: 0.27013641  - Train acc: 0.8979 - Val loss: 0.52196938\n",
      "(1.00 min) Epoch 2/300 -- Iteration 4725 - Batch 874/3851 - Train loss: 0.26948164  - Train acc: 0.8982 - Val loss: 0.52196938\n",
      "(1.01 min) Epoch 2/300 -- Iteration 4763 - Batch 912/3851 - Train loss: 0.26881558  - Train acc: 0.8985 - Val loss: 0.52196938\n",
      "(1.01 min) Epoch 2/300 -- Iteration 4801 - Batch 950/3851 - Train loss: 0.26935923  - Train acc: 0.8982 - Val loss: 0.52196938\n",
      "(1.02 min) Epoch 2/300 -- Iteration 4839 - Batch 988/3851 - Train loss: 0.26932560  - Train acc: 0.8981 - Val loss: 0.52196938\n",
      "(1.03 min) Epoch 2/300 -- Iteration 4877 - Batch 1026/3851 - Train loss: 0.26931126  - Train acc: 0.8979 - Val loss: 0.52196938\n",
      "(1.04 min) Epoch 2/300 -- Iteration 4915 - Batch 1064/3851 - Train loss: 0.26905173  - Train acc: 0.8980 - Val loss: 0.52196938\n",
      "(1.04 min) Epoch 2/300 -- Iteration 4953 - Batch 1102/3851 - Train loss: 0.26900924  - Train acc: 0.8979 - Val loss: 0.52196938\n",
      "(1.05 min) Epoch 2/300 -- Iteration 4991 - Batch 1140/3851 - Train loss: 0.26845241  - Train acc: 0.8980 - Val loss: 0.52196938\n",
      "(1.06 min) Epoch 2/300 -- Iteration 5029 - Batch 1178/3851 - Train loss: 0.26815572  - Train acc: 0.8981 - Val loss: 0.52196938\n",
      "(1.07 min) Epoch 2/300 -- Iteration 5067 - Batch 1216/3851 - Train loss: 0.26743528  - Train acc: 0.8985 - Val loss: 0.52196938\n",
      "(1.08 min) Epoch 2/300 -- Iteration 5105 - Batch 1254/3851 - Train loss: 0.26729823  - Train acc: 0.8986 - Val loss: 0.52196938\n",
      "(1.08 min) Epoch 2/300 -- Iteration 5143 - Batch 1292/3851 - Train loss: 0.26698221  - Train acc: 0.8987 - Val loss: 0.52196938\n",
      "(1.09 min) Epoch 2/300 -- Iteration 5181 - Batch 1330/3851 - Train loss: 0.26696743  - Train acc: 0.8987 - Val loss: 0.52196938\n",
      "(1.10 min) Epoch 2/300 -- Iteration 5219 - Batch 1368/3851 - Train loss: 0.26705505  - Train acc: 0.8987 - Val loss: 0.52196938\n",
      "(1.11 min) Epoch 2/300 -- Iteration 5257 - Batch 1406/3851 - Train loss: 0.26633570  - Train acc: 0.8991 - Val loss: 0.52196938\n",
      "(1.11 min) Epoch 2/300 -- Iteration 5295 - Batch 1444/3851 - Train loss: 0.26658975  - Train acc: 0.8989 - Val loss: 0.52196938\n",
      "(1.12 min) Epoch 2/300 -- Iteration 5333 - Batch 1482/3851 - Train loss: 0.26625261  - Train acc: 0.8990 - Val loss: 0.52196938\n",
      "(1.13 min) Epoch 2/300 -- Iteration 5371 - Batch 1520/3851 - Train loss: 0.26542421  - Train acc: 0.8993 - Val loss: 0.52196938\n",
      "(1.14 min) Epoch 2/300 -- Iteration 5409 - Batch 1558/3851 - Train loss: 0.26523649  - Train acc: 0.8994 - Val loss: 0.52196938\n",
      "(1.15 min) Epoch 2/300 -- Iteration 5447 - Batch 1596/3851 - Train loss: 0.26513178  - Train acc: 0.8995 - Val loss: 0.52196938\n",
      "(1.15 min) Epoch 2/300 -- Iteration 5485 - Batch 1634/3851 - Train loss: 0.26499924  - Train acc: 0.8995 - Val loss: 0.52196938\n",
      "(1.16 min) Epoch 2/300 -- Iteration 5523 - Batch 1672/3851 - Train loss: 0.26467933  - Train acc: 0.8996 - Val loss: 0.52196938\n",
      "(1.17 min) Epoch 2/300 -- Iteration 5561 - Batch 1710/3851 - Train loss: 0.26448858  - Train acc: 0.8996 - Val loss: 0.52196938\n",
      "(1.18 min) Epoch 2/300 -- Iteration 5599 - Batch 1748/3851 - Train loss: 0.26425515  - Train acc: 0.8997 - Val loss: 0.52196938\n",
      "(1.18 min) Epoch 2/300 -- Iteration 5637 - Batch 1786/3851 - Train loss: 0.26399243  - Train acc: 0.8997 - Val loss: 0.52196938\n",
      "(1.19 min) Epoch 2/300 -- Iteration 5675 - Batch 1824/3851 - Train loss: 0.26399130  - Train acc: 0.8998 - Val loss: 0.52196938\n",
      "(1.20 min) Epoch 2/300 -- Iteration 5713 - Batch 1862/3851 - Train loss: 0.26380148  - Train acc: 0.8998 - Val loss: 0.52196938\n",
      "(1.21 min) Epoch 2/300 -- Iteration 5751 - Batch 1900/3851 - Train loss: 0.26400213  - Train acc: 0.8998 - Val loss: 0.52196938\n",
      "(1.22 min) Epoch 2/300 -- Iteration 5789 - Batch 1938/3851 - Train loss: 0.26359567  - Train acc: 0.9000 - Val loss: 0.52196938\n",
      "(1.22 min) Epoch 2/300 -- Iteration 5827 - Batch 1976/3851 - Train loss: 0.26351841  - Train acc: 0.8999 - Val loss: 0.52196938\n",
      "(1.23 min) Epoch 2/300 -- Iteration 5865 - Batch 2014/3851 - Train loss: 0.26350444  - Train acc: 0.9000 - Val loss: 0.52196938\n",
      "(1.24 min) Epoch 2/300 -- Iteration 5903 - Batch 2052/3851 - Train loss: 0.26340513  - Train acc: 0.9000 - Val loss: 0.52196938\n",
      "(1.25 min) Epoch 2/300 -- Iteration 5941 - Batch 2090/3851 - Train loss: 0.26334497  - Train acc: 0.9000 - Val loss: 0.52196938\n",
      "(1.25 min) Epoch 2/300 -- Iteration 5979 - Batch 2128/3851 - Train loss: 0.26314684  - Train acc: 0.9001 - Val loss: 0.52196938\n",
      "(1.26 min) Epoch 2/300 -- Iteration 6017 - Batch 2166/3851 - Train loss: 0.26296109  - Train acc: 0.9002 - Val loss: 0.52196938\n",
      "(1.27 min) Epoch 2/300 -- Iteration 6055 - Batch 2204/3851 - Train loss: 0.26273059  - Train acc: 0.9004 - Val loss: 0.52196938\n",
      "(1.28 min) Epoch 2/300 -- Iteration 6093 - Batch 2242/3851 - Train loss: 0.26270236  - Train acc: 0.9004 - Val loss: 0.52196938\n",
      "(1.29 min) Epoch 2/300 -- Iteration 6131 - Batch 2280/3851 - Train loss: 0.26247928  - Train acc: 0.9005 - Val loss: 0.52196938\n",
      "(1.29 min) Epoch 2/300 -- Iteration 6169 - Batch 2318/3851 - Train loss: 0.26241382  - Train acc: 0.9004 - Val loss: 0.52196938\n",
      "(1.30 min) Epoch 2/300 -- Iteration 6207 - Batch 2356/3851 - Train loss: 0.26224216  - Train acc: 0.9005 - Val loss: 0.52196938\n",
      "(1.31 min) Epoch 2/300 -- Iteration 6245 - Batch 2394/3851 - Train loss: 0.26223969  - Train acc: 0.9005 - Val loss: 0.52196938\n",
      "(1.32 min) Epoch 2/300 -- Iteration 6283 - Batch 2432/3851 - Train loss: 0.26211111  - Train acc: 0.9006 - Val loss: 0.52196938\n",
      "(1.32 min) Epoch 2/300 -- Iteration 6321 - Batch 2470/3851 - Train loss: 0.26212284  - Train acc: 0.9006 - Val loss: 0.52196938\n",
      "(1.33 min) Epoch 2/300 -- Iteration 6359 - Batch 2508/3851 - Train loss: 0.26176633  - Train acc: 0.9008 - Val loss: 0.52196938\n",
      "(1.34 min) Epoch 2/300 -- Iteration 6397 - Batch 2546/3851 - Train loss: 0.26178547  - Train acc: 0.9008 - Val loss: 0.52196938\n",
      "(1.35 min) Epoch 2/300 -- Iteration 6435 - Batch 2584/3851 - Train loss: 0.26167092  - Train acc: 0.9008 - Val loss: 0.52196938\n",
      "(1.35 min) Epoch 2/300 -- Iteration 6473 - Batch 2622/3851 - Train loss: 0.26151893  - Train acc: 0.9009 - Val loss: 0.52196938\n",
      "(1.36 min) Epoch 2/300 -- Iteration 6511 - Batch 2660/3851 - Train loss: 0.26142872  - Train acc: 0.9010 - Val loss: 0.52196938\n",
      "(1.37 min) Epoch 2/300 -- Iteration 6549 - Batch 2698/3851 - Train loss: 0.26134870  - Train acc: 0.9010 - Val loss: 0.52196938\n",
      "(1.38 min) Epoch 2/300 -- Iteration 6587 - Batch 2736/3851 - Train loss: 0.26092758  - Train acc: 0.9012 - Val loss: 0.52196938\n",
      "(1.39 min) Epoch 2/300 -- Iteration 6625 - Batch 2774/3851 - Train loss: 0.26106125  - Train acc: 0.9011 - Val loss: 0.52196938\n",
      "(1.39 min) Epoch 2/300 -- Iteration 6663 - Batch 2812/3851 - Train loss: 0.26092710  - Train acc: 0.9011 - Val loss: 0.52196938\n",
      "(1.40 min) Epoch 2/300 -- Iteration 6701 - Batch 2850/3851 - Train loss: 0.26069357  - Train acc: 0.9012 - Val loss: 0.52196938\n",
      "(1.41 min) Epoch 2/300 -- Iteration 6739 - Batch 2888/3851 - Train loss: 0.26065056  - Train acc: 0.9012 - Val loss: 0.52196938\n",
      "(1.42 min) Epoch 2/300 -- Iteration 6777 - Batch 2926/3851 - Train loss: 0.26050489  - Train acc: 0.9013 - Val loss: 0.52196938\n",
      "(1.42 min) Epoch 2/300 -- Iteration 6815 - Batch 2964/3851 - Train loss: 0.26034199  - Train acc: 0.9014 - Val loss: 0.52196938\n",
      "(1.43 min) Epoch 2/300 -- Iteration 6853 - Batch 3002/3851 - Train loss: 0.26022106  - Train acc: 0.9015 - Val loss: 0.52196938\n",
      "(1.44 min) Epoch 2/300 -- Iteration 6891 - Batch 3040/3851 - Train loss: 0.26021533  - Train acc: 0.9014 - Val loss: 0.52196938\n",
      "(1.45 min) Epoch 2/300 -- Iteration 6929 - Batch 3078/3851 - Train loss: 0.26004990  - Train acc: 0.9014 - Val loss: 0.52196938\n",
      "(1.46 min) Epoch 2/300 -- Iteration 6967 - Batch 3116/3851 - Train loss: 0.25992429  - Train acc: 0.9015 - Val loss: 0.52196938\n",
      "(1.46 min) Epoch 2/300 -- Iteration 7005 - Batch 3154/3851 - Train loss: 0.25963336  - Train acc: 0.9016 - Val loss: 0.52196938\n",
      "(1.47 min) Epoch 2/300 -- Iteration 7043 - Batch 3192/3851 - Train loss: 0.25939960  - Train acc: 0.9017 - Val loss: 0.52196938\n",
      "(1.48 min) Epoch 2/300 -- Iteration 7081 - Batch 3230/3851 - Train loss: 0.25918436  - Train acc: 0.9018 - Val loss: 0.52196938\n",
      "(1.49 min) Epoch 2/300 -- Iteration 7119 - Batch 3268/3851 - Train loss: 0.25901621  - Train acc: 0.9018 - Val loss: 0.52196938\n",
      "(1.49 min) Epoch 2/300 -- Iteration 7157 - Batch 3306/3851 - Train loss: 0.25877041  - Train acc: 0.9019 - Val loss: 0.52196938\n",
      "(1.50 min) Epoch 2/300 -- Iteration 7195 - Batch 3344/3851 - Train loss: 0.25862097  - Train acc: 0.9020 - Val loss: 0.52196938\n",
      "(1.51 min) Epoch 2/300 -- Iteration 7233 - Batch 3382/3851 - Train loss: 0.25833362  - Train acc: 0.9021 - Val loss: 0.52196938\n",
      "(1.52 min) Epoch 2/300 -- Iteration 7271 - Batch 3420/3851 - Train loss: 0.25822700  - Train acc: 0.9022 - Val loss: 0.52196938\n",
      "(1.53 min) Epoch 2/300 -- Iteration 7309 - Batch 3458/3851 - Train loss: 0.25807527  - Train acc: 0.9023 - Val loss: 0.52196938\n",
      "(1.53 min) Epoch 2/300 -- Iteration 7347 - Batch 3496/3851 - Train loss: 0.25798225  - Train acc: 0.9024 - Val loss: 0.52196938\n",
      "(1.54 min) Epoch 2/300 -- Iteration 7385 - Batch 3534/3851 - Train loss: 0.25786417  - Train acc: 0.9024 - Val loss: 0.52196938\n",
      "(1.55 min) Epoch 2/300 -- Iteration 7423 - Batch 3572/3851 - Train loss: 0.25761287  - Train acc: 0.9025 - Val loss: 0.52196938\n",
      "(1.56 min) Epoch 2/300 -- Iteration 7461 - Batch 3610/3851 - Train loss: 0.25755065  - Train acc: 0.9025 - Val loss: 0.52196938\n",
      "(1.56 min) Epoch 2/300 -- Iteration 7499 - Batch 3648/3851 - Train loss: 0.25730865  - Train acc: 0.9026 - Val loss: 0.52196938\n",
      "(1.57 min) Epoch 2/300 -- Iteration 7537 - Batch 3686/3851 - Train loss: 0.25726545  - Train acc: 0.9026 - Val loss: 0.52196938\n",
      "(1.58 min) Epoch 2/300 -- Iteration 7575 - Batch 3724/3851 - Train loss: 0.25707428  - Train acc: 0.9027 - Val loss: 0.52196938\n",
      "(1.59 min) Epoch 2/300 -- Iteration 7613 - Batch 3762/3851 - Train loss: 0.25690177  - Train acc: 0.9028 - Val loss: 0.52196938\n",
      "(1.59 min) Epoch 2/300 -- Iteration 7651 - Batch 3800/3851 - Train loss: 0.25669171  - Train acc: 0.9029 - Val loss: 0.52196938\n",
      "(1.60 min) Epoch 2/300 -- Iteration 7689 - Batch 3838/3851 - Train loss: 0.25648240  - Train acc: 0.9029 - Val loss: 0.52196938\n",
      "(1.61 min) Epoch 2/300 -- Iteration 7702 - Batch 3850/3851 - Train loss: 0.25644946  - Train acc: 0.9029 - Val loss: 0.59709674 - Val acc: 0.7700\n",
      "(1.61 min) Epoch 3/300 -- Iteration 7740 - Batch 38/3851 - Train loss: 0.24516746  - Train acc: 0.9093 - Val loss: 0.59709674\n",
      "(1.62 min) Epoch 3/300 -- Iteration 7778 - Batch 76/3851 - Train loss: 0.24114336  - Train acc: 0.9105 - Val loss: 0.59709674\n",
      "(1.63 min) Epoch 3/300 -- Iteration 7816 - Batch 114/3851 - Train loss: 0.23816474  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.64 min) Epoch 3/300 -- Iteration 7854 - Batch 152/3851 - Train loss: 0.24002062  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(1.65 min) Epoch 3/300 -- Iteration 7892 - Batch 190/3851 - Train loss: 0.24261386  - Train acc: 0.9089 - Val loss: 0.59709674\n",
      "(1.65 min) Epoch 3/300 -- Iteration 7930 - Batch 228/3851 - Train loss: 0.24100245  - Train acc: 0.9097 - Val loss: 0.59709674\n",
      "(1.66 min) Epoch 3/300 -- Iteration 7968 - Batch 266/3851 - Train loss: 0.23941320  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(1.67 min) Epoch 3/300 -- Iteration 8006 - Batch 304/3851 - Train loss: 0.23941179  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.68 min) Epoch 3/300 -- Iteration 8044 - Batch 342/3851 - Train loss: 0.23842997  - Train acc: 0.9108 - Val loss: 0.59709674\n",
      "(1.68 min) Epoch 3/300 -- Iteration 8082 - Batch 380/3851 - Train loss: 0.23792811  - Train acc: 0.9111 - Val loss: 0.59709674\n",
      "(1.69 min) Epoch 3/300 -- Iteration 8120 - Batch 418/3851 - Train loss: 0.23827572  - Train acc: 0.9106 - Val loss: 0.59709674\n",
      "(1.70 min) Epoch 3/300 -- Iteration 8158 - Batch 456/3851 - Train loss: 0.23923193  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(1.71 min) Epoch 3/300 -- Iteration 8196 - Batch 494/3851 - Train loss: 0.23993188  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.71 min) Epoch 3/300 -- Iteration 8234 - Batch 532/3851 - Train loss: 0.24077431  - Train acc: 0.9097 - Val loss: 0.59709674\n",
      "(1.72 min) Epoch 3/300 -- Iteration 8272 - Batch 570/3851 - Train loss: 0.23955016  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(1.73 min) Epoch 3/300 -- Iteration 8310 - Batch 608/3851 - Train loss: 0.23954192  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(1.74 min) Epoch 3/300 -- Iteration 8348 - Batch 646/3851 - Train loss: 0.23904199  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(1.75 min) Epoch 3/300 -- Iteration 8386 - Batch 684/3851 - Train loss: 0.23850468  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(1.75 min) Epoch 3/300 -- Iteration 8424 - Batch 722/3851 - Train loss: 0.23869703  - Train acc: 0.9105 - Val loss: 0.59709674\n",
      "(1.76 min) Epoch 3/300 -- Iteration 8462 - Batch 760/3851 - Train loss: 0.23834911  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(1.77 min) Epoch 3/300 -- Iteration 8500 - Batch 798/3851 - Train loss: 0.23835033  - Train acc: 0.9106 - Val loss: 0.59709674\n",
      "(1.78 min) Epoch 3/300 -- Iteration 8538 - Batch 836/3851 - Train loss: 0.23876355  - Train acc: 0.9104 - Val loss: 0.59709674\n",
      "(1.78 min) Epoch 3/300 -- Iteration 8576 - Batch 874/3851 - Train loss: 0.23882703  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(1.79 min) Epoch 3/300 -- Iteration 8614 - Batch 912/3851 - Train loss: 0.23899236  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.80 min) Epoch 3/300 -- Iteration 8652 - Batch 950/3851 - Train loss: 0.23900778  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.81 min) Epoch 3/300 -- Iteration 8690 - Batch 988/3851 - Train loss: 0.23905110  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(1.81 min) Epoch 3/300 -- Iteration 8728 - Batch 1026/3851 - Train loss: 0.23873645  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.82 min) Epoch 3/300 -- Iteration 8766 - Batch 1064/3851 - Train loss: 0.23884285  - Train acc: 0.9099 - Val loss: 0.59709674\n",
      "(1.83 min) Epoch 3/300 -- Iteration 8804 - Batch 1102/3851 - Train loss: 0.23883069  - Train acc: 0.9099 - Val loss: 0.59709674\n",
      "(1.84 min) Epoch 3/300 -- Iteration 8842 - Batch 1140/3851 - Train loss: 0.23925447  - Train acc: 0.9097 - Val loss: 0.59709674\n",
      "(1.85 min) Epoch 3/300 -- Iteration 8880 - Batch 1178/3851 - Train loss: 0.23851377  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.85 min) Epoch 3/300 -- Iteration 8918 - Batch 1216/3851 - Train loss: 0.23857711  - Train acc: 0.9100 - Val loss: 0.59709674\n",
      "(1.86 min) Epoch 3/300 -- Iteration 8956 - Batch 1254/3851 - Train loss: 0.23848507  - Train acc: 0.9099 - Val loss: 0.59709674\n",
      "(1.87 min) Epoch 3/300 -- Iteration 8994 - Batch 1292/3851 - Train loss: 0.23820791  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.88 min) Epoch 3/300 -- Iteration 9032 - Batch 1330/3851 - Train loss: 0.23828226  - Train acc: 0.9100 - Val loss: 0.59709674\n",
      "(1.88 min) Epoch 3/300 -- Iteration 9070 - Batch 1368/3851 - Train loss: 0.23777413  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(1.89 min) Epoch 3/300 -- Iteration 9108 - Batch 1406/3851 - Train loss: 0.23789542  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(1.90 min) Epoch 3/300 -- Iteration 9146 - Batch 1444/3851 - Train loss: 0.23765784  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(1.91 min) Epoch 3/300 -- Iteration 9184 - Batch 1482/3851 - Train loss: 0.23772734  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.91 min) Epoch 3/300 -- Iteration 9222 - Batch 1520/3851 - Train loss: 0.23781050  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.92 min) Epoch 3/300 -- Iteration 9260 - Batch 1558/3851 - Train loss: 0.23790849  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.93 min) Epoch 3/300 -- Iteration 9298 - Batch 1596/3851 - Train loss: 0.23776938  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(1.94 min) Epoch 3/300 -- Iteration 9336 - Batch 1634/3851 - Train loss: 0.23752801  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(1.95 min) Epoch 3/300 -- Iteration 9374 - Batch 1672/3851 - Train loss: 0.23757119  - Train acc: 0.9101 - Val loss: 0.59709674\n",
      "(1.95 min) Epoch 3/300 -- Iteration 9412 - Batch 1710/3851 - Train loss: 0.23728428  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(1.96 min) Epoch 3/300 -- Iteration 9450 - Batch 1748/3851 - Train loss: 0.23721350  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(1.97 min) Epoch 3/300 -- Iteration 9488 - Batch 1786/3851 - Train loss: 0.23711319  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(1.98 min) Epoch 3/300 -- Iteration 9526 - Batch 1824/3851 - Train loss: 0.23719861  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(1.98 min) Epoch 3/300 -- Iteration 9564 - Batch 1862/3851 - Train loss: 0.23713511  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(1.99 min) Epoch 3/300 -- Iteration 9602 - Batch 1900/3851 - Train loss: 0.23721578  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(2.00 min) Epoch 3/300 -- Iteration 9640 - Batch 1938/3851 - Train loss: 0.23708375  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(2.01 min) Epoch 3/300 -- Iteration 9678 - Batch 1976/3851 - Train loss: 0.23706197  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(2.01 min) Epoch 3/300 -- Iteration 9716 - Batch 2014/3851 - Train loss: 0.23689608  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(2.02 min) Epoch 3/300 -- Iteration 9754 - Batch 2052/3851 - Train loss: 0.23689869  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(2.03 min) Epoch 3/300 -- Iteration 9792 - Batch 2090/3851 - Train loss: 0.23691026  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(2.04 min) Epoch 3/300 -- Iteration 9830 - Batch 2128/3851 - Train loss: 0.23691225  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(2.05 min) Epoch 3/300 -- Iteration 9868 - Batch 2166/3851 - Train loss: 0.23692209  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(2.05 min) Epoch 3/300 -- Iteration 9906 - Batch 2204/3851 - Train loss: 0.23655449  - Train acc: 0.9104 - Val loss: 0.59709674\n",
      "(2.06 min) Epoch 3/300 -- Iteration 9944 - Batch 2242/3851 - Train loss: 0.23638694  - Train acc: 0.9104 - Val loss: 0.59709674\n",
      "(2.07 min) Epoch 3/300 -- Iteration 9982 - Batch 2280/3851 - Train loss: 0.23664432  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(2.08 min) Epoch 3/300 -- Iteration 10020 - Batch 2318/3851 - Train loss: 0.23653394  - Train acc: 0.9104 - Val loss: 0.59709674\n",
      "(2.08 min) Epoch 3/300 -- Iteration 10058 - Batch 2356/3851 - Train loss: 0.23668202  - Train acc: 0.9103 - Val loss: 0.59709674\n",
      "(2.09 min) Epoch 3/300 -- Iteration 10096 - Batch 2394/3851 - Train loss: 0.23676597  - Train acc: 0.9102 - Val loss: 0.59709674\n",
      "(2.10 min) Epoch 3/300 -- Iteration 10134 - Batch 2432/3851 - Train loss: 0.23664277  - Train acc: 0.9104 - Val loss: 0.59709674\n",
      "(2.11 min) Epoch 3/300 -- Iteration 10172 - Batch 2470/3851 - Train loss: 0.23629183  - Train acc: 0.9105 - Val loss: 0.59709674\n",
      "(2.11 min) Epoch 3/300 -- Iteration 10210 - Batch 2508/3851 - Train loss: 0.23610608  - Train acc: 0.9106 - Val loss: 0.59709674\n",
      "(2.12 min) Epoch 3/300 -- Iteration 10248 - Batch 2546/3851 - Train loss: 0.23608817  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(2.13 min) Epoch 3/300 -- Iteration 10286 - Batch 2584/3851 - Train loss: 0.23586889  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(2.14 min) Epoch 3/300 -- Iteration 10324 - Batch 2622/3851 - Train loss: 0.23603841  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(2.14 min) Epoch 3/300 -- Iteration 10362 - Batch 2660/3851 - Train loss: 0.23590825  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(2.15 min) Epoch 3/300 -- Iteration 10400 - Batch 2698/3851 - Train loss: 0.23600165  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(2.16 min) Epoch 3/300 -- Iteration 10438 - Batch 2736/3851 - Train loss: 0.23588469  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(2.17 min) Epoch 3/300 -- Iteration 10476 - Batch 2774/3851 - Train loss: 0.23576748  - Train acc: 0.9108 - Val loss: 0.59709674\n",
      "(2.18 min) Epoch 3/300 -- Iteration 10514 - Batch 2812/3851 - Train loss: 0.23569389  - Train acc: 0.9107 - Val loss: 0.59709674\n",
      "(2.18 min) Epoch 3/300 -- Iteration 10552 - Batch 2850/3851 - Train loss: 0.23537592  - Train acc: 0.9109 - Val loss: 0.59709674\n",
      "(2.19 min) Epoch 3/300 -- Iteration 10590 - Batch 2888/3851 - Train loss: 0.23526958  - Train acc: 0.9110 - Val loss: 0.59709674\n",
      "(2.20 min) Epoch 3/300 -- Iteration 10628 - Batch 2926/3851 - Train loss: 0.23526423  - Train acc: 0.9110 - Val loss: 0.59709674\n",
      "(2.21 min) Epoch 3/300 -- Iteration 10666 - Batch 2964/3851 - Train loss: 0.23517803  - Train acc: 0.9110 - Val loss: 0.59709674\n",
      "(2.22 min) Epoch 3/300 -- Iteration 10704 - Batch 3002/3851 - Train loss: 0.23531682  - Train acc: 0.9110 - Val loss: 0.59709674\n",
      "(2.22 min) Epoch 3/300 -- Iteration 10742 - Batch 3040/3851 - Train loss: 0.23544838  - Train acc: 0.9109 - Val loss: 0.59709674\n",
      "(2.23 min) Epoch 3/300 -- Iteration 10780 - Batch 3078/3851 - Train loss: 0.23553940  - Train acc: 0.9109 - Val loss: 0.59709674\n",
      "(2.24 min) Epoch 3/300 -- Iteration 10818 - Batch 3116/3851 - Train loss: 0.23541613  - Train acc: 0.9109 - Val loss: 0.59709674\n",
      "(2.25 min) Epoch 3/300 -- Iteration 10856 - Batch 3154/3851 - Train loss: 0.23543410  - Train acc: 0.9109 - Val loss: 0.59709674\n",
      "(2.25 min) Epoch 3/300 -- Iteration 10894 - Batch 3192/3851 - Train loss: 0.23538885  - Train acc: 0.9109 - Val loss: 0.59709674\n",
      "(2.26 min) Epoch 3/300 -- Iteration 10932 - Batch 3230/3851 - Train loss: 0.23541294  - Train acc: 0.9109 - Val loss: 0.59709674\n",
      "(2.27 min) Epoch 3/300 -- Iteration 10970 - Batch 3268/3851 - Train loss: 0.23548746  - Train acc: 0.9109 - Val loss: 0.59709674\n",
      "(2.28 min) Epoch 3/300 -- Iteration 11008 - Batch 3306/3851 - Train loss: 0.23535691  - Train acc: 0.9110 - Val loss: 0.59709674\n",
      "(2.28 min) Epoch 3/300 -- Iteration 11046 - Batch 3344/3851 - Train loss: 0.23513542  - Train acc: 0.9110 - Val loss: 0.59709674\n",
      "(2.29 min) Epoch 3/300 -- Iteration 11084 - Batch 3382/3851 - Train loss: 0.23518315  - Train acc: 0.9110 - Val loss: 0.59709674\n",
      "(2.30 min) Epoch 3/300 -- Iteration 11122 - Batch 3420/3851 - Train loss: 0.23528797  - Train acc: 0.9110 - Val loss: 0.59709674\n",
      "(2.31 min) Epoch 3/300 -- Iteration 11160 - Batch 3458/3851 - Train loss: 0.23514393  - Train acc: 0.9111 - Val loss: 0.59709674\n",
      "(2.32 min) Epoch 3/300 -- Iteration 11198 - Batch 3496/3851 - Train loss: 0.23509601  - Train acc: 0.9111 - Val loss: 0.59709674\n",
      "(2.32 min) Epoch 3/300 -- Iteration 11236 - Batch 3534/3851 - Train loss: 0.23507203  - Train acc: 0.9112 - Val loss: 0.59709674\n",
      "(2.33 min) Epoch 3/300 -- Iteration 11274 - Batch 3572/3851 - Train loss: 0.23493065  - Train acc: 0.9112 - Val loss: 0.59709674\n",
      "(2.34 min) Epoch 3/300 -- Iteration 11312 - Batch 3610/3851 - Train loss: 0.23488582  - Train acc: 0.9113 - Val loss: 0.59709674\n",
      "(2.35 min) Epoch 3/300 -- Iteration 11350 - Batch 3648/3851 - Train loss: 0.23492887  - Train acc: 0.9112 - Val loss: 0.59709674\n",
      "(2.35 min) Epoch 3/300 -- Iteration 11388 - Batch 3686/3851 - Train loss: 0.23487174  - Train acc: 0.9113 - Val loss: 0.59709674\n",
      "(2.36 min) Epoch 3/300 -- Iteration 11426 - Batch 3724/3851 - Train loss: 0.23496950  - Train acc: 0.9113 - Val loss: 0.59709674\n",
      "(2.37 min) Epoch 3/300 -- Iteration 11464 - Batch 3762/3851 - Train loss: 0.23483213  - Train acc: 0.9114 - Val loss: 0.59709674\n",
      "(2.38 min) Epoch 3/300 -- Iteration 11502 - Batch 3800/3851 - Train loss: 0.23479409  - Train acc: 0.9114 - Val loss: 0.59709674\n",
      "(2.38 min) Epoch 3/300 -- Iteration 11540 - Batch 3838/3851 - Train loss: 0.23473660  - Train acc: 0.9114 - Val loss: 0.59709674\n",
      "(2.39 min) Epoch 3/300 -- Iteration 11553 - Batch 3850/3851 - Train loss: 0.23467193  - Train acc: 0.9114 - Val loss: 0.61637032 - Val acc: 0.6717\n",
      "(2.40 min) Epoch 4/300 -- Iteration 11591 - Batch 38/3851 - Train loss: 0.22413973  - Train acc: 0.9185 - Val loss: 0.61637032\n",
      "(2.40 min) Epoch 4/300 -- Iteration 11629 - Batch 76/3851 - Train loss: 0.21906023  - Train acc: 0.9196 - Val loss: 0.61637032\n",
      "(2.41 min) Epoch 4/300 -- Iteration 11667 - Batch 114/3851 - Train loss: 0.21841207  - Train acc: 0.9198 - Val loss: 0.61637032\n",
      "(2.42 min) Epoch 4/300 -- Iteration 11705 - Batch 152/3851 - Train loss: 0.21923723  - Train acc: 0.9195 - Val loss: 0.61637032\n",
      "(2.43 min) Epoch 4/300 -- Iteration 11743 - Batch 190/3851 - Train loss: 0.21984254  - Train acc: 0.9188 - Val loss: 0.61637032\n",
      "(2.43 min) Epoch 4/300 -- Iteration 11781 - Batch 228/3851 - Train loss: 0.22072241  - Train acc: 0.9184 - Val loss: 0.61637032\n",
      "(2.44 min) Epoch 4/300 -- Iteration 11819 - Batch 266/3851 - Train loss: 0.22068840  - Train acc: 0.9185 - Val loss: 0.61637032\n",
      "(2.45 min) Epoch 4/300 -- Iteration 11857 - Batch 304/3851 - Train loss: 0.21868463  - Train acc: 0.9195 - Val loss: 0.61637032\n",
      "(2.46 min) Epoch 4/300 -- Iteration 11895 - Batch 342/3851 - Train loss: 0.21842128  - Train acc: 0.9193 - Val loss: 0.61637032\n",
      "(2.47 min) Epoch 4/300 -- Iteration 11933 - Batch 380/3851 - Train loss: 0.22050840  - Train acc: 0.9183 - Val loss: 0.61637032\n",
      "(2.47 min) Epoch 4/300 -- Iteration 11971 - Batch 418/3851 - Train loss: 0.22098495  - Train acc: 0.9178 - Val loss: 0.61637032\n",
      "(2.48 min) Epoch 4/300 -- Iteration 12009 - Batch 456/3851 - Train loss: 0.22113482  - Train acc: 0.9176 - Val loss: 0.61637032\n",
      "(2.49 min) Epoch 4/300 -- Iteration 12047 - Batch 494/3851 - Train loss: 0.22217071  - Train acc: 0.9172 - Val loss: 0.61637032\n",
      "(2.50 min) Epoch 4/300 -- Iteration 12085 - Batch 532/3851 - Train loss: 0.22344866  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(2.50 min) Epoch 4/300 -- Iteration 12123 - Batch 570/3851 - Train loss: 0.22396810  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(2.51 min) Epoch 4/300 -- Iteration 12161 - Batch 608/3851 - Train loss: 0.22441600  - Train acc: 0.9164 - Val loss: 0.61637032\n",
      "(2.52 min) Epoch 4/300 -- Iteration 12199 - Batch 646/3851 - Train loss: 0.22445587  - Train acc: 0.9162 - Val loss: 0.61637032\n",
      "(2.53 min) Epoch 4/300 -- Iteration 12237 - Batch 684/3851 - Train loss: 0.22475377  - Train acc: 0.9161 - Val loss: 0.61637032\n",
      "(2.53 min) Epoch 4/300 -- Iteration 12275 - Batch 722/3851 - Train loss: 0.22408199  - Train acc: 0.9162 - Val loss: 0.61637032\n",
      "(2.54 min) Epoch 4/300 -- Iteration 12313 - Batch 760/3851 - Train loss: 0.22352116  - Train acc: 0.9164 - Val loss: 0.61637032\n",
      "(2.55 min) Epoch 4/300 -- Iteration 12351 - Batch 798/3851 - Train loss: 0.22368816  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.56 min) Epoch 4/300 -- Iteration 12389 - Batch 836/3851 - Train loss: 0.22337772  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(2.56 min) Epoch 4/300 -- Iteration 12427 - Batch 874/3851 - Train loss: 0.22343654  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(2.57 min) Epoch 4/300 -- Iteration 12465 - Batch 912/3851 - Train loss: 0.22388910  - Train acc: 0.9164 - Val loss: 0.61637032\n",
      "(2.58 min) Epoch 4/300 -- Iteration 12503 - Batch 950/3851 - Train loss: 0.22412033  - Train acc: 0.9164 - Val loss: 0.61637032\n",
      "(2.59 min) Epoch 4/300 -- Iteration 12541 - Batch 988/3851 - Train loss: 0.22403485  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.59 min) Epoch 4/300 -- Iteration 12579 - Batch 1026/3851 - Train loss: 0.22368171  - Train acc: 0.9165 - Val loss: 0.61637032\n",
      "(2.60 min) Epoch 4/300 -- Iteration 12617 - Batch 1064/3851 - Train loss: 0.22393502  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.61 min) Epoch 4/300 -- Iteration 12655 - Batch 1102/3851 - Train loss: 0.22444824  - Train acc: 0.9160 - Val loss: 0.61637032\n",
      "(2.62 min) Epoch 4/300 -- Iteration 12693 - Batch 1140/3851 - Train loss: 0.22463145  - Train acc: 0.9159 - Val loss: 0.61637032\n",
      "(2.62 min) Epoch 4/300 -- Iteration 12731 - Batch 1178/3851 - Train loss: 0.22415910  - Train acc: 0.9162 - Val loss: 0.61637032\n",
      "(2.63 min) Epoch 4/300 -- Iteration 12769 - Batch 1216/3851 - Train loss: 0.22402985  - Train acc: 0.9162 - Val loss: 0.61637032\n",
      "(2.64 min) Epoch 4/300 -- Iteration 12807 - Batch 1254/3851 - Train loss: 0.22430710  - Train acc: 0.9161 - Val loss: 0.61637032\n",
      "(2.65 min) Epoch 4/300 -- Iteration 12845 - Batch 1292/3851 - Train loss: 0.22437110  - Train acc: 0.9161 - Val loss: 0.61637032\n",
      "(2.66 min) Epoch 4/300 -- Iteration 12883 - Batch 1330/3851 - Train loss: 0.22456751  - Train acc: 0.9160 - Val loss: 0.61637032\n",
      "(2.66 min) Epoch 4/300 -- Iteration 12921 - Batch 1368/3851 - Train loss: 0.22453846  - Train acc: 0.9159 - Val loss: 0.61637032\n",
      "(2.67 min) Epoch 4/300 -- Iteration 12959 - Batch 1406/3851 - Train loss: 0.22500505  - Train acc: 0.9158 - Val loss: 0.61637032\n",
      "(2.68 min) Epoch 4/300 -- Iteration 12997 - Batch 1444/3851 - Train loss: 0.22502838  - Train acc: 0.9158 - Val loss: 0.61637032\n",
      "(2.69 min) Epoch 4/300 -- Iteration 13035 - Batch 1482/3851 - Train loss: 0.22501727  - Train acc: 0.9158 - Val loss: 0.61637032\n",
      "(2.69 min) Epoch 4/300 -- Iteration 13073 - Batch 1520/3851 - Train loss: 0.22487304  - Train acc: 0.9158 - Val loss: 0.61637032\n",
      "(2.70 min) Epoch 4/300 -- Iteration 13111 - Batch 1558/3851 - Train loss: 0.22486840  - Train acc: 0.9159 - Val loss: 0.61637032\n",
      "(2.71 min) Epoch 4/300 -- Iteration 13149 - Batch 1596/3851 - Train loss: 0.22473785  - Train acc: 0.9159 - Val loss: 0.61637032\n",
      "(2.72 min) Epoch 4/300 -- Iteration 13187 - Batch 1634/3851 - Train loss: 0.22454459  - Train acc: 0.9160 - Val loss: 0.61637032\n",
      "(2.72 min) Epoch 4/300 -- Iteration 13225 - Batch 1672/3851 - Train loss: 0.22447968  - Train acc: 0.9160 - Val loss: 0.61637032\n",
      "(2.73 min) Epoch 4/300 -- Iteration 13263 - Batch 1710/3851 - Train loss: 0.22406143  - Train acc: 0.9161 - Val loss: 0.61637032\n",
      "(2.74 min) Epoch 4/300 -- Iteration 13301 - Batch 1748/3851 - Train loss: 0.22416156  - Train acc: 0.9160 - Val loss: 0.61637032\n",
      "(2.75 min) Epoch 4/300 -- Iteration 13339 - Batch 1786/3851 - Train loss: 0.22430407  - Train acc: 0.9159 - Val loss: 0.61637032\n",
      "(2.76 min) Epoch 4/300 -- Iteration 13377 - Batch 1824/3851 - Train loss: 0.22403488  - Train acc: 0.9161 - Val loss: 0.61637032\n",
      "(2.76 min) Epoch 4/300 -- Iteration 13415 - Batch 1862/3851 - Train loss: 0.22420419  - Train acc: 0.9160 - Val loss: 0.61637032\n",
      "(2.77 min) Epoch 4/300 -- Iteration 13453 - Batch 1900/3851 - Train loss: 0.22406896  - Train acc: 0.9161 - Val loss: 0.61637032\n",
      "(2.78 min) Epoch 4/300 -- Iteration 13491 - Batch 1938/3851 - Train loss: 0.22400320  - Train acc: 0.9161 - Val loss: 0.61637032\n",
      "(2.79 min) Epoch 4/300 -- Iteration 13529 - Batch 1976/3851 - Train loss: 0.22385287  - Train acc: 0.9162 - Val loss: 0.61637032\n",
      "(2.79 min) Epoch 4/300 -- Iteration 13567 - Batch 2014/3851 - Train loss: 0.22389633  - Train acc: 0.9161 - Val loss: 0.61637032\n",
      "(2.80 min) Epoch 4/300 -- Iteration 13605 - Batch 2052/3851 - Train loss: 0.22368685  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.81 min) Epoch 4/300 -- Iteration 13643 - Batch 2090/3851 - Train loss: 0.22368103  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.82 min) Epoch 4/300 -- Iteration 13681 - Batch 2128/3851 - Train loss: 0.22362003  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.82 min) Epoch 4/300 -- Iteration 13719 - Batch 2166/3851 - Train loss: 0.22349315  - Train acc: 0.9165 - Val loss: 0.61637032\n",
      "(2.83 min) Epoch 4/300 -- Iteration 13757 - Batch 2204/3851 - Train loss: 0.22345653  - Train acc: 0.9165 - Val loss: 0.61637032\n",
      "(2.84 min) Epoch 4/300 -- Iteration 13795 - Batch 2242/3851 - Train loss: 0.22358414  - Train acc: 0.9164 - Val loss: 0.61637032\n",
      "(2.85 min) Epoch 4/300 -- Iteration 13833 - Batch 2280/3851 - Train loss: 0.22346690  - Train acc: 0.9164 - Val loss: 0.61637032\n",
      "(2.85 min) Epoch 4/300 -- Iteration 13871 - Batch 2318/3851 - Train loss: 0.22364039  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.86 min) Epoch 4/300 -- Iteration 13909 - Batch 2356/3851 - Train loss: 0.22369196  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.87 min) Epoch 4/300 -- Iteration 13947 - Batch 2394/3851 - Train loss: 0.22354201  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.88 min) Epoch 4/300 -- Iteration 13985 - Batch 2432/3851 - Train loss: 0.22353969  - Train acc: 0.9162 - Val loss: 0.61637032\n",
      "(2.88 min) Epoch 4/300 -- Iteration 14023 - Batch 2470/3851 - Train loss: 0.22343133  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.89 min) Epoch 4/300 -- Iteration 14061 - Batch 2508/3851 - Train loss: 0.22335984  - Train acc: 0.9164 - Val loss: 0.61637032\n",
      "(2.90 min) Epoch 4/300 -- Iteration 14099 - Batch 2546/3851 - Train loss: 0.22349127  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.91 min) Epoch 4/300 -- Iteration 14137 - Batch 2584/3851 - Train loss: 0.22349535  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.92 min) Epoch 4/300 -- Iteration 14175 - Batch 2622/3851 - Train loss: 0.22341388  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.92 min) Epoch 4/300 -- Iteration 14213 - Batch 2660/3851 - Train loss: 0.22346136  - Train acc: 0.9162 - Val loss: 0.61637032\n",
      "(2.93 min) Epoch 4/300 -- Iteration 14251 - Batch 2698/3851 - Train loss: 0.22341262  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.94 min) Epoch 4/300 -- Iteration 14289 - Batch 2736/3851 - Train loss: 0.22333983  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.95 min) Epoch 4/300 -- Iteration 14327 - Batch 2774/3851 - Train loss: 0.22326951  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.95 min) Epoch 4/300 -- Iteration 14365 - Batch 2812/3851 - Train loss: 0.22337904  - Train acc: 0.9162 - Val loss: 0.61637032\n",
      "(2.96 min) Epoch 4/300 -- Iteration 14403 - Batch 2850/3851 - Train loss: 0.22325568  - Train acc: 0.9162 - Val loss: 0.61637032\n",
      "(2.97 min) Epoch 4/300 -- Iteration 14441 - Batch 2888/3851 - Train loss: 0.22313582  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.98 min) Epoch 4/300 -- Iteration 14479 - Batch 2926/3851 - Train loss: 0.22308829  - Train acc: 0.9163 - Val loss: 0.61637032\n",
      "(2.98 min) Epoch 4/300 -- Iteration 14517 - Batch 2964/3851 - Train loss: 0.22274483  - Train acc: 0.9164 - Val loss: 0.61637032\n",
      "(2.99 min) Epoch 4/300 -- Iteration 14555 - Batch 3002/3851 - Train loss: 0.22248755  - Train acc: 0.9165 - Val loss: 0.61637032\n",
      "(3.00 min) Epoch 4/300 -- Iteration 14593 - Batch 3040/3851 - Train loss: 0.22247127  - Train acc: 0.9165 - Val loss: 0.61637032\n",
      "(3.01 min) Epoch 4/300 -- Iteration 14631 - Batch 3078/3851 - Train loss: 0.22225591  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(3.01 min) Epoch 4/300 -- Iteration 14669 - Batch 3116/3851 - Train loss: 0.22211497  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(3.02 min) Epoch 4/300 -- Iteration 14707 - Batch 3154/3851 - Train loss: 0.22221871  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(3.03 min) Epoch 4/300 -- Iteration 14745 - Batch 3192/3851 - Train loss: 0.22218952  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(3.04 min) Epoch 4/300 -- Iteration 14783 - Batch 3230/3851 - Train loss: 0.22202892  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(3.05 min) Epoch 4/300 -- Iteration 14821 - Batch 3268/3851 - Train loss: 0.22216864  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(3.05 min) Epoch 4/300 -- Iteration 14859 - Batch 3306/3851 - Train loss: 0.22209815  - Train acc: 0.9166 - Val loss: 0.61637032\n",
      "(3.06 min) Epoch 4/300 -- Iteration 14897 - Batch 3344/3851 - Train loss: 0.22181894  - Train acc: 0.9168 - Val loss: 0.61637032\n",
      "(3.07 min) Epoch 4/300 -- Iteration 14935 - Batch 3382/3851 - Train loss: 0.22172992  - Train acc: 0.9168 - Val loss: 0.61637032\n",
      "(3.08 min) Epoch 4/300 -- Iteration 14973 - Batch 3420/3851 - Train loss: 0.22161824  - Train acc: 0.9169 - Val loss: 0.61637032\n",
      "(3.08 min) Epoch 4/300 -- Iteration 15011 - Batch 3458/3851 - Train loss: 0.22143556  - Train acc: 0.9170 - Val loss: 0.61637032\n",
      "(3.09 min) Epoch 4/300 -- Iteration 15049 - Batch 3496/3851 - Train loss: 0.22128613  - Train acc: 0.9171 - Val loss: 0.61637032\n",
      "(3.10 min) Epoch 4/300 -- Iteration 15087 - Batch 3534/3851 - Train loss: 0.22138159  - Train acc: 0.9170 - Val loss: 0.61637032\n",
      "(3.11 min) Epoch 4/300 -- Iteration 15125 - Batch 3572/3851 - Train loss: 0.22142200  - Train acc: 0.9170 - Val loss: 0.61637032\n",
      "(3.11 min) Epoch 4/300 -- Iteration 15163 - Batch 3610/3851 - Train loss: 0.22134356  - Train acc: 0.9171 - Val loss: 0.61637032\n",
      "(3.12 min) Epoch 4/300 -- Iteration 15201 - Batch 3648/3851 - Train loss: 0.22133786  - Train acc: 0.9171 - Val loss: 0.61637032\n",
      "(3.13 min) Epoch 4/300 -- Iteration 15239 - Batch 3686/3851 - Train loss: 0.22131784  - Train acc: 0.9170 - Val loss: 0.61637032\n",
      "(3.14 min) Epoch 4/300 -- Iteration 15277 - Batch 3724/3851 - Train loss: 0.22115714  - Train acc: 0.9171 - Val loss: 0.61637032\n",
      "(3.14 min) Epoch 4/300 -- Iteration 15315 - Batch 3762/3851 - Train loss: 0.22124438  - Train acc: 0.9170 - Val loss: 0.61637032\n",
      "(3.15 min) Epoch 4/300 -- Iteration 15353 - Batch 3800/3851 - Train loss: 0.22111913  - Train acc: 0.9170 - Val loss: 0.61637032\n",
      "(3.16 min) Epoch 4/300 -- Iteration 15391 - Batch 3838/3851 - Train loss: 0.22102050  - Train acc: 0.9171 - Val loss: 0.61637032\n",
      "(3.16 min) Epoch 4/300 -- Iteration 15404 - Batch 3850/3851 - Train loss: 0.22099008  - Train acc: 0.9171 - Val loss: 0.65647310 - Val acc: 0.6500\n",
      "(3.17 min) Epoch 5/300 -- Iteration 15442 - Batch 38/3851 - Train loss: 0.20830588  - Train acc: 0.9205 - Val loss: 0.65647310\n",
      "(3.18 min) Epoch 5/300 -- Iteration 15480 - Batch 76/3851 - Train loss: 0.21026599  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.19 min) Epoch 5/300 -- Iteration 15518 - Batch 114/3851 - Train loss: 0.21983871  - Train acc: 0.9161 - Val loss: 0.65647310\n",
      "(3.19 min) Epoch 5/300 -- Iteration 15556 - Batch 152/3851 - Train loss: 0.22016893  - Train acc: 0.9161 - Val loss: 0.65647310\n",
      "(3.20 min) Epoch 5/300 -- Iteration 15594 - Batch 190/3851 - Train loss: 0.22235012  - Train acc: 0.9157 - Val loss: 0.65647310\n",
      "(3.21 min) Epoch 5/300 -- Iteration 15632 - Batch 228/3851 - Train loss: 0.22269039  - Train acc: 0.9160 - Val loss: 0.65647310\n",
      "(3.22 min) Epoch 5/300 -- Iteration 15670 - Batch 266/3851 - Train loss: 0.22240005  - Train acc: 0.9166 - Val loss: 0.65647310\n",
      "(3.23 min) Epoch 5/300 -- Iteration 15708 - Batch 304/3851 - Train loss: 0.21887710  - Train acc: 0.9181 - Val loss: 0.65647310\n",
      "(3.23 min) Epoch 5/300 -- Iteration 15746 - Batch 342/3851 - Train loss: 0.21848558  - Train acc: 0.9184 - Val loss: 0.65647310\n",
      "(3.24 min) Epoch 5/300 -- Iteration 15784 - Batch 380/3851 - Train loss: 0.21823042  - Train acc: 0.9183 - Val loss: 0.65647310\n",
      "(3.25 min) Epoch 5/300 -- Iteration 15822 - Batch 418/3851 - Train loss: 0.21901190  - Train acc: 0.9180 - Val loss: 0.65647310\n",
      "(3.26 min) Epoch 5/300 -- Iteration 15860 - Batch 456/3851 - Train loss: 0.21817187  - Train acc: 0.9182 - Val loss: 0.65647310\n",
      "(3.26 min) Epoch 5/300 -- Iteration 15898 - Batch 494/3851 - Train loss: 0.21802995  - Train acc: 0.9182 - Val loss: 0.65647310\n",
      "(3.27 min) Epoch 5/300 -- Iteration 15936 - Batch 532/3851 - Train loss: 0.21783589  - Train acc: 0.9184 - Val loss: 0.65647310\n",
      "(3.28 min) Epoch 5/300 -- Iteration 15974 - Batch 570/3851 - Train loss: 0.21796565  - Train acc: 0.9184 - Val loss: 0.65647310\n",
      "(3.29 min) Epoch 5/300 -- Iteration 16012 - Batch 608/3851 - Train loss: 0.21738310  - Train acc: 0.9185 - Val loss: 0.65647310\n",
      "(3.29 min) Epoch 5/300 -- Iteration 16050 - Batch 646/3851 - Train loss: 0.21726164  - Train acc: 0.9185 - Val loss: 0.65647310\n",
      "(3.30 min) Epoch 5/300 -- Iteration 16088 - Batch 684/3851 - Train loss: 0.21641636  - Train acc: 0.9187 - Val loss: 0.65647310\n",
      "(3.31 min) Epoch 5/300 -- Iteration 16126 - Batch 722/3851 - Train loss: 0.21572235  - Train acc: 0.9188 - Val loss: 0.65647310\n",
      "(3.32 min) Epoch 5/300 -- Iteration 16164 - Batch 760/3851 - Train loss: 0.21503633  - Train acc: 0.9191 - Val loss: 0.65647310\n",
      "(3.32 min) Epoch 5/300 -- Iteration 16202 - Batch 798/3851 - Train loss: 0.21490612  - Train acc: 0.9193 - Val loss: 0.65647310\n",
      "(3.33 min) Epoch 5/300 -- Iteration 16240 - Batch 836/3851 - Train loss: 0.21477831  - Train acc: 0.9193 - Val loss: 0.65647310\n",
      "(3.34 min) Epoch 5/300 -- Iteration 16278 - Batch 874/3851 - Train loss: 0.21504975  - Train acc: 0.9192 - Val loss: 0.65647310\n",
      "(3.35 min) Epoch 5/300 -- Iteration 16316 - Batch 912/3851 - Train loss: 0.21507574  - Train acc: 0.9191 - Val loss: 0.65647310\n",
      "(3.35 min) Epoch 5/300 -- Iteration 16354 - Batch 950/3851 - Train loss: 0.21457798  - Train acc: 0.9194 - Val loss: 0.65647310\n",
      "(3.36 min) Epoch 5/300 -- Iteration 16392 - Batch 988/3851 - Train loss: 0.21472729  - Train acc: 0.9194 - Val loss: 0.65647310\n",
      "(3.37 min) Epoch 5/300 -- Iteration 16430 - Batch 1026/3851 - Train loss: 0.21488014  - Train acc: 0.9193 - Val loss: 0.65647310\n",
      "(3.38 min) Epoch 5/300 -- Iteration 16468 - Batch 1064/3851 - Train loss: 0.21468001  - Train acc: 0.9194 - Val loss: 0.65647310\n",
      "(3.39 min) Epoch 5/300 -- Iteration 16506 - Batch 1102/3851 - Train loss: 0.21440179  - Train acc: 0.9195 - Val loss: 0.65647310\n",
      "(3.39 min) Epoch 5/300 -- Iteration 16544 - Batch 1140/3851 - Train loss: 0.21378438  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.40 min) Epoch 5/300 -- Iteration 16582 - Batch 1178/3851 - Train loss: 0.21368226  - Train acc: 0.9197 - Val loss: 0.65647310\n",
      "(3.41 min) Epoch 5/300 -- Iteration 16620 - Batch 1216/3851 - Train loss: 0.21390092  - Train acc: 0.9197 - Val loss: 0.65647310\n",
      "(3.42 min) Epoch 5/300 -- Iteration 16658 - Batch 1254/3851 - Train loss: 0.21358763  - Train acc: 0.9197 - Val loss: 0.65647310\n",
      "(3.42 min) Epoch 5/300 -- Iteration 16696 - Batch 1292/3851 - Train loss: 0.21331590  - Train acc: 0.9200 - Val loss: 0.65647310\n",
      "(3.43 min) Epoch 5/300 -- Iteration 16734 - Batch 1330/3851 - Train loss: 0.21297554  - Train acc: 0.9201 - Val loss: 0.65647310\n",
      "(3.44 min) Epoch 5/300 -- Iteration 16772 - Batch 1368/3851 - Train loss: 0.21324313  - Train acc: 0.9200 - Val loss: 0.65647310\n",
      "(3.45 min) Epoch 5/300 -- Iteration 16810 - Batch 1406/3851 - Train loss: 0.21330833  - Train acc: 0.9200 - Val loss: 0.65647310\n",
      "(3.45 min) Epoch 5/300 -- Iteration 16848 - Batch 1444/3851 - Train loss: 0.21320017  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.46 min) Epoch 5/300 -- Iteration 16886 - Batch 1482/3851 - Train loss: 0.21320695  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.47 min) Epoch 5/300 -- Iteration 16924 - Batch 1520/3851 - Train loss: 0.21320320  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.48 min) Epoch 5/300 -- Iteration 16962 - Batch 1558/3851 - Train loss: 0.21315759  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.48 min) Epoch 5/300 -- Iteration 17000 - Batch 1596/3851 - Train loss: 0.21331338  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.49 min) Epoch 5/300 -- Iteration 17038 - Batch 1634/3851 - Train loss: 0.21392697  - Train acc: 0.9196 - Val loss: 0.65647310\n",
      "(3.50 min) Epoch 5/300 -- Iteration 17076 - Batch 1672/3851 - Train loss: 0.21378202  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.51 min) Epoch 5/300 -- Iteration 17114 - Batch 1710/3851 - Train loss: 0.21378499  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.51 min) Epoch 5/300 -- Iteration 17152 - Batch 1748/3851 - Train loss: 0.21362157  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.52 min) Epoch 5/300 -- Iteration 17190 - Batch 1786/3851 - Train loss: 0.21346862  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.53 min) Epoch 5/300 -- Iteration 17228 - Batch 1824/3851 - Train loss: 0.21350650  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.54 min) Epoch 5/300 -- Iteration 17266 - Batch 1862/3851 - Train loss: 0.21350125  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.55 min) Epoch 5/300 -- Iteration 17304 - Batch 1900/3851 - Train loss: 0.21344789  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.55 min) Epoch 5/300 -- Iteration 17342 - Batch 1938/3851 - Train loss: 0.21341635  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.56 min) Epoch 5/300 -- Iteration 17380 - Batch 1976/3851 - Train loss: 0.21349277  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.57 min) Epoch 5/300 -- Iteration 17418 - Batch 2014/3851 - Train loss: 0.21344389  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.58 min) Epoch 5/300 -- Iteration 17456 - Batch 2052/3851 - Train loss: 0.21330429  - Train acc: 0.9200 - Val loss: 0.65647310\n",
      "(3.58 min) Epoch 5/300 -- Iteration 17494 - Batch 2090/3851 - Train loss: 0.21338630  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.59 min) Epoch 5/300 -- Iteration 17532 - Batch 2128/3851 - Train loss: 0.21348258  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.60 min) Epoch 5/300 -- Iteration 17570 - Batch 2166/3851 - Train loss: 0.21358434  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.61 min) Epoch 5/300 -- Iteration 17608 - Batch 2204/3851 - Train loss: 0.21353267  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.61 min) Epoch 5/300 -- Iteration 17646 - Batch 2242/3851 - Train loss: 0.21361129  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.62 min) Epoch 5/300 -- Iteration 17684 - Batch 2280/3851 - Train loss: 0.21350697  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.63 min) Epoch 5/300 -- Iteration 17722 - Batch 2318/3851 - Train loss: 0.21321654  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.64 min) Epoch 5/300 -- Iteration 17760 - Batch 2356/3851 - Train loss: 0.21324383  - Train acc: 0.9200 - Val loss: 0.65647310\n",
      "(3.64 min) Epoch 5/300 -- Iteration 17798 - Batch 2394/3851 - Train loss: 0.21349266  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.65 min) Epoch 5/300 -- Iteration 17836 - Batch 2432/3851 - Train loss: 0.21355203  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.66 min) Epoch 5/300 -- Iteration 17874 - Batch 2470/3851 - Train loss: 0.21354394  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.67 min) Epoch 5/300 -- Iteration 17912 - Batch 2508/3851 - Train loss: 0.21363983  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.67 min) Epoch 5/300 -- Iteration 17950 - Batch 2546/3851 - Train loss: 0.21366475  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.68 min) Epoch 5/300 -- Iteration 17988 - Batch 2584/3851 - Train loss: 0.21351431  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.69 min) Epoch 5/300 -- Iteration 18026 - Batch 2622/3851 - Train loss: 0.21338800  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.70 min) Epoch 5/300 -- Iteration 18064 - Batch 2660/3851 - Train loss: 0.21328204  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.71 min) Epoch 5/300 -- Iteration 18102 - Batch 2698/3851 - Train loss: 0.21324491  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.71 min) Epoch 5/300 -- Iteration 18140 - Batch 2736/3851 - Train loss: 0.21336197  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.72 min) Epoch 5/300 -- Iteration 18178 - Batch 2774/3851 - Train loss: 0.21336388  - Train acc: 0.9198 - Val loss: 0.65647310\n",
      "(3.73 min) Epoch 5/300 -- Iteration 18216 - Batch 2812/3851 - Train loss: 0.21323261  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.74 min) Epoch 5/300 -- Iteration 18254 - Batch 2850/3851 - Train loss: 0.21310059  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.74 min) Epoch 5/300 -- Iteration 18292 - Batch 2888/3851 - Train loss: 0.21308042  - Train acc: 0.9199 - Val loss: 0.65647310\n",
      "(3.75 min) Epoch 5/300 -- Iteration 18330 - Batch 2926/3851 - Train loss: 0.21292494  - Train acc: 0.9200 - Val loss: 0.65647310\n",
      "(3.76 min) Epoch 5/300 -- Iteration 18368 - Batch 2964/3851 - Train loss: 0.21267742  - Train acc: 0.9202 - Val loss: 0.65647310\n",
      "(3.77 min) Epoch 5/300 -- Iteration 18406 - Batch 3002/3851 - Train loss: 0.21286672  - Train acc: 0.9201 - Val loss: 0.65647310\n",
      "(3.77 min) Epoch 5/300 -- Iteration 18444 - Batch 3040/3851 - Train loss: 0.21266334  - Train acc: 0.9202 - Val loss: 0.65647310\n",
      "(3.78 min) Epoch 5/300 -- Iteration 18482 - Batch 3078/3851 - Train loss: 0.21271908  - Train acc: 0.9201 - Val loss: 0.65647310\n",
      "(3.79 min) Epoch 5/300 -- Iteration 18520 - Batch 3116/3851 - Train loss: 0.21277755  - Train acc: 0.9200 - Val loss: 0.65647310\n",
      "(3.80 min) Epoch 5/300 -- Iteration 18558 - Batch 3154/3851 - Train loss: 0.21269625  - Train acc: 0.9201 - Val loss: 0.65647310\n",
      "(3.80 min) Epoch 5/300 -- Iteration 18596 - Batch 3192/3851 - Train loss: 0.21273702  - Train acc: 0.9201 - Val loss: 0.65647310\n",
      "(3.81 min) Epoch 5/300 -- Iteration 18634 - Batch 3230/3851 - Train loss: 0.21265995  - Train acc: 0.9201 - Val loss: 0.65647310\n",
      "(3.82 min) Epoch 5/300 -- Iteration 18672 - Batch 3268/3851 - Train loss: 0.21261846  - Train acc: 0.9201 - Val loss: 0.65647310\n",
      "(3.83 min) Epoch 5/300 -- Iteration 18710 - Batch 3306/3851 - Train loss: 0.21260809  - Train acc: 0.9201 - Val loss: 0.65647310\n",
      "(3.84 min) Epoch 5/300 -- Iteration 18748 - Batch 3344/3851 - Train loss: 0.21243798  - Train acc: 0.9202 - Val loss: 0.65647310\n",
      "(3.84 min) Epoch 5/300 -- Iteration 18786 - Batch 3382/3851 - Train loss: 0.21251162  - Train acc: 0.9202 - Val loss: 0.65647310\n",
      "(3.85 min) Epoch 5/300 -- Iteration 18824 - Batch 3420/3851 - Train loss: 0.21239418  - Train acc: 0.9202 - Val loss: 0.65647310\n",
      "(3.86 min) Epoch 5/300 -- Iteration 18862 - Batch 3458/3851 - Train loss: 0.21223071  - Train acc: 0.9203 - Val loss: 0.65647310\n",
      "(3.87 min) Epoch 5/300 -- Iteration 18900 - Batch 3496/3851 - Train loss: 0.21219944  - Train acc: 0.9203 - Val loss: 0.65647310\n",
      "(3.87 min) Epoch 5/300 -- Iteration 18938 - Batch 3534/3851 - Train loss: 0.21223490  - Train acc: 0.9203 - Val loss: 0.65647310\n",
      "(3.88 min) Epoch 5/300 -- Iteration 18976 - Batch 3572/3851 - Train loss: 0.21213186  - Train acc: 0.9203 - Val loss: 0.65647310\n",
      "(3.89 min) Epoch 5/300 -- Iteration 19014 - Batch 3610/3851 - Train loss: 0.21201978  - Train acc: 0.9204 - Val loss: 0.65647310\n",
      "(3.90 min) Epoch 5/300 -- Iteration 19052 - Batch 3648/3851 - Train loss: 0.21181247  - Train acc: 0.9204 - Val loss: 0.65647310\n",
      "(3.90 min) Epoch 5/300 -- Iteration 19090 - Batch 3686/3851 - Train loss: 0.21174904  - Train acc: 0.9205 - Val loss: 0.65647310\n",
      "(3.91 min) Epoch 5/300 -- Iteration 19128 - Batch 3724/3851 - Train loss: 0.21174360  - Train acc: 0.9205 - Val loss: 0.65647310\n",
      "(3.92 min) Epoch 5/300 -- Iteration 19166 - Batch 3762/3851 - Train loss: 0.21179826  - Train acc: 0.9205 - Val loss: 0.65647310\n",
      "(3.93 min) Epoch 5/300 -- Iteration 19204 - Batch 3800/3851 - Train loss: 0.21169451  - Train acc: 0.9205 - Val loss: 0.65647310\n",
      "(3.93 min) Epoch 5/300 -- Iteration 19242 - Batch 3838/3851 - Train loss: 0.21168790  - Train acc: 0.9205 - Val loss: 0.65647310\n",
      "(3.94 min) Epoch 5/300 -- Iteration 19255 - Batch 3850/3851 - Train loss: 0.21163606  - Train acc: 0.9205 - Val loss: 0.66149038 - Val acc: 0.6600\n",
      "(3.95 min) Epoch 6/300 -- Iteration 19293 - Batch 38/3851 - Train loss: 0.19687507  - Train acc: 0.9251 - Val loss: 0.66149038\n",
      "(3.95 min) Epoch 6/300 -- Iteration 19331 - Batch 76/3851 - Train loss: 0.20322381  - Train acc: 0.9228 - Val loss: 0.66149038\n",
      "(3.96 min) Epoch 6/300 -- Iteration 19369 - Batch 114/3851 - Train loss: 0.20287590  - Train acc: 0.9227 - Val loss: 0.66149038\n",
      "(3.97 min) Epoch 6/300 -- Iteration 19407 - Batch 152/3851 - Train loss: 0.20492047  - Train acc: 0.9220 - Val loss: 0.66149038\n",
      "(3.98 min) Epoch 6/300 -- Iteration 19445 - Batch 190/3851 - Train loss: 0.20758045  - Train acc: 0.9217 - Val loss: 0.66149038\n",
      "(3.98 min) Epoch 6/300 -- Iteration 19483 - Batch 228/3851 - Train loss: 0.20649437  - Train acc: 0.9225 - Val loss: 0.66149038\n",
      "(3.99 min) Epoch 6/300 -- Iteration 19521 - Batch 266/3851 - Train loss: 0.20588675  - Train acc: 0.9223 - Val loss: 0.66149038\n",
      "(4.00 min) Epoch 6/300 -- Iteration 19559 - Batch 304/3851 - Train loss: 0.20641475  - Train acc: 0.9222 - Val loss: 0.66149038\n",
      "(4.01 min) Epoch 6/300 -- Iteration 19597 - Batch 342/3851 - Train loss: 0.20605162  - Train acc: 0.9222 - Val loss: 0.66149038\n",
      "(4.02 min) Epoch 6/300 -- Iteration 19635 - Batch 380/3851 - Train loss: 0.20720422  - Train acc: 0.9219 - Val loss: 0.66149038\n",
      "(4.02 min) Epoch 6/300 -- Iteration 19673 - Batch 418/3851 - Train loss: 0.20630625  - Train acc: 0.9226 - Val loss: 0.66149038\n",
      "(4.03 min) Epoch 6/300 -- Iteration 19711 - Batch 456/3851 - Train loss: 0.20732325  - Train acc: 0.9223 - Val loss: 0.66149038\n",
      "(4.04 min) Epoch 6/300 -- Iteration 19749 - Batch 494/3851 - Train loss: 0.20796222  - Train acc: 0.9221 - Val loss: 0.66149038\n",
      "(4.05 min) Epoch 6/300 -- Iteration 19787 - Batch 532/3851 - Train loss: 0.20738138  - Train acc: 0.9224 - Val loss: 0.66149038\n",
      "(4.05 min) Epoch 6/300 -- Iteration 19825 - Batch 570/3851 - Train loss: 0.20673107  - Train acc: 0.9224 - Val loss: 0.66149038\n",
      "(4.06 min) Epoch 6/300 -- Iteration 19863 - Batch 608/3851 - Train loss: 0.20614631  - Train acc: 0.9226 - Val loss: 0.66149038\n",
      "(4.07 min) Epoch 6/300 -- Iteration 19901 - Batch 646/3851 - Train loss: 0.20521629  - Train acc: 0.9231 - Val loss: 0.66149038\n",
      "(4.08 min) Epoch 6/300 -- Iteration 19939 - Batch 684/3851 - Train loss: 0.20486182  - Train acc: 0.9231 - Val loss: 0.66149038\n",
      "(4.08 min) Epoch 6/300 -- Iteration 19977 - Batch 722/3851 - Train loss: 0.20477776  - Train acc: 0.9234 - Val loss: 0.66149038\n",
      "(4.09 min) Epoch 6/300 -- Iteration 20015 - Batch 760/3851 - Train loss: 0.20480148  - Train acc: 0.9236 - Val loss: 0.66149038\n",
      "(4.10 min) Epoch 6/300 -- Iteration 20053 - Batch 798/3851 - Train loss: 0.20497167  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.11 min) Epoch 6/300 -- Iteration 20091 - Batch 836/3851 - Train loss: 0.20533224  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.11 min) Epoch 6/300 -- Iteration 20129 - Batch 874/3851 - Train loss: 0.20574036  - Train acc: 0.9233 - Val loss: 0.66149038\n",
      "(4.12 min) Epoch 6/300 -- Iteration 20167 - Batch 912/3851 - Train loss: 0.20580145  - Train acc: 0.9231 - Val loss: 0.66149038\n",
      "(4.13 min) Epoch 6/300 -- Iteration 20205 - Batch 950/3851 - Train loss: 0.20615040  - Train acc: 0.9230 - Val loss: 0.66149038\n",
      "(4.14 min) Epoch 6/300 -- Iteration 20243 - Batch 988/3851 - Train loss: 0.20601802  - Train acc: 0.9231 - Val loss: 0.66149038\n",
      "(4.14 min) Epoch 6/300 -- Iteration 20281 - Batch 1026/3851 - Train loss: 0.20573942  - Train acc: 0.9233 - Val loss: 0.66149038\n",
      "(4.15 min) Epoch 6/300 -- Iteration 20319 - Batch 1064/3851 - Train loss: 0.20569602  - Train acc: 0.9233 - Val loss: 0.66149038\n",
      "(4.16 min) Epoch 6/300 -- Iteration 20357 - Batch 1102/3851 - Train loss: 0.20508708  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.17 min) Epoch 6/300 -- Iteration 20395 - Batch 1140/3851 - Train loss: 0.20491275  - Train acc: 0.9236 - Val loss: 0.66149038\n",
      "(4.17 min) Epoch 6/300 -- Iteration 20433 - Batch 1178/3851 - Train loss: 0.20514604  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.18 min) Epoch 6/300 -- Iteration 20471 - Batch 1216/3851 - Train loss: 0.20512849  - Train acc: 0.9234 - Val loss: 0.66149038\n",
      "(4.19 min) Epoch 6/300 -- Iteration 20509 - Batch 1254/3851 - Train loss: 0.20491579  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.20 min) Epoch 6/300 -- Iteration 20547 - Batch 1292/3851 - Train loss: 0.20482873  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.21 min) Epoch 6/300 -- Iteration 20585 - Batch 1330/3851 - Train loss: 0.20494073  - Train acc: 0.9234 - Val loss: 0.66149038\n",
      "(4.21 min) Epoch 6/300 -- Iteration 20623 - Batch 1368/3851 - Train loss: 0.20489143  - Train acc: 0.9236 - Val loss: 0.66149038\n",
      "(4.22 min) Epoch 6/300 -- Iteration 20661 - Batch 1406/3851 - Train loss: 0.20458091  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.23 min) Epoch 6/300 -- Iteration 20699 - Batch 1444/3851 - Train loss: 0.20471732  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.24 min) Epoch 6/300 -- Iteration 20737 - Batch 1482/3851 - Train loss: 0.20470444  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.24 min) Epoch 6/300 -- Iteration 20775 - Batch 1520/3851 - Train loss: 0.20450072  - Train acc: 0.9239 - Val loss: 0.66149038\n",
      "(4.25 min) Epoch 6/300 -- Iteration 20813 - Batch 1558/3851 - Train loss: 0.20474114  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.26 min) Epoch 6/300 -- Iteration 20851 - Batch 1596/3851 - Train loss: 0.20464878  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.27 min) Epoch 6/300 -- Iteration 20889 - Batch 1634/3851 - Train loss: 0.20467291  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.27 min) Epoch 6/300 -- Iteration 20927 - Batch 1672/3851 - Train loss: 0.20457839  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.28 min) Epoch 6/300 -- Iteration 20965 - Batch 1710/3851 - Train loss: 0.20450918  - Train acc: 0.9236 - Val loss: 0.66149038\n",
      "(4.29 min) Epoch 6/300 -- Iteration 21003 - Batch 1748/3851 - Train loss: 0.20465243  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.30 min) Epoch 6/300 -- Iteration 21041 - Batch 1786/3851 - Train loss: 0.20469638  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.30 min) Epoch 6/300 -- Iteration 21079 - Batch 1824/3851 - Train loss: 0.20483454  - Train acc: 0.9234 - Val loss: 0.66149038\n",
      "(4.31 min) Epoch 6/300 -- Iteration 21117 - Batch 1862/3851 - Train loss: 0.20480121  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.32 min) Epoch 6/300 -- Iteration 21155 - Batch 1900/3851 - Train loss: 0.20473244  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.33 min) Epoch 6/300 -- Iteration 21193 - Batch 1938/3851 - Train loss: 0.20483900  - Train acc: 0.9234 - Val loss: 0.66149038\n",
      "(4.33 min) Epoch 6/300 -- Iteration 21231 - Batch 1976/3851 - Train loss: 0.20470989  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.34 min) Epoch 6/300 -- Iteration 21269 - Batch 2014/3851 - Train loss: 0.20461656  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.35 min) Epoch 6/300 -- Iteration 21307 - Batch 2052/3851 - Train loss: 0.20457832  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.36 min) Epoch 6/300 -- Iteration 21345 - Batch 2090/3851 - Train loss: 0.20449836  - Train acc: 0.9235 - Val loss: 0.66149038\n",
      "(4.36 min) Epoch 6/300 -- Iteration 21383 - Batch 2128/3851 - Train loss: 0.20407016  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.37 min) Epoch 6/300 -- Iteration 21421 - Batch 2166/3851 - Train loss: 0.20409827  - Train acc: 0.9236 - Val loss: 0.66149038\n",
      "(4.38 min) Epoch 6/300 -- Iteration 21459 - Batch 2204/3851 - Train loss: 0.20396254  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.39 min) Epoch 6/300 -- Iteration 21497 - Batch 2242/3851 - Train loss: 0.20396273  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.40 min) Epoch 6/300 -- Iteration 21535 - Batch 2280/3851 - Train loss: 0.20389498  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.40 min) Epoch 6/300 -- Iteration 21573 - Batch 2318/3851 - Train loss: 0.20393708  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.41 min) Epoch 6/300 -- Iteration 21611 - Batch 2356/3851 - Train loss: 0.20410266  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.42 min) Epoch 6/300 -- Iteration 21649 - Batch 2394/3851 - Train loss: 0.20401829  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.43 min) Epoch 6/300 -- Iteration 21687 - Batch 2432/3851 - Train loss: 0.20402298  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.43 min) Epoch 6/300 -- Iteration 21725 - Batch 2470/3851 - Train loss: 0.20408058  - Train acc: 0.9236 - Val loss: 0.66149038\n",
      "(4.44 min) Epoch 6/300 -- Iteration 21763 - Batch 2508/3851 - Train loss: 0.20411825  - Train acc: 0.9236 - Val loss: 0.66149038\n",
      "(4.45 min) Epoch 6/300 -- Iteration 21801 - Batch 2546/3851 - Train loss: 0.20406649  - Train acc: 0.9236 - Val loss: 0.66149038\n",
      "(4.46 min) Epoch 6/300 -- Iteration 21839 - Batch 2584/3851 - Train loss: 0.20405875  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.46 min) Epoch 6/300 -- Iteration 21877 - Batch 2622/3851 - Train loss: 0.20397353  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.47 min) Epoch 6/300 -- Iteration 21915 - Batch 2660/3851 - Train loss: 0.20391224  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.48 min) Epoch 6/300 -- Iteration 21953 - Batch 2698/3851 - Train loss: 0.20385752  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.49 min) Epoch 6/300 -- Iteration 21991 - Batch 2736/3851 - Train loss: 0.20364433  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.49 min) Epoch 6/300 -- Iteration 22029 - Batch 2774/3851 - Train loss: 0.20370519  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.50 min) Epoch 6/300 -- Iteration 22067 - Batch 2812/3851 - Train loss: 0.20367307  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.51 min) Epoch 6/300 -- Iteration 22105 - Batch 2850/3851 - Train loss: 0.20375212  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.52 min) Epoch 6/300 -- Iteration 22143 - Batch 2888/3851 - Train loss: 0.20364005  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.52 min) Epoch 6/300 -- Iteration 22181 - Batch 2926/3851 - Train loss: 0.20356836  - Train acc: 0.9239 - Val loss: 0.66149038\n",
      "(4.53 min) Epoch 6/300 -- Iteration 22219 - Batch 2964/3851 - Train loss: 0.20377759  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.54 min) Epoch 6/300 -- Iteration 22257 - Batch 3002/3851 - Train loss: 0.20361739  - Train acc: 0.9239 - Val loss: 0.66149038\n",
      "(4.55 min) Epoch 6/300 -- Iteration 22295 - Batch 3040/3851 - Train loss: 0.20353359  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.56 min) Epoch 6/300 -- Iteration 22333 - Batch 3078/3851 - Train loss: 0.20361865  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.56 min) Epoch 6/300 -- Iteration 22371 - Batch 3116/3851 - Train loss: 0.20382256  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.57 min) Epoch 6/300 -- Iteration 22409 - Batch 3154/3851 - Train loss: 0.20386248  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.58 min) Epoch 6/300 -- Iteration 22447 - Batch 3192/3851 - Train loss: 0.20386411  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.59 min) Epoch 6/300 -- Iteration 22485 - Batch 3230/3851 - Train loss: 0.20387347  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.59 min) Epoch 6/300 -- Iteration 22523 - Batch 3268/3851 - Train loss: 0.20383343  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.60 min) Epoch 6/300 -- Iteration 22561 - Batch 3306/3851 - Train loss: 0.20394683  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.61 min) Epoch 6/300 -- Iteration 22599 - Batch 3344/3851 - Train loss: 0.20383504  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.62 min) Epoch 6/300 -- Iteration 22637 - Batch 3382/3851 - Train loss: 0.20389364  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.62 min) Epoch 6/300 -- Iteration 22675 - Batch 3420/3851 - Train loss: 0.20389787  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.63 min) Epoch 6/300 -- Iteration 22713 - Batch 3458/3851 - Train loss: 0.20388593  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.64 min) Epoch 6/300 -- Iteration 22751 - Batch 3496/3851 - Train loss: 0.20393083  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.65 min) Epoch 6/300 -- Iteration 22789 - Batch 3534/3851 - Train loss: 0.20392176  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.65 min) Epoch 6/300 -- Iteration 22827 - Batch 3572/3851 - Train loss: 0.20394269  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.66 min) Epoch 6/300 -- Iteration 22865 - Batch 3610/3851 - Train loss: 0.20404466  - Train acc: 0.9237 - Val loss: 0.66149038\n",
      "(4.67 min) Epoch 6/300 -- Iteration 22903 - Batch 3648/3851 - Train loss: 0.20389921  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.68 min) Epoch 6/300 -- Iteration 22941 - Batch 3686/3851 - Train loss: 0.20385896  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.68 min) Epoch 6/300 -- Iteration 22979 - Batch 3724/3851 - Train loss: 0.20396790  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.69 min) Epoch 6/300 -- Iteration 23017 - Batch 3762/3851 - Train loss: 0.20392800  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.70 min) Epoch 6/300 -- Iteration 23055 - Batch 3800/3851 - Train loss: 0.20396795  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.71 min) Epoch 6/300 -- Iteration 23093 - Batch 3838/3851 - Train loss: 0.20382079  - Train acc: 0.9238 - Val loss: 0.66149038\n",
      "(4.71 min) Epoch 6/300 -- Iteration 23106 - Batch 3850/3851 - Train loss: 0.20390869  - Train acc: 0.9238 - Val loss: 0.74990296 - Val acc: 0.6233\n",
      "(4.72 min) Epoch 7/300 -- Iteration 23144 - Batch 38/3851 - Train loss: 0.20474792  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(4.73 min) Epoch 7/300 -- Iteration 23182 - Batch 76/3851 - Train loss: 0.20864240  - Train acc: 0.9235 - Val loss: 0.74990296\n",
      "(4.73 min) Epoch 7/300 -- Iteration 23220 - Batch 114/3851 - Train loss: 0.20725831  - Train acc: 0.9230 - Val loss: 0.74990296\n",
      "(4.74 min) Epoch 7/300 -- Iteration 23258 - Batch 152/3851 - Train loss: 0.20662767  - Train acc: 0.9237 - Val loss: 0.74990296\n",
      "(4.75 min) Epoch 7/300 -- Iteration 23296 - Batch 190/3851 - Train loss: 0.20384424  - Train acc: 0.9243 - Val loss: 0.74990296\n",
      "(4.76 min) Epoch 7/300 -- Iteration 23334 - Batch 228/3851 - Train loss: 0.20317125  - Train acc: 0.9241 - Val loss: 0.74990296\n",
      "(4.76 min) Epoch 7/300 -- Iteration 23372 - Batch 266/3851 - Train loss: 0.20110381  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(4.77 min) Epoch 7/300 -- Iteration 23410 - Batch 304/3851 - Train loss: 0.20212680  - Train acc: 0.9243 - Val loss: 0.74990296\n",
      "(4.78 min) Epoch 7/300 -- Iteration 23448 - Batch 342/3851 - Train loss: 0.20195070  - Train acc: 0.9241 - Val loss: 0.74990296\n",
      "(4.79 min) Epoch 7/300 -- Iteration 23486 - Batch 380/3851 - Train loss: 0.20279805  - Train acc: 0.9242 - Val loss: 0.74990296\n",
      "(4.80 min) Epoch 7/300 -- Iteration 23524 - Batch 418/3851 - Train loss: 0.20316502  - Train acc: 0.9243 - Val loss: 0.74990296\n",
      "(4.80 min) Epoch 7/300 -- Iteration 23562 - Batch 456/3851 - Train loss: 0.20342431  - Train acc: 0.9244 - Val loss: 0.74990296\n",
      "(4.81 min) Epoch 7/300 -- Iteration 23600 - Batch 494/3851 - Train loss: 0.20310976  - Train acc: 0.9243 - Val loss: 0.74990296\n",
      "(4.82 min) Epoch 7/300 -- Iteration 23638 - Batch 532/3851 - Train loss: 0.20232088  - Train acc: 0.9246 - Val loss: 0.74990296\n",
      "(4.83 min) Epoch 7/300 -- Iteration 23676 - Batch 570/3851 - Train loss: 0.20263878  - Train acc: 0.9244 - Val loss: 0.74990296\n",
      "(4.83 min) Epoch 7/300 -- Iteration 23714 - Batch 608/3851 - Train loss: 0.20339977  - Train acc: 0.9238 - Val loss: 0.74990296\n",
      "(4.84 min) Epoch 7/300 -- Iteration 23752 - Batch 646/3851 - Train loss: 0.20342567  - Train acc: 0.9240 - Val loss: 0.74990296\n",
      "(4.85 min) Epoch 7/300 -- Iteration 23790 - Batch 684/3851 - Train loss: 0.20374662  - Train acc: 0.9236 - Val loss: 0.74990296\n",
      "(4.86 min) Epoch 7/300 -- Iteration 23828 - Batch 722/3851 - Train loss: 0.20448229  - Train acc: 0.9232 - Val loss: 0.74990296\n",
      "(4.86 min) Epoch 7/300 -- Iteration 23866 - Batch 760/3851 - Train loss: 0.20431140  - Train acc: 0.9234 - Val loss: 0.74990296\n",
      "(4.87 min) Epoch 7/300 -- Iteration 23904 - Batch 798/3851 - Train loss: 0.20446140  - Train acc: 0.9233 - Val loss: 0.74990296\n",
      "(4.88 min) Epoch 7/300 -- Iteration 23942 - Batch 836/3851 - Train loss: 0.20432756  - Train acc: 0.9235 - Val loss: 0.74990296\n",
      "(4.89 min) Epoch 7/300 -- Iteration 23980 - Batch 874/3851 - Train loss: 0.20436826  - Train acc: 0.9235 - Val loss: 0.74990296\n",
      "(4.89 min) Epoch 7/300 -- Iteration 24018 - Batch 912/3851 - Train loss: 0.20420196  - Train acc: 0.9235 - Val loss: 0.74990296\n",
      "(4.90 min) Epoch 7/300 -- Iteration 24056 - Batch 950/3851 - Train loss: 0.20392626  - Train acc: 0.9235 - Val loss: 0.74990296\n",
      "(4.91 min) Epoch 7/300 -- Iteration 24094 - Batch 988/3851 - Train loss: 0.20364921  - Train acc: 0.9236 - Val loss: 0.74990296\n",
      "(4.92 min) Epoch 7/300 -- Iteration 24132 - Batch 1026/3851 - Train loss: 0.20374728  - Train acc: 0.9235 - Val loss: 0.74990296\n",
      "(4.92 min) Epoch 7/300 -- Iteration 24170 - Batch 1064/3851 - Train loss: 0.20411488  - Train acc: 0.9233 - Val loss: 0.74990296\n",
      "(4.93 min) Epoch 7/300 -- Iteration 24208 - Batch 1102/3851 - Train loss: 0.20380460  - Train acc: 0.9235 - Val loss: 0.74990296\n",
      "(4.94 min) Epoch 7/300 -- Iteration 24246 - Batch 1140/3851 - Train loss: 0.20355966  - Train acc: 0.9236 - Val loss: 0.74990296\n",
      "(4.95 min) Epoch 7/300 -- Iteration 24284 - Batch 1178/3851 - Train loss: 0.20352293  - Train acc: 0.9236 - Val loss: 0.74990296\n",
      "(4.96 min) Epoch 7/300 -- Iteration 24322 - Batch 1216/3851 - Train loss: 0.20306043  - Train acc: 0.9238 - Val loss: 0.74990296\n",
      "(4.96 min) Epoch 7/300 -- Iteration 24360 - Batch 1254/3851 - Train loss: 0.20298594  - Train acc: 0.9239 - Val loss: 0.74990296\n",
      "(4.97 min) Epoch 7/300 -- Iteration 24398 - Batch 1292/3851 - Train loss: 0.20279688  - Train acc: 0.9241 - Val loss: 0.74990296\n",
      "(4.98 min) Epoch 7/300 -- Iteration 24436 - Batch 1330/3851 - Train loss: 0.20258936  - Train acc: 0.9241 - Val loss: 0.74990296\n",
      "(4.99 min) Epoch 7/300 -- Iteration 24474 - Batch 1368/3851 - Train loss: 0.20212641  - Train acc: 0.9243 - Val loss: 0.74990296\n",
      "(4.99 min) Epoch 7/300 -- Iteration 24512 - Batch 1406/3851 - Train loss: 0.20190972  - Train acc: 0.9243 - Val loss: 0.74990296\n",
      "(5.00 min) Epoch 7/300 -- Iteration 24550 - Batch 1444/3851 - Train loss: 0.20157530  - Train acc: 0.9245 - Val loss: 0.74990296\n",
      "(5.01 min) Epoch 7/300 -- Iteration 24588 - Batch 1482/3851 - Train loss: 0.20126259  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.02 min) Epoch 7/300 -- Iteration 24626 - Batch 1520/3851 - Train loss: 0.20122202  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.02 min) Epoch 7/300 -- Iteration 24664 - Batch 1558/3851 - Train loss: 0.20132814  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.03 min) Epoch 7/300 -- Iteration 24702 - Batch 1596/3851 - Train loss: 0.20154997  - Train acc: 0.9246 - Val loss: 0.74990296\n",
      "(5.04 min) Epoch 7/300 -- Iteration 24740 - Batch 1634/3851 - Train loss: 0.20136570  - Train acc: 0.9246 - Val loss: 0.74990296\n",
      "(5.05 min) Epoch 7/300 -- Iteration 24778 - Batch 1672/3851 - Train loss: 0.20178058  - Train acc: 0.9245 - Val loss: 0.74990296\n",
      "(5.05 min) Epoch 7/300 -- Iteration 24816 - Batch 1710/3851 - Train loss: 0.20142705  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.06 min) Epoch 7/300 -- Iteration 24854 - Batch 1748/3851 - Train loss: 0.20153292  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.07 min) Epoch 7/300 -- Iteration 24892 - Batch 1786/3851 - Train loss: 0.20161019  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.08 min) Epoch 7/300 -- Iteration 24930 - Batch 1824/3851 - Train loss: 0.20146149  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.09 min) Epoch 7/300 -- Iteration 24968 - Batch 1862/3851 - Train loss: 0.20149792  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.09 min) Epoch 7/300 -- Iteration 25006 - Batch 1900/3851 - Train loss: 0.20147404  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.10 min) Epoch 7/300 -- Iteration 25044 - Batch 1938/3851 - Train loss: 0.20141304  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.11 min) Epoch 7/300 -- Iteration 25082 - Batch 1976/3851 - Train loss: 0.20143051  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.12 min) Epoch 7/300 -- Iteration 25120 - Batch 2014/3851 - Train loss: 0.20130969  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.12 min) Epoch 7/300 -- Iteration 25158 - Batch 2052/3851 - Train loss: 0.20149654  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.13 min) Epoch 7/300 -- Iteration 25196 - Batch 2090/3851 - Train loss: 0.20121620  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.14 min) Epoch 7/300 -- Iteration 25234 - Batch 2128/3851 - Train loss: 0.20133341  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.15 min) Epoch 7/300 -- Iteration 25272 - Batch 2166/3851 - Train loss: 0.20122747  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.15 min) Epoch 7/300 -- Iteration 25310 - Batch 2204/3851 - Train loss: 0.20105247  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(5.16 min) Epoch 7/300 -- Iteration 25348 - Batch 2242/3851 - Train loss: 0.20110126  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.17 min) Epoch 7/300 -- Iteration 25386 - Batch 2280/3851 - Train loss: 0.20116427  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.18 min) Epoch 7/300 -- Iteration 25424 - Batch 2318/3851 - Train loss: 0.20127377  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.18 min) Epoch 7/300 -- Iteration 25462 - Batch 2356/3851 - Train loss: 0.20124651  - Train acc: 0.9247 - Val loss: 0.74990296\n",
      "(5.19 min) Epoch 7/300 -- Iteration 25500 - Batch 2394/3851 - Train loss: 0.20115904  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.20 min) Epoch 7/300 -- Iteration 25538 - Batch 2432/3851 - Train loss: 0.20112717  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.21 min) Epoch 7/300 -- Iteration 25576 - Batch 2470/3851 - Train loss: 0.20116312  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.21 min) Epoch 7/300 -- Iteration 25614 - Batch 2508/3851 - Train loss: 0.20110294  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.22 min) Epoch 7/300 -- Iteration 25652 - Batch 2546/3851 - Train loss: 0.20097762  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(5.23 min) Epoch 7/300 -- Iteration 25690 - Batch 2584/3851 - Train loss: 0.20104449  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(5.24 min) Epoch 7/300 -- Iteration 25728 - Batch 2622/3851 - Train loss: 0.20116324  - Train acc: 0.9248 - Val loss: 0.74990296\n",
      "(5.25 min) Epoch 7/300 -- Iteration 25766 - Batch 2660/3851 - Train loss: 0.20091428  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(5.25 min) Epoch 7/300 -- Iteration 25804 - Batch 2698/3851 - Train loss: 0.20094890  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(5.26 min) Epoch 7/300 -- Iteration 25842 - Batch 2736/3851 - Train loss: 0.20093193  - Train acc: 0.9250 - Val loss: 0.74990296\n",
      "(5.27 min) Epoch 7/300 -- Iteration 25880 - Batch 2774/3851 - Train loss: 0.20081213  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.28 min) Epoch 7/300 -- Iteration 25918 - Batch 2812/3851 - Train loss: 0.20065475  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.28 min) Epoch 7/300 -- Iteration 25956 - Batch 2850/3851 - Train loss: 0.20058892  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.29 min) Epoch 7/300 -- Iteration 25994 - Batch 2888/3851 - Train loss: 0.20051239  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.30 min) Epoch 7/300 -- Iteration 26032 - Batch 2926/3851 - Train loss: 0.20077896  - Train acc: 0.9250 - Val loss: 0.74990296\n",
      "(5.31 min) Epoch 7/300 -- Iteration 26070 - Batch 2964/3851 - Train loss: 0.20085323  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(5.31 min) Epoch 7/300 -- Iteration 26108 - Batch 3002/3851 - Train loss: 0.20089107  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(5.32 min) Epoch 7/300 -- Iteration 26146 - Batch 3040/3851 - Train loss: 0.20083116  - Train acc: 0.9249 - Val loss: 0.74990296\n",
      "(5.33 min) Epoch 7/300 -- Iteration 26184 - Batch 3078/3851 - Train loss: 0.20069342  - Train acc: 0.9250 - Val loss: 0.74990296\n",
      "(5.34 min) Epoch 7/300 -- Iteration 26222 - Batch 3116/3851 - Train loss: 0.20058327  - Train acc: 0.9250 - Val loss: 0.74990296\n",
      "(5.34 min) Epoch 7/300 -- Iteration 26260 - Batch 3154/3851 - Train loss: 0.20062528  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.35 min) Epoch 7/300 -- Iteration 26298 - Batch 3192/3851 - Train loss: 0.20045453  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.36 min) Epoch 7/300 -- Iteration 26336 - Batch 3230/3851 - Train loss: 0.20028125  - Train acc: 0.9252 - Val loss: 0.74990296\n",
      "(5.37 min) Epoch 7/300 -- Iteration 26374 - Batch 3268/3851 - Train loss: 0.20035768  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.37 min) Epoch 7/300 -- Iteration 26412 - Batch 3306/3851 - Train loss: 0.20034135  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.38 min) Epoch 7/300 -- Iteration 26450 - Batch 3344/3851 - Train loss: 0.20043816  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.39 min) Epoch 7/300 -- Iteration 26488 - Batch 3382/3851 - Train loss: 0.20051426  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.40 min) Epoch 7/300 -- Iteration 26526 - Batch 3420/3851 - Train loss: 0.20041908  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.40 min) Epoch 7/300 -- Iteration 26564 - Batch 3458/3851 - Train loss: 0.20038415  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.41 min) Epoch 7/300 -- Iteration 26602 - Batch 3496/3851 - Train loss: 0.20023969  - Train acc: 0.9252 - Val loss: 0.74990296\n",
      "(5.42 min) Epoch 7/300 -- Iteration 26640 - Batch 3534/3851 - Train loss: 0.20025250  - Train acc: 0.9252 - Val loss: 0.74990296\n",
      "(5.43 min) Epoch 7/300 -- Iteration 26678 - Batch 3572/3851 - Train loss: 0.20047044  - Train acc: 0.9250 - Val loss: 0.74990296\n",
      "(5.44 min) Epoch 7/300 -- Iteration 26716 - Batch 3610/3851 - Train loss: 0.20055361  - Train acc: 0.9250 - Val loss: 0.74990296\n",
      "(5.44 min) Epoch 7/300 -- Iteration 26754 - Batch 3648/3851 - Train loss: 0.20055202  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.45 min) Epoch 7/300 -- Iteration 26792 - Batch 3686/3851 - Train loss: 0.20046434  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.46 min) Epoch 7/300 -- Iteration 26830 - Batch 3724/3851 - Train loss: 0.20060355  - Train acc: 0.9250 - Val loss: 0.74990296\n",
      "(5.47 min) Epoch 7/300 -- Iteration 26868 - Batch 3762/3851 - Train loss: 0.20061881  - Train acc: 0.9250 - Val loss: 0.74990296\n",
      "(5.47 min) Epoch 7/300 -- Iteration 26906 - Batch 3800/3851 - Train loss: 0.20051508  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.48 min) Epoch 7/300 -- Iteration 26944 - Batch 3838/3851 - Train loss: 0.20053686  - Train acc: 0.9251 - Val loss: 0.74990296\n",
      "(5.48 min) Epoch 7/300 -- Iteration 26957 - Batch 3850/3851 - Train loss: 0.20053111  - Train acc: 0.9251 - Val loss: 0.70115739 - Val acc: 0.6433\n",
      "(5.49 min) Epoch 8/300 -- Iteration 26995 - Batch 38/3851 - Train loss: 0.19848166  - Train acc: 0.9227 - Val loss: 0.70115739\n",
      "(5.50 min) Epoch 8/300 -- Iteration 27033 - Batch 76/3851 - Train loss: 0.20040941  - Train acc: 0.9245 - Val loss: 0.70115739\n",
      "(5.51 min) Epoch 8/300 -- Iteration 27071 - Batch 114/3851 - Train loss: 0.19890307  - Train acc: 0.9254 - Val loss: 0.70115739\n",
      "(5.52 min) Epoch 8/300 -- Iteration 27109 - Batch 152/3851 - Train loss: 0.19869345  - Train acc: 0.9248 - Val loss: 0.70115739\n",
      "(5.52 min) Epoch 8/300 -- Iteration 27147 - Batch 190/3851 - Train loss: 0.19852629  - Train acc: 0.9247 - Val loss: 0.70115739\n",
      "(5.53 min) Epoch 8/300 -- Iteration 27185 - Batch 228/3851 - Train loss: 0.19915449  - Train acc: 0.9241 - Val loss: 0.70115739\n",
      "(5.54 min) Epoch 8/300 -- Iteration 27223 - Batch 266/3851 - Train loss: 0.19948829  - Train acc: 0.9241 - Val loss: 0.70115739\n",
      "(5.55 min) Epoch 8/300 -- Iteration 27261 - Batch 304/3851 - Train loss: 0.19875236  - Train acc: 0.9246 - Val loss: 0.70115739\n",
      "(5.55 min) Epoch 8/300 -- Iteration 27299 - Batch 342/3851 - Train loss: 0.19828637  - Train acc: 0.9249 - Val loss: 0.70115739\n",
      "(5.56 min) Epoch 8/300 -- Iteration 27337 - Batch 380/3851 - Train loss: 0.19778899  - Train acc: 0.9253 - Val loss: 0.70115739\n",
      "(5.57 min) Epoch 8/300 -- Iteration 27375 - Batch 418/3851 - Train loss: 0.19822467  - Train acc: 0.9252 - Val loss: 0.70115739\n",
      "(5.58 min) Epoch 8/300 -- Iteration 27413 - Batch 456/3851 - Train loss: 0.19851449  - Train acc: 0.9253 - Val loss: 0.70115739\n",
      "(5.59 min) Epoch 8/300 -- Iteration 27451 - Batch 494/3851 - Train loss: 0.19757736  - Train acc: 0.9257 - Val loss: 0.70115739\n",
      "(5.59 min) Epoch 8/300 -- Iteration 27489 - Batch 532/3851 - Train loss: 0.19767452  - Train acc: 0.9256 - Val loss: 0.70115739\n",
      "(5.60 min) Epoch 8/300 -- Iteration 27527 - Batch 570/3851 - Train loss: 0.19829732  - Train acc: 0.9256 - Val loss: 0.70115739\n",
      "(5.61 min) Epoch 8/300 -- Iteration 27565 - Batch 608/3851 - Train loss: 0.19865029  - Train acc: 0.9256 - Val loss: 0.70115739\n",
      "(5.62 min) Epoch 8/300 -- Iteration 27603 - Batch 646/3851 - Train loss: 0.19862454  - Train acc: 0.9257 - Val loss: 0.70115739\n",
      "(5.62 min) Epoch 8/300 -- Iteration 27641 - Batch 684/3851 - Train loss: 0.19822146  - Train acc: 0.9256 - Val loss: 0.70115739\n",
      "(5.63 min) Epoch 8/300 -- Iteration 27679 - Batch 722/3851 - Train loss: 0.19844341  - Train acc: 0.9256 - Val loss: 0.70115739\n",
      "(5.64 min) Epoch 8/300 -- Iteration 27717 - Batch 760/3851 - Train loss: 0.19832980  - Train acc: 0.9256 - Val loss: 0.70115739\n",
      "(5.65 min) Epoch 8/300 -- Iteration 27755 - Batch 798/3851 - Train loss: 0.19819891  - Train acc: 0.9258 - Val loss: 0.70115739\n",
      "(5.65 min) Epoch 8/300 -- Iteration 27793 - Batch 836/3851 - Train loss: 0.19802708  - Train acc: 0.9258 - Val loss: 0.70115739\n",
      "(5.66 min) Epoch 8/300 -- Iteration 27831 - Batch 874/3851 - Train loss: 0.19695459  - Train acc: 0.9262 - Val loss: 0.70115739\n",
      "(5.67 min) Epoch 8/300 -- Iteration 27869 - Batch 912/3851 - Train loss: 0.19688805  - Train acc: 0.9261 - Val loss: 0.70115739\n",
      "(5.68 min) Epoch 8/300 -- Iteration 27907 - Batch 950/3851 - Train loss: 0.19688086  - Train acc: 0.9261 - Val loss: 0.70115739\n",
      "(5.68 min) Epoch 8/300 -- Iteration 27945 - Batch 988/3851 - Train loss: 0.19677883  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.69 min) Epoch 8/300 -- Iteration 27983 - Batch 1026/3851 - Train loss: 0.19706255  - Train acc: 0.9258 - Val loss: 0.70115739\n",
      "(5.70 min) Epoch 8/300 -- Iteration 28021 - Batch 1064/3851 - Train loss: 0.19735204  - Train acc: 0.9258 - Val loss: 0.70115739\n",
      "(5.71 min) Epoch 8/300 -- Iteration 28059 - Batch 1102/3851 - Train loss: 0.19747617  - Train acc: 0.9258 - Val loss: 0.70115739\n",
      "(5.71 min) Epoch 8/300 -- Iteration 28097 - Batch 1140/3851 - Train loss: 0.19742551  - Train acc: 0.9257 - Val loss: 0.70115739\n",
      "(5.72 min) Epoch 8/300 -- Iteration 28135 - Batch 1178/3851 - Train loss: 0.19761355  - Train acc: 0.9257 - Val loss: 0.70115739\n",
      "(5.73 min) Epoch 8/300 -- Iteration 28173 - Batch 1216/3851 - Train loss: 0.19735839  - Train acc: 0.9257 - Val loss: 0.70115739\n",
      "(5.74 min) Epoch 8/300 -- Iteration 28211 - Batch 1254/3851 - Train loss: 0.19726136  - Train acc: 0.9257 - Val loss: 0.70115739\n",
      "(5.74 min) Epoch 8/300 -- Iteration 28249 - Batch 1292/3851 - Train loss: 0.19734332  - Train acc: 0.9258 - Val loss: 0.70115739\n",
      "(5.75 min) Epoch 8/300 -- Iteration 28287 - Batch 1330/3851 - Train loss: 0.19758697  - Train acc: 0.9257 - Val loss: 0.70115739\n",
      "(5.76 min) Epoch 8/300 -- Iteration 28325 - Batch 1368/3851 - Train loss: 0.19755175  - Train acc: 0.9258 - Val loss: 0.70115739\n",
      "(5.77 min) Epoch 8/300 -- Iteration 28363 - Batch 1406/3851 - Train loss: 0.19746795  - Train acc: 0.9258 - Val loss: 0.70115739\n",
      "(5.78 min) Epoch 8/300 -- Iteration 28401 - Batch 1444/3851 - Train loss: 0.19731504  - Train acc: 0.9259 - Val loss: 0.70115739\n",
      "(5.78 min) Epoch 8/300 -- Iteration 28439 - Batch 1482/3851 - Train loss: 0.19716449  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.79 min) Epoch 8/300 -- Iteration 28477 - Batch 1520/3851 - Train loss: 0.19716631  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.80 min) Epoch 8/300 -- Iteration 28515 - Batch 1558/3851 - Train loss: 0.19671546  - Train acc: 0.9262 - Val loss: 0.70115739\n",
      "(5.81 min) Epoch 8/300 -- Iteration 28553 - Batch 1596/3851 - Train loss: 0.19682273  - Train acc: 0.9261 - Val loss: 0.70115739\n",
      "(5.81 min) Epoch 8/300 -- Iteration 28591 - Batch 1634/3851 - Train loss: 0.19666206  - Train acc: 0.9262 - Val loss: 0.70115739\n",
      "(5.82 min) Epoch 8/300 -- Iteration 28629 - Batch 1672/3851 - Train loss: 0.19682671  - Train acc: 0.9261 - Val loss: 0.70115739\n",
      "(5.83 min) Epoch 8/300 -- Iteration 28667 - Batch 1710/3851 - Train loss: 0.19693251  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.84 min) Epoch 8/300 -- Iteration 28705 - Batch 1748/3851 - Train loss: 0.19710711  - Train acc: 0.9261 - Val loss: 0.70115739\n",
      "(5.84 min) Epoch 8/300 -- Iteration 28743 - Batch 1786/3851 - Train loss: 0.19692968  - Train acc: 0.9261 - Val loss: 0.70115739\n",
      "(5.85 min) Epoch 8/300 -- Iteration 28781 - Batch 1824/3851 - Train loss: 0.19717098  - Train acc: 0.9259 - Val loss: 0.70115739\n",
      "(5.86 min) Epoch 8/300 -- Iteration 28819 - Batch 1862/3851 - Train loss: 0.19702733  - Train acc: 0.9259 - Val loss: 0.70115739\n",
      "(5.87 min) Epoch 8/300 -- Iteration 28857 - Batch 1900/3851 - Train loss: 0.19696182  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.87 min) Epoch 8/300 -- Iteration 28895 - Batch 1938/3851 - Train loss: 0.19709764  - Train acc: 0.9259 - Val loss: 0.70115739\n",
      "(5.88 min) Epoch 8/300 -- Iteration 28933 - Batch 1976/3851 - Train loss: 0.19720156  - Train acc: 0.9258 - Val loss: 0.70115739\n",
      "(5.89 min) Epoch 8/300 -- Iteration 28971 - Batch 2014/3851 - Train loss: 0.19684497  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.90 min) Epoch 8/300 -- Iteration 29009 - Batch 2052/3851 - Train loss: 0.19672412  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.90 min) Epoch 8/300 -- Iteration 29047 - Batch 2090/3851 - Train loss: 0.19665098  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.91 min) Epoch 8/300 -- Iteration 29085 - Batch 2128/3851 - Train loss: 0.19666489  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.92 min) Epoch 8/300 -- Iteration 29123 - Batch 2166/3851 - Train loss: 0.19667418  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.93 min) Epoch 8/300 -- Iteration 29161 - Batch 2204/3851 - Train loss: 0.19677758  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.94 min) Epoch 8/300 -- Iteration 29199 - Batch 2242/3851 - Train loss: 0.19664886  - Train acc: 0.9260 - Val loss: 0.70115739\n",
      "(5.94 min) Epoch 8/300 -- Iteration 29237 - Batch 2280/3851 - Train loss: 0.19656454  - Train acc: 0.9261 - Val loss: 0.70115739\n",
      "(5.95 min) Epoch 8/300 -- Iteration 29275 - Batch 2318/3851 - Train loss: 0.19662505  - Train acc: 0.9261 - Val loss: 0.70115739\n",
      "(5.96 min) Epoch 8/300 -- Iteration 29313 - Batch 2356/3851 - Train loss: 0.19655069  - Train acc: 0.9262 - Val loss: 0.70115739\n",
      "(5.97 min) Epoch 8/300 -- Iteration 29351 - Batch 2394/3851 - Train loss: 0.19649736  - Train acc: 0.9263 - Val loss: 0.70115739\n",
      "(5.97 min) Epoch 8/300 -- Iteration 29389 - Batch 2432/3851 - Train loss: 0.19663808  - Train acc: 0.9263 - Val loss: 0.70115739\n",
      "(5.98 min) Epoch 8/300 -- Iteration 29427 - Batch 2470/3851 - Train loss: 0.19669919  - Train acc: 0.9262 - Val loss: 0.70115739\n",
      "(5.99 min) Epoch 8/300 -- Iteration 29465 - Batch 2508/3851 - Train loss: 0.19679354  - Train acc: 0.9262 - Val loss: 0.70115739\n",
      "(6.00 min) Epoch 8/300 -- Iteration 29503 - Batch 2546/3851 - Train loss: 0.19694055  - Train acc: 0.9261 - Val loss: 0.70115739\n",
      "(6.00 min) Epoch 8/300 -- Iteration 29541 - Batch 2584/3851 - Train loss: 0.19684151  - Train acc: 0.9262 - Val loss: 0.70115739\n",
      "(6.01 min) Epoch 8/300 -- Iteration 29579 - Batch 2622/3851 - Train loss: 0.19671028  - Train acc: 0.9262 - Val loss: 0.70115739\n",
      "(6.02 min) Epoch 8/300 -- Iteration 29617 - Batch 2660/3851 - Train loss: 0.19673300  - Train acc: 0.9262 - Val loss: 0.70115739\n",
      "(6.03 min) Epoch 8/300 -- Iteration 29655 - Batch 2698/3851 - Train loss: 0.19653709  - Train acc: 0.9263 - Val loss: 0.70115739\n",
      "(6.03 min) Epoch 8/300 -- Iteration 29693 - Batch 2736/3851 - Train loss: 0.19652221  - Train acc: 0.9263 - Val loss: 0.70115739\n",
      "(6.04 min) Epoch 8/300 -- Iteration 29731 - Batch 2774/3851 - Train loss: 0.19659477  - Train acc: 0.9263 - Val loss: 0.70115739\n",
      "(6.05 min) Epoch 8/300 -- Iteration 29769 - Batch 2812/3851 - Train loss: 0.19640203  - Train acc: 0.9264 - Val loss: 0.70115739\n",
      "(6.06 min) Epoch 8/300 -- Iteration 29807 - Batch 2850/3851 - Train loss: 0.19644090  - Train acc: 0.9263 - Val loss: 0.70115739\n",
      "(6.06 min) Epoch 8/300 -- Iteration 29845 - Batch 2888/3851 - Train loss: 0.19638761  - Train acc: 0.9264 - Val loss: 0.70115739\n",
      "(6.07 min) Epoch 8/300 -- Iteration 29883 - Batch 2926/3851 - Train loss: 0.19631707  - Train acc: 0.9264 - Val loss: 0.70115739\n",
      "(6.08 min) Epoch 8/300 -- Iteration 29921 - Batch 2964/3851 - Train loss: 0.19618318  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.09 min) Epoch 8/300 -- Iteration 29959 - Batch 3002/3851 - Train loss: 0.19612062  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.09 min) Epoch 8/300 -- Iteration 29997 - Batch 3040/3851 - Train loss: 0.19620559  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.10 min) Epoch 8/300 -- Iteration 30035 - Batch 3078/3851 - Train loss: 0.19597282  - Train acc: 0.9266 - Val loss: 0.70115739\n",
      "(6.11 min) Epoch 8/300 -- Iteration 30073 - Batch 3116/3851 - Train loss: 0.19600040  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.12 min) Epoch 8/300 -- Iteration 30111 - Batch 3154/3851 - Train loss: 0.19600206  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.12 min) Epoch 8/300 -- Iteration 30149 - Batch 3192/3851 - Train loss: 0.19609527  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.13 min) Epoch 8/300 -- Iteration 30187 - Batch 3230/3851 - Train loss: 0.19603550  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.14 min) Epoch 8/300 -- Iteration 30225 - Batch 3268/3851 - Train loss: 0.19602185  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.15 min) Epoch 8/300 -- Iteration 30263 - Batch 3306/3851 - Train loss: 0.19595640  - Train acc: 0.9266 - Val loss: 0.70115739\n",
      "(6.16 min) Epoch 8/300 -- Iteration 30301 - Batch 3344/3851 - Train loss: 0.19605471  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.16 min) Epoch 8/300 -- Iteration 30339 - Batch 3382/3851 - Train loss: 0.19605982  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.17 min) Epoch 8/300 -- Iteration 30377 - Batch 3420/3851 - Train loss: 0.19600855  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.18 min) Epoch 8/300 -- Iteration 30415 - Batch 3458/3851 - Train loss: 0.19605628  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.19 min) Epoch 8/300 -- Iteration 30453 - Batch 3496/3851 - Train loss: 0.19607510  - Train acc: 0.9264 - Val loss: 0.70115739\n",
      "(6.19 min) Epoch 8/300 -- Iteration 30491 - Batch 3534/3851 - Train loss: 0.19599950  - Train acc: 0.9264 - Val loss: 0.70115739\n",
      "(6.20 min) Epoch 8/300 -- Iteration 30529 - Batch 3572/3851 - Train loss: 0.19592551  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.21 min) Epoch 8/300 -- Iteration 30567 - Batch 3610/3851 - Train loss: 0.19579409  - Train acc: 0.9265 - Val loss: 0.70115739\n",
      "(6.22 min) Epoch 8/300 -- Iteration 30605 - Batch 3648/3851 - Train loss: 0.19567792  - Train acc: 0.9266 - Val loss: 0.70115739\n",
      "(6.22 min) Epoch 8/300 -- Iteration 30643 - Batch 3686/3851 - Train loss: 0.19564129  - Train acc: 0.9266 - Val loss: 0.70115739\n",
      "(6.23 min) Epoch 8/300 -- Iteration 30681 - Batch 3724/3851 - Train loss: 0.19558099  - Train acc: 0.9267 - Val loss: 0.70115739\n",
      "(6.24 min) Epoch 8/300 -- Iteration 30719 - Batch 3762/3851 - Train loss: 0.19566164  - Train acc: 0.9266 - Val loss: 0.70115739\n",
      "(6.25 min) Epoch 8/300 -- Iteration 30757 - Batch 3800/3851 - Train loss: 0.19562206  - Train acc: 0.9267 - Val loss: 0.70115739\n",
      "(6.25 min) Epoch 8/300 -- Iteration 30795 - Batch 3838/3851 - Train loss: 0.19564385  - Train acc: 0.9267 - Val loss: 0.70115739\n",
      "(6.26 min) Epoch 8/300 -- Iteration 30808 - Batch 3850/3851 - Train loss: 0.19564385  - Train acc: 0.9267 - Val loss: 0.72253627 - Val acc: 0.6383\n",
      "(6.27 min) Epoch 9/300 -- Iteration 30846 - Batch 38/3851 - Train loss: 0.18999091  - Train acc: 0.9323 - Val loss: 0.72253627\n",
      "(6.27 min) Epoch 9/300 -- Iteration 30884 - Batch 76/3851 - Train loss: 0.19411766  - Train acc: 0.9276 - Val loss: 0.72253627\n",
      "(6.28 min) Epoch 9/300 -- Iteration 30922 - Batch 114/3851 - Train loss: 0.19602126  - Train acc: 0.9270 - Val loss: 0.72253627\n",
      "(6.29 min) Epoch 9/300 -- Iteration 30960 - Batch 152/3851 - Train loss: 0.19832732  - Train acc: 0.9254 - Val loss: 0.72253627\n",
      "(6.30 min) Epoch 9/300 -- Iteration 30998 - Batch 190/3851 - Train loss: 0.19610029  - Train acc: 0.9265 - Val loss: 0.72253627\n",
      "(6.30 min) Epoch 9/300 -- Iteration 31036 - Batch 228/3851 - Train loss: 0.19735969  - Train acc: 0.9268 - Val loss: 0.72253627\n",
      "(6.31 min) Epoch 9/300 -- Iteration 31074 - Batch 266/3851 - Train loss: 0.19597853  - Train acc: 0.9274 - Val loss: 0.72253627\n",
      "(6.32 min) Epoch 9/300 -- Iteration 31112 - Batch 304/3851 - Train loss: 0.19456529  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.33 min) Epoch 9/300 -- Iteration 31150 - Batch 342/3851 - Train loss: 0.19555968  - Train acc: 0.9274 - Val loss: 0.72253627\n",
      "(6.33 min) Epoch 9/300 -- Iteration 31188 - Batch 380/3851 - Train loss: 0.19578353  - Train acc: 0.9272 - Val loss: 0.72253627\n",
      "(6.34 min) Epoch 9/300 -- Iteration 31226 - Batch 418/3851 - Train loss: 0.19516004  - Train acc: 0.9276 - Val loss: 0.72253627\n",
      "(6.35 min) Epoch 9/300 -- Iteration 31264 - Batch 456/3851 - Train loss: 0.19442144  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.36 min) Epoch 9/300 -- Iteration 31302 - Batch 494/3851 - Train loss: 0.19314753  - Train acc: 0.9284 - Val loss: 0.72253627\n",
      "(6.36 min) Epoch 9/300 -- Iteration 31340 - Batch 532/3851 - Train loss: 0.19296726  - Train acc: 0.9283 - Val loss: 0.72253627\n",
      "(6.37 min) Epoch 9/300 -- Iteration 31378 - Batch 570/3851 - Train loss: 0.19315307  - Train acc: 0.9283 - Val loss: 0.72253627\n",
      "(6.38 min) Epoch 9/300 -- Iteration 31416 - Batch 608/3851 - Train loss: 0.19360376  - Train acc: 0.9282 - Val loss: 0.72253627\n",
      "(6.39 min) Epoch 9/300 -- Iteration 31454 - Batch 646/3851 - Train loss: 0.19406862  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.40 min) Epoch 9/300 -- Iteration 31492 - Batch 684/3851 - Train loss: 0.19386278  - Train acc: 0.9282 - Val loss: 0.72253627\n",
      "(6.40 min) Epoch 9/300 -- Iteration 31530 - Batch 722/3851 - Train loss: 0.19393082  - Train acc: 0.9283 - Val loss: 0.72253627\n",
      "(6.41 min) Epoch 9/300 -- Iteration 31568 - Batch 760/3851 - Train loss: 0.19397356  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.42 min) Epoch 9/300 -- Iteration 31606 - Batch 798/3851 - Train loss: 0.19359329  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.43 min) Epoch 9/300 -- Iteration 31644 - Batch 836/3851 - Train loss: 0.19407499  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.43 min) Epoch 9/300 -- Iteration 31682 - Batch 874/3851 - Train loss: 0.19399891  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.44 min) Epoch 9/300 -- Iteration 31720 - Batch 912/3851 - Train loss: 0.19401390  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.45 min) Epoch 9/300 -- Iteration 31758 - Batch 950/3851 - Train loss: 0.19426340  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.46 min) Epoch 9/300 -- Iteration 31796 - Batch 988/3851 - Train loss: 0.19417964  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.46 min) Epoch 9/300 -- Iteration 31834 - Batch 1026/3851 - Train loss: 0.19387495  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.47 min) Epoch 9/300 -- Iteration 31872 - Batch 1064/3851 - Train loss: 0.19386730  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.48 min) Epoch 9/300 -- Iteration 31910 - Batch 1102/3851 - Train loss: 0.19385297  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.49 min) Epoch 9/300 -- Iteration 31948 - Batch 1140/3851 - Train loss: 0.19357205  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.49 min) Epoch 9/300 -- Iteration 31986 - Batch 1178/3851 - Train loss: 0.19374170  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.50 min) Epoch 9/300 -- Iteration 32024 - Batch 1216/3851 - Train loss: 0.19386339  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.51 min) Epoch 9/300 -- Iteration 32062 - Batch 1254/3851 - Train loss: 0.19416654  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.52 min) Epoch 9/300 -- Iteration 32100 - Batch 1292/3851 - Train loss: 0.19448223  - Train acc: 0.9276 - Val loss: 0.72253627\n",
      "(6.52 min) Epoch 9/300 -- Iteration 32138 - Batch 1330/3851 - Train loss: 0.19439523  - Train acc: 0.9276 - Val loss: 0.72253627\n",
      "(6.53 min) Epoch 9/300 -- Iteration 32176 - Batch 1368/3851 - Train loss: 0.19443718  - Train acc: 0.9277 - Val loss: 0.72253627\n",
      "(6.54 min) Epoch 9/300 -- Iteration 32214 - Batch 1406/3851 - Train loss: 0.19481391  - Train acc: 0.9275 - Val loss: 0.72253627\n",
      "(6.55 min) Epoch 9/300 -- Iteration 32252 - Batch 1444/3851 - Train loss: 0.19494203  - Train acc: 0.9274 - Val loss: 0.72253627\n",
      "(6.55 min) Epoch 9/300 -- Iteration 32290 - Batch 1482/3851 - Train loss: 0.19474084  - Train acc: 0.9275 - Val loss: 0.72253627\n",
      "(6.56 min) Epoch 9/300 -- Iteration 32328 - Batch 1520/3851 - Train loss: 0.19472907  - Train acc: 0.9275 - Val loss: 0.72253627\n",
      "(6.57 min) Epoch 9/300 -- Iteration 32366 - Batch 1558/3851 - Train loss: 0.19444893  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.58 min) Epoch 9/300 -- Iteration 32404 - Batch 1596/3851 - Train loss: 0.19433100  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.58 min) Epoch 9/300 -- Iteration 32442 - Batch 1634/3851 - Train loss: 0.19411561  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.59 min) Epoch 9/300 -- Iteration 32480 - Batch 1672/3851 - Train loss: 0.19426345  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.60 min) Epoch 9/300 -- Iteration 32518 - Batch 1710/3851 - Train loss: 0.19426393  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.61 min) Epoch 9/300 -- Iteration 32556 - Batch 1748/3851 - Train loss: 0.19404037  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.62 min) Epoch 9/300 -- Iteration 32594 - Batch 1786/3851 - Train loss: 0.19392844  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.62 min) Epoch 9/300 -- Iteration 32632 - Batch 1824/3851 - Train loss: 0.19393385  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.63 min) Epoch 9/300 -- Iteration 32670 - Batch 1862/3851 - Train loss: 0.19378007  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.64 min) Epoch 9/300 -- Iteration 32708 - Batch 1900/3851 - Train loss: 0.19372207  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.65 min) Epoch 9/300 -- Iteration 32746 - Batch 1938/3851 - Train loss: 0.19408340  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.65 min) Epoch 9/300 -- Iteration 32784 - Batch 1976/3851 - Train loss: 0.19392165  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.66 min) Epoch 9/300 -- Iteration 32822 - Batch 2014/3851 - Train loss: 0.19397566  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.67 min) Epoch 9/300 -- Iteration 32860 - Batch 2052/3851 - Train loss: 0.19370946  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.68 min) Epoch 9/300 -- Iteration 32898 - Batch 2090/3851 - Train loss: 0.19372245  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.68 min) Epoch 9/300 -- Iteration 32936 - Batch 2128/3851 - Train loss: 0.19367468  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.69 min) Epoch 9/300 -- Iteration 32974 - Batch 2166/3851 - Train loss: 0.19349579  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.70 min) Epoch 9/300 -- Iteration 33012 - Batch 2204/3851 - Train loss: 0.19356497  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.71 min) Epoch 9/300 -- Iteration 33050 - Batch 2242/3851 - Train loss: 0.19350725  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.71 min) Epoch 9/300 -- Iteration 33088 - Batch 2280/3851 - Train loss: 0.19344588  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.72 min) Epoch 9/300 -- Iteration 33126 - Batch 2318/3851 - Train loss: 0.19351873  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.73 min) Epoch 9/300 -- Iteration 33164 - Batch 2356/3851 - Train loss: 0.19334145  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.74 min) Epoch 9/300 -- Iteration 33202 - Batch 2394/3851 - Train loss: 0.19325230  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.75 min) Epoch 9/300 -- Iteration 33240 - Batch 2432/3851 - Train loss: 0.19340947  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.75 min) Epoch 9/300 -- Iteration 33278 - Batch 2470/3851 - Train loss: 0.19341826  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.76 min) Epoch 9/300 -- Iteration 33316 - Batch 2508/3851 - Train loss: 0.19358660  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.77 min) Epoch 9/300 -- Iteration 33354 - Batch 2546/3851 - Train loss: 0.19347286  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.78 min) Epoch 9/300 -- Iteration 33392 - Batch 2584/3851 - Train loss: 0.19347917  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.78 min) Epoch 9/300 -- Iteration 33430 - Batch 2622/3851 - Train loss: 0.19371761  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.79 min) Epoch 9/300 -- Iteration 33468 - Batch 2660/3851 - Train loss: 0.19368929  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.80 min) Epoch 9/300 -- Iteration 33506 - Batch 2698/3851 - Train loss: 0.19373966  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.81 min) Epoch 9/300 -- Iteration 33544 - Batch 2736/3851 - Train loss: 0.19366879  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.81 min) Epoch 9/300 -- Iteration 33582 - Batch 2774/3851 - Train loss: 0.19377298  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.82 min) Epoch 9/300 -- Iteration 33620 - Batch 2812/3851 - Train loss: 0.19388622  - Train acc: 0.9278 - Val loss: 0.72253627\n",
      "(6.83 min) Epoch 9/300 -- Iteration 33658 - Batch 2850/3851 - Train loss: 0.19364611  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.84 min) Epoch 9/300 -- Iteration 33696 - Batch 2888/3851 - Train loss: 0.19348212  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.84 min) Epoch 9/300 -- Iteration 33734 - Batch 2926/3851 - Train loss: 0.19347286  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.85 min) Epoch 9/300 -- Iteration 33772 - Batch 2964/3851 - Train loss: 0.19352426  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.86 min) Epoch 9/300 -- Iteration 33810 - Batch 3002/3851 - Train loss: 0.19340439  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.87 min) Epoch 9/300 -- Iteration 33848 - Batch 3040/3851 - Train loss: 0.19340725  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.87 min) Epoch 9/300 -- Iteration 33886 - Batch 3078/3851 - Train loss: 0.19337774  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.88 min) Epoch 9/300 -- Iteration 33924 - Batch 3116/3851 - Train loss: 0.19336302  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.89 min) Epoch 9/300 -- Iteration 33962 - Batch 3154/3851 - Train loss: 0.19348159  - Train acc: 0.9279 - Val loss: 0.72253627\n",
      "(6.90 min) Epoch 9/300 -- Iteration 34000 - Batch 3192/3851 - Train loss: 0.19323870  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.91 min) Epoch 9/300 -- Iteration 34038 - Batch 3230/3851 - Train loss: 0.19319276  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.91 min) Epoch 9/300 -- Iteration 34076 - Batch 3268/3851 - Train loss: 0.19316571  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.92 min) Epoch 9/300 -- Iteration 34114 - Batch 3306/3851 - Train loss: 0.19313314  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.93 min) Epoch 9/300 -- Iteration 34152 - Batch 3344/3851 - Train loss: 0.19302705  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(6.94 min) Epoch 9/300 -- Iteration 34190 - Batch 3382/3851 - Train loss: 0.19285010  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.94 min) Epoch 9/300 -- Iteration 34228 - Batch 3420/3851 - Train loss: 0.19272725  - Train acc: 0.9282 - Val loss: 0.72253627\n",
      "(6.95 min) Epoch 9/300 -- Iteration 34266 - Batch 3458/3851 - Train loss: 0.19294119  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.96 min) Epoch 9/300 -- Iteration 34304 - Batch 3496/3851 - Train loss: 0.19295665  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.97 min) Epoch 9/300 -- Iteration 34342 - Batch 3534/3851 - Train loss: 0.19301139  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.97 min) Epoch 9/300 -- Iteration 34380 - Batch 3572/3851 - Train loss: 0.19291216  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.98 min) Epoch 9/300 -- Iteration 34418 - Batch 3610/3851 - Train loss: 0.19290569  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(6.99 min) Epoch 9/300 -- Iteration 34456 - Batch 3648/3851 - Train loss: 0.19292627  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(7.00 min) Epoch 9/300 -- Iteration 34494 - Batch 3686/3851 - Train loss: 0.19304750  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(7.00 min) Epoch 9/300 -- Iteration 34532 - Batch 3724/3851 - Train loss: 0.19295704  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(7.01 min) Epoch 9/300 -- Iteration 34570 - Batch 3762/3851 - Train loss: 0.19290159  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(7.02 min) Epoch 9/300 -- Iteration 34608 - Batch 3800/3851 - Train loss: 0.19300800  - Train acc: 0.9281 - Val loss: 0.72253627\n",
      "(7.03 min) Epoch 9/300 -- Iteration 34646 - Batch 3838/3851 - Train loss: 0.19296820  - Train acc: 0.9280 - Val loss: 0.72253627\n",
      "(7.03 min) Epoch 9/300 -- Iteration 34659 - Batch 3850/3851 - Train loss: 0.19299797  - Train acc: 0.9280 - Val loss: 0.76876038 - Val acc: 0.6400\n",
      "(7.04 min) Epoch 10/300 -- Iteration 34697 - Batch 38/3851 - Train loss: 0.19050659  - Train acc: 0.9265 - Val loss: 0.76876038\n",
      "(7.05 min) Epoch 10/300 -- Iteration 34735 - Batch 76/3851 - Train loss: 0.18888766  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.05 min) Epoch 10/300 -- Iteration 34773 - Batch 114/3851 - Train loss: 0.18702550  - Train acc: 0.9302 - Val loss: 0.76876038\n",
      "(7.06 min) Epoch 10/300 -- Iteration 34811 - Batch 152/3851 - Train loss: 0.18506186  - Train acc: 0.9316 - Val loss: 0.76876038\n",
      "(7.07 min) Epoch 10/300 -- Iteration 34849 - Batch 190/3851 - Train loss: 0.18749863  - Train acc: 0.9310 - Val loss: 0.76876038\n",
      "(7.08 min) Epoch 10/300 -- Iteration 34887 - Batch 228/3851 - Train loss: 0.18949109  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.08 min) Epoch 10/300 -- Iteration 34925 - Batch 266/3851 - Train loss: 0.18822114  - Train acc: 0.9305 - Val loss: 0.76876038\n",
      "(7.09 min) Epoch 10/300 -- Iteration 34963 - Batch 304/3851 - Train loss: 0.18856566  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.10 min) Epoch 10/300 -- Iteration 35001 - Batch 342/3851 - Train loss: 0.18772858  - Train acc: 0.9305 - Val loss: 0.76876038\n",
      "(7.11 min) Epoch 10/300 -- Iteration 35039 - Batch 380/3851 - Train loss: 0.18805738  - Train acc: 0.9305 - Val loss: 0.76876038\n",
      "(7.12 min) Epoch 10/300 -- Iteration 35077 - Batch 418/3851 - Train loss: 0.18749825  - Train acc: 0.9303 - Val loss: 0.76876038\n",
      "(7.12 min) Epoch 10/300 -- Iteration 35115 - Batch 456/3851 - Train loss: 0.18712117  - Train acc: 0.9306 - Val loss: 0.76876038\n",
      "(7.13 min) Epoch 10/300 -- Iteration 35153 - Batch 494/3851 - Train loss: 0.18638892  - Train acc: 0.9309 - Val loss: 0.76876038\n",
      "(7.14 min) Epoch 10/300 -- Iteration 35191 - Batch 532/3851 - Train loss: 0.18626755  - Train acc: 0.9308 - Val loss: 0.76876038\n",
      "(7.15 min) Epoch 10/300 -- Iteration 35229 - Batch 570/3851 - Train loss: 0.18647021  - Train acc: 0.9306 - Val loss: 0.76876038\n",
      "(7.15 min) Epoch 10/300 -- Iteration 35267 - Batch 608/3851 - Train loss: 0.18781161  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.16 min) Epoch 10/300 -- Iteration 35305 - Batch 646/3851 - Train loss: 0.18821817  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.17 min) Epoch 10/300 -- Iteration 35343 - Batch 684/3851 - Train loss: 0.18854238  - Train acc: 0.9296 - Val loss: 0.76876038\n",
      "(7.18 min) Epoch 10/300 -- Iteration 35381 - Batch 722/3851 - Train loss: 0.18891413  - Train acc: 0.9294 - Val loss: 0.76876038\n",
      "(7.18 min) Epoch 10/300 -- Iteration 35419 - Batch 760/3851 - Train loss: 0.18847191  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.19 min) Epoch 10/300 -- Iteration 35457 - Batch 798/3851 - Train loss: 0.18862829  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.20 min) Epoch 10/300 -- Iteration 35495 - Batch 836/3851 - Train loss: 0.18876564  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.21 min) Epoch 10/300 -- Iteration 35533 - Batch 874/3851 - Train loss: 0.18843584  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.21 min) Epoch 10/300 -- Iteration 35571 - Batch 912/3851 - Train loss: 0.18802824  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.22 min) Epoch 10/300 -- Iteration 35609 - Batch 950/3851 - Train loss: 0.18825656  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.23 min) Epoch 10/300 -- Iteration 35647 - Batch 988/3851 - Train loss: 0.18800199  - Train acc: 0.9303 - Val loss: 0.76876038\n",
      "(7.24 min) Epoch 10/300 -- Iteration 35685 - Batch 1026/3851 - Train loss: 0.18802256  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.24 min) Epoch 10/300 -- Iteration 35723 - Batch 1064/3851 - Train loss: 0.18800981  - Train acc: 0.9302 - Val loss: 0.76876038\n",
      "(7.25 min) Epoch 10/300 -- Iteration 35761 - Batch 1102/3851 - Train loss: 0.18792922  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.26 min) Epoch 10/300 -- Iteration 35799 - Batch 1140/3851 - Train loss: 0.18798670  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.27 min) Epoch 10/300 -- Iteration 35837 - Batch 1178/3851 - Train loss: 0.18820470  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.28 min) Epoch 10/300 -- Iteration 35875 - Batch 1216/3851 - Train loss: 0.18812074  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.28 min) Epoch 10/300 -- Iteration 35913 - Batch 1254/3851 - Train loss: 0.18826895  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.29 min) Epoch 10/300 -- Iteration 35951 - Batch 1292/3851 - Train loss: 0.18810028  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.30 min) Epoch 10/300 -- Iteration 35989 - Batch 1330/3851 - Train loss: 0.18765422  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.31 min) Epoch 10/300 -- Iteration 36027 - Batch 1368/3851 - Train loss: 0.18772638  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.31 min) Epoch 10/300 -- Iteration 36065 - Batch 1406/3851 - Train loss: 0.18743466  - Train acc: 0.9302 - Val loss: 0.76876038\n",
      "(7.32 min) Epoch 10/300 -- Iteration 36103 - Batch 1444/3851 - Train loss: 0.18748030  - Train acc: 0.9302 - Val loss: 0.76876038\n",
      "(7.33 min) Epoch 10/300 -- Iteration 36141 - Batch 1482/3851 - Train loss: 0.18723802  - Train acc: 0.9304 - Val loss: 0.76876038\n",
      "(7.34 min) Epoch 10/300 -- Iteration 36179 - Batch 1520/3851 - Train loss: 0.18721185  - Train acc: 0.9304 - Val loss: 0.76876038\n",
      "(7.34 min) Epoch 10/300 -- Iteration 36217 - Batch 1558/3851 - Train loss: 0.18687536  - Train acc: 0.9304 - Val loss: 0.76876038\n",
      "(7.35 min) Epoch 10/300 -- Iteration 36255 - Batch 1596/3851 - Train loss: 0.18687552  - Train acc: 0.9305 - Val loss: 0.76876038\n",
      "(7.36 min) Epoch 10/300 -- Iteration 36293 - Batch 1634/3851 - Train loss: 0.18666103  - Train acc: 0.9305 - Val loss: 0.76876038\n",
      "(7.37 min) Epoch 10/300 -- Iteration 36331 - Batch 1672/3851 - Train loss: 0.18671865  - Train acc: 0.9305 - Val loss: 0.76876038\n",
      "(7.37 min) Epoch 10/300 -- Iteration 36369 - Batch 1710/3851 - Train loss: 0.18667132  - Train acc: 0.9305 - Val loss: 0.76876038\n",
      "(7.38 min) Epoch 10/300 -- Iteration 36407 - Batch 1748/3851 - Train loss: 0.18671981  - Train acc: 0.9305 - Val loss: 0.76876038\n",
      "(7.39 min) Epoch 10/300 -- Iteration 36445 - Batch 1786/3851 - Train loss: 0.18681600  - Train acc: 0.9304 - Val loss: 0.76876038\n",
      "(7.40 min) Epoch 10/300 -- Iteration 36483 - Batch 1824/3851 - Train loss: 0.18702130  - Train acc: 0.9303 - Val loss: 0.76876038\n",
      "(7.40 min) Epoch 10/300 -- Iteration 36521 - Batch 1862/3851 - Train loss: 0.18686909  - Train acc: 0.9304 - Val loss: 0.76876038\n",
      "(7.41 min) Epoch 10/300 -- Iteration 36559 - Batch 1900/3851 - Train loss: 0.18674601  - Train acc: 0.9305 - Val loss: 0.76876038\n",
      "(7.42 min) Epoch 10/300 -- Iteration 36597 - Batch 1938/3851 - Train loss: 0.18685131  - Train acc: 0.9304 - Val loss: 0.76876038\n",
      "(7.43 min) Epoch 10/300 -- Iteration 36635 - Batch 1976/3851 - Train loss: 0.18708935  - Train acc: 0.9303 - Val loss: 0.76876038\n",
      "(7.44 min) Epoch 10/300 -- Iteration 36673 - Batch 2014/3851 - Train loss: 0.18722990  - Train acc: 0.9303 - Val loss: 0.76876038\n",
      "(7.44 min) Epoch 10/300 -- Iteration 36711 - Batch 2052/3851 - Train loss: 0.18735458  - Train acc: 0.9302 - Val loss: 0.76876038\n",
      "(7.45 min) Epoch 10/300 -- Iteration 36749 - Batch 2090/3851 - Train loss: 0.18717363  - Train acc: 0.9302 - Val loss: 0.76876038\n",
      "(7.46 min) Epoch 10/300 -- Iteration 36787 - Batch 2128/3851 - Train loss: 0.18731478  - Train acc: 0.9302 - Val loss: 0.76876038\n",
      "(7.47 min) Epoch 10/300 -- Iteration 36825 - Batch 2166/3851 - Train loss: 0.18720497  - Train acc: 0.9302 - Val loss: 0.76876038\n",
      "(7.47 min) Epoch 10/300 -- Iteration 36863 - Batch 2204/3851 - Train loss: 0.18718562  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.48 min) Epoch 10/300 -- Iteration 36901 - Batch 2242/3851 - Train loss: 0.18724383  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.49 min) Epoch 10/300 -- Iteration 36939 - Batch 2280/3851 - Train loss: 0.18743535  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.50 min) Epoch 10/300 -- Iteration 36977 - Batch 2318/3851 - Train loss: 0.18724460  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.50 min) Epoch 10/300 -- Iteration 37015 - Batch 2356/3851 - Train loss: 0.18726409  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.51 min) Epoch 10/300 -- Iteration 37053 - Batch 2394/3851 - Train loss: 0.18741421  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.52 min) Epoch 10/300 -- Iteration 37091 - Batch 2432/3851 - Train loss: 0.18746243  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.53 min) Epoch 10/300 -- Iteration 37129 - Batch 2470/3851 - Train loss: 0.18753795  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.53 min) Epoch 10/300 -- Iteration 37167 - Batch 2508/3851 - Train loss: 0.18732908  - Train acc: 0.9301 - Val loss: 0.76876038\n",
      "(7.54 min) Epoch 10/300 -- Iteration 37205 - Batch 2546/3851 - Train loss: 0.18749999  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.55 min) Epoch 10/300 -- Iteration 37243 - Batch 2584/3851 - Train loss: 0.18772502  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.56 min) Epoch 10/300 -- Iteration 37281 - Batch 2622/3851 - Train loss: 0.18766717  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.56 min) Epoch 10/300 -- Iteration 37319 - Batch 2660/3851 - Train loss: 0.18776148  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.57 min) Epoch 10/300 -- Iteration 37357 - Batch 2698/3851 - Train loss: 0.18786889  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.58 min) Epoch 10/300 -- Iteration 37395 - Batch 2736/3851 - Train loss: 0.18793223  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.59 min) Epoch 10/300 -- Iteration 37433 - Batch 2774/3851 - Train loss: 0.18794081  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.59 min) Epoch 10/300 -- Iteration 37471 - Batch 2812/3851 - Train loss: 0.18819391  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.60 min) Epoch 10/300 -- Iteration 37509 - Batch 2850/3851 - Train loss: 0.18807391  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.61 min) Epoch 10/300 -- Iteration 37547 - Batch 2888/3851 - Train loss: 0.18807057  - Train acc: 0.9300 - Val loss: 0.76876038\n",
      "(7.62 min) Epoch 10/300 -- Iteration 37585 - Batch 2926/3851 - Train loss: 0.18818313  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.63 min) Epoch 10/300 -- Iteration 37623 - Batch 2964/3851 - Train loss: 0.18818388  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.63 min) Epoch 10/300 -- Iteration 37661 - Batch 3002/3851 - Train loss: 0.18813183  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.64 min) Epoch 10/300 -- Iteration 37699 - Batch 3040/3851 - Train loss: 0.18816782  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.65 min) Epoch 10/300 -- Iteration 37737 - Batch 3078/3851 - Train loss: 0.18827264  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.66 min) Epoch 10/300 -- Iteration 37775 - Batch 3116/3851 - Train loss: 0.18828408  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.66 min) Epoch 10/300 -- Iteration 37813 - Batch 3154/3851 - Train loss: 0.18827813  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.67 min) Epoch 10/300 -- Iteration 37851 - Batch 3192/3851 - Train loss: 0.18813389  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.68 min) Epoch 10/300 -- Iteration 37889 - Batch 3230/3851 - Train loss: 0.18816083  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.69 min) Epoch 10/300 -- Iteration 37927 - Batch 3268/3851 - Train loss: 0.18818850  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.69 min) Epoch 10/300 -- Iteration 37965 - Batch 3306/3851 - Train loss: 0.18814289  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.70 min) Epoch 10/300 -- Iteration 38003 - Batch 3344/3851 - Train loss: 0.18818677  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.71 min) Epoch 10/300 -- Iteration 38041 - Batch 3382/3851 - Train loss: 0.18815275  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.72 min) Epoch 10/300 -- Iteration 38079 - Batch 3420/3851 - Train loss: 0.18811468  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.72 min) Epoch 10/300 -- Iteration 38117 - Batch 3458/3851 - Train loss: 0.18809773  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.73 min) Epoch 10/300 -- Iteration 38155 - Batch 3496/3851 - Train loss: 0.18816290  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.74 min) Epoch 10/300 -- Iteration 38193 - Batch 3534/3851 - Train loss: 0.18821254  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.75 min) Epoch 10/300 -- Iteration 38231 - Batch 3572/3851 - Train loss: 0.18817771  - Train acc: 0.9296 - Val loss: 0.76876038\n",
      "(7.76 min) Epoch 10/300 -- Iteration 38269 - Batch 3610/3851 - Train loss: 0.18805695  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.76 min) Epoch 10/300 -- Iteration 38307 - Batch 3648/3851 - Train loss: 0.18793352  - Train acc: 0.9297 - Val loss: 0.76876038\n",
      "(7.77 min) Epoch 10/300 -- Iteration 38345 - Batch 3686/3851 - Train loss: 0.18783966  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.78 min) Epoch 10/300 -- Iteration 38383 - Batch 3724/3851 - Train loss: 0.18788780  - Train acc: 0.9298 - Val loss: 0.76876038\n",
      "(7.79 min) Epoch 10/300 -- Iteration 38421 - Batch 3762/3851 - Train loss: 0.18780393  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.79 min) Epoch 10/300 -- Iteration 38459 - Batch 3800/3851 - Train loss: 0.18778189  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.80 min) Epoch 10/300 -- Iteration 38497 - Batch 3838/3851 - Train loss: 0.18780075  - Train acc: 0.9299 - Val loss: 0.76876038\n",
      "(7.80 min) Epoch 10/300 -- Iteration 38510 - Batch 3850/3851 - Train loss: 0.18778833  - Train acc: 0.9299 - Val loss: 0.70729250 - Val acc: 0.6433\n",
      "(7.81 min) Epoch 11/300 -- Iteration 38548 - Batch 38/3851 - Train loss: 0.18982875  - Train acc: 0.9239 - Val loss: 0.70729250\n",
      "(7.82 min) Epoch 11/300 -- Iteration 38586 - Batch 76/3851 - Train loss: 0.18665936  - Train acc: 0.9268 - Val loss: 0.70729250\n",
      "(7.83 min) Epoch 11/300 -- Iteration 38624 - Batch 114/3851 - Train loss: 0.18915355  - Train acc: 0.9255 - Val loss: 0.70729250\n",
      "(7.84 min) Epoch 11/300 -- Iteration 38662 - Batch 152/3851 - Train loss: 0.18647778  - Train acc: 0.9285 - Val loss: 0.70729250\n",
      "(7.84 min) Epoch 11/300 -- Iteration 38700 - Batch 190/3851 - Train loss: 0.18478117  - Train acc: 0.9296 - Val loss: 0.70729250\n",
      "(7.85 min) Epoch 11/300 -- Iteration 38738 - Batch 228/3851 - Train loss: 0.18542108  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(7.86 min) Epoch 11/300 -- Iteration 38776 - Batch 266/3851 - Train loss: 0.18552576  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(7.87 min) Epoch 11/300 -- Iteration 38814 - Batch 304/3851 - Train loss: 0.18444558  - Train acc: 0.9301 - Val loss: 0.70729250\n",
      "(7.87 min) Epoch 11/300 -- Iteration 38852 - Batch 342/3851 - Train loss: 0.18498101  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(7.88 min) Epoch 11/300 -- Iteration 38890 - Batch 380/3851 - Train loss: 0.18479554  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(7.89 min) Epoch 11/300 -- Iteration 38928 - Batch 418/3851 - Train loss: 0.18380395  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(7.90 min) Epoch 11/300 -- Iteration 38966 - Batch 456/3851 - Train loss: 0.18470118  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(7.90 min) Epoch 11/300 -- Iteration 39004 - Batch 494/3851 - Train loss: 0.18362739  - Train acc: 0.9304 - Val loss: 0.70729250\n",
      "(7.91 min) Epoch 11/300 -- Iteration 39042 - Batch 532/3851 - Train loss: 0.18383607  - Train acc: 0.9306 - Val loss: 0.70729250\n",
      "(7.92 min) Epoch 11/300 -- Iteration 39080 - Batch 570/3851 - Train loss: 0.18387772  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(7.93 min) Epoch 11/300 -- Iteration 39118 - Batch 608/3851 - Train loss: 0.18457849  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(7.93 min) Epoch 11/300 -- Iteration 39156 - Batch 646/3851 - Train loss: 0.18473909  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(7.94 min) Epoch 11/300 -- Iteration 39194 - Batch 684/3851 - Train loss: 0.18534954  - Train acc: 0.9296 - Val loss: 0.70729250\n",
      "(7.95 min) Epoch 11/300 -- Iteration 39232 - Batch 722/3851 - Train loss: 0.18545754  - Train acc: 0.9295 - Val loss: 0.70729250\n",
      "(7.96 min) Epoch 11/300 -- Iteration 39270 - Batch 760/3851 - Train loss: 0.18593811  - Train acc: 0.9293 - Val loss: 0.70729250\n",
      "(7.97 min) Epoch 11/300 -- Iteration 39308 - Batch 798/3851 - Train loss: 0.18528795  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(7.97 min) Epoch 11/300 -- Iteration 39346 - Batch 836/3851 - Train loss: 0.18525469  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(7.98 min) Epoch 11/300 -- Iteration 39384 - Batch 874/3851 - Train loss: 0.18522442  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(7.99 min) Epoch 11/300 -- Iteration 39422 - Batch 912/3851 - Train loss: 0.18526468  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.00 min) Epoch 11/300 -- Iteration 39460 - Batch 950/3851 - Train loss: 0.18493685  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.00 min) Epoch 11/300 -- Iteration 39498 - Batch 988/3851 - Train loss: 0.18519390  - Train acc: 0.9297 - Val loss: 0.70729250\n",
      "(8.01 min) Epoch 11/300 -- Iteration 39536 - Batch 1026/3851 - Train loss: 0.18526921  - Train acc: 0.9297 - Val loss: 0.70729250\n",
      "(8.02 min) Epoch 11/300 -- Iteration 39574 - Batch 1064/3851 - Train loss: 0.18525165  - Train acc: 0.9296 - Val loss: 0.70729250\n",
      "(8.03 min) Epoch 11/300 -- Iteration 39612 - Batch 1102/3851 - Train loss: 0.18485782  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.03 min) Epoch 11/300 -- Iteration 39650 - Batch 1140/3851 - Train loss: 0.18468344  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.04 min) Epoch 11/300 -- Iteration 39688 - Batch 1178/3851 - Train loss: 0.18458709  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.05 min) Epoch 11/300 -- Iteration 39726 - Batch 1216/3851 - Train loss: 0.18488866  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.06 min) Epoch 11/300 -- Iteration 39764 - Batch 1254/3851 - Train loss: 0.18488104  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.06 min) Epoch 11/300 -- Iteration 39802 - Batch 1292/3851 - Train loss: 0.18487121  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.07 min) Epoch 11/300 -- Iteration 39840 - Batch 1330/3851 - Train loss: 0.18438525  - Train acc: 0.9301 - Val loss: 0.70729250\n",
      "(8.08 min) Epoch 11/300 -- Iteration 39878 - Batch 1368/3851 - Train loss: 0.18401526  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.09 min) Epoch 11/300 -- Iteration 39916 - Batch 1406/3851 - Train loss: 0.18395198  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.09 min) Epoch 11/300 -- Iteration 39954 - Batch 1444/3851 - Train loss: 0.18441131  - Train acc: 0.9301 - Val loss: 0.70729250\n",
      "(8.10 min) Epoch 11/300 -- Iteration 39992 - Batch 1482/3851 - Train loss: 0.18441457  - Train acc: 0.9301 - Val loss: 0.70729250\n",
      "(8.11 min) Epoch 11/300 -- Iteration 40030 - Batch 1520/3851 - Train loss: 0.18432921  - Train acc: 0.9301 - Val loss: 0.70729250\n",
      "(8.12 min) Epoch 11/300 -- Iteration 40068 - Batch 1558/3851 - Train loss: 0.18472988  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(8.13 min) Epoch 11/300 -- Iteration 40106 - Batch 1596/3851 - Train loss: 0.18481460  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(8.13 min) Epoch 11/300 -- Iteration 40144 - Batch 1634/3851 - Train loss: 0.18485707  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(8.14 min) Epoch 11/300 -- Iteration 40182 - Batch 1672/3851 - Train loss: 0.18491441  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.15 min) Epoch 11/300 -- Iteration 40220 - Batch 1710/3851 - Train loss: 0.18502999  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.16 min) Epoch 11/300 -- Iteration 40258 - Batch 1748/3851 - Train loss: 0.18512741  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.16 min) Epoch 11/300 -- Iteration 40296 - Batch 1786/3851 - Train loss: 0.18502764  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.17 min) Epoch 11/300 -- Iteration 40334 - Batch 1824/3851 - Train loss: 0.18540070  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.18 min) Epoch 11/300 -- Iteration 40372 - Batch 1862/3851 - Train loss: 0.18511665  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(8.19 min) Epoch 11/300 -- Iteration 40410 - Batch 1900/3851 - Train loss: 0.18489698  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(8.19 min) Epoch 11/300 -- Iteration 40448 - Batch 1938/3851 - Train loss: 0.18484984  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(8.20 min) Epoch 11/300 -- Iteration 40486 - Batch 1976/3851 - Train loss: 0.18520913  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.21 min) Epoch 11/300 -- Iteration 40524 - Batch 2014/3851 - Train loss: 0.18532648  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.22 min) Epoch 11/300 -- Iteration 40562 - Batch 2052/3851 - Train loss: 0.18560798  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.22 min) Epoch 11/300 -- Iteration 40600 - Batch 2090/3851 - Train loss: 0.18570303  - Train acc: 0.9297 - Val loss: 0.70729250\n",
      "(8.23 min) Epoch 11/300 -- Iteration 40638 - Batch 2128/3851 - Train loss: 0.18562366  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.24 min) Epoch 11/300 -- Iteration 40676 - Batch 2166/3851 - Train loss: 0.18549843  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.25 min) Epoch 11/300 -- Iteration 40714 - Batch 2204/3851 - Train loss: 0.18571177  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.25 min) Epoch 11/300 -- Iteration 40752 - Batch 2242/3851 - Train loss: 0.18558239  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.26 min) Epoch 11/300 -- Iteration 40790 - Batch 2280/3851 - Train loss: 0.18569478  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.27 min) Epoch 11/300 -- Iteration 40828 - Batch 2318/3851 - Train loss: 0.18558994  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.28 min) Epoch 11/300 -- Iteration 40866 - Batch 2356/3851 - Train loss: 0.18563899  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.28 min) Epoch 11/300 -- Iteration 40904 - Batch 2394/3851 - Train loss: 0.18578110  - Train acc: 0.9298 - Val loss: 0.70729250\n",
      "(8.29 min) Epoch 11/300 -- Iteration 40942 - Batch 2432/3851 - Train loss: 0.18571601  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.30 min) Epoch 11/300 -- Iteration 40980 - Batch 2470/3851 - Train loss: 0.18561756  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.31 min) Epoch 11/300 -- Iteration 41018 - Batch 2508/3851 - Train loss: 0.18579274  - Train acc: 0.9299 - Val loss: 0.70729250\n",
      "(8.32 min) Epoch 11/300 -- Iteration 41056 - Batch 2546/3851 - Train loss: 0.18569052  - Train acc: 0.9300 - Val loss: 0.70729250\n",
      "(8.32 min) Epoch 11/300 -- Iteration 41094 - Batch 2584/3851 - Train loss: 0.18559661  - Train acc: 0.9301 - Val loss: 0.70729250\n",
      "(8.33 min) Epoch 11/300 -- Iteration 41132 - Batch 2622/3851 - Train loss: 0.18549205  - Train acc: 0.9301 - Val loss: 0.70729250\n",
      "(8.34 min) Epoch 11/300 -- Iteration 41170 - Batch 2660/3851 - Train loss: 0.18535503  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.35 min) Epoch 11/300 -- Iteration 41208 - Batch 2698/3851 - Train loss: 0.18528376  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.35 min) Epoch 11/300 -- Iteration 41246 - Batch 2736/3851 - Train loss: 0.18521655  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.36 min) Epoch 11/300 -- Iteration 41284 - Batch 2774/3851 - Train loss: 0.18511993  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.37 min) Epoch 11/300 -- Iteration 41322 - Batch 2812/3851 - Train loss: 0.18503884  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.38 min) Epoch 11/300 -- Iteration 41360 - Batch 2850/3851 - Train loss: 0.18511347  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.38 min) Epoch 11/300 -- Iteration 41398 - Batch 2888/3851 - Train loss: 0.18512863  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.39 min) Epoch 11/300 -- Iteration 41436 - Batch 2926/3851 - Train loss: 0.18504376  - Train acc: 0.9304 - Val loss: 0.70729250\n",
      "(8.40 min) Epoch 11/300 -- Iteration 41474 - Batch 2964/3851 - Train loss: 0.18526104  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.41 min) Epoch 11/300 -- Iteration 41512 - Batch 3002/3851 - Train loss: 0.18538948  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.41 min) Epoch 11/300 -- Iteration 41550 - Batch 3040/3851 - Train loss: 0.18545108  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.42 min) Epoch 11/300 -- Iteration 41588 - Batch 3078/3851 - Train loss: 0.18533630  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.43 min) Epoch 11/300 -- Iteration 41626 - Batch 3116/3851 - Train loss: 0.18523997  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.44 min) Epoch 11/300 -- Iteration 41664 - Batch 3154/3851 - Train loss: 0.18532247  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.44 min) Epoch 11/300 -- Iteration 41702 - Batch 3192/3851 - Train loss: 0.18538255  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.45 min) Epoch 11/300 -- Iteration 41740 - Batch 3230/3851 - Train loss: 0.18533476  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.46 min) Epoch 11/300 -- Iteration 41778 - Batch 3268/3851 - Train loss: 0.18530590  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.47 min) Epoch 11/300 -- Iteration 41816 - Batch 3306/3851 - Train loss: 0.18531848  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.47 min) Epoch 11/300 -- Iteration 41854 - Batch 3344/3851 - Train loss: 0.18550135  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.48 min) Epoch 11/300 -- Iteration 41892 - Batch 3382/3851 - Train loss: 0.18538307  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.49 min) Epoch 11/300 -- Iteration 41930 - Batch 3420/3851 - Train loss: 0.18549959  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.50 min) Epoch 11/300 -- Iteration 41968 - Batch 3458/3851 - Train loss: 0.18529291  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.51 min) Epoch 11/300 -- Iteration 42006 - Batch 3496/3851 - Train loss: 0.18528424  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.51 min) Epoch 11/300 -- Iteration 42044 - Batch 3534/3851 - Train loss: 0.18529503  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.52 min) Epoch 11/300 -- Iteration 42082 - Batch 3572/3851 - Train loss: 0.18541372  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.53 min) Epoch 11/300 -- Iteration 42120 - Batch 3610/3851 - Train loss: 0.18533868  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.54 min) Epoch 11/300 -- Iteration 42158 - Batch 3648/3851 - Train loss: 0.18541972  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.54 min) Epoch 11/300 -- Iteration 42196 - Batch 3686/3851 - Train loss: 0.18535890  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.55 min) Epoch 11/300 -- Iteration 42234 - Batch 3724/3851 - Train loss: 0.18531749  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.56 min) Epoch 11/300 -- Iteration 42272 - Batch 3762/3851 - Train loss: 0.18534680  - Train acc: 0.9302 - Val loss: 0.70729250\n",
      "(8.57 min) Epoch 11/300 -- Iteration 42310 - Batch 3800/3851 - Train loss: 0.18528682  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.57 min) Epoch 11/300 -- Iteration 42348 - Batch 3838/3851 - Train loss: 0.18526020  - Train acc: 0.9303 - Val loss: 0.70729250\n",
      "(8.58 min) Epoch 11/300 -- Iteration 42361 - Batch 3850/3851 - Train loss: 0.18531447  - Train acc: 0.9303 - Val loss: 0.70118403 - Val acc: 0.6433\n",
      "(8.59 min) Epoch 12/300 -- Iteration 42399 - Batch 38/3851 - Train loss: 0.17904225  - Train acc: 0.9335 - Val loss: 0.70118403\n",
      "(8.59 min) Epoch 12/300 -- Iteration 42437 - Batch 76/3851 - Train loss: 0.18260146  - Train acc: 0.9318 - Val loss: 0.70118403\n",
      "(8.60 min) Epoch 12/300 -- Iteration 42475 - Batch 114/3851 - Train loss: 0.19154462  - Train acc: 0.9287 - Val loss: 0.70118403\n",
      "(8.61 min) Epoch 12/300 -- Iteration 42513 - Batch 152/3851 - Train loss: 0.18988395  - Train acc: 0.9285 - Val loss: 0.70118403\n",
      "(8.62 min) Epoch 12/300 -- Iteration 42551 - Batch 190/3851 - Train loss: 0.18796621  - Train acc: 0.9296 - Val loss: 0.70118403\n",
      "(8.62 min) Epoch 12/300 -- Iteration 42589 - Batch 228/3851 - Train loss: 0.18483285  - Train acc: 0.9304 - Val loss: 0.70118403\n",
      "(8.63 min) Epoch 12/300 -- Iteration 42627 - Batch 266/3851 - Train loss: 0.18567350  - Train acc: 0.9303 - Val loss: 0.70118403\n",
      "(8.64 min) Epoch 12/300 -- Iteration 42665 - Batch 304/3851 - Train loss: 0.18453705  - Train acc: 0.9313 - Val loss: 0.70118403\n",
      "(8.65 min) Epoch 12/300 -- Iteration 42703 - Batch 342/3851 - Train loss: 0.18508837  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.65 min) Epoch 12/300 -- Iteration 42741 - Batch 380/3851 - Train loss: 0.18476998  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.66 min) Epoch 12/300 -- Iteration 42779 - Batch 418/3851 - Train loss: 0.18518246  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.67 min) Epoch 12/300 -- Iteration 42817 - Batch 456/3851 - Train loss: 0.18523885  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.68 min) Epoch 12/300 -- Iteration 42855 - Batch 494/3851 - Train loss: 0.18466933  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(8.68 min) Epoch 12/300 -- Iteration 42893 - Batch 532/3851 - Train loss: 0.18330474  - Train acc: 0.9315 - Val loss: 0.70118403\n",
      "(8.69 min) Epoch 12/300 -- Iteration 42931 - Batch 570/3851 - Train loss: 0.18374584  - Train acc: 0.9313 - Val loss: 0.70118403\n",
      "(8.70 min) Epoch 12/300 -- Iteration 42969 - Batch 608/3851 - Train loss: 0.18356604  - Train acc: 0.9317 - Val loss: 0.70118403\n",
      "(8.71 min) Epoch 12/300 -- Iteration 43007 - Batch 646/3851 - Train loss: 0.18432353  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.71 min) Epoch 12/300 -- Iteration 43045 - Batch 684/3851 - Train loss: 0.18400587  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.72 min) Epoch 12/300 -- Iteration 43083 - Batch 722/3851 - Train loss: 0.18443557  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.73 min) Epoch 12/300 -- Iteration 43121 - Batch 760/3851 - Train loss: 0.18538453  - Train acc: 0.9306 - Val loss: 0.70118403\n",
      "(8.74 min) Epoch 12/300 -- Iteration 43159 - Batch 798/3851 - Train loss: 0.18538168  - Train acc: 0.9307 - Val loss: 0.70118403\n",
      "(8.75 min) Epoch 12/300 -- Iteration 43197 - Batch 836/3851 - Train loss: 0.18527698  - Train acc: 0.9308 - Val loss: 0.70118403\n",
      "(8.75 min) Epoch 12/300 -- Iteration 43235 - Batch 874/3851 - Train loss: 0.18509062  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(8.76 min) Epoch 12/300 -- Iteration 43273 - Batch 912/3851 - Train loss: 0.18487460  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.77 min) Epoch 12/300 -- Iteration 43311 - Batch 950/3851 - Train loss: 0.18446573  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(8.78 min) Epoch 12/300 -- Iteration 43349 - Batch 988/3851 - Train loss: 0.18468846  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.78 min) Epoch 12/300 -- Iteration 43387 - Batch 1026/3851 - Train loss: 0.18489060  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.79 min) Epoch 12/300 -- Iteration 43425 - Batch 1064/3851 - Train loss: 0.18468444  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.80 min) Epoch 12/300 -- Iteration 43463 - Batch 1102/3851 - Train loss: 0.18490888  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.81 min) Epoch 12/300 -- Iteration 43501 - Batch 1140/3851 - Train loss: 0.18486007  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(8.81 min) Epoch 12/300 -- Iteration 43539 - Batch 1178/3851 - Train loss: 0.18500602  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(8.82 min) Epoch 12/300 -- Iteration 43577 - Batch 1216/3851 - Train loss: 0.18522435  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(8.83 min) Epoch 12/300 -- Iteration 43615 - Batch 1254/3851 - Train loss: 0.18530089  - Train acc: 0.9308 - Val loss: 0.70118403\n",
      "(8.84 min) Epoch 12/300 -- Iteration 43653 - Batch 1292/3851 - Train loss: 0.18523991  - Train acc: 0.9308 - Val loss: 0.70118403\n",
      "(8.84 min) Epoch 12/300 -- Iteration 43691 - Batch 1330/3851 - Train loss: 0.18515799  - Train acc: 0.9308 - Val loss: 0.70118403\n",
      "(8.85 min) Epoch 12/300 -- Iteration 43729 - Batch 1368/3851 - Train loss: 0.18568866  - Train acc: 0.9305 - Val loss: 0.70118403\n",
      "(8.86 min) Epoch 12/300 -- Iteration 43767 - Batch 1406/3851 - Train loss: 0.18559692  - Train acc: 0.9306 - Val loss: 0.70118403\n",
      "(8.87 min) Epoch 12/300 -- Iteration 43805 - Batch 1444/3851 - Train loss: 0.18555895  - Train acc: 0.9307 - Val loss: 0.70118403\n",
      "(8.87 min) Epoch 12/300 -- Iteration 43843 - Batch 1482/3851 - Train loss: 0.18538698  - Train acc: 0.9308 - Val loss: 0.70118403\n",
      "(8.88 min) Epoch 12/300 -- Iteration 43881 - Batch 1520/3851 - Train loss: 0.18530152  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(8.89 min) Epoch 12/300 -- Iteration 43919 - Batch 1558/3851 - Train loss: 0.18517098  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.90 min) Epoch 12/300 -- Iteration 43957 - Batch 1596/3851 - Train loss: 0.18502980  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(8.91 min) Epoch 12/300 -- Iteration 43995 - Batch 1634/3851 - Train loss: 0.18489074  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.91 min) Epoch 12/300 -- Iteration 44033 - Batch 1672/3851 - Train loss: 0.18497302  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.92 min) Epoch 12/300 -- Iteration 44071 - Batch 1710/3851 - Train loss: 0.18470900  - Train acc: 0.9313 - Val loss: 0.70118403\n",
      "(8.93 min) Epoch 12/300 -- Iteration 44109 - Batch 1748/3851 - Train loss: 0.18495659  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.94 min) Epoch 12/300 -- Iteration 44147 - Batch 1786/3851 - Train loss: 0.18504821  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.94 min) Epoch 12/300 -- Iteration 44185 - Batch 1824/3851 - Train loss: 0.18506238  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.95 min) Epoch 12/300 -- Iteration 44223 - Batch 1862/3851 - Train loss: 0.18507803  - Train acc: 0.9312 - Val loss: 0.70118403\n",
      "(8.96 min) Epoch 12/300 -- Iteration 44261 - Batch 1900/3851 - Train loss: 0.18546596  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.97 min) Epoch 12/300 -- Iteration 44299 - Batch 1938/3851 - Train loss: 0.18562781  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.97 min) Epoch 12/300 -- Iteration 44337 - Batch 1976/3851 - Train loss: 0.18543512  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(8.98 min) Epoch 12/300 -- Iteration 44375 - Batch 2014/3851 - Train loss: 0.18537018  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(8.99 min) Epoch 12/300 -- Iteration 44413 - Batch 2052/3851 - Train loss: 0.18537401  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.00 min) Epoch 12/300 -- Iteration 44451 - Batch 2090/3851 - Train loss: 0.18532901  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.00 min) Epoch 12/300 -- Iteration 44489 - Batch 2128/3851 - Train loss: 0.18518262  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.01 min) Epoch 12/300 -- Iteration 44527 - Batch 2166/3851 - Train loss: 0.18511622  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.02 min) Epoch 12/300 -- Iteration 44565 - Batch 2204/3851 - Train loss: 0.18521680  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.03 min) Epoch 12/300 -- Iteration 44603 - Batch 2242/3851 - Train loss: 0.18504306  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(9.04 min) Epoch 12/300 -- Iteration 44641 - Batch 2280/3851 - Train loss: 0.18495626  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(9.04 min) Epoch 12/300 -- Iteration 44679 - Batch 2318/3851 - Train loss: 0.18483408  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(9.05 min) Epoch 12/300 -- Iteration 44717 - Batch 2356/3851 - Train loss: 0.18508214  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.06 min) Epoch 12/300 -- Iteration 44755 - Batch 2394/3851 - Train loss: 0.18473149  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.07 min) Epoch 12/300 -- Iteration 44793 - Batch 2432/3851 - Train loss: 0.18468121  - Train acc: 0.9311 - Val loss: 0.70118403\n",
      "(9.07 min) Epoch 12/300 -- Iteration 44831 - Batch 2470/3851 - Train loss: 0.18483158  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.08 min) Epoch 12/300 -- Iteration 44869 - Batch 2508/3851 - Train loss: 0.18504304  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.09 min) Epoch 12/300 -- Iteration 44907 - Batch 2546/3851 - Train loss: 0.18512459  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.10 min) Epoch 12/300 -- Iteration 44945 - Batch 2584/3851 - Train loss: 0.18511316  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.10 min) Epoch 12/300 -- Iteration 44983 - Batch 2622/3851 - Train loss: 0.18515633  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.11 min) Epoch 12/300 -- Iteration 45021 - Batch 2660/3851 - Train loss: 0.18532135  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.12 min) Epoch 12/300 -- Iteration 45059 - Batch 2698/3851 - Train loss: 0.18542820  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.13 min) Epoch 12/300 -- Iteration 45097 - Batch 2736/3851 - Train loss: 0.18536115  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.13 min) Epoch 12/300 -- Iteration 45135 - Batch 2774/3851 - Train loss: 0.18527623  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.14 min) Epoch 12/300 -- Iteration 45173 - Batch 2812/3851 - Train loss: 0.18505121  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.15 min) Epoch 12/300 -- Iteration 45211 - Batch 2850/3851 - Train loss: 0.18499128  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.16 min) Epoch 12/300 -- Iteration 45249 - Batch 2888/3851 - Train loss: 0.18510310  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.16 min) Epoch 12/300 -- Iteration 45287 - Batch 2926/3851 - Train loss: 0.18507524  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.17 min) Epoch 12/300 -- Iteration 45325 - Batch 2964/3851 - Train loss: 0.18497840  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.18 min) Epoch 12/300 -- Iteration 45363 - Batch 3002/3851 - Train loss: 0.18505442  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.19 min) Epoch 12/300 -- Iteration 45401 - Batch 3040/3851 - Train loss: 0.18506639  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.19 min) Epoch 12/300 -- Iteration 45439 - Batch 3078/3851 - Train loss: 0.18512724  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.20 min) Epoch 12/300 -- Iteration 45477 - Batch 3116/3851 - Train loss: 0.18515926  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.21 min) Epoch 12/300 -- Iteration 45515 - Batch 3154/3851 - Train loss: 0.18492758  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.22 min) Epoch 12/300 -- Iteration 45553 - Batch 3192/3851 - Train loss: 0.18498550  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.22 min) Epoch 12/300 -- Iteration 45591 - Batch 3230/3851 - Train loss: 0.18509515  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.23 min) Epoch 12/300 -- Iteration 45629 - Batch 3268/3851 - Train loss: 0.18494287  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.24 min) Epoch 12/300 -- Iteration 45667 - Batch 3306/3851 - Train loss: 0.18502878  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.25 min) Epoch 12/300 -- Iteration 45705 - Batch 3344/3851 - Train loss: 0.18495138  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.25 min) Epoch 12/300 -- Iteration 45743 - Batch 3382/3851 - Train loss: 0.18490115  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.26 min) Epoch 12/300 -- Iteration 45781 - Batch 3420/3851 - Train loss: 0.18497855  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.27 min) Epoch 12/300 -- Iteration 45819 - Batch 3458/3851 - Train loss: 0.18480198  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.28 min) Epoch 12/300 -- Iteration 45857 - Batch 3496/3851 - Train loss: 0.18478277  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.29 min) Epoch 12/300 -- Iteration 45895 - Batch 3534/3851 - Train loss: 0.18473198  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.29 min) Epoch 12/300 -- Iteration 45933 - Batch 3572/3851 - Train loss: 0.18487040  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.30 min) Epoch 12/300 -- Iteration 45971 - Batch 3610/3851 - Train loss: 0.18491616  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.31 min) Epoch 12/300 -- Iteration 46009 - Batch 3648/3851 - Train loss: 0.18496550  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.32 min) Epoch 12/300 -- Iteration 46047 - Batch 3686/3851 - Train loss: 0.18496550  - Train acc: 0.9309 - Val loss: 0.70118403\n",
      "(9.32 min) Epoch 12/300 -- Iteration 46085 - Batch 3724/3851 - Train loss: 0.18486715  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.33 min) Epoch 12/300 -- Iteration 46123 - Batch 3762/3851 - Train loss: 0.18476439  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.34 min) Epoch 12/300 -- Iteration 46161 - Batch 3800/3851 - Train loss: 0.18482193  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.35 min) Epoch 12/300 -- Iteration 46199 - Batch 3838/3851 - Train loss: 0.18473286  - Train acc: 0.9310 - Val loss: 0.70118403\n",
      "(9.35 min) Epoch 12/300 -- Iteration 46212 - Batch 3850/3851 - Train loss: 0.18472117  - Train acc: 0.9310 - Val loss: 0.75449258 - Val acc: 0.6467\n",
      "(9.36 min) Epoch 13/300 -- Iteration 46250 - Batch 38/3851 - Train loss: 0.19246646  - Train acc: 0.9273 - Val loss: 0.75449258\n",
      "(9.37 min) Epoch 13/300 -- Iteration 46288 - Batch 76/3851 - Train loss: 0.18082269  - Train acc: 0.9316 - Val loss: 0.75449258\n",
      "(9.37 min) Epoch 13/300 -- Iteration 46326 - Batch 114/3851 - Train loss: 0.18141395  - Train acc: 0.9313 - Val loss: 0.75449258\n",
      "(9.38 min) Epoch 13/300 -- Iteration 46364 - Batch 152/3851 - Train loss: 0.18417293  - Train acc: 0.9304 - Val loss: 0.75449258\n",
      "(9.39 min) Epoch 13/300 -- Iteration 46402 - Batch 190/3851 - Train loss: 0.18590286  - Train acc: 0.9297 - Val loss: 0.75449258\n",
      "(9.40 min) Epoch 13/300 -- Iteration 46440 - Batch 228/3851 - Train loss: 0.18422964  - Train acc: 0.9308 - Val loss: 0.75449258\n",
      "(9.40 min) Epoch 13/300 -- Iteration 46478 - Batch 266/3851 - Train loss: 0.18708710  - Train acc: 0.9298 - Val loss: 0.75449258\n",
      "(9.41 min) Epoch 13/300 -- Iteration 46516 - Batch 304/3851 - Train loss: 0.18667826  - Train acc: 0.9296 - Val loss: 0.75449258\n",
      "(9.42 min) Epoch 13/300 -- Iteration 46554 - Batch 342/3851 - Train loss: 0.18546584  - Train acc: 0.9302 - Val loss: 0.75449258\n",
      "(9.43 min) Epoch 13/300 -- Iteration 46592 - Batch 380/3851 - Train loss: 0.18471430  - Train acc: 0.9302 - Val loss: 0.75449258\n",
      "(9.43 min) Epoch 13/300 -- Iteration 46630 - Batch 418/3851 - Train loss: 0.18460994  - Train acc: 0.9303 - Val loss: 0.75449258\n",
      "(9.44 min) Epoch 13/300 -- Iteration 46668 - Batch 456/3851 - Train loss: 0.18400291  - Train acc: 0.9307 - Val loss: 0.75449258\n",
      "(9.45 min) Epoch 13/300 -- Iteration 46706 - Batch 494/3851 - Train loss: 0.18387217  - Train acc: 0.9307 - Val loss: 0.75449258\n",
      "(9.46 min) Epoch 13/300 -- Iteration 46744 - Batch 532/3851 - Train loss: 0.18355001  - Train acc: 0.9308 - Val loss: 0.75449258\n",
      "(9.47 min) Epoch 13/300 -- Iteration 46782 - Batch 570/3851 - Train loss: 0.18341234  - Train acc: 0.9309 - Val loss: 0.75449258\n",
      "(9.47 min) Epoch 13/300 -- Iteration 46820 - Batch 608/3851 - Train loss: 0.18383474  - Train acc: 0.9307 - Val loss: 0.75449258\n",
      "(9.48 min) Epoch 13/300 -- Iteration 46858 - Batch 646/3851 - Train loss: 0.18268676  - Train acc: 0.9309 - Val loss: 0.75449258\n",
      "(9.49 min) Epoch 13/300 -- Iteration 46896 - Batch 684/3851 - Train loss: 0.18296447  - Train acc: 0.9307 - Val loss: 0.75449258\n",
      "(9.50 min) Epoch 13/300 -- Iteration 46934 - Batch 722/3851 - Train loss: 0.18335320  - Train acc: 0.9305 - Val loss: 0.75449258\n",
      "(9.50 min) Epoch 13/300 -- Iteration 46972 - Batch 760/3851 - Train loss: 0.18276516  - Train acc: 0.9307 - Val loss: 0.75449258\n",
      "(9.51 min) Epoch 13/300 -- Iteration 47010 - Batch 798/3851 - Train loss: 0.18269489  - Train acc: 0.9307 - Val loss: 0.75449258\n",
      "(9.52 min) Epoch 13/300 -- Iteration 47048 - Batch 836/3851 - Train loss: 0.18212439  - Train acc: 0.9310 - Val loss: 0.75449258\n",
      "(9.53 min) Epoch 13/300 -- Iteration 47086 - Batch 874/3851 - Train loss: 0.18230101  - Train acc: 0.9310 - Val loss: 0.75449258\n",
      "(9.53 min) Epoch 13/300 -- Iteration 47124 - Batch 912/3851 - Train loss: 0.18229504  - Train acc: 0.9310 - Val loss: 0.75449258\n",
      "(9.54 min) Epoch 13/300 -- Iteration 47162 - Batch 950/3851 - Train loss: 0.18269944  - Train acc: 0.9309 - Val loss: 0.75449258\n",
      "(9.55 min) Epoch 13/300 -- Iteration 47200 - Batch 988/3851 - Train loss: 0.18248978  - Train acc: 0.9311 - Val loss: 0.75449258\n",
      "(9.56 min) Epoch 13/300 -- Iteration 47238 - Batch 1026/3851 - Train loss: 0.18242946  - Train acc: 0.9312 - Val loss: 0.75449258\n",
      "(9.56 min) Epoch 13/300 -- Iteration 47276 - Batch 1064/3851 - Train loss: 0.18267362  - Train acc: 0.9312 - Val loss: 0.75449258\n",
      "(9.57 min) Epoch 13/300 -- Iteration 47314 - Batch 1102/3851 - Train loss: 0.18240366  - Train acc: 0.9315 - Val loss: 0.75449258\n",
      "(9.58 min) Epoch 13/300 -- Iteration 47352 - Batch 1140/3851 - Train loss: 0.18227123  - Train acc: 0.9315 - Val loss: 0.75449258\n",
      "(9.59 min) Epoch 13/300 -- Iteration 47390 - Batch 1178/3851 - Train loss: 0.18202287  - Train acc: 0.9316 - Val loss: 0.75449258\n",
      "(9.59 min) Epoch 13/300 -- Iteration 47428 - Batch 1216/3851 - Train loss: 0.18204526  - Train acc: 0.9315 - Val loss: 0.75449258\n",
      "(9.60 min) Epoch 13/300 -- Iteration 47466 - Batch 1254/3851 - Train loss: 0.18207725  - Train acc: 0.9315 - Val loss: 0.75449258\n",
      "(9.61 min) Epoch 13/300 -- Iteration 47504 - Batch 1292/3851 - Train loss: 0.18199820  - Train acc: 0.9315 - Val loss: 0.75449258\n",
      "(9.62 min) Epoch 13/300 -- Iteration 47542 - Batch 1330/3851 - Train loss: 0.18172116  - Train acc: 0.9316 - Val loss: 0.75449258\n",
      "(9.63 min) Epoch 13/300 -- Iteration 47580 - Batch 1368/3851 - Train loss: 0.18150771  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.63 min) Epoch 13/300 -- Iteration 47618 - Batch 1406/3851 - Train loss: 0.18161702  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.64 min) Epoch 13/300 -- Iteration 47656 - Batch 1444/3851 - Train loss: 0.18152567  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.65 min) Epoch 13/300 -- Iteration 47694 - Batch 1482/3851 - Train loss: 0.18166977  - Train acc: 0.9315 - Val loss: 0.75449258\n",
      "(9.66 min) Epoch 13/300 -- Iteration 47732 - Batch 1520/3851 - Train loss: 0.18141648  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.66 min) Epoch 13/300 -- Iteration 47770 - Batch 1558/3851 - Train loss: 0.18135769  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.67 min) Epoch 13/300 -- Iteration 47808 - Batch 1596/3851 - Train loss: 0.18120703  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.68 min) Epoch 13/300 -- Iteration 47846 - Batch 1634/3851 - Train loss: 0.18105242  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.69 min) Epoch 13/300 -- Iteration 47884 - Batch 1672/3851 - Train loss: 0.18090999  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.69 min) Epoch 13/300 -- Iteration 47922 - Batch 1710/3851 - Train loss: 0.18088778  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.70 min) Epoch 13/300 -- Iteration 47960 - Batch 1748/3851 - Train loss: 0.18097654  - Train acc: 0.9317 - Val loss: 0.75449258\n",
      "(9.71 min) Epoch 13/300 -- Iteration 47998 - Batch 1786/3851 - Train loss: 0.18073899  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.72 min) Epoch 13/300 -- Iteration 48036 - Batch 1824/3851 - Train loss: 0.18083385  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.72 min) Epoch 13/300 -- Iteration 48074 - Batch 1862/3851 - Train loss: 0.18096648  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.73 min) Epoch 13/300 -- Iteration 48112 - Batch 1900/3851 - Train loss: 0.18075079  - Train acc: 0.9319 - Val loss: 0.75449258\n",
      "(9.74 min) Epoch 13/300 -- Iteration 48150 - Batch 1938/3851 - Train loss: 0.18107032  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.75 min) Epoch 13/300 -- Iteration 48188 - Batch 1976/3851 - Train loss: 0.18112742  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.75 min) Epoch 13/300 -- Iteration 48226 - Batch 2014/3851 - Train loss: 0.18116808  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.76 min) Epoch 13/300 -- Iteration 48264 - Batch 2052/3851 - Train loss: 0.18112557  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.77 min) Epoch 13/300 -- Iteration 48302 - Batch 2090/3851 - Train loss: 0.18119697  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.78 min) Epoch 13/300 -- Iteration 48340 - Batch 2128/3851 - Train loss: 0.18109252  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.78 min) Epoch 13/300 -- Iteration 48378 - Batch 2166/3851 - Train loss: 0.18117793  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.79 min) Epoch 13/300 -- Iteration 48416 - Batch 2204/3851 - Train loss: 0.18132692  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.80 min) Epoch 13/300 -- Iteration 48454 - Batch 2242/3851 - Train loss: 0.18130286  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.81 min) Epoch 13/300 -- Iteration 48492 - Batch 2280/3851 - Train loss: 0.18121522  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.81 min) Epoch 13/300 -- Iteration 48530 - Batch 2318/3851 - Train loss: 0.18103808  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.82 min) Epoch 13/300 -- Iteration 48568 - Batch 2356/3851 - Train loss: 0.18102851  - Train acc: 0.9319 - Val loss: 0.75449258\n",
      "(9.83 min) Epoch 13/300 -- Iteration 48606 - Batch 2394/3851 - Train loss: 0.18100468  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.84 min) Epoch 13/300 -- Iteration 48644 - Batch 2432/3851 - Train loss: 0.18098351  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.85 min) Epoch 13/300 -- Iteration 48682 - Batch 2470/3851 - Train loss: 0.18086086  - Train acc: 0.9319 - Val loss: 0.75449258\n",
      "(9.85 min) Epoch 13/300 -- Iteration 48720 - Batch 2508/3851 - Train loss: 0.18095241  - Train acc: 0.9318 - Val loss: 0.75449258\n",
      "(9.86 min) Epoch 13/300 -- Iteration 48758 - Batch 2546/3851 - Train loss: 0.18087653  - Train acc: 0.9319 - Val loss: 0.75449258\n",
      "(9.87 min) Epoch 13/300 -- Iteration 48796 - Batch 2584/3851 - Train loss: 0.18087594  - Train acc: 0.9319 - Val loss: 0.75449258\n",
      "(9.88 min) Epoch 13/300 -- Iteration 48834 - Batch 2622/3851 - Train loss: 0.18082789  - Train acc: 0.9319 - Val loss: 0.75449258\n",
      "(9.88 min) Epoch 13/300 -- Iteration 48872 - Batch 2660/3851 - Train loss: 0.18072281  - Train acc: 0.9320 - Val loss: 0.75449258\n",
      "(9.89 min) Epoch 13/300 -- Iteration 48910 - Batch 2698/3851 - Train loss: 0.18073826  - Train acc: 0.9320 - Val loss: 0.75449258\n",
      "(9.90 min) Epoch 13/300 -- Iteration 48948 - Batch 2736/3851 - Train loss: 0.18077459  - Train acc: 0.9320 - Val loss: 0.75449258\n",
      "(9.91 min) Epoch 13/300 -- Iteration 48986 - Batch 2774/3851 - Train loss: 0.18058330  - Train acc: 0.9321 - Val loss: 0.75449258\n",
      "(9.91 min) Epoch 13/300 -- Iteration 49024 - Batch 2812/3851 - Train loss: 0.18043281  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(9.92 min) Epoch 13/300 -- Iteration 49062 - Batch 2850/3851 - Train loss: 0.18037149  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(9.93 min) Epoch 13/300 -- Iteration 49100 - Batch 2888/3851 - Train loss: 0.18068664  - Train acc: 0.9321 - Val loss: 0.75449258\n",
      "(9.94 min) Epoch 13/300 -- Iteration 49138 - Batch 2926/3851 - Train loss: 0.18067421  - Train acc: 0.9321 - Val loss: 0.75449258\n",
      "(9.94 min) Epoch 13/300 -- Iteration 49176 - Batch 2964/3851 - Train loss: 0.18050902  - Train acc: 0.9321 - Val loss: 0.75449258\n",
      "(9.95 min) Epoch 13/300 -- Iteration 49214 - Batch 3002/3851 - Train loss: 0.18062924  - Train acc: 0.9321 - Val loss: 0.75449258\n",
      "(9.96 min) Epoch 13/300 -- Iteration 49252 - Batch 3040/3851 - Train loss: 0.18052631  - Train acc: 0.9321 - Val loss: 0.75449258\n",
      "(9.97 min) Epoch 13/300 -- Iteration 49290 - Batch 3078/3851 - Train loss: 0.18045826  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(9.97 min) Epoch 13/300 -- Iteration 49328 - Batch 3116/3851 - Train loss: 0.18039502  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(9.98 min) Epoch 13/300 -- Iteration 49366 - Batch 3154/3851 - Train loss: 0.18050172  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(9.99 min) Epoch 13/300 -- Iteration 49404 - Batch 3192/3851 - Train loss: 0.18055806  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(10.00 min) Epoch 13/300 -- Iteration 49442 - Batch 3230/3851 - Train loss: 0.18058227  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(10.00 min) Epoch 13/300 -- Iteration 49480 - Batch 3268/3851 - Train loss: 0.18059522  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(10.01 min) Epoch 13/300 -- Iteration 49518 - Batch 3306/3851 - Train loss: 0.18079926  - Train acc: 0.9321 - Val loss: 0.75449258\n",
      "(10.02 min) Epoch 13/300 -- Iteration 49556 - Batch 3344/3851 - Train loss: 0.18071882  - Train acc: 0.9321 - Val loss: 0.75449258\n",
      "(10.03 min) Epoch 13/300 -- Iteration 49594 - Batch 3382/3851 - Train loss: 0.18048340  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(10.04 min) Epoch 13/300 -- Iteration 49632 - Batch 3420/3851 - Train loss: 0.18056440  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(10.04 min) Epoch 13/300 -- Iteration 49670 - Batch 3458/3851 - Train loss: 0.18054564  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(10.05 min) Epoch 13/300 -- Iteration 49708 - Batch 3496/3851 - Train loss: 0.18044271  - Train acc: 0.9322 - Val loss: 0.75449258\n",
      "(10.06 min) Epoch 13/300 -- Iteration 49746 - Batch 3534/3851 - Train loss: 0.18036227  - Train acc: 0.9323 - Val loss: 0.75449258\n",
      "(10.07 min) Epoch 13/300 -- Iteration 49784 - Batch 3572/3851 - Train loss: 0.18036129  - Train acc: 0.9323 - Val loss: 0.75449258\n",
      "(10.07 min) Epoch 13/300 -- Iteration 49822 - Batch 3610/3851 - Train loss: 0.18036370  - Train acc: 0.9323 - Val loss: 0.75449258\n",
      "(10.08 min) Epoch 13/300 -- Iteration 49860 - Batch 3648/3851 - Train loss: 0.18034651  - Train acc: 0.9323 - Val loss: 0.75449258\n",
      "(10.09 min) Epoch 13/300 -- Iteration 49898 - Batch 3686/3851 - Train loss: 0.18020242  - Train acc: 0.9323 - Val loss: 0.75449258\n",
      "(10.10 min) Epoch 13/300 -- Iteration 49936 - Batch 3724/3851 - Train loss: 0.18022232  - Train acc: 0.9323 - Val loss: 0.75449258\n",
      "(10.10 min) Epoch 13/300 -- Iteration 49974 - Batch 3762/3851 - Train loss: 0.18028621  - Train acc: 0.9323 - Val loss: 0.75449258\n",
      "(10.11 min) Epoch 13/300 -- Iteration 50012 - Batch 3800/3851 - Train loss: 0.18037535  - Train acc: 0.9323 - Val loss: 0.75449258\n",
      "(10.12 min) Epoch 13/300 -- Iteration 50050 - Batch 3838/3851 - Train loss: 0.18048364  - Train acc: 0.9323 - Val loss: 0.75449258\n",
      "(10.12 min) Epoch 13/300 -- Iteration 50063 - Batch 3850/3851 - Train loss: 0.18048007  - Train acc: 0.9323 - Val loss: 0.75449312 - Val acc: 0.6450\n",
      "(10.13 min) Epoch 14/300 -- Iteration 50101 - Batch 38/3851 - Train loss: 0.17566339  - Train acc: 0.9347 - Val loss: 0.75449312\n",
      "(10.14 min) Epoch 14/300 -- Iteration 50139 - Batch 76/3851 - Train loss: 0.17565735  - Train acc: 0.9328 - Val loss: 0.75449312\n",
      "(10.15 min) Epoch 14/300 -- Iteration 50177 - Batch 114/3851 - Train loss: 0.17628541  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.15 min) Epoch 14/300 -- Iteration 50215 - Batch 152/3851 - Train loss: 0.17738813  - Train acc: 0.9329 - Val loss: 0.75449312\n",
      "(10.16 min) Epoch 14/300 -- Iteration 50253 - Batch 190/3851 - Train loss: 0.17660287  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.17 min) Epoch 14/300 -- Iteration 50291 - Batch 228/3851 - Train loss: 0.17959360  - Train acc: 0.9322 - Val loss: 0.75449312\n",
      "(10.18 min) Epoch 14/300 -- Iteration 50329 - Batch 266/3851 - Train loss: 0.18029430  - Train acc: 0.9318 - Val loss: 0.75449312\n",
      "(10.18 min) Epoch 14/300 -- Iteration 50367 - Batch 304/3851 - Train loss: 0.18033928  - Train acc: 0.9320 - Val loss: 0.75449312\n",
      "(10.19 min) Epoch 14/300 -- Iteration 50405 - Batch 342/3851 - Train loss: 0.17887777  - Train acc: 0.9327 - Val loss: 0.75449312\n",
      "(10.20 min) Epoch 14/300 -- Iteration 50443 - Batch 380/3851 - Train loss: 0.17926546  - Train acc: 0.9326 - Val loss: 0.75449312\n",
      "(10.21 min) Epoch 14/300 -- Iteration 50481 - Batch 418/3851 - Train loss: 0.17851312  - Train acc: 0.9330 - Val loss: 0.75449312\n",
      "(10.21 min) Epoch 14/300 -- Iteration 50519 - Batch 456/3851 - Train loss: 0.17741905  - Train acc: 0.9336 - Val loss: 0.75449312\n",
      "(10.22 min) Epoch 14/300 -- Iteration 50557 - Batch 494/3851 - Train loss: 0.17789667  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.23 min) Epoch 14/300 -- Iteration 50595 - Batch 532/3851 - Train loss: 0.17874534  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.24 min) Epoch 14/300 -- Iteration 50633 - Batch 570/3851 - Train loss: 0.17923788  - Train acc: 0.9330 - Val loss: 0.75449312\n",
      "(10.25 min) Epoch 14/300 -- Iteration 50671 - Batch 608/3851 - Train loss: 0.17900256  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.25 min) Epoch 14/300 -- Iteration 50709 - Batch 646/3851 - Train loss: 0.17855652  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.26 min) Epoch 14/300 -- Iteration 50747 - Batch 684/3851 - Train loss: 0.17832895  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.27 min) Epoch 14/300 -- Iteration 50785 - Batch 722/3851 - Train loss: 0.17872229  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.28 min) Epoch 14/300 -- Iteration 50823 - Batch 760/3851 - Train loss: 0.17896149  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.28 min) Epoch 14/300 -- Iteration 50861 - Batch 798/3851 - Train loss: 0.17942060  - Train acc: 0.9327 - Val loss: 0.75449312\n",
      "(10.29 min) Epoch 14/300 -- Iteration 50899 - Batch 836/3851 - Train loss: 0.17892276  - Train acc: 0.9328 - Val loss: 0.75449312\n",
      "(10.30 min) Epoch 14/300 -- Iteration 50937 - Batch 874/3851 - Train loss: 0.17854603  - Train acc: 0.9329 - Val loss: 0.75449312\n",
      "(10.31 min) Epoch 14/300 -- Iteration 50975 - Batch 912/3851 - Train loss: 0.17861609  - Train acc: 0.9329 - Val loss: 0.75449312\n",
      "(10.31 min) Epoch 14/300 -- Iteration 51013 - Batch 950/3851 - Train loss: 0.17888162  - Train acc: 0.9329 - Val loss: 0.75449312\n",
      "(10.32 min) Epoch 14/300 -- Iteration 51051 - Batch 988/3851 - Train loss: 0.17872130  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.33 min) Epoch 14/300 -- Iteration 51089 - Batch 1026/3851 - Train loss: 0.17881325  - Train acc: 0.9330 - Val loss: 0.75449312\n",
      "(10.34 min) Epoch 14/300 -- Iteration 51127 - Batch 1064/3851 - Train loss: 0.17884536  - Train acc: 0.9330 - Val loss: 0.75449312\n",
      "(10.34 min) Epoch 14/300 -- Iteration 51165 - Batch 1102/3851 - Train loss: 0.17849585  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.35 min) Epoch 14/300 -- Iteration 51203 - Batch 1140/3851 - Train loss: 0.17847370  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.36 min) Epoch 14/300 -- Iteration 51241 - Batch 1178/3851 - Train loss: 0.17857507  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.37 min) Epoch 14/300 -- Iteration 51279 - Batch 1216/3851 - Train loss: 0.17856489  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.37 min) Epoch 14/300 -- Iteration 51317 - Batch 1254/3851 - Train loss: 0.17850338  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.38 min) Epoch 14/300 -- Iteration 51355 - Batch 1292/3851 - Train loss: 0.17824548  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.39 min) Epoch 14/300 -- Iteration 51393 - Batch 1330/3851 - Train loss: 0.17817809  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.40 min) Epoch 14/300 -- Iteration 51431 - Batch 1368/3851 - Train loss: 0.17815207  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.41 min) Epoch 14/300 -- Iteration 51469 - Batch 1406/3851 - Train loss: 0.17818138  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.41 min) Epoch 14/300 -- Iteration 51507 - Batch 1444/3851 - Train loss: 0.17814031  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.42 min) Epoch 14/300 -- Iteration 51545 - Batch 1482/3851 - Train loss: 0.17799448  - Train acc: 0.9335 - Val loss: 0.75449312\n",
      "(10.43 min) Epoch 14/300 -- Iteration 51583 - Batch 1520/3851 - Train loss: 0.17821200  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.44 min) Epoch 14/300 -- Iteration 51621 - Batch 1558/3851 - Train loss: 0.17812109  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.44 min) Epoch 14/300 -- Iteration 51659 - Batch 1596/3851 - Train loss: 0.17834984  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.45 min) Epoch 14/300 -- Iteration 51697 - Batch 1634/3851 - Train loss: 0.17838174  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.46 min) Epoch 14/300 -- Iteration 51735 - Batch 1672/3851 - Train loss: 0.17835918  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.47 min) Epoch 14/300 -- Iteration 51773 - Batch 1710/3851 - Train loss: 0.17815369  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.47 min) Epoch 14/300 -- Iteration 51811 - Batch 1748/3851 - Train loss: 0.17815963  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.48 min) Epoch 14/300 -- Iteration 51849 - Batch 1786/3851 - Train loss: 0.17816158  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.49 min) Epoch 14/300 -- Iteration 51887 - Batch 1824/3851 - Train loss: 0.17809849  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.50 min) Epoch 14/300 -- Iteration 51925 - Batch 1862/3851 - Train loss: 0.17824650  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.50 min) Epoch 14/300 -- Iteration 51963 - Batch 1900/3851 - Train loss: 0.17809969  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.51 min) Epoch 14/300 -- Iteration 52001 - Batch 1938/3851 - Train loss: 0.17792694  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.52 min) Epoch 14/300 -- Iteration 52039 - Batch 1976/3851 - Train loss: 0.17796602  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.53 min) Epoch 14/300 -- Iteration 52077 - Batch 2014/3851 - Train loss: 0.17808043  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.53 min) Epoch 14/300 -- Iteration 52115 - Batch 2052/3851 - Train loss: 0.17829250  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.54 min) Epoch 14/300 -- Iteration 52153 - Batch 2090/3851 - Train loss: 0.17815101  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.55 min) Epoch 14/300 -- Iteration 52191 - Batch 2128/3851 - Train loss: 0.17817368  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.56 min) Epoch 14/300 -- Iteration 52229 - Batch 2166/3851 - Train loss: 0.17820152  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.57 min) Epoch 14/300 -- Iteration 52267 - Batch 2204/3851 - Train loss: 0.17804628  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.57 min) Epoch 14/300 -- Iteration 52305 - Batch 2242/3851 - Train loss: 0.17808202  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.58 min) Epoch 14/300 -- Iteration 52343 - Batch 2280/3851 - Train loss: 0.17802360  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.59 min) Epoch 14/300 -- Iteration 52381 - Batch 2318/3851 - Train loss: 0.17786054  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.60 min) Epoch 14/300 -- Iteration 52419 - Batch 2356/3851 - Train loss: 0.17782184  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.60 min) Epoch 14/300 -- Iteration 52457 - Batch 2394/3851 - Train loss: 0.17783192  - Train acc: 0.9335 - Val loss: 0.75449312\n",
      "(10.61 min) Epoch 14/300 -- Iteration 52495 - Batch 2432/3851 - Train loss: 0.17774861  - Train acc: 0.9335 - Val loss: 0.75449312\n",
      "(10.62 min) Epoch 14/300 -- Iteration 52533 - Batch 2470/3851 - Train loss: 0.17787787  - Train acc: 0.9335 - Val loss: 0.75449312\n",
      "(10.63 min) Epoch 14/300 -- Iteration 52571 - Batch 2508/3851 - Train loss: 0.17777613  - Train acc: 0.9336 - Val loss: 0.75449312\n",
      "(10.63 min) Epoch 14/300 -- Iteration 52609 - Batch 2546/3851 - Train loss: 0.17767117  - Train acc: 0.9336 - Val loss: 0.75449312\n",
      "(10.64 min) Epoch 14/300 -- Iteration 52647 - Batch 2584/3851 - Train loss: 0.17767399  - Train acc: 0.9336 - Val loss: 0.75449312\n",
      "(10.65 min) Epoch 14/300 -- Iteration 52685 - Batch 2622/3851 - Train loss: 0.17764254  - Train acc: 0.9336 - Val loss: 0.75449312\n",
      "(10.66 min) Epoch 14/300 -- Iteration 52723 - Batch 2660/3851 - Train loss: 0.17798987  - Train acc: 0.9335 - Val loss: 0.75449312\n",
      "(10.66 min) Epoch 14/300 -- Iteration 52761 - Batch 2698/3851 - Train loss: 0.17805588  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.67 min) Epoch 14/300 -- Iteration 52799 - Batch 2736/3851 - Train loss: 0.17822177  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.68 min) Epoch 14/300 -- Iteration 52837 - Batch 2774/3851 - Train loss: 0.17825621  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.69 min) Epoch 14/300 -- Iteration 52875 - Batch 2812/3851 - Train loss: 0.17822344  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.69 min) Epoch 14/300 -- Iteration 52913 - Batch 2850/3851 - Train loss: 0.17803454  - Train acc: 0.9334 - Val loss: 0.75449312\n",
      "(10.70 min) Epoch 14/300 -- Iteration 52951 - Batch 2888/3851 - Train loss: 0.17810475  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.71 min) Epoch 14/300 -- Iteration 52989 - Batch 2926/3851 - Train loss: 0.17827453  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.72 min) Epoch 14/300 -- Iteration 53027 - Batch 2964/3851 - Train loss: 0.17832245  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.72 min) Epoch 14/300 -- Iteration 53065 - Batch 3002/3851 - Train loss: 0.17835640  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.73 min) Epoch 14/300 -- Iteration 53103 - Batch 3040/3851 - Train loss: 0.17829442  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.74 min) Epoch 14/300 -- Iteration 53141 - Batch 3078/3851 - Train loss: 0.17845255  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.75 min) Epoch 14/300 -- Iteration 53179 - Batch 3116/3851 - Train loss: 0.17851134  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.76 min) Epoch 14/300 -- Iteration 53217 - Batch 3154/3851 - Train loss: 0.17863093  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.76 min) Epoch 14/300 -- Iteration 53255 - Batch 3192/3851 - Train loss: 0.17841888  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.77 min) Epoch 14/300 -- Iteration 53293 - Batch 3230/3851 - Train loss: 0.17838306  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.78 min) Epoch 14/300 -- Iteration 53331 - Batch 3268/3851 - Train loss: 0.17824882  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.79 min) Epoch 14/300 -- Iteration 53369 - Batch 3306/3851 - Train loss: 0.17838111  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.79 min) Epoch 14/300 -- Iteration 53407 - Batch 3344/3851 - Train loss: 0.17835091  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.80 min) Epoch 14/300 -- Iteration 53445 - Batch 3382/3851 - Train loss: 0.17834016  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.81 min) Epoch 14/300 -- Iteration 53483 - Batch 3420/3851 - Train loss: 0.17833099  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.82 min) Epoch 14/300 -- Iteration 53521 - Batch 3458/3851 - Train loss: 0.17835087  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.82 min) Epoch 14/300 -- Iteration 53559 - Batch 3496/3851 - Train loss: 0.17835229  - Train acc: 0.9333 - Val loss: 0.75449312\n",
      "(10.83 min) Epoch 14/300 -- Iteration 53597 - Batch 3534/3851 - Train loss: 0.17845963  - Train acc: 0.9332 - Val loss: 0.75449312\n",
      "(10.84 min) Epoch 14/300 -- Iteration 53635 - Batch 3572/3851 - Train loss: 0.17848909  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.85 min) Epoch 14/300 -- Iteration 53673 - Batch 3610/3851 - Train loss: 0.17853321  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.85 min) Epoch 14/300 -- Iteration 53711 - Batch 3648/3851 - Train loss: 0.17864727  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.86 min) Epoch 14/300 -- Iteration 53749 - Batch 3686/3851 - Train loss: 0.17853202  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.87 min) Epoch 14/300 -- Iteration 53787 - Batch 3724/3851 - Train loss: 0.17856139  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.88 min) Epoch 14/300 -- Iteration 53825 - Batch 3762/3851 - Train loss: 0.17851732  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.88 min) Epoch 14/300 -- Iteration 53863 - Batch 3800/3851 - Train loss: 0.17857096  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.89 min) Epoch 14/300 -- Iteration 53901 - Batch 3838/3851 - Train loss: 0.17863410  - Train acc: 0.9331 - Val loss: 0.75449312\n",
      "(10.90 min) Epoch 14/300 -- Iteration 53914 - Batch 3850/3851 - Train loss: 0.17865903  - Train acc: 0.9331 - Val loss: 0.75619942 - Val acc: 0.6400\n",
      "(10.90 min) Epoch 15/300 -- Iteration 53952 - Batch 38/3851 - Train loss: 0.17608961  - Train acc: 0.9327 - Val loss: 0.75619942\n",
      "(10.91 min) Epoch 15/300 -- Iteration 53990 - Batch 76/3851 - Train loss: 0.17847443  - Train acc: 0.9323 - Val loss: 0.75619942\n",
      "(10.92 min) Epoch 15/300 -- Iteration 54028 - Batch 114/3851 - Train loss: 0.17904521  - Train acc: 0.9327 - Val loss: 0.75619942\n",
      "(10.93 min) Epoch 15/300 -- Iteration 54066 - Batch 152/3851 - Train loss: 0.17715423  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(10.93 min) Epoch 15/300 -- Iteration 54104 - Batch 190/3851 - Train loss: 0.17685273  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(10.94 min) Epoch 15/300 -- Iteration 54142 - Batch 228/3851 - Train loss: 0.17651517  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(10.95 min) Epoch 15/300 -- Iteration 54180 - Batch 266/3851 - Train loss: 0.17709260  - Train acc: 0.9331 - Val loss: 0.75619942\n",
      "(10.96 min) Epoch 15/300 -- Iteration 54218 - Batch 304/3851 - Train loss: 0.17763746  - Train acc: 0.9328 - Val loss: 0.75619942\n",
      "(10.97 min) Epoch 15/300 -- Iteration 54256 - Batch 342/3851 - Train loss: 0.17732948  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(10.97 min) Epoch 15/300 -- Iteration 54294 - Batch 380/3851 - Train loss: 0.17642293  - Train acc: 0.9337 - Val loss: 0.75619942\n",
      "(10.98 min) Epoch 15/300 -- Iteration 54332 - Batch 418/3851 - Train loss: 0.17755145  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(10.99 min) Epoch 15/300 -- Iteration 54370 - Batch 456/3851 - Train loss: 0.17768663  - Train acc: 0.9330 - Val loss: 0.75619942\n",
      "(11.00 min) Epoch 15/300 -- Iteration 54408 - Batch 494/3851 - Train loss: 0.17740412  - Train acc: 0.9330 - Val loss: 0.75619942\n",
      "(11.00 min) Epoch 15/300 -- Iteration 54446 - Batch 532/3851 - Train loss: 0.17736730  - Train acc: 0.9330 - Val loss: 0.75619942\n",
      "(11.01 min) Epoch 15/300 -- Iteration 54484 - Batch 570/3851 - Train loss: 0.17779598  - Train acc: 0.9328 - Val loss: 0.75619942\n",
      "(11.02 min) Epoch 15/300 -- Iteration 54522 - Batch 608/3851 - Train loss: 0.17823129  - Train acc: 0.9327 - Val loss: 0.75619942\n",
      "(11.03 min) Epoch 15/300 -- Iteration 54560 - Batch 646/3851 - Train loss: 0.17859695  - Train acc: 0.9327 - Val loss: 0.75619942\n",
      "(11.03 min) Epoch 15/300 -- Iteration 54598 - Batch 684/3851 - Train loss: 0.17859847  - Train acc: 0.9327 - Val loss: 0.75619942\n",
      "(11.04 min) Epoch 15/300 -- Iteration 54636 - Batch 722/3851 - Train loss: 0.17807870  - Train acc: 0.9329 - Val loss: 0.75619942\n",
      "(11.05 min) Epoch 15/300 -- Iteration 54674 - Batch 760/3851 - Train loss: 0.17776122  - Train acc: 0.9327 - Val loss: 0.75619942\n",
      "(11.06 min) Epoch 15/300 -- Iteration 54712 - Batch 798/3851 - Train loss: 0.17796447  - Train acc: 0.9328 - Val loss: 0.75619942\n",
      "(11.06 min) Epoch 15/300 -- Iteration 54750 - Batch 836/3851 - Train loss: 0.17769756  - Train acc: 0.9327 - Val loss: 0.75619942\n",
      "(11.07 min) Epoch 15/300 -- Iteration 54788 - Batch 874/3851 - Train loss: 0.17744829  - Train acc: 0.9329 - Val loss: 0.75619942\n",
      "(11.08 min) Epoch 15/300 -- Iteration 54826 - Batch 912/3851 - Train loss: 0.17739151  - Train acc: 0.9330 - Val loss: 0.75619942\n",
      "(11.09 min) Epoch 15/300 -- Iteration 54864 - Batch 950/3851 - Train loss: 0.17733246  - Train acc: 0.9331 - Val loss: 0.75619942\n",
      "(11.09 min) Epoch 15/300 -- Iteration 54902 - Batch 988/3851 - Train loss: 0.17715050  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.10 min) Epoch 15/300 -- Iteration 54940 - Batch 1026/3851 - Train loss: 0.17698298  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(11.11 min) Epoch 15/300 -- Iteration 54978 - Batch 1064/3851 - Train loss: 0.17730262  - Train acc: 0.9331 - Val loss: 0.75619942\n",
      "(11.12 min) Epoch 15/300 -- Iteration 55016 - Batch 1102/3851 - Train loss: 0.17733287  - Train acc: 0.9331 - Val loss: 0.75619942\n",
      "(11.13 min) Epoch 15/300 -- Iteration 55054 - Batch 1140/3851 - Train loss: 0.17748324  - Train acc: 0.9331 - Val loss: 0.75619942\n",
      "(11.13 min) Epoch 15/300 -- Iteration 55092 - Batch 1178/3851 - Train loss: 0.17766545  - Train acc: 0.9330 - Val loss: 0.75619942\n",
      "(11.14 min) Epoch 15/300 -- Iteration 55130 - Batch 1216/3851 - Train loss: 0.17737988  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.15 min) Epoch 15/300 -- Iteration 55168 - Batch 1254/3851 - Train loss: 0.17762290  - Train acc: 0.9331 - Val loss: 0.75619942\n",
      "(11.16 min) Epoch 15/300 -- Iteration 55206 - Batch 1292/3851 - Train loss: 0.17760112  - Train acc: 0.9331 - Val loss: 0.75619942\n",
      "(11.16 min) Epoch 15/300 -- Iteration 55244 - Batch 1330/3851 - Train loss: 0.17757157  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.17 min) Epoch 15/300 -- Iteration 55282 - Batch 1368/3851 - Train loss: 0.17786793  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.18 min) Epoch 15/300 -- Iteration 55320 - Batch 1406/3851 - Train loss: 0.17793908  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(11.19 min) Epoch 15/300 -- Iteration 55358 - Batch 1444/3851 - Train loss: 0.17797900  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.19 min) Epoch 15/300 -- Iteration 55396 - Batch 1482/3851 - Train loss: 0.17787063  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(11.20 min) Epoch 15/300 -- Iteration 55434 - Batch 1520/3851 - Train loss: 0.17820231  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.21 min) Epoch 15/300 -- Iteration 55472 - Batch 1558/3851 - Train loss: 0.17799419  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(11.22 min) Epoch 15/300 -- Iteration 55510 - Batch 1596/3851 - Train loss: 0.17822240  - Train acc: 0.9331 - Val loss: 0.75619942\n",
      "(11.22 min) Epoch 15/300 -- Iteration 55548 - Batch 1634/3851 - Train loss: 0.17811019  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.23 min) Epoch 15/300 -- Iteration 55586 - Batch 1672/3851 - Train loss: 0.17799885  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.24 min) Epoch 15/300 -- Iteration 55624 - Batch 1710/3851 - Train loss: 0.17792415  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.25 min) Epoch 15/300 -- Iteration 55662 - Batch 1748/3851 - Train loss: 0.17784512  - Train acc: 0.9332 - Val loss: 0.75619942\n",
      "(11.25 min) Epoch 15/300 -- Iteration 55700 - Batch 1786/3851 - Train loss: 0.17768180  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(11.26 min) Epoch 15/300 -- Iteration 55738 - Batch 1824/3851 - Train loss: 0.17764400  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(11.27 min) Epoch 15/300 -- Iteration 55776 - Batch 1862/3851 - Train loss: 0.17772201  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(11.28 min) Epoch 15/300 -- Iteration 55814 - Batch 1900/3851 - Train loss: 0.17756291  - Train acc: 0.9334 - Val loss: 0.75619942\n",
      "(11.28 min) Epoch 15/300 -- Iteration 55852 - Batch 1938/3851 - Train loss: 0.17768418  - Train acc: 0.9333 - Val loss: 0.75619942\n",
      "(11.29 min) Epoch 15/300 -- Iteration 55890 - Batch 1976/3851 - Train loss: 0.17749300  - Train acc: 0.9334 - Val loss: 0.75619942\n",
      "(11.30 min) Epoch 15/300 -- Iteration 55928 - Batch 2014/3851 - Train loss: 0.17747278  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.31 min) Epoch 15/300 -- Iteration 55966 - Batch 2052/3851 - Train loss: 0.17719311  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.32 min) Epoch 15/300 -- Iteration 56004 - Batch 2090/3851 - Train loss: 0.17709656  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.32 min) Epoch 15/300 -- Iteration 56042 - Batch 2128/3851 - Train loss: 0.17702061  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.33 min) Epoch 15/300 -- Iteration 56080 - Batch 2166/3851 - Train loss: 0.17706446  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.34 min) Epoch 15/300 -- Iteration 56118 - Batch 2204/3851 - Train loss: 0.17709555  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.35 min) Epoch 15/300 -- Iteration 56156 - Batch 2242/3851 - Train loss: 0.17715729  - Train acc: 0.9334 - Val loss: 0.75619942\n",
      "(11.35 min) Epoch 15/300 -- Iteration 56194 - Batch 2280/3851 - Train loss: 0.17704285  - Train acc: 0.9334 - Val loss: 0.75619942\n",
      "(11.36 min) Epoch 15/300 -- Iteration 56232 - Batch 2318/3851 - Train loss: 0.17710354  - Train acc: 0.9334 - Val loss: 0.75619942\n",
      "(11.37 min) Epoch 15/300 -- Iteration 56270 - Batch 2356/3851 - Train loss: 0.17711738  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.38 min) Epoch 15/300 -- Iteration 56308 - Batch 2394/3851 - Train loss: 0.17693493  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.38 min) Epoch 15/300 -- Iteration 56346 - Batch 2432/3851 - Train loss: 0.17687667  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.39 min) Epoch 15/300 -- Iteration 56384 - Batch 2470/3851 - Train loss: 0.17670882  - Train acc: 0.9336 - Val loss: 0.75619942\n",
      "(11.40 min) Epoch 15/300 -- Iteration 56422 - Batch 2508/3851 - Train loss: 0.17647791  - Train acc: 0.9337 - Val loss: 0.75619942\n",
      "(11.41 min) Epoch 15/300 -- Iteration 56460 - Batch 2546/3851 - Train loss: 0.17650123  - Train acc: 0.9336 - Val loss: 0.75619942\n",
      "(11.41 min) Epoch 15/300 -- Iteration 56498 - Batch 2584/3851 - Train loss: 0.17638202  - Train acc: 0.9336 - Val loss: 0.75619942\n",
      "(11.42 min) Epoch 15/300 -- Iteration 56536 - Batch 2622/3851 - Train loss: 0.17657368  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.43 min) Epoch 15/300 -- Iteration 56574 - Batch 2660/3851 - Train loss: 0.17655795  - Train acc: 0.9335 - Val loss: 0.75619942\n",
      "(11.44 min) Epoch 15/300 -- Iteration 56612 - Batch 2698/3851 - Train loss: 0.17656410  - Train acc: 0.9336 - Val loss: 0.75619942\n",
      "(11.44 min) Epoch 15/300 -- Iteration 56650 - Batch 2736/3851 - Train loss: 0.17642742  - Train acc: 0.9336 - Val loss: 0.75619942\n",
      "(11.45 min) Epoch 15/300 -- Iteration 56688 - Batch 2774/3851 - Train loss: 0.17638953  - Train acc: 0.9336 - Val loss: 0.75619942\n",
      "(11.46 min) Epoch 15/300 -- Iteration 56726 - Batch 2812/3851 - Train loss: 0.17630604  - Train acc: 0.9337 - Val loss: 0.75619942\n",
      "(11.47 min) Epoch 15/300 -- Iteration 56764 - Batch 2850/3851 - Train loss: 0.17633850  - Train acc: 0.9336 - Val loss: 0.75619942\n",
      "(11.48 min) Epoch 15/300 -- Iteration 56802 - Batch 2888/3851 - Train loss: 0.17635605  - Train acc: 0.9336 - Val loss: 0.75619942\n",
      "(11.48 min) Epoch 15/300 -- Iteration 56840 - Batch 2926/3851 - Train loss: 0.17630964  - Train acc: 0.9337 - Val loss: 0.75619942\n",
      "(11.49 min) Epoch 15/300 -- Iteration 56878 - Batch 2964/3851 - Train loss: 0.17615892  - Train acc: 0.9337 - Val loss: 0.75619942\n",
      "(11.50 min) Epoch 15/300 -- Iteration 56916 - Batch 3002/3851 - Train loss: 0.17605394  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.51 min) Epoch 15/300 -- Iteration 56954 - Batch 3040/3851 - Train loss: 0.17630104  - Train acc: 0.9337 - Val loss: 0.75619942\n",
      "(11.51 min) Epoch 15/300 -- Iteration 56992 - Batch 3078/3851 - Train loss: 0.17622739  - Train acc: 0.9337 - Val loss: 0.75619942\n",
      "(11.52 min) Epoch 15/300 -- Iteration 57030 - Batch 3116/3851 - Train loss: 0.17618960  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.53 min) Epoch 15/300 -- Iteration 57068 - Batch 3154/3851 - Train loss: 0.17626110  - Train acc: 0.9337 - Val loss: 0.75619942\n",
      "(11.54 min) Epoch 15/300 -- Iteration 57106 - Batch 3192/3851 - Train loss: 0.17631590  - Train acc: 0.9337 - Val loss: 0.75619942\n",
      "(11.54 min) Epoch 15/300 -- Iteration 57144 - Batch 3230/3851 - Train loss: 0.17623338  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.55 min) Epoch 15/300 -- Iteration 57182 - Batch 3268/3851 - Train loss: 0.17623689  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.56 min) Epoch 15/300 -- Iteration 57220 - Batch 3306/3851 - Train loss: 0.17619270  - Train acc: 0.9339 - Val loss: 0.75619942\n",
      "(11.57 min) Epoch 15/300 -- Iteration 57258 - Batch 3344/3851 - Train loss: 0.17629159  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.57 min) Epoch 15/300 -- Iteration 57296 - Batch 3382/3851 - Train loss: 0.17614164  - Train acc: 0.9339 - Val loss: 0.75619942\n",
      "(11.58 min) Epoch 15/300 -- Iteration 57334 - Batch 3420/3851 - Train loss: 0.17617604  - Train acc: 0.9339 - Val loss: 0.75619942\n",
      "(11.59 min) Epoch 15/300 -- Iteration 57372 - Batch 3458/3851 - Train loss: 0.17625787  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.60 min) Epoch 15/300 -- Iteration 57410 - Batch 3496/3851 - Train loss: 0.17616571  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.60 min) Epoch 15/300 -- Iteration 57448 - Batch 3534/3851 - Train loss: 0.17619147  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.61 min) Epoch 15/300 -- Iteration 57486 - Batch 3572/3851 - Train loss: 0.17609676  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.62 min) Epoch 15/300 -- Iteration 57524 - Batch 3610/3851 - Train loss: 0.17600025  - Train acc: 0.9339 - Val loss: 0.75619942\n",
      "(11.63 min) Epoch 15/300 -- Iteration 57562 - Batch 3648/3851 - Train loss: 0.17596821  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.63 min) Epoch 15/300 -- Iteration 57600 - Batch 3686/3851 - Train loss: 0.17600192  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.64 min) Epoch 15/300 -- Iteration 57638 - Batch 3724/3851 - Train loss: 0.17593264  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.65 min) Epoch 15/300 -- Iteration 57676 - Batch 3762/3851 - Train loss: 0.17582838  - Train acc: 0.9338 - Val loss: 0.75619942\n",
      "(11.66 min) Epoch 15/300 -- Iteration 57714 - Batch 3800/3851 - Train loss: 0.17578109  - Train acc: 0.9339 - Val loss: 0.75619942\n",
      "(11.67 min) Epoch 15/300 -- Iteration 57752 - Batch 3838/3851 - Train loss: 0.17584297  - Train acc: 0.9339 - Val loss: 0.75619942\n",
      "(11.67 min) Epoch 15/300 -- Iteration 57765 - Batch 3850/3851 - Train loss: 0.17586753  - Train acc: 0.9338 - Val loss: 0.73075205 - Val acc: 0.6500\n",
      "(11.68 min) Epoch 16/300 -- Iteration 57803 - Batch 38/3851 - Train loss: 0.16923008  - Train acc: 0.9393 - Val loss: 0.73075205\n",
      "(11.68 min) Epoch 16/300 -- Iteration 57841 - Batch 76/3851 - Train loss: 0.17069767  - Train acc: 0.9364 - Val loss: 0.73075205\n",
      "(11.69 min) Epoch 16/300 -- Iteration 57879 - Batch 114/3851 - Train loss: 0.17553122  - Train acc: 0.9342 - Val loss: 0.73075205\n",
      "(11.70 min) Epoch 16/300 -- Iteration 57917 - Batch 152/3851 - Train loss: 0.17377612  - Train acc: 0.9342 - Val loss: 0.73075205\n",
      "(11.71 min) Epoch 16/300 -- Iteration 57955 - Batch 190/3851 - Train loss: 0.17477992  - Train acc: 0.9335 - Val loss: 0.73075205\n",
      "(11.72 min) Epoch 16/300 -- Iteration 57993 - Batch 228/3851 - Train loss: 0.17614828  - Train acc: 0.9333 - Val loss: 0.73075205\n",
      "(11.72 min) Epoch 16/300 -- Iteration 58031 - Batch 266/3851 - Train loss: 0.17546799  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(11.73 min) Epoch 16/300 -- Iteration 58069 - Batch 304/3851 - Train loss: 0.17496292  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(11.74 min) Epoch 16/300 -- Iteration 58107 - Batch 342/3851 - Train loss: 0.17439892  - Train acc: 0.9351 - Val loss: 0.73075205\n",
      "(11.75 min) Epoch 16/300 -- Iteration 58145 - Batch 380/3851 - Train loss: 0.17351157  - Train acc: 0.9351 - Val loss: 0.73075205\n",
      "(11.75 min) Epoch 16/300 -- Iteration 58183 - Batch 418/3851 - Train loss: 0.17339510  - Train acc: 0.9350 - Val loss: 0.73075205\n",
      "(11.76 min) Epoch 16/300 -- Iteration 58221 - Batch 456/3851 - Train loss: 0.17377277  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(11.77 min) Epoch 16/300 -- Iteration 58259 - Batch 494/3851 - Train loss: 0.17382140  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(11.78 min) Epoch 16/300 -- Iteration 58297 - Batch 532/3851 - Train loss: 0.17405153  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(11.78 min) Epoch 16/300 -- Iteration 58335 - Batch 570/3851 - Train loss: 0.17383001  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(11.79 min) Epoch 16/300 -- Iteration 58373 - Batch 608/3851 - Train loss: 0.17413209  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(11.80 min) Epoch 16/300 -- Iteration 58411 - Batch 646/3851 - Train loss: 0.17443233  - Train acc: 0.9344 - Val loss: 0.73075205\n",
      "(11.81 min) Epoch 16/300 -- Iteration 58449 - Batch 684/3851 - Train loss: 0.17456594  - Train acc: 0.9343 - Val loss: 0.73075205\n",
      "(11.81 min) Epoch 16/300 -- Iteration 58487 - Batch 722/3851 - Train loss: 0.17462771  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(11.82 min) Epoch 16/300 -- Iteration 58525 - Batch 760/3851 - Train loss: 0.17446714  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(11.83 min) Epoch 16/300 -- Iteration 58563 - Batch 798/3851 - Train loss: 0.17451728  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(11.84 min) Epoch 16/300 -- Iteration 58601 - Batch 836/3851 - Train loss: 0.17454224  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(11.85 min) Epoch 16/300 -- Iteration 58639 - Batch 874/3851 - Train loss: 0.17461944  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(11.85 min) Epoch 16/300 -- Iteration 58677 - Batch 912/3851 - Train loss: 0.17450724  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(11.86 min) Epoch 16/300 -- Iteration 58715 - Batch 950/3851 - Train loss: 0.17436523  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(11.87 min) Epoch 16/300 -- Iteration 58753 - Batch 988/3851 - Train loss: 0.17439237  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(11.88 min) Epoch 16/300 -- Iteration 58791 - Batch 1026/3851 - Train loss: 0.17406331  - Train acc: 0.9349 - Val loss: 0.73075205\n",
      "(11.88 min) Epoch 16/300 -- Iteration 58829 - Batch 1064/3851 - Train loss: 0.17410891  - Train acc: 0.9349 - Val loss: 0.73075205\n",
      "(11.89 min) Epoch 16/300 -- Iteration 58867 - Batch 1102/3851 - Train loss: 0.17437754  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(11.90 min) Epoch 16/300 -- Iteration 58905 - Batch 1140/3851 - Train loss: 0.17448369  - Train acc: 0.9348 - Val loss: 0.73075205\n",
      "(11.91 min) Epoch 16/300 -- Iteration 58943 - Batch 1178/3851 - Train loss: 0.17459292  - Train acc: 0.9348 - Val loss: 0.73075205\n",
      "(11.91 min) Epoch 16/300 -- Iteration 58981 - Batch 1216/3851 - Train loss: 0.17484975  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(11.92 min) Epoch 16/300 -- Iteration 59019 - Batch 1254/3851 - Train loss: 0.17527852  - Train acc: 0.9344 - Val loss: 0.73075205\n",
      "(11.93 min) Epoch 16/300 -- Iteration 59057 - Batch 1292/3851 - Train loss: 0.17558817  - Train acc: 0.9342 - Val loss: 0.73075205\n",
      "(11.94 min) Epoch 16/300 -- Iteration 59095 - Batch 1330/3851 - Train loss: 0.17561177  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(11.94 min) Epoch 16/300 -- Iteration 59133 - Batch 1368/3851 - Train loss: 0.17559159  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(11.95 min) Epoch 16/300 -- Iteration 59171 - Batch 1406/3851 - Train loss: 0.17557466  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(11.96 min) Epoch 16/300 -- Iteration 59209 - Batch 1444/3851 - Train loss: 0.17560377  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(11.97 min) Epoch 16/300 -- Iteration 59247 - Batch 1482/3851 - Train loss: 0.17581189  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(11.97 min) Epoch 16/300 -- Iteration 59285 - Batch 1520/3851 - Train loss: 0.17599668  - Train acc: 0.9340 - Val loss: 0.73075205\n",
      "(11.98 min) Epoch 16/300 -- Iteration 59323 - Batch 1558/3851 - Train loss: 0.17628195  - Train acc: 0.9340 - Val loss: 0.73075205\n",
      "(11.99 min) Epoch 16/300 -- Iteration 59361 - Batch 1596/3851 - Train loss: 0.17607153  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(12.00 min) Epoch 16/300 -- Iteration 59399 - Batch 1634/3851 - Train loss: 0.17624883  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(12.00 min) Epoch 16/300 -- Iteration 59437 - Batch 1672/3851 - Train loss: 0.17622361  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(12.01 min) Epoch 16/300 -- Iteration 59475 - Batch 1710/3851 - Train loss: 0.17611282  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(12.02 min) Epoch 16/300 -- Iteration 59513 - Batch 1748/3851 - Train loss: 0.17615157  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(12.03 min) Epoch 16/300 -- Iteration 59551 - Batch 1786/3851 - Train loss: 0.17621999  - Train acc: 0.9340 - Val loss: 0.73075205\n",
      "(12.04 min) Epoch 16/300 -- Iteration 59589 - Batch 1824/3851 - Train loss: 0.17603260  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(12.04 min) Epoch 16/300 -- Iteration 59627 - Batch 1862/3851 - Train loss: 0.17603488  - Train acc: 0.9340 - Val loss: 0.73075205\n",
      "(12.05 min) Epoch 16/300 -- Iteration 59665 - Batch 1900/3851 - Train loss: 0.17580063  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(12.06 min) Epoch 16/300 -- Iteration 59703 - Batch 1938/3851 - Train loss: 0.17575437  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(12.07 min) Epoch 16/300 -- Iteration 59741 - Batch 1976/3851 - Train loss: 0.17583308  - Train acc: 0.9341 - Val loss: 0.73075205\n",
      "(12.07 min) Epoch 16/300 -- Iteration 59779 - Batch 2014/3851 - Train loss: 0.17570726  - Train acc: 0.9342 - Val loss: 0.73075205\n",
      "(12.08 min) Epoch 16/300 -- Iteration 59817 - Batch 2052/3851 - Train loss: 0.17539959  - Train acc: 0.9343 - Val loss: 0.73075205\n",
      "(12.09 min) Epoch 16/300 -- Iteration 59855 - Batch 2090/3851 - Train loss: 0.17513988  - Train acc: 0.9343 - Val loss: 0.73075205\n",
      "(12.10 min) Epoch 16/300 -- Iteration 59893 - Batch 2128/3851 - Train loss: 0.17508655  - Train acc: 0.9343 - Val loss: 0.73075205\n",
      "(12.10 min) Epoch 16/300 -- Iteration 59931 - Batch 2166/3851 - Train loss: 0.17501980  - Train acc: 0.9344 - Val loss: 0.73075205\n",
      "(12.11 min) Epoch 16/300 -- Iteration 59969 - Batch 2204/3851 - Train loss: 0.17492171  - Train acc: 0.9343 - Val loss: 0.73075205\n",
      "(12.12 min) Epoch 16/300 -- Iteration 60007 - Batch 2242/3851 - Train loss: 0.17496967  - Train acc: 0.9343 - Val loss: 0.73075205\n",
      "(12.13 min) Epoch 16/300 -- Iteration 60045 - Batch 2280/3851 - Train loss: 0.17486387  - Train acc: 0.9344 - Val loss: 0.73075205\n",
      "(12.13 min) Epoch 16/300 -- Iteration 60083 - Batch 2318/3851 - Train loss: 0.17473727  - Train acc: 0.9344 - Val loss: 0.73075205\n",
      "(12.14 min) Epoch 16/300 -- Iteration 60121 - Batch 2356/3851 - Train loss: 0.17471754  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.15 min) Epoch 16/300 -- Iteration 60159 - Batch 2394/3851 - Train loss: 0.17481590  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.16 min) Epoch 16/300 -- Iteration 60197 - Batch 2432/3851 - Train loss: 0.17483821  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.17 min) Epoch 16/300 -- Iteration 60235 - Batch 2470/3851 - Train loss: 0.17486160  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.17 min) Epoch 16/300 -- Iteration 60273 - Batch 2508/3851 - Train loss: 0.17471074  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.18 min) Epoch 16/300 -- Iteration 60311 - Batch 2546/3851 - Train loss: 0.17476283  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.19 min) Epoch 16/300 -- Iteration 60349 - Batch 2584/3851 - Train loss: 0.17482889  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.20 min) Epoch 16/300 -- Iteration 60387 - Batch 2622/3851 - Train loss: 0.17487020  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.20 min) Epoch 16/300 -- Iteration 60425 - Batch 2660/3851 - Train loss: 0.17468897  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.21 min) Epoch 16/300 -- Iteration 60463 - Batch 2698/3851 - Train loss: 0.17458712  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.22 min) Epoch 16/300 -- Iteration 60501 - Batch 2736/3851 - Train loss: 0.17457916  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.23 min) Epoch 16/300 -- Iteration 60539 - Batch 2774/3851 - Train loss: 0.17450758  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.23 min) Epoch 16/300 -- Iteration 60577 - Batch 2812/3851 - Train loss: 0.17447660  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.24 min) Epoch 16/300 -- Iteration 60615 - Batch 2850/3851 - Train loss: 0.17461192  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.25 min) Epoch 16/300 -- Iteration 60653 - Batch 2888/3851 - Train loss: 0.17461056  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.26 min) Epoch 16/300 -- Iteration 60691 - Batch 2926/3851 - Train loss: 0.17463080  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.26 min) Epoch 16/300 -- Iteration 60729 - Batch 2964/3851 - Train loss: 0.17457394  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.27 min) Epoch 16/300 -- Iteration 60767 - Batch 3002/3851 - Train loss: 0.17470869  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.28 min) Epoch 16/300 -- Iteration 60805 - Batch 3040/3851 - Train loss: 0.17459285  - Train acc: 0.9345 - Val loss: 0.73075205\n",
      "(12.29 min) Epoch 16/300 -- Iteration 60843 - Batch 3078/3851 - Train loss: 0.17450724  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.30 min) Epoch 16/300 -- Iteration 60881 - Batch 3116/3851 - Train loss: 0.17443599  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.30 min) Epoch 16/300 -- Iteration 60919 - Batch 3154/3851 - Train loss: 0.17437100  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.31 min) Epoch 16/300 -- Iteration 60957 - Batch 3192/3851 - Train loss: 0.17435871  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.32 min) Epoch 16/300 -- Iteration 60995 - Batch 3230/3851 - Train loss: 0.17427964  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.33 min) Epoch 16/300 -- Iteration 61033 - Batch 3268/3851 - Train loss: 0.17437150  - Train acc: 0.9348 - Val loss: 0.73075205\n",
      "(12.33 min) Epoch 16/300 -- Iteration 61071 - Batch 3306/3851 - Train loss: 0.17437668  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.34 min) Epoch 16/300 -- Iteration 61109 - Batch 3344/3851 - Train loss: 0.17443548  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.35 min) Epoch 16/300 -- Iteration 61147 - Batch 3382/3851 - Train loss: 0.17456249  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.36 min) Epoch 16/300 -- Iteration 61185 - Batch 3420/3851 - Train loss: 0.17455892  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.36 min) Epoch 16/300 -- Iteration 61223 - Batch 3458/3851 - Train loss: 0.17464198  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.37 min) Epoch 16/300 -- Iteration 61261 - Batch 3496/3851 - Train loss: 0.17462039  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.38 min) Epoch 16/300 -- Iteration 61299 - Batch 3534/3851 - Train loss: 0.17450413  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.39 min) Epoch 16/300 -- Iteration 61337 - Batch 3572/3851 - Train loss: 0.17441477  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.39 min) Epoch 16/300 -- Iteration 61375 - Batch 3610/3851 - Train loss: 0.17435170  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.40 min) Epoch 16/300 -- Iteration 61413 - Batch 3648/3851 - Train loss: 0.17433413  - Train acc: 0.9346 - Val loss: 0.73075205\n",
      "(12.41 min) Epoch 16/300 -- Iteration 61451 - Batch 3686/3851 - Train loss: 0.17439879  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.42 min) Epoch 16/300 -- Iteration 61489 - Batch 3724/3851 - Train loss: 0.17431334  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.43 min) Epoch 16/300 -- Iteration 61527 - Batch 3762/3851 - Train loss: 0.17423355  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.43 min) Epoch 16/300 -- Iteration 61565 - Batch 3800/3851 - Train loss: 0.17423224  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.44 min) Epoch 16/300 -- Iteration 61603 - Batch 3838/3851 - Train loss: 0.17429571  - Train acc: 0.9347 - Val loss: 0.73075205\n",
      "(12.44 min) Epoch 16/300 -- Iteration 61616 - Batch 3850/3851 - Train loss: 0.17434256  - Train acc: 0.9347 - Val loss: 0.74213606 - Val acc: 0.6500\n",
      "(12.44 min) Epoch 16/300 -- Iteration 61616 - Batch 3850/3851 - Train loss: 0.17434256  - Train acc: 0.9347 - Val loss: 0.74213606 - Val acc: 0.6500\n"
     ]
    }
   ],
   "source": [
    "import src.final_model as fm\n",
    "import src.final_training as ft\n",
    "importlib.reload(fm)\n",
    "importlib.reload(ft)\n",
    "importlib.reload(src.final_training)\n",
    "importlib.reload(src.final_model)\n",
    "\n",
    "only_classifier = False\n",
    "early_stop = 15\n",
    "augmentation = True\n",
    "batch_size = 128\n",
    "\n",
    "stage = 'rnn'\n",
    "curves = ft.train_final_model(modelo_final,\n",
    "                                              stage,\n",
    "                                                train_dataset,\n",
    "                                                validation_dataset,\n",
    "                                                ae_loss,\n",
    "                                                rnn_loss,\n",
    "                                                max_epochs,\n",
    "                                                max_time,\n",
    "                                                batch_size,\n",
    "                                                10e-6,\n",
    "                                                random_sampler,\n",
    "                                                only_classifier,\n",
    "                                                augmentation,\n",
    "                                                early_stop,\n",
    "                                                use_gpu,\n",
    "                                                num_cpu,\n",
    "                                                0.015)\n",
    "\n",
    "torch.save(modelo_final.state_dict(), f'models/{modelo_final.name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1950x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.show_curves([curves], [modelo_final], 'RNN')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmarc\\AppData\\Local\\Temp\\ipykernel_8956\\625530811.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  modelo_final.load_state_dict(torch.load(f'models/1st_iteration.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo RNN model\n",
      "Precision: 0.767\n",
      "Recall: 0.750\n",
      "F1: 0.754\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1950x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#validation\n",
    "import src.final_model as fm\n",
    "import src.final_training as ft\n",
    "importlib.reload(fm)\n",
    "importlib.reload(ft)\n",
    "importlib.reload(src.final_training)\n",
    "importlib.reload(src.final_model)\n",
    "importlib.reload(src.utils.metrics)\n",
    "importlib.reload(metrics)\n",
    "\n",
    "modelo_final = fm.FinalModel(128, n_channels, 'RNN', 128, 1, dropout_prob,  n_classes, name='RNN model')\n",
    "modelo_final.load_state_dict(torch.load(f'models/1st_iteration.pth'))\n",
    "\n",
    "modelo_final.cpu()\n",
    "modelo_final.eval()\n",
    "\n",
    "\n",
    "prediction = modelo_final.rnn_classifier_test(validation_dataset.tensors[0])\n",
    "\n",
    "cm = confusion_matrix(validation_dataset.tensors[1], prediction.argmax(dim=1), normalize='true')\n",
    "print(f'\\nModelo {modelo_final.name}')\n",
    "metrics.plot_matrix([modelo_final], [cm], 'Validation')\n",
    "precision, recall, f1 = metrics.performance_metrics(validation_dataset.tensors[1], prediction.argmax(dim=1))\n",
    "\n",
    "print(f'Precision: {precision:.3f}'\n",
    "      f'\\nRecall: {recall:.3f}'\n",
    "      f'\\nF1: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo RNN model\n",
      "Precision: 0.704\n",
      "Recall: 0.698\n",
      "F1: 0.693\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1950x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test\n",
    "modelo_final.eval()\n",
    "\n",
    "prediction = modelo_final.rnn_classifier_test(test_dataset.tensors[0])\n",
    "cm = confusion_matrix(test_dataset.tensors[1], prediction.argmax(dim=1), normalize='true')\n",
    "print(f'\\nModelo {modelo_final.name}')\n",
    "metrics.plot_matrix([modelo_final], [cm], 'Test')\n",
    "precision, recall, f1 = metrics.performance_metrics(test_dataset.tensors[1], prediction.argmax(dim=1))\n",
    "\n",
    "print(f'Precision: {precision:.3f}'\n",
    "      f'\\nRecall: {recall:.3f}'\n",
    "      f'\\nF1: {f1:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "import src.final_model as fm\n",
    "import src.final_training as ft\n",
    "importlib.reload(fm)\n",
    "importlib.reload(ft)\n",
    "importlib.reload(src.final_training)\n",
    "importlib.reload(src.final_model)\n",
    "\n",
    "# modelo_final = fm.FinalModel(128, 3, 'LSTM', 128, 1, 0.35,  3, name='2nd_iteration')\n",
    "\n",
    "# modelo_final.load_state_dict(torch.load(f'models/0_8_aernn.pth'))\n",
    "modelo_final.eval()\n",
    "modelo_final.cuda()\n",
    "\n",
    "class_counts = torch.bincount(train_dataset.tensors[1].long())\n",
    "class_weights = 1. / class_counts.float()\n",
    "weights = class_weights[train_dataset.tensors[1].long()]\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, sampler=sampler, num_workers=0, pin_memory=True)\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, y_batch in train_loader:\n",
    "        img = img.cuda()\n",
    "        lat_spc = modelo_final.rnn_latent(img)\n",
    "        features.append(lat_spc.cpu().numpy())\n",
    "        labels.append(y_batch.numpy())\n",
    "\n",
    "features = np.concatenate(features, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123227, 21)\n",
      "(123227,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=15000, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=15000, n_jobs=-1, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=15000, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=15000, random_state=42, n_jobs = -1)\n",
    "rf_classifier.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "# Extract features from the validation dataset\n",
    "val_loader = DataLoader(validation_dataset, batch_size=len(validation_dataset), shuffle=False)\n",
    "val_features = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, y_batch in val_loader:\n",
    "        if True:\n",
    "            img = img.cuda()\n",
    "        lat_spc = modelo_final.rnn_latent(img)\n",
    "        val_features.append(lat_spc.cpu().numpy())\n",
    "        val_labels.append(y_batch.numpy())\n",
    "\n",
    "val_features = np.concatenate(val_features, axis=0)\n",
    "val_labels = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "# Predict and evaluate\n",
    "val_predictions = rf_classifier.predict(val_features)\n",
    "accuracy = np.mean(val_predictions == val_labels)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inteli_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
