{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset de entrenamiento completo tiene: 55000 elementos.\n",
      "El dataset de validación tiene: 5000 elementos.\n",
      "El dataset de test tiene: 10000 elementos.\n",
      "\n",
      "Primer ejemplo obtenido del dataset:\n",
      "Tensor de tamaño: torch.Size([784]) con valor mínimo: 0.0 y máximo: 1.0\n",
      "Clase: 0\n"
     ]
    }
   ],
   "source": [
    "# Dígito escogido para clasificar\n",
    "chosen_digit = 8\n",
    "\n",
    "# Se define una transformación para las imágenes (La primera transforma las imágenes a tensor y la segunda aplana los tensores en vectores)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda t: t.flatten()),\n",
    "])\n",
    "\n",
    "# Se define una transformación para los labels. Esta toma cada label y los transforma a 1 y 0,\n",
    "# donde 1 representa que el dígito es igual al dígito escogido (en este caso 8) y 0 representa que es otro dígito.\n",
    "target_transform = torchvision.transforms.Lambda(lambda label: 1 if label == chosen_digit else 0)\n",
    "\n",
    "# Se descargan los datasets de train y test de mnist y se transforman\n",
    "root_dataset_dir = \"./mnist\"\n",
    "\n",
    "train_mnist_dataset = torchvision.datasets.MNIST(\n",
    "    root=root_dataset_dir,\n",
    "    train=True,\n",
    "    transform=transform, # Acá definimos la transformación a las imágenes\n",
    "    target_transform=target_transform, # Acá definimos la transformación a los labels\n",
    "    download=True\n",
    ")\n",
    "test_mnist_dataset = torchvision.datasets.MNIST(\n",
    "    root=root_dataset_dir,\n",
    "    train=False,\n",
    "    transform=transform, # Acá definimos la transformación a las imágenes\n",
    "    target_transform=target_transform, # Acá definimos la transformación a los labels\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Del dataset de entrenamieno obtenemos una porción que utilizaremos como dataset de validación\n",
    "# Sabemos que son 60000 datos totales del dataset original de entrenamiento y 10000 de test.\n",
    "# Por lo tanto tomamos 10000 datos del dataset de entrenamiento y los utilizamos como validación (eliminándolos del dataset de entrenamiento)\n",
    "train_size = 55000\n",
    "val_size = len(train_mnist_dataset) - train_size\n",
    "train_mnist_dataset, val_mnist_dataset = torch.utils.data.random_split(train_mnist_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(f\"El dataset de entrenamiento completo tiene: {len(train_mnist_dataset)} elementos.\")\n",
    "print(f\"El dataset de validación tiene: {len(val_mnist_dataset)} elementos.\")\n",
    "print(f\"El dataset de test tiene: {len(test_mnist_dataset)} elementos.\")\n",
    "\n",
    "print()\n",
    "\n",
    "x, y = train_mnist_dataset[0]\n",
    "print(f\"Primer ejemplo obtenido del dataset:\")\n",
    "print(f\"Tensor de tamaño: {x.shape} con valor mínimo: {x.min()} y máximo: {x.max()}\")\n",
    "print(\"Clase:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(dataset):\n",
    "    data_tuples = [(x, y) for (x, y) in dataset]  # Separamos el dataset en X, y\n",
    "    X = torch.stack([t[0] for t in data_tuples])\n",
    "    y = torch.tensor([t[1] for t in data_tuples])\n",
    "\n",
    "    final_X = X.float()\n",
    "    final_y = y\n",
    "\n",
    "    final_dataset = torch.utils.data.TensorDataset(\n",
    "        final_X,\n",
    "        final_y,\n",
    "    )\n",
    "\n",
    "    return final_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanceando dataset de entrenamiento.\n",
      "\n",
      "Balanceando dataset de validación.\n",
      "\n",
      "Balanceando dataset de test.\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanceando dataset de entrenamiento.\")\n",
    "train_mnist_dataset = balance_dataset(train_mnist_dataset)\n",
    "print()\n",
    "\n",
    "print(\"Balanceando dataset de validación.\")\n",
    "val_mnist_dataset = balance_dataset(val_mnist_dataset)\n",
    "print()\n",
    "\n",
    "print(\"Balanceando dataset de test.\")\n",
    "test_mnist_dataset = balance_dataset(test_mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist_dataset = torch.utils.data.TensorDataset(\n",
    "    train_mnist_dataset.tensors[0].view(-1, 1, 28, 28),\n",
    "    train_mnist_dataset.tensors[1]\n",
    ")\n",
    "\n",
    "val_mnist_dataset = torch.utils.data.TensorDataset(\n",
    "    val_mnist_dataset.tensors[0].view(-1, 1, 28, 28),\n",
    "    val_mnist_dataset.tensors[1]\n",
    ")\n",
    "\n",
    "test_mnist_dataset = torch.utils.data.TensorDataset(\n",
    "    test_mnist_dataset.tensors[0].view(-1, 1, 28, 28),\n",
    "    test_mnist_dataset.tensors[1]\n",
    ")\n",
    "\n",
    "train_mnist_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.nn.functional.interpolate(train_mnist_dataset.tensors[0], size=(21, 21)),\n",
    "    train_mnist_dataset.tensors[1]\n",
    ")\n",
    "\n",
    "val_mnist_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.nn.functional.interpolate(val_mnist_dataset.tensors[0], size=(21, 21)),\n",
    "    val_mnist_dataset.tensors[1]\n",
    ")\n",
    "\n",
    "test_mnist_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.nn.functional.interpolate(test_mnist_dataset.tensors[0], size=(21, 21)),\n",
    "    test_mnist_dataset.tensors[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 -- Iteration 25798 - Batch 858/860 - Train loss: 840.210693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import src.model.train as train\n",
    "importlib.reload(train)\n",
    "import src.model.metrics as metrics\n",
    "importlib.reload(metrics)\n",
    "import src.model.vae as vae\n",
    "importlib.reload(vae)\n",
    "\n",
    "model_ones = vae.VAE(latent_dim=21)\n",
    "\n",
    "max_epochs = 30\n",
    "criterion = vae.loss_function\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "early_stop = 20\n",
    "use_gpu = False\n",
    "\n",
    "curves, tiempo_ejecucion = train.train_model(model_ones,\n",
    "                                            train_mnist_dataset,\n",
    "                                            val_mnist_dataset,\n",
    "                                            [0],\n",
    "                                            max_epochs=max_epochs,\n",
    "                                            criterion=criterion,\n",
    "                                            batch_size=batch_size,\n",
    "                                            lr=lr,\n",
    "                                            early_stopping_tolerance=early_stop,\n",
    "                                            use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAHWklEQVR4nO3dsYskZhnA4W/2NiZK9i5gt7lFTJMiJIUSsJBIIgimSeHfIBYKok16LfwDRDgLCxGsTcCktQg5EEwhgoh2F6YIBs8Mh4bbnbHRn7FYsmG+2R3vnqfd5eUthv3x3sB9i81msxkAMMY4uOoFANgfogBARAGAiAIAEQUAIgoARBQAyOFFfmm9Xo/lcjmOjo7GYrHY9U4ATLbZbMZqtRrHx8fj4OD8e+BCUVgul+Pk5GTacgBcjTt37oybN2+e+/MLReHo6GiMMcaXx8vjcDwyZzMALs3puD/eGm/09/w8F4rCf/7J6HA8Mg4XogDwf+ff/6HRx30F4ItmACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAOTwqheAh83ii89MmfPr138xZc6zt76z9YyTH749YRP2gUsBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAPLIDl+y9569PmXM6zqbM+cxyM2UODwaXAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCAPHyGlyyvz0358W0d08/nDLnsz+7PWUODwaXAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCAPHyGlyyv7xya8qcF7/9/SlzPj1+O2UODwaXAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgHhkBy7Zr+49MWXO47/505Q5Z1Om8KBwKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABAvr8EncO2Zp7ee8f7ZexM2GePs7t+nzIGPcikAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQL6/BJ/DH717fesb7P3plwiZjPDFuT5kDH+VSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEI/s8FD48OvPT5nz2td+vPWMV7/31QmbjLGeMgX+l0sBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAeHmNh8K7L835qD/3qce2nrG+d2/CJrAbLgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgDxyA577/Dzn9t6xs+/8ZMJm4xx6+7JlDmwr1wKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoAxMtr7L0/f+t46xlfenTCImOMb77z4tYzTsYfJmwCu+FSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIF5eY++tT/551SvkH3cfu+oVYKdcCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFAOKRHfbe77/y0wlT5nzUn3zz2pQ5sK9cCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAMTLa+y9Rxfbf0x/8NdnJ2wyxuOv/W7rGZsJe8CuuBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDEIzvszLXr1696hfzyzRemzHnq9PaUObCvXAoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDEy2vszNkHH0yZ8/KTX9h6xlPDi2lwES4FACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMjhRX5ps9mMMcY4HffH2Ox0HwB24HTcH2P89+/5eS4UhdVqNcYY463xxpZrAXCVVqvVuHHjxrk/X2w+LhtjjPV6PZbL5Tg6OhqLxWLqggDs3mazGavVahwfH4+Dg/O/ObhQFAB4OPiiGYCIAgARBQAiCgBEFACIKAAQUQAg/wLCUIry8ZkehAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ones.eval()\n",
    "img = test_mnist_dataset[2][0][0]\n",
    "plt.imshow(img)\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32297/3418156530.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_tensor = torch.tensor(img).unsqueeze(0).unsqueeze(0)  # Convertir la imagen a tensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_tensor = torch.tensor(img).unsqueeze(0).unsqueeze(0)  # Convertir la imagen a tensor\n",
    "    reconstructed_img = model_ones(img_tensor)[0].squeeze().detach().numpy()\n",
    "\n",
    "plt.imshow(reconstructed_img)\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ones.eval()\n",
    "img = test_mnist_dataset[0][0][0]\n",
    "plt.imshow(img)\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32297/1527667916.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_tensor = torch.tensor(img).unsqueeze(0).unsqueeze(0)  # Convertir la imagen a tensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    img_tensor = torch.tensor(img).unsqueeze(0).unsqueeze(0)  # Convertir la imagen a tensor\n",
    "    reconstructed_img = model_ones(img_tensor)[0].squeeze().detach().numpy()\n",
    "\n",
    "plt.imshow(reconstructed_img)\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inteli_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
